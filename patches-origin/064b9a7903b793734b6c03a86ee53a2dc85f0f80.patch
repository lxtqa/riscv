From 064b9a7903b793734b6c03a86ee53a2dc85f0f80 Mon Sep 17 00:00:00 2001
From: Leszek Swirski <leszeks@chromium.org>
Date: Fri, 1 Sep 2023 10:53:32 +0200
Subject: [PATCH] [tagged-ptr] Convert more Objects to Tagged<>

Convert a large amount of Objects to Tagged<Object>, by using a clang
tool to convert all variables and parameters to Tagged<>, and manually
fixing up any build failures this caused.

Bug: v8:12710
Change-Id: I2bfc90220fe92ebc6487b5cddbc4166164637a71
Reviewed-on: https://chromium-review.googlesource.com/c/v8/v8/+/4827204
Commit-Queue: Leszek Swirski <leszeks@chromium.org>
Reviewed-by: Michael Lippautz <mlippautz@chromium.org>
Auto-Submit: Leszek Swirski <leszeks@chromium.org>
Cr-Commit-Position: refs/heads/main@{#89750}
---
 src/api/api-arguments-inl.h                   |  11 +-
 src/api/api-arguments.cc                      |  14 +-
 src/api/api-arguments.h                       |  19 +-
 src/api/api-inl.h                             |  20 +-
 src/api/api-natives.cc                        |  28 +-
 src/api/api.cc                                | 240 +++---
 src/api/api.h                                 |  24 +-
 src/ast/ast-value-factory.cc                  |   3 +-
 src/ast/ast-value-factory.h                   |   2 +-
 src/ast/ast.cc                                |  10 +-
 src/ast/ast.h                                 |   4 +-
 src/ast/scopes.cc                             |   8 +-
 src/ast/scopes.h                              |   2 +-
 src/base/macros.h                             |   4 +
 src/baseline/arm/baseline-assembler-arm-inl.h |   8 +-
 .../arm64/baseline-assembler-arm64-inl.h      |   8 +-
 src/baseline/baseline-assembler-inl.h         |   2 +-
 src/baseline/baseline-assembler.h             |  13 +-
 src/baseline/baseline-batch-compiler.cc       |  18 +-
 src/baseline/baseline-batch-compiler.h        |   6 +-
 src/baseline/baseline-compiler.cc             |  24 +-
 src/baseline/baseline-compiler.h              |  16 +-
 src/baseline/baseline.cc                      |   3 +-
 src/baseline/baseline.h                       |   3 +-
 src/baseline/bytecode-offset-iterator.cc      |   4 +-
 src/baseline/bytecode-offset-iterator.h       |   4 +-
 .../ia32/baseline-assembler-ia32-inl.h        |  14 +-
 .../loong64/baseline-assembler-loong64-inl.h  |   8 +-
 .../mips64/baseline-assembler-mips64-inl.h    |   8 +-
 src/baseline/ppc/baseline-assembler-ppc-inl.h |   8 +-
 .../riscv/baseline-assembler-riscv-inl.h      |   8 +-
 .../s390/baseline-assembler-s390-inl.h        |   8 +-
 src/baseline/x64/baseline-assembler-x64-inl.h |  14 +-
 src/builtins/accessors.cc                     |  18 +-
 src/builtins/arm/builtins-arm.cc              |   2 +-
 src/builtins/arm64/builtins-arm64.cc          |   2 +-
 src/builtins/builtins-api.cc                  |  41 +-
 src/builtins/builtins-array.cc                |  79 +-
 src/builtins/builtins-arraybuffer.cc          |  24 +-
 src/builtins/builtins-bigint.cc               |   6 +-
 src/builtins/builtins-callsite.cc             |   4 +-
 src/builtins/builtins-dataview.cc             |   2 +-
 src/builtins/builtins-function.cc             |   2 +-
 src/builtins/builtins-intl.cc                 |  28 +-
 src/builtins/builtins-object.cc               |  14 +-
 src/builtins/builtins-sharedarraybuffer.cc    |   6 +-
 src/builtins/builtins-symbol.cc               |   2 +-
 src/builtins/builtins-utils.h                 |   4 +-
 src/builtins/builtins.cc                      |  14 +-
 src/builtins/builtins.h                       |   8 +-
 src/builtins/constants-table-builder.cc       |   2 +-
 src/builtins/setup-builtins-internal.cc       |  48 +-
 src/builtins/x64/builtins-x64.cc              |   2 +-
 src/codegen/arm/assembler-arm-inl.h           |   8 +-
 src/codegen/arm/assembler-arm.h               |   4 +-
 src/codegen/arm/macro-assembler-arm.cc        |   8 +-
 src/codegen/arm/macro-assembler-arm.h         |   6 +-
 src/codegen/arm64/assembler-arm64-inl.h       |  13 +-
 src/codegen/arm64/assembler-arm64.h           |   2 +-
 src/codegen/assembler.h                       |   2 +-
 src/codegen/code-stub-assembler.cc            |  10 +-
 src/codegen/code-stub-assembler.h             |   8 +-
 src/codegen/compilation-cache.cc              |  16 +-
 src/codegen/compiler.cc                       |  32 +-
 src/codegen/external-reference.cc             |  26 +-
 src/codegen/handler-table.cc                  |   6 +-
 src/codegen/handler-table.h                   |   8 +-
 src/codegen/ia32/assembler-ia32-inl.h         |   6 +-
 src/codegen/ia32/assembler-ia32.h             |   4 +-
 src/codegen/ia32/macro-assembler-ia32.h       |   4 +-
 src/codegen/macro-assembler-base.cc           |   2 +-
 src/codegen/maglev-safepoint-table.cc         |   6 +-
 src/codegen/maglev-safepoint-table.h          |   9 +-
 src/codegen/optimized-compilation-info.cc     |   6 +-
 src/codegen/optimized-compilation-info.h      |  12 +-
 src/codegen/pending-optimization-table.cc     |   4 +-
 src/codegen/pending-optimization-table.h      |   4 +-
 src/codegen/reloc-info-inl.h                  |   3 +-
 src/codegen/reloc-info.cc                     |  28 +-
 src/codegen/reloc-info.h                      |  22 +-
 src/codegen/safepoint-table.cc                |   9 +-
 src/codegen/safepoint-table.h                 |   9 +-
 src/codegen/source-position-table.cc          |   4 +-
 src/codegen/source-position-table.h           |   3 +-
 src/codegen/source-position.cc                |  26 +-
 src/codegen/source-position.h                 |   8 +-
 src/codegen/x64/assembler-x64-inl.h           |   8 +-
 src/codegen/x64/assembler-x64.h               |   4 +-
 src/codegen/x64/macro-assembler-x64.cc        |  18 +-
 src/codegen/x64/macro-assembler-x64.h         |  30 +-
 src/common/globals.h                          |   2 +-
 src/common/ptr-compr-inl.h                    |   2 +-
 .../lazy-compile-dispatcher.cc                |   6 +-
 src/compiler/access-info.cc                   |   9 +-
 src/compiler/backend/code-generator.cc        |   4 +-
 src/compiler/backend/instruction-selector.cc  |  61 +-
 src/compiler/code-assembler.cc                |   7 +-
 src/compiler/code-assembler.h                 |   6 +-
 src/compiler/compilation-dependencies.cc      |  20 +-
 src/compiler/graph-visualizer.cc              |   9 +-
 src/compiler/heap-refs.cc                     |  44 +-
 src/compiler/heap-refs.h                      |   6 +-
 src/compiler/js-heap-broker.cc                |  30 +-
 src/compiler/js-heap-broker.h                 |  19 +-
 src/compiler/linkage.cc                       |   2 +-
 src/compiler/property-access-builder.cc       |   2 +-
 src/compiler/turboshaft/assembler.h           |   3 +-
 .../turboshaft/assert-types-reducer.h         |   6 +-
 src/compiler/wasm-compiler.cc                 |   4 +-
 src/date/date.h                               |   2 +-
 src/debug/debug-coverage.cc                   |  48 +-
 src/debug/debug-frames.cc                     |   9 +-
 src/debug/debug-frames.h                      |   2 +-
 src/debug/debug-interface.cc                  |  17 +-
 src/debug/debug-scopes.cc                     |  19 +-
 src/debug/debug-wasm-objects.cc               |   6 +-
 src/debug/debug.cc                            | 119 +--
 src/debug/debug.h                             |  34 +-
 src/debug/liveedit.cc                         |  51 +-
 src/deoptimizer/deoptimizer.cc                |  99 +--
 src/deoptimizer/deoptimizer.h                 |  24 +-
 src/deoptimizer/materialized-object-store.cc  |   5 +-
 src/deoptimizer/translated-state.cc           |  57 +-
 src/deoptimizer/translated-state.h            |  63 +-
 src/diagnostics/basic-block-profiler.cc       |   9 +-
 src/diagnostics/basic-block-profiler.h        |   4 +-
 src/diagnostics/disassembler.cc               |   2 +-
 src/diagnostics/objects-debug.cc              | 188 ++---
 src/diagnostics/objects-printer.cc            | 198 ++---
 src/diagnostics/perf-jit.cc                   |  29 +-
 src/diagnostics/perf-jit.h                    |  12 +-
 src/execution/arguments-inl.h                 |   4 +-
 src/execution/arguments.h                     |   4 +-
 src/execution/arm/simulator-arm.cc            |   4 +-
 src/execution/arm64/simulator-arm64.cc        |   4 +-
 src/execution/execution.cc                    |   4 +-
 src/execution/frames-inl.h                    |  16 +-
 src/execution/frames.cc                       | 269 +++----
 src/execution/frames.h                        | 128 ++--
 src/execution/futex-emulation.cc              |  85 ++-
 src/execution/futex-emulation.h               |  70 +-
 src/execution/isolate-inl.h                   |  24 +-
 src/execution/isolate-utils-inl.h             |   9 +-
 src/execution/isolate-utils.h                 |   9 +-
 src/execution/isolate.cc                      |  78 +-
 src/execution/isolate.h                       |  61 +-
 src/execution/local-isolate-inl.h             |   2 +-
 src/execution/local-isolate.h                 |   2 +-
 src/execution/messages.cc                     |  16 +-
 src/execution/messages.h                      |  11 +-
 src/execution/microtask-queue.cc              |   8 +-
 src/execution/microtask-queue.h               |   6 +-
 src/execution/simulator.h                     |   2 +-
 src/execution/stack-guard.cc                  |   2 +-
 src/execution/stack-guard.h                   |   3 +-
 src/execution/tiering-manager.cc              |  48 +-
 src/execution/tiering-manager.h               |  18 +-
 src/extensions/statistics-extension.cc        |  10 +-
 src/handles/global-handles.cc                 |  14 +-
 src/handles/global-handles.h                  |   3 +-
 src/handles/maybe-handles-inl.h               |  18 +-
 src/handles/maybe-handles.h                   |   8 +-
 src/handles/shared-object-conveyor-handles.cc |   3 +-
 src/handles/shared-object-conveyor-handles.h  |   4 +-
 src/handles/traced-handles.cc                 |  34 +-
 src/handles/traced-handles.h                  |   8 +-
 src/heap/allocation-result.h                  |   9 +-
 src/heap/array-buffer-sweeper.cc              |   4 +-
 src/heap/array-buffer-sweeper.h               |   4 +-
 src/heap/code-stats.cc                        |  13 +-
 src/heap/code-stats.h                         |   7 +-
 src/heap/combined-heap.h                      |   4 +-
 src/heap/concurrent-allocator-inl.h           |   6 +-
 src/heap/concurrent-allocator.cc              |   2 +-
 src/heap/concurrent-marking.cc                |  19 +-
 src/heap/cppgc-js/cpp-heap.cc                 |   2 +-
 src/heap/cppgc-js/cpp-heap.h                  |   6 +-
 src/heap/cppgc-js/cpp-marking-state-inl.h     |   2 +-
 src/heap/cppgc-js/cpp-marking-state.h         |   3 +-
 src/heap/cppgc-js/cpp-snapshot.cc             |   2 +-
 .../cppgc-js/cross-heap-remembered-set.cc     |   5 +-
 src/heap/cppgc-js/cross-heap-remembered-set.h |   2 +-
 .../cppgc-js/unified-heap-marking-state-inl.h |   8 +-
 .../cppgc-js/unified-heap-marking-state.h     |   2 +-
 src/heap/cppgc-js/wrappable-info-inl.h        |   2 +-
 src/heap/cppgc-js/wrappable-info.h            |   3 +-
 src/heap/ephemeron-remembered-set.cc          |   8 +-
 src/heap/ephemeron-remembered-set.h           |  12 +-
 src/heap/evacuation-allocator-inl.h           |   8 +-
 src/heap/evacuation-allocator.h               |   7 +-
 src/heap/evacuation-verifier-inl.h            |   6 +-
 src/heap/evacuation-verifier.cc               |  23 +-
 src/heap/evacuation-verifier.h                |  18 +-
 src/heap/factory-base.cc                      |  80 +-
 src/heap/factory-inl.h                        |   2 +-
 src/heap/factory.cc                           |  11 +-
 src/heap/factory.h                            |   6 +-
 .../finalization-registry-cleanup-task.cc     |   2 +-
 src/heap/free-list.cc                         |  67 +-
 src/heap/free-list.h                          |  49 +-
 src/heap/heap-allocator-inl.h                 |  11 +-
 src/heap/heap-allocator.h                     |   8 +-
 src/heap/heap-inl.h                           |  59 +-
 src/heap/heap-verifier.cc                     | 129 ++--
 src/heap/heap-verifier.h                      |  18 +-
 src/heap/heap-write-barrier-inl.h             | 104 +--
 src/heap/heap-write-barrier.cc                |  37 +-
 src/heap/heap-write-barrier.h                 | 100 +--
 src/heap/heap.cc                              |  30 +-
 src/heap/heap.h                               |  45 +-
 src/heap/incremental-marking-inl.h            |   3 +-
 src/heap/incremental-marking.cc               |  14 +-
 src/heap/incremental-marking.h                |   6 +-
 src/heap/large-page.h                         |   2 +-
 src/heap/large-spaces.cc                      |  22 +-
 src/heap/large-spaces.h                       |   8 +-
 src/heap/local-heap-inl.h                     |  11 +-
 src/heap/local-heap.cc                        |   2 +-
 src/heap/local-heap.h                         |   5 +-
 src/heap/mark-compact-inl.h                   |  18 +-
 src/heap/mark-compact.cc                      | 532 +++++++-------
 src/heap/mark-compact.h                       |  70 +-
 src/heap/mark-sweep-utilities.cc              |  10 +-
 src/heap/mark-sweep-utilities.h               |  12 +-
 src/heap/marking-barrier-inl.h                |  16 +-
 src/heap/marking-barrier.cc                   |  25 +-
 src/heap/marking-barrier.h                    |  31 +-
 src/heap/marking-inl.h                        |   2 +-
 src/heap/marking-state-inl.h                  |  11 +-
 src/heap/marking-state.h                      |   8 +-
 src/heap/marking-visitor-inl.h                | 149 ++--
 src/heap/marking-visitor.h                    | 127 ++--
 src/heap/marking-worklist-inl.h               |  13 +-
 src/heap/marking-worklist.cc                  |   4 +-
 src/heap/marking-worklist.h                   |  14 +-
 src/heap/marking.cc                           |   2 +-
 src/heap/marking.h                            |   4 +-
 src/heap/memory-chunk.h                       |   2 +-
 src/heap/memory-measurement-inl.h             |  10 +-
 src/heap/memory-measurement.cc                |  29 +-
 src/heap/memory-measurement.h                 |  21 +-
 src/heap/minor-mark-sweep-inl.h               |   4 +-
 src/heap/minor-mark-sweep.cc                  |  38 +-
 src/heap/new-spaces-inl.h                     |  14 +-
 src/heap/new-spaces.cc                        |   4 +-
 src/heap/new-spaces.h                         |  10 +-
 src/heap/object-lock.h                        |  10 +-
 src/heap/object-stats.cc                      | 187 ++---
 src/heap/objects-visiting-inl.h               |  60 +-
 src/heap/objects-visiting.cc                  |  73 +-
 src/heap/objects-visiting.h                   |  58 +-
 src/heap/page.cc                              |   4 +-
 src/heap/page.h                               |   2 +-
 src/heap/paged-spaces-inl.h                   |   6 +-
 src/heap/paged-spaces.cc                      |  10 +-
 src/heap/paged-spaces.h                       |   2 +-
 src/heap/pretenuring-handler-inl.h            |  14 +-
 src/heap/pretenuring-handler.cc               |  40 +-
 src/heap/pretenuring-handler.h                |  10 +-
 src/heap/read-only-heap-inl.h                 |   4 +-
 src/heap/read-only-heap.cc                    |   6 +-
 src/heap/read-only-heap.h                     |   8 +-
 src/heap/read-only-promotion.cc               | 118 +--
 src/heap/read-only-spaces.cc                  |  17 +-
 src/heap/read-only-spaces.h                   |   6 +-
 src/heap/reference-summarizer.cc              |  40 +-
 src/heap/reference-summarizer.h               |   3 +-
 src/heap/remembered-set-inl.h                 |  10 +-
 src/heap/remembered-set.h                     |  18 +-
 src/heap/scavenger-inl.h                      | 117 +--
 src/heap/scavenger.cc                         |  73 +-
 src/heap/scavenger.h                          |  59 +-
 src/heap/setup-heap-internal.cc               |  36 +-
 src/heap/spaces-inl.h                         |  11 +-
 src/heap/spaces.h                             |   2 +-
 src/heap/sweeper.cc                           |  42 +-
 src/heap/third-party/heap-api-stub.cc         |   8 +-
 src/heap/third-party/heap-api.h               |   8 +-
 src/heap/traced-handles-marking-visitor.cc    |   4 +-
 src/heap/traced-handles-marking-visitor.h     |   2 +-
 src/heap/weak-object-worklists.cc             |  69 +-
 src/heap/weak-object-worklists.h              |  26 +-
 .../young-generation-marking-visitor-inl.h    |  30 +-
 src/heap/young-generation-marking-visitor.h   |  35 +-
 src/ic/call-optimization.cc                   |  21 +-
 src/ic/call-optimization.h                    |   7 +-
 src/ic/handler-configuration-inl.h            |   4 +-
 src/ic/handler-configuration.cc               |   6 +-
 src/ic/handler-configuration.h                |  10 +-
 src/ic/ic-inl.h                               |   2 +-
 src/ic/ic-stats.cc                            |   8 +-
 src/ic/ic-stats.h                             |   6 +-
 src/ic/ic.cc                                  |  46 +-
 src/ic/ic.h                                   |   7 +-
 src/ic/stub-cache.cc                          |  22 +-
 src/ic/stub-cache.h                           |  12 +-
 src/init/bootstrapper.cc                      |  43 +-
 src/init/setup-isolate.h                      |   5 +-
 src/interpreter/bytecode-array-builder.cc     |   6 +-
 src/interpreter/bytecode-array-builder.h      |   6 +-
 src/interpreter/bytecode-array-iterator.cc    |   4 +-
 src/interpreter/bytecode-array-iterator.h     |   2 +-
 src/interpreter/bytecode-array-writer.cc      |   2 +-
 src/interpreter/bytecode-array-writer.h       |   2 +-
 src/interpreter/bytecode-generator.cc         |   4 +-
 src/interpreter/bytecode-generator.h          |   2 +-
 src/interpreter/constant-array-builder.cc     |   8 +-
 src/interpreter/constant-array-builder.h      |  12 +-
 src/interpreter/interpreter.cc                |  11 +-
 src/interpreter/interpreter.h                 |   6 +-
 src/json/json-parser.cc                       |  35 +-
 src/json/json-stringifier.cc                  |   4 +-
 src/logging/code-events.h                     |  11 +-
 src/logging/local-logger.cc                   |  16 +-
 src/logging/local-logger.h                    |   8 +-
 src/logging/log-file.cc                       |  26 +-
 src/logging/log-file.h                        |  14 +-
 src/logging/log-inl.h                         |   2 +-
 src/logging/log.cc                            | 141 ++--
 src/logging/log.h                             |  45 +-
 src/maglev/arm/maglev-assembler-arm-inl.h     |   9 +-
 src/maglev/arm64/maglev-assembler-arm64-inl.h |   9 +-
 src/maglev/maglev-assembler-inl.h             |   7 +-
 src/maglev/maglev-assembler.h                 |  12 +-
 src/maglev/maglev-code-generator.cc           |   2 +-
 src/maglev/maglev-graph-printer.cc            |   2 +-
 src/maglev/maglev-ir.cc                       |   7 +-
 src/maglev/maglev-ir.h                        |   6 +-
 src/maglev/x64/maglev-assembler-x64-inl.h     |   9 +-
 src/numbers/conversions-inl.h                 |  14 +-
 src/numbers/conversions.cc                    |   5 +-
 src/numbers/conversions.h                     |  19 +-
 src/numbers/math-random.cc                    |  10 +-
 src/numbers/math-random.h                     |   2 +-
 src/objects/abstract-code-inl.h               |  32 +-
 src/objects/abstract-code.cc                  |   4 +-
 src/objects/abstract-code.h                   |  11 +-
 src/objects/allocation-site-inl.h             |   4 +-
 src/objects/allocation-site-scopes-inl.h      |   2 +-
 src/objects/allocation-site-scopes.h          |   4 +-
 src/objects/allocation-site.h                 |   2 +-
 src/objects/api-callbacks.h                   |   5 +-
 src/objects/backing-store.cc                  |   4 +-
 src/objects/bigint.cc                         | 122 ++--
 src/objects/bigint.h                          |  11 +-
 src/objects/bytecode-array-inl.h              |  16 +-
 src/objects/bytecode-array.cc                 |   2 +-
 src/objects/bytecode-array.h                  |   2 +-
 src/objects/call-site-info.cc                 |  40 +-
 src/objects/call-site-info.h                  |  12 +-
 src/objects/code-inl.h                        |  61 +-
 src/objects/code.cc                           |  25 +-
 src/objects/code.h                            |  55 +-
 src/objects/compilation-cache-table-inl.h     |  43 +-
 src/objects/compilation-cache-table.cc        |  81 ++-
 src/objects/compilation-cache-table.h         |  28 +-
 src/objects/compressed-slots-inl.h            |  46 +-
 src/objects/compressed-slots.h                |  58 +-
 src/objects/contexts-inl.h                    |  56 +-
 src/objects/contexts.cc                       |  56 +-
 src/objects/contexts.h                        |  76 +-
 src/objects/debug-objects-inl.h               |   4 +-
 src/objects/debug-objects.cc                  |  29 +-
 src/objects/debug-objects.h                   |   4 +-
 src/objects/deoptimization-data-inl.h         |   8 +-
 src/objects/deoptimization-data.cc            |  10 +-
 src/objects/deoptimization-data.h             |  15 +-
 src/objects/dependent-code.cc                 |   6 +-
 src/objects/dependent-code.h                  |   7 +-
 src/objects/descriptor-array-inl.h            |  55 +-
 src/objects/descriptor-array.h                |  59 +-
 src/objects/dictionary-inl.h                  |  94 +--
 src/objects/dictionary.h                      |  82 ++-
 src/objects/elements-inl.h                    |   3 +-
 src/objects/elements.cc                       | 684 ++++++++++--------
 src/objects/elements.h                        |  55 +-
 src/objects/embedder-data-slot-inl.h          |  23 +-
 src/objects/embedder-data-slot.h              |  23 +-
 src/objects/feedback-cell-inl.h               |   6 +-
 src/objects/feedback-cell.h                   |   5 +-
 src/objects/feedback-vector-inl.h             |  24 +-
 src/objects/feedback-vector.cc                |  51 +-
 src/objects/feedback-vector.h                 |  42 +-
 src/objects/field-index-inl.h                 |  12 +-
 src/objects/field-index.h                     |  11 +-
 src/objects/field-type.cc                     |  20 +-
 src/objects/field-type.h                      |  14 +-
 src/objects/fixed-array-inl.h                 |  28 +-
 src/objects/fixed-array.cc                    |   8 +-
 src/objects/fixed-array.h                     |  18 +-
 src/objects/free-space-inl.h                  |  10 +-
 src/objects/free-space.h                      |   8 +-
 src/objects/hash-table-inl.h                  |  56 +-
 src/objects/hash-table.h                      |  65 +-
 src/objects/heap-object.h                     |  58 +-
 src/objects/instance-type-checker.h           |   2 +-
 src/objects/instance-type-inl.h               |  26 +-
 src/objects/instruction-stream-inl.h          |  45 +-
 src/objects/instruction-stream.cc             |   9 +-
 src/objects/instruction-stream.h              |  31 +-
 src/objects/intl-objects.cc                   |   5 +-
 src/objects/intl-objects.h                    |   3 +-
 src/objects/js-array-buffer.cc                |   6 +-
 src/objects/js-atomics-synchronization.cc     |   2 +-
 src/objects/js-atomics-synchronization.h      |   2 +-
 src/objects/js-break-iterator-inl.h           |   8 +-
 src/objects/js-break-iterator.cc              |   4 +-
 src/objects/js-break-iterator.h               |   8 +-
 src/objects/js-collator-inl.h                 |   3 +-
 src/objects/js-collator.h                     |   2 +-
 src/objects/js-collection-inl.h               |   6 +-
 src/objects/js-collection-iterator.h          |   2 +-
 src/objects/js-collection.h                   |   2 +-
 src/objects/js-date-time-format-inl.h         |   8 +-
 src/objects/js-date-time-format.cc            |   2 +-
 src/objects/js-date-time-format.h             |   7 +-
 src/objects/js-display-names-inl.h            |   2 +-
 src/objects/js-display-names.h                |   2 +-
 src/objects/js-duration-format-inl.h          |   5 +-
 src/objects/js-duration-format.cc             |   4 +-
 src/objects/js-duration-format.h              |   4 +-
 src/objects/js-function-inl.h                 |  28 +-
 src/objects/js-function.cc                    |   8 +-
 src/objects/js-function.h                     |  15 +-
 src/objects/js-list-format-inl.h              |   2 +-
 src/objects/js-list-format.h                  |   2 +-
 src/objects/js-locale-inl.h                   |   2 +-
 src/objects/js-locale.h                       |   2 +-
 src/objects/js-number-format-inl.h            |   2 +-
 src/objects/js-number-format.cc               |   2 +-
 src/objects/js-number-format.h                |   4 +-
 src/objects/js-objects-inl.h                  | 124 ++--
 src/objects/js-objects.cc                     | 294 ++++----
 src/objects/js-objects.h                      | 154 ++--
 src/objects/js-plural-rules-inl.h             |   4 +-
 src/objects/js-plural-rules.h                 |   4 +-
 src/objects/js-promise-inl.h                  |   4 +-
 src/objects/js-promise.h                      |   4 +-
 src/objects/js-regexp-inl.h                   |  24 +-
 src/objects/js-regexp.cc                      |  10 +-
 src/objects/js-regexp.h                       |  16 +-
 src/objects/js-relative-time-format-inl.h     |   2 +-
 src/objects/js-relative-time-format.h         |   2 +-
 src/objects/js-segment-iterator-inl.h         |   8 +-
 src/objects/js-segment-iterator.h             |   4 +-
 src/objects/js-segmenter-inl.h                |   2 +-
 src/objects/js-segmenter.h                    |   2 +-
 src/objects/js-segments-inl.h                 |   4 +-
 src/objects/js-segments.h                     |   4 +-
 src/objects/js-struct.cc                      |   3 +-
 src/objects/js-struct.h                       |   2 +-
 src/objects/js-temporal-objects.cc            |   2 +-
 src/objects/js-weak-refs-inl.h                |  44 +-
 src/objects/js-weak-refs.h                    |   6 +-
 src/objects/keys.cc                           |  66 +-
 src/objects/keys.h                            |   6 +-
 src/objects/literal-objects-inl.h             |  16 +-
 src/objects/literal-objects.cc                |  37 +-
 src/objects/literal-objects.h                 |  15 +-
 src/objects/lookup-cache-inl.h                |   7 +-
 src/objects/lookup-cache.h                    |   6 +-
 src/objects/lookup-inl.h                      |  14 +-
 src/objects/lookup.cc                         | 158 ++--
 src/objects/lookup.h                          |  45 +-
 src/objects/managed.h                         |   4 +-
 src/objects/map-inl.h                         |  75 +-
 src/objects/map-updater.cc                    |  66 +-
 src/objects/map-updater.h                     |  15 +-
 src/objects/map.cc                            | 227 +++---
 src/objects/map.h                             |  90 +--
 src/objects/maybe-object-inl.h                |  25 +-
 src/objects/maybe-object.h                    |  18 +-
 src/objects/module-inl.h                      |  14 +-
 src/objects/module.cc                         |  24 +-
 src/objects/module.h                          |   4 +-
 src/objects/name-inl.h                        |   2 +-
 src/objects/name.h                            |   2 +-
 src/objects/object-macros.h                   |   4 +-
 src/objects/objects-body-descriptors-inl.h    | 399 +++++-----
 src/objects/objects-body-descriptors.h        |  76 +-
 src/objects/objects-inl.h                     |  96 +--
 src/objects/objects.cc                        | 267 +++----
 src/objects/objects.h                         | 100 ++-
 src/objects/ordered-hash-table-inl.h          |  33 +-
 src/objects/ordered-hash-table.cc             | 213 +++---
 src/objects/ordered-hash-table.h              |  93 +--
 src/objects/property-array-inl.h              |  37 +-
 src/objects/property-array.h                  |  35 +-
 src/objects/property-cell.h                   |  14 +-
 src/objects/property-descriptor.cc            |   4 +-
 src/objects/property-details.h                |   2 +-
 src/objects/prototype-info-inl.h              |  11 +-
 src/objects/prototype-info.h                  |  19 +-
 src/objects/prototype-inl.h                   |  15 +-
 src/objects/prototype.h                       |   4 +-
 src/objects/regexp-match-info-inl.h           |  14 +-
 src/objects/regexp-match-info.h               |   8 +-
 src/objects/scope-info-inl.h                  |  20 +-
 src/objects/scope-info.cc                     |  70 +-
 src/objects/scope-info.h                      |  49 +-
 src/objects/script-inl.h                      |   2 +-
 src/objects/script.h                          |   4 +-
 src/objects/shared-function-info-inl.h        |  82 ++-
 src/objects/shared-function-info.cc           |  84 ++-
 src/objects/shared-function-info.h            |  62 +-
 src/objects/simd.cc                           |   5 +-
 src/objects/slots-inl.h                       |  65 +-
 src/objects/slots.h                           |  59 +-
 src/objects/smi.h                             |   2 +-
 src/objects/source-text-module.cc             |  18 +-
 src/objects/source-text-module.h              |  24 +-
 src/objects/string-comparator.cc              |  10 +-
 src/objects/string-comparator.h               |   4 +-
 src/objects/string-forwarding-table-inl.h     |  30 +-
 src/objects/string-forwarding-table.cc        |  33 +-
 src/objects/string-forwarding-table.h         |  13 +-
 src/objects/string-inl.h                      |  91 ++-
 src/objects/string-set-inl.h                  |   7 +-
 src/objects/string-set.h                      |   7 +-
 src/objects/string-table.h                    |   4 +-
 src/objects/string.cc                         | 160 ++--
 src/objects/string.h                          |  98 +--
 src/objects/struct-inl.h                      |  11 +-
 src/objects/struct.h                          |  10 +-
 src/objects/swiss-name-dictionary-inl.h       |  62 +-
 src/objects/swiss-name-dictionary.cc          |  25 +-
 src/objects/swiss-name-dictionary.h           |  54 +-
 src/objects/symbol-table.cc                   |   6 +-
 src/objects/tagged-impl-inl.h                 |  51 +-
 src/objects/tagged-impl.cc                    |  37 +-
 src/objects/tagged-impl.h                     |  44 +-
 src/objects/tagged-index.h                    |  28 +-
 src/objects/tagged-value-inl.h                |   5 +-
 src/objects/tagged-value.h                    |   5 +-
 src/objects/tagged.h                          |  28 +-
 src/objects/template-objects.cc               |  32 +-
 src/objects/templates-inl.h                   |  23 +-
 src/objects/templates.cc                      |  23 +-
 src/objects/templates.h                       |  14 +-
 src/objects/transitions-inl.h                 |  99 +--
 src/objects/transitions.cc                    | 127 ++--
 src/objects/transitions.h                     | 138 ++--
 src/objects/value-serializer.cc               |  52 +-
 src/objects/value-serializer.h                |  18 +-
 src/objects/visitors-inl.h                    |  13 +-
 src/objects/visitors.cc                       |   3 +-
 src/objects/visitors.h                        |  69 +-
 src/parsing/parse-info.cc                     |  17 +-
 src/parsing/parse-info.h                      |  12 +-
 src/parsing/preparse-data-impl.h              |  56 +-
 src/parsing/preparse-data.cc                  |   8 +-
 src/parsing/scanner-character-streams.cc      |   6 +-
 src/profiler/allocation-tracker.cc            |  10 +-
 src/profiler/allocation-tracker.h             |   5 +-
 src/profiler/cpu-profiler.cc                  |   6 +-
 src/profiler/cpu-profiler.h                   |   4 +-
 src/profiler/heap-profiler.cc                 |  16 +-
 src/profiler/heap-snapshot-generator.cc       | 574 ++++++++-------
 src/profiler/heap-snapshot-generator.h        | 187 ++---
 src/profiler/profile-generator.cc             |   4 +-
 src/profiler/profile-generator.h              |   6 +-
 src/profiler/profiler-listener.cc             |  15 +-
 src/profiler/profiler-listener.h              |  11 +-
 src/profiler/sampling-heap-profiler.cc        |  12 +-
 src/profiler/strings-storage.cc               |  12 +-
 src/profiler/strings-storage.h                |   6 +-
 src/profiler/tick-sample.cc                   |   4 +-
 src/regexp/arm/regexp-macro-assembler-arm.cc  |   2 +-
 .../arm64/regexp-macro-assembler-arm64.cc     |   2 +-
 .../experimental/experimental-interpreter.cc  |  21 +-
 .../experimental/experimental-interpreter.h   |   7 +-
 src/regexp/experimental/experimental.cc       |  25 +-
 src/regexp/experimental/experimental.h        |   2 +-
 .../ia32/regexp-macro-assembler-ia32.cc       |   2 +-
 src/regexp/regexp-interpreter.cc              |  33 +-
 src/regexp/regexp-interpreter.h               |  13 +-
 src/regexp/regexp-macro-assembler.cc          |  21 +-
 src/regexp/regexp-macro-assembler.h           |  21 +-
 src/regexp/regexp.cc                          |  52 +-
 src/regexp/regexp.h                           |   8 +-
 src/regexp/x64/regexp-macro-assembler-x64.cc  |   2 +-
 src/roots/roots-inl.h                         |   2 +-
 src/roots/roots.cc                            |   4 +-
 src/roots/roots.h                             |   2 +-
 src/runtime/runtime-array.cc                  |   2 +-
 src/runtime/runtime-compiler.cc               |  42 +-
 src/runtime/runtime-debug.cc                  |  14 +-
 src/runtime/runtime-function.cc               |   2 +-
 src/runtime/runtime-internal.cc               |  12 +-
 src/runtime/runtime-literals.cc               |  16 +-
 src/runtime/runtime-promise.cc                |   4 +-
 src/runtime/runtime-proxy.cc                  |   2 +-
 src/runtime/runtime-test-wasm.cc              |  15 +-
 src/runtime/runtime-test.cc                   |  42 +-
 src/runtime/runtime-trace.cc                  |   2 +-
 src/runtime/runtime-utils.h                   |   4 +-
 src/runtime/runtime-wasm.cc                   |   2 +-
 src/sandbox/code-pointer-inl.h                |   2 +-
 src/sandbox/code-pointer.h                    |   2 +-
 src/sandbox/indirect-pointer-inl.h            |   2 +-
 src/sandbox/testing.cc                        |   4 +-
 src/snapshot/code-serializer.cc               |  25 +-
 src/snapshot/context-serializer.cc            |  17 +-
 src/snapshot/context-serializer.h             |   6 +-
 src/snapshot/deserializer.cc                  |  93 +--
 src/snapshot/deserializer.h                   |  16 +-
 src/snapshot/embedded/embedded-data.cc        |  17 +-
 src/snapshot/embedded/embedded-file-writer.cc |   4 +-
 src/snapshot/read-only-serializer.cc          |  54 +-
 src/snapshot/references.h                     |   6 +-
 src/snapshot/roots-serializer.cc              |   2 +-
 src/snapshot/roots-serializer.h               |   4 +-
 src/snapshot/serializer-deserializer.cc       |   7 +-
 src/snapshot/serializer-deserializer.h        |  10 +-
 src/snapshot/serializer-inl.h                 |   2 +-
 src/snapshot/serializer.cc                    |  78 +-
 src/snapshot/serializer.h                     |  76 +-
 src/snapshot/shared-heap-serializer.cc        |  15 +-
 src/snapshot/shared-heap-serializer.h         |   4 +-
 src/snapshot/snapshot.cc                      |  18 +-
 src/snapshot/startup-serializer.cc            |   6 +-
 src/snapshot/startup-serializer.h             |   2 +-
 src/strings/string-builder-inl.h              |  16 +-
 src/strings/string-builder.cc                 |  34 +-
 src/strings/string-stream.cc                  |  14 +-
 src/strings/string-stream.h                   |   9 +-
 src/torque/implementation-visitor.cc          |   8 +-
 src/utils/address-map.h                       |   8 +-
 src/wasm/c-api.cc                             |  20 +-
 src/wasm/module-compiler.cc                   |  36 +-
 src/wasm/module-compiler.h                    |   6 +-
 src/wasm/module-instantiate.cc                |  14 +-
 src/wasm/wasm-code-manager.cc                 |   4 +-
 src/wasm/wasm-code-manager.h                  |   2 +-
 src/wasm/wasm-debug.cc                        |  18 +-
 src/wasm/wasm-js.cc                           |   2 +-
 src/wasm/wasm-objects-inl.h                   |  41 +-
 src/wasm/wasm-objects.cc                      |  87 ++-
 src/wasm/wasm-objects.h                       |  58 +-
 test/cctest/compiler/codegen-tester.h         |   4 +-
 test/cctest/compiler/function-tester.cc       |   2 +-
 .../test-atomic-load-store-codegen.cc         |  53 +-
 test/cctest/compiler/test-code-generator.cc   |   8 +-
 .../test-concurrent-shared-function-info.cc   |  13 +-
 .../compiler/test-representation-change.cc    |   2 +-
 test/cctest/compiler/test-run-load-store.cc   |  34 +-
 test/cctest/compiler/test-run-machops.cc      |  12 +-
 test/cctest/heap/heap-utils.cc                |  26 +-
 test/cctest/heap/heap-utils.h                 |   2 +-
 test/cctest/heap/test-alloc.cc                |   2 +-
 test/cctest/heap/test-array-buffer-tracker.cc |   4 +-
 .../cctest/heap/test-concurrent-allocation.cc |  23 +-
 test/cctest/heap/test-heap.cc                 |   2 +-
 test/cctest/heap/test-mark-compact.cc         |   6 +-
 test/cctest/heap/test-spaces.cc               |  38 +-
 test/cctest/heap/test-weak-references.cc      |  26 +-
 test/cctest/heap/test-write-barrier.cc        |   8 +-
 test/cctest/test-accessor-assembler.cc        |   2 +-
 test/cctest/test-api-typed-array.cc           |   2 +-
 test/cctest/test-api.cc                       |  12 +-
 test/cctest/test-code-stub-assembler.cc       |  32 +-
 test/cctest/test-debug.cc                     |  10 +-
 test/cctest/test-descriptor-array.cc          |  12 +-
 test/cctest/test-field-type-tracking.cc       |  35 +-
 test/cctest/test-inobject-slack-tracking.cc   |  11 +-
 test/cctest/test-js-weak-refs.cc              |  81 ++-
 test/cctest/test-log-stack-tracer.cc          |   8 +-
 test/cctest/test-mementos.cc                  |   2 +-
 test/cctest/test-orderedhashtable.cc          |  13 +-
 test/cctest/test-regexp.cc                    |   7 +-
 test/cctest/test-serialize.cc                 |   2 +-
 test/cctest/test-shared-strings.cc            |  19 +-
 test/cctest/test-smi-lexicographic-compare.cc |   8 +-
 test/cctest/test-strings.cc                   |  11 +-
 test/cctest/test-swiss-name-dictionary.cc     |   2 +-
 test/cctest/test-transitions.cc               |  16 +-
 test/cctest/test-transitions.h                |   6 +-
 test/cctest/test-unwinder-code-pages.cc       |  10 +-
 test/cctest/wasm/test-run-wasm-wrappers.cc    |   6 +-
 test/cctest/wasm/test-wasm-breakpoints.cc     |   2 +-
 test/cctest/wasm/test-wasm-serialization.cc   |   2 +-
 test/cctest/wasm/test-wasm-stack.cc           |   2 +-
 test/cctest/wasm/wasm-run-utils.cc            |   3 +-
 test/common/call-tester.h                     |   4 +-
 test/mkgrokdump/mkgrokdump.cc                 |  12 +-
 test/unittests/api/deserialize-unittest.cc    |  26 +-
 .../assembler/macro-assembler-x64-unittest.cc |   9 +-
 test/unittests/codegen/code-pages-unittest.cc |   8 +-
 test/unittests/compiler/codegen-tester.h      |   4 +-
 test/unittests/compiler/codegen-unittest.cc   |  12 +-
 test/unittests/compiler/compiler-unittest.cc  |   6 +-
 test/unittests/heap/heap-utils.cc             |  26 +-
 test/unittests/heap/heap-utils.h              |   2 +-
 .../heap/inner-pointer-resolution-unittest.cc |  12 +-
 test/unittests/heap/iterators-unittest.cc     |  14 +-
 test/unittests/heap/lab-unittest.cc           |   4 +-
 .../heap/marking-worklist-unittest.cc         |  28 +-
 .../heap/persistent-handles-unittest.cc       |   3 +-
 test/unittests/heap/shared-heap-unittest.cc   |   2 +-
 test/unittests/heap/spaces-unittest.cc        |   6 +-
 .../bytecode-array-iterator-unittest.cc       |   6 +-
 ...bytecode-array-random-iterator-unittest.cc |  42 +-
 .../bytecode-expectations-printer.cc          |   6 +-
 .../bytecode-expectations-printer.h           |   4 +-
 .../constant-array-builder-unittest.cc        |  17 +-
 .../interpreter/interpreter-tester.h          |   2 +-
 .../interpreter/interpreter-unittest.cc       |  27 +-
 test/unittests/logging/log-unittest.cc        |   2 +-
 test/unittests/maglev/node-type-unittest.cc   |   8 +-
 .../unittests/numbers/conversions-unittest.cc |   2 +-
 .../objects/concurrent-js-array-unittest.cc   |   2 +-
 ...oncurrent-script-context-table-unittest.cc |   2 +-
 test/unittests/objects/dictionary-unittest.cc |  21 +-
 .../objects/feedback-vector-unittest.cc       |  12 +-
 test/unittests/objects/object-unittest.cc     |  14 +-
 test/unittests/objects/roots-unittest.cc      |   2 +-
 test/unittests/objects/weakmaps-unittest.cc   |   3 +-
 .../parser/parse-decision-unittest.cc         |   2 +-
 test/unittests/parser/preparser-unittest.cc   |   4 +-
 .../parser/scanner-streams-unittest.cc        |   4 +-
 test/unittests/regexp/regexp-unittest.cc      |  12 +-
 .../tasks/background-compile-task-unittest.cc |   2 +-
 test/unittests/test-utils.h                   |  19 +-
 tools/debug_helper/debug-macro-shims.h        |   2 +-
 724 files changed, 9811 insertions(+), 8753 deletions(-)

diff --git a/src/api/api-arguments-inl.h b/src/api/api-arguments-inl.h
index a781b6e6428..b5aba465de7 100644
--- a/src/api/api-arguments-inl.h
+++ b/src/api/api-arguments-inl.h
@@ -41,21 +41,21 @@ Handle<V> CustomArguments<T>::GetReturnValue(Isolate* isolate) const {
   // Check the ReturnValue.
   FullObjectSlot slot = slot_at(kReturnValueIndex);
   // Nothing was set, return empty handle as per previous behaviour.
-  Object raw_object = *slot;
+  Tagged<Object> raw_object = *slot;
   if (IsTheHole(raw_object, isolate)) return Handle<V>();
   DCHECK(IsApiCallResultType(raw_object));
   return Handle<V>::cast(Handle<Object>(slot.location()));
 }
 
-inline JSObject PropertyCallbackArguments::holder() const {
+inline Tagged<JSObject> PropertyCallbackArguments::holder() const {
   return JSObject::cast(*slot_at(T::kHolderIndex));
 }
 
-inline Object PropertyCallbackArguments::receiver() const {
+inline Tagged<Object> PropertyCallbackArguments::receiver() const {
   return *slot_at(T::kThisIndex);
 }
 
-inline JSReceiver FunctionCallbackArguments::holder() const {
+inline Tagged<JSReceiver> FunctionCallbackArguments::holder() const {
   return JSReceiver::cast(*slot_at(T::kHolderIndex));
 }
 
@@ -84,7 +84,8 @@ inline JSReceiver FunctionCallbackArguments::holder() const {
   ExternalCallbackScope call_scope(ISOLATE, FUNCTION_ADDR(F));         \
   PropertyCallbackInfo<API_RETURN_TYPE> callback_info(values_);
 
-Handle<Object> FunctionCallbackArguments::Call(CallHandlerInfo handler) {
+Handle<Object> FunctionCallbackArguments::Call(
+    Tagged<CallHandlerInfo> handler) {
   Isolate* isolate = this->isolate();
   RCS_SCOPE(isolate, RuntimeCallCounterId::kFunctionCallback);
   v8::FunctionCallback f =
diff --git a/src/api/api-arguments.cc b/src/api/api-arguments.cc
index 980d8f3b198..08511a0ac4a 100644
--- a/src/api/api-arguments.cc
+++ b/src/api/api-arguments.cc
@@ -10,8 +10,8 @@ namespace v8 {
 namespace internal {
 
 PropertyCallbackArguments::PropertyCallbackArguments(
-    Isolate* isolate, Object data, Object self, JSObject holder,
-    Maybe<ShouldThrow> should_throw)
+    Isolate* isolate, Tagged<Object> data, Tagged<Object> self,
+    Tagged<JSObject> holder, Maybe<ShouldThrow> should_throw)
     : Super(isolate)
 #ifdef DEBUG
       ,
@@ -29,7 +29,7 @@ PropertyCallbackArguments::PropertyCallbackArguments(
   slot_at(T::kShouldThrowOnErrorIndex).store(Smi::FromInt(value));
   // Here the hole is set as default value.
   // It cannot escape into js as it's removed in Call below.
-  HeapObject the_hole_value = ReadOnlyRoots(isolate).the_hole_value();
+  Tagged<HeapObject> the_hole_value = ReadOnlyRoots(isolate).the_hole_value();
   slot_at(T::kReturnValueIndex).store(the_hole_value);
   slot_at(T::kUnusedIndex).store(Smi::zero());
   DCHECK(IsHeapObject(*slot_at(T::kHolderIndex)));
@@ -37,8 +37,10 @@ PropertyCallbackArguments::PropertyCallbackArguments(
 }
 
 FunctionCallbackArguments::FunctionCallbackArguments(
-    internal::Isolate* isolate, internal::Object data, internal::Object holder,
-    internal::HeapObject new_target, internal::Address* argv, int argc)
+    internal::Isolate* isolate, internal::Tagged<internal::Object> data,
+    internal::Tagged<internal::Object> holder,
+    internal::Tagged<internal::HeapObject> new_target, internal::Address* argv,
+    int argc)
     : Super(isolate), argv_(argv), argc_(argc) {
   slot_at(T::kDataIndex).store(data);
   slot_at(T::kHolderIndex).store(holder);
@@ -47,7 +49,7 @@ FunctionCallbackArguments::FunctionCallbackArguments(
   // Here the hole is set as default value. It's converted to and not
   // directly exposed to js.
   // TODO(cbruni): Remove and/or use custom sentinel value.
-  HeapObject the_hole_value = ReadOnlyRoots(isolate).the_hole_value();
+  Tagged<HeapObject> the_hole_value = ReadOnlyRoots(isolate).the_hole_value();
   slot_at(T::kReturnValueIndex).store(the_hole_value);
   slot_at(T::kUnusedIndex).store(Smi::zero());
   DCHECK(IsHeapObject(*slot_at(T::kHolderIndex)));
diff --git a/src/api/api-arguments.h b/src/api/api-arguments.h
index 2c02d36ce38..18f28ce7bdf 100644
--- a/src/api/api-arguments.h
+++ b/src/api/api-arguments.h
@@ -80,8 +80,9 @@ class PropertyCallbackArguments final
   static constexpr int kIsolateIndex = T::kIsolateIndex;
   static constexpr int kShouldThrowOnErrorIndex = T::kShouldThrowOnErrorIndex;
 
-  PropertyCallbackArguments(Isolate* isolate, Object data, Object self,
-                            JSObject holder, Maybe<ShouldThrow> should_throw);
+  PropertyCallbackArguments(Isolate* isolate, Tagged<Object> data,
+                            Tagged<Object> self, Tagged<JSObject> holder,
+                            Maybe<ShouldThrow> should_throw);
   inline ~PropertyCallbackArguments();
 
   // Don't copy PropertyCallbackArguments, because they would both have the
@@ -157,8 +158,8 @@ class PropertyCallbackArguments final
   inline Handle<JSObject> CallPropertyEnumerator(
       Handle<InterceptorInfo> interceptor);
 
-  inline JSObject holder() const;
-  inline Object receiver() const;
+  inline Tagged<JSObject> holder() const;
+  inline Tagged<Object> receiver() const;
 
 #ifdef DEBUG
   // This stores current value of Isolate::javascript_execution_counter().
@@ -195,8 +196,10 @@ class FunctionCallbackArguments
   static_assert(T::kValuesOffset == offsetof(T, values_));
   static_assert(T::kLengthOffset == offsetof(T, length_));
 
-  FunctionCallbackArguments(Isolate* isolate, Object data, Object holder,
-                            HeapObject new_target, Address* argv, int argc);
+  FunctionCallbackArguments(Isolate* isolate, Tagged<Object> data,
+                            Tagged<Object> holder,
+                            Tagged<HeapObject> new_target, Address* argv,
+                            int argc);
 
   /*
    * The following Call function wraps the calling of all callbacks to handle
@@ -206,10 +209,10 @@ class FunctionCallbackArguments
    * and used if it's been set to anything inside the callback.
    * New style callbacks always use the return value.
    */
-  inline Handle<Object> Call(CallHandlerInfo handler);
+  inline Handle<Object> Call(Tagged<CallHandlerInfo> handler);
 
  private:
-  inline JSReceiver holder() const;
+  inline Tagged<JSReceiver> holder() const;
 
   internal::Address* argv_;
   int const argc_;
diff --git a/src/api/api-inl.h b/src/api/api-inl.h
index 85d9a98c2bd..154ac355a1a 100644
--- a/src/api/api-inl.h
+++ b/src/api/api-inl.h
@@ -19,7 +19,7 @@
 namespace v8 {
 
 template <typename T>
-inline T ToCData(v8::internal::Object obj) {
+inline T ToCData(v8::internal::Tagged<v8::internal::Object> obj) {
   static_assert(sizeof(T) == sizeof(v8::internal::Address));
   if (obj == v8::internal::Smi::zero()) return nullptr;
   return reinterpret_cast<T>(
@@ -27,7 +27,8 @@ inline T ToCData(v8::internal::Object obj) {
 }
 
 template <>
-inline v8::internal::Address ToCData(v8::internal::Object obj) {
+inline v8::internal::Address ToCData(
+    v8::internal::Tagged<v8::internal::Object> obj) {
   if (obj == v8::internal::Smi::zero()) return v8::internal::kNullAddress;
   return v8::internal::Foreign::cast(obj)->foreign_address();
 }
@@ -191,7 +192,7 @@ class V8_NODISCARD CallDepthScope {
     isolate_->set_next_v8_call_is_safe_for_termination(false);
     if (!context.IsEmpty()) {
       i::DisallowGarbageCollection no_gc;
-      i::Context env = *Utils::OpenHandle(*context);
+      i::Tagged<i::Context> env = *Utils::OpenHandle(*context);
       i::HandleScopeImplementer* impl = isolate->handle_scope_implementer();
       if (isolate->context().is_null() ||
           isolate->context()->native_context() != env->native_context()) {
@@ -276,7 +277,7 @@ class V8_NODISCARD InternalEscapableScope : public EscapableHandleScope {
 
 template <typename T>
 void CopySmiElementsToTypedBuffer(T* dst, uint32_t length,
-                                  i::FixedArray elements) {
+                                  i::Tagged<i::FixedArray> elements) {
   for (uint32_t i = 0; i < length; ++i) {
     double value = i::Object::Number(elements->get(static_cast<int>(i)));
     // TODO(mslekova): Avoid converting back-and-forth when possible, e.g
@@ -287,7 +288,7 @@ void CopySmiElementsToTypedBuffer(T* dst, uint32_t length,
 
 template <typename T>
 void CopyDoubleElementsToTypedBuffer(T* dst, uint32_t length,
-                                     i::FixedDoubleArray elements) {
+                                     i::Tagged<i::FixedDoubleArray> elements) {
   for (uint32_t i = 0; i < length; ++i) {
     double value = elements->get_scalar(static_cast<int>(i));
     // TODO(mslekova): There are certain cases, e.g. double->double, in which
@@ -311,13 +312,13 @@ bool CopyAndConvertArrayToCppBuffer(Local<Array> src, T* dst,
   }
 
   i::DisallowGarbageCollection no_gc;
-  i::JSArray obj = *reinterpret_cast<i::JSArray*>(*src);
+  i::Tagged<i::JSArray> obj = *reinterpret_cast<i::JSArray*>(*src);
   if (i::Object::IterationHasObservableEffects(obj)) {
     // The array has a custom iterator.
     return false;
   }
 
-  i::FixedArrayBase elements = obj->elements();
+  i::Tagged<i::FixedArrayBase> elements = obj->elements();
   switch (obj->GetElementsKind()) {
     case i::PACKED_SMI_ELEMENTS:
       CopySmiElementsToTypedBuffer(dst, length, i::FixedArray::cast(elements));
@@ -349,14 +350,15 @@ inline bool V8_EXPORT TryToCopyAndConvertArrayToCppBuffer(Local<Array> src,
 
 namespace internal {
 
-void HandleScopeImplementer::EnterContext(NativeContext context) {
+void HandleScopeImplementer::EnterContext(Tagged<NativeContext> context) {
   DCHECK_EQ(entered_contexts_.capacity(), is_microtask_context_.capacity());
   DCHECK_EQ(entered_contexts_.size(), is_microtask_context_.size());
   entered_contexts_.push_back(context);
   is_microtask_context_.push_back(0);
 }
 
-void HandleScopeImplementer::EnterMicrotaskContext(NativeContext context) {
+void HandleScopeImplementer::EnterMicrotaskContext(
+    Tagged<NativeContext> context) {
   DCHECK_EQ(entered_contexts_.capacity(), is_microtask_context_.capacity());
   DCHECK_EQ(entered_contexts_.size(), is_microtask_context_.size());
   entered_contexts_.push_back(context);
diff --git a/src/api/api-natives.cc b/src/api/api-natives.cc
index c06f45dca75..3015c2f80fb 100644
--- a/src/api/api-natives.cc
+++ b/src/api/api-natives.cc
@@ -172,7 +172,7 @@ class V8_NODISCARD AccessCheckDisableScope {
   Handle<JSObject> obj_;
 };
 
-Object GetIntrinsic(Isolate* isolate, v8::Intrinsic intrinsic) {
+Tagged<Object> GetIntrinsic(Isolate* isolate, v8::Intrinsic intrinsic) {
   Handle<Context> native_context = isolate->native_context();
   DCHECK(!native_context.is_null());
   switch (intrinsic) {
@@ -197,7 +197,7 @@ MaybeHandle<JSObject> ConfigureInstance(Isolate* isolate, Handle<JSObject> obj,
   int max_number_of_properties = 0;
   TemplateInfoT info = *data;
   while (!info.is_null()) {
-    Object props = info.property_accessors();
+    Tagged<Object> props = info.property_accessors();
     if (!IsUndefined(props, isolate)) {
       max_number_of_properties += TemplateList::cast(props)->length();
     }
@@ -213,7 +213,7 @@ MaybeHandle<JSObject> ConfigureInstance(Isolate* isolate, Handle<JSObject> obj,
     for (Handle<TemplateInfoT> temp(*data, isolate); !temp->is_null();
          temp = handle(temp->GetParent(isolate), isolate)) {
       // Accumulate accessors.
-      Object maybe_properties = temp->property_accessors();
+      Tagged<Object> maybe_properties = temp->property_accessors();
       if (!IsUndefined(maybe_properties, isolate)) {
         valid_descriptors = AccessorInfo::AppendUnique(
             isolate, handle(maybe_properties, isolate), array,
@@ -231,7 +231,7 @@ MaybeHandle<JSObject> ConfigureInstance(Isolate* isolate, Handle<JSObject> obj,
     }
   }
 
-  Object maybe_property_list = data->property_list();
+  Tagged<Object> maybe_property_list = data->property_list();
   if (IsUndefined(maybe_property_list, isolate)) return obj;
   Handle<TemplateList> properties(TemplateList::cast(maybe_property_list),
                                   isolate);
@@ -240,7 +240,7 @@ MaybeHandle<JSObject> ConfigureInstance(Isolate* isolate, Handle<JSObject> obj,
   int i = 0;
   for (int c = 0; c < data->number_of_properties(); c++) {
     auto name = handle(Name::cast(properties->get(i++)), isolate);
-    Object bit = properties->get(i++);
+    Tagged<Object> bit = properties->get(i++);
     if (IsSmi(bit)) {
       PropertyDetails details(Smi::cast(bit));
       PropertyAttributes attributes = details.attributes();
@@ -297,7 +297,7 @@ MaybeHandle<JSObject> ProbeInstantiationsCache(
   }
 
   if (serial_number < TemplateInfo::kFastTemplateInstantiationsCacheSize) {
-    FixedArray fast_cache =
+    Tagged<FixedArray> fast_cache =
         native_context->fast_template_instantiations_cache();
     Handle<Object> object{fast_cache->get(serial_number), isolate};
     if (IsTheHole(*object, isolate)) return {};
@@ -305,7 +305,7 @@ MaybeHandle<JSObject> ProbeInstantiationsCache(
   }
   if (caching_mode == CachingMode::kUnlimited ||
       (serial_number < TemplateInfo::kSlowTemplateInstantiationsCacheSize)) {
-    SimpleNumberDictionary slow_cache =
+    Tagged<SimpleNumberDictionary> slow_cache =
         native_context->slow_template_instantiations_cache();
     InternalIndex entry = slow_cache->FindEntry(isolate, serial_number);
     if (entry.is_found()) {
@@ -361,7 +361,7 @@ void UncacheTemplateInstantiation(Isolate* isolate,
   if (serial_number < 0) return;
 
   if (serial_number < TemplateInfo::kFastTemplateInstantiationsCacheSize) {
-    FixedArray fast_cache =
+    Tagged<FixedArray> fast_cache =
         native_context->fast_template_instantiations_cache();
     DCHECK(!IsUndefined(fast_cache->get(serial_number), isolate));
     fast_cache->set_undefined(serial_number);
@@ -379,12 +379,12 @@ void UncacheTemplateInstantiation(Isolate* isolate,
   }
 }
 
-bool IsSimpleInstantiation(Isolate* isolate, ObjectTemplateInfo info,
-                           JSReceiver new_target) {
+bool IsSimpleInstantiation(Isolate* isolate, Tagged<ObjectTemplateInfo> info,
+                           Tagged<JSReceiver> new_target) {
   DisallowGarbageCollection no_gc;
 
   if (!IsJSFunction(new_target)) return false;
-  JSFunction fun = JSFunction::cast(new_target);
+  Tagged<JSFunction> fun = JSFunction::cast(new_target);
   if (fun->shared()->function_data(kAcquireLoad) != info->constructor())
     return false;
   if (info->immutable_proto()) return false;
@@ -417,7 +417,7 @@ MaybeHandle<JSObject> InstantiateObject(Isolate* isolate,
   }
 
   if (constructor.is_null()) {
-    Object maybe_constructor_info = info->constructor();
+    Tagged<Object> maybe_constructor_info = info->constructor();
     if (IsUndefined(maybe_constructor_info, isolate)) {
       constructor = isolate->object_function();
     } else {
@@ -559,7 +559,7 @@ MaybeHandle<JSFunction> InstantiateFunction(
 
 void AddPropertyToPropertyList(Isolate* isolate, Handle<TemplateInfo> templ,
                                int length, Handle<Object>* data) {
-  Object maybe_list = templ->property_list();
+  Tagged<Object> maybe_list = templ->property_list();
   Handle<TemplateList> list;
   if (IsUndefined(maybe_list, isolate)) {
     list = TemplateList::New(isolate, length);
@@ -677,7 +677,7 @@ void ApiNatives::AddAccessorProperty(Isolate* isolate,
 void ApiNatives::AddNativeDataProperty(Isolate* isolate,
                                        Handle<TemplateInfo> info,
                                        Handle<AccessorInfo> property) {
-  Object maybe_list = info->property_accessors();
+  Tagged<Object> maybe_list = info->property_accessors();
   Handle<TemplateList> list;
   if (IsUndefined(maybe_list, isolate)) {
     list = TemplateList::New(isolate, 1);
diff --git a/src/api/api.cc b/src/api/api.cc
index ba7960b265d..1f99076b952 100644
--- a/src/api/api.cc
+++ b/src/api/api.cc
@@ -839,7 +839,7 @@ void DisposeGlobal(i::Address* location) {
 
 i::Address* Eternalize(Isolate* v8_isolate, Value* value) {
   i::Isolate* i_isolate = reinterpret_cast<i::Isolate*>(v8_isolate);
-  i::Object object = *Utils::OpenDirectHandle(value);
+  i::Tagged<i::Object> object = *Utils::OpenDirectHandle(value);
   int index = -1;
   i_isolate->eternal_handles()->Create(i_isolate, object, &index);
   return i_isolate->eternal_handles()->Get(index).location();
@@ -968,9 +968,9 @@ bool Data::IsFixedArray() const {
 
 bool Data::IsValue() const {
   i::DisallowGarbageCollection no_gc;
-  i::Object self = *Utils::OpenDirectHandle(this);
+  i::Tagged<i::Object> self = *Utils::OpenDirectHandle(this);
   if (i::IsSmi(self)) return true;
-  i::HeapObject heap_object = i::HeapObject::cast(self);
+  i::Tagged<i::HeapObject> heap_object = i::HeapObject::cast(self);
   DCHECK(!IsTheHole(heap_object));
   if (i::IsSymbol(heap_object)) {
     return !i::Symbol::cast(heap_object)->is_private();
@@ -996,7 +996,7 @@ bool Data::IsContext() const {
 
 void Context::Enter() {
   i::DisallowGarbageCollection no_gc;
-  i::NativeContext env = *Utils::OpenDirectHandle(this);
+  i::Tagged<i::NativeContext> env = *Utils::OpenDirectHandle(this);
   i::Isolate* i_isolate = env->GetIsolate();
   ENTER_V8_NO_SCRIPT_NO_EXCEPTION(i_isolate);
   i::HandleScopeImplementer* impl = i_isolate->handle_scope_implementer();
@@ -1131,7 +1131,7 @@ void Context::SetAlignedPointerInEmbedderData(int index, void* value) {
 
 // --- T e m p l a t e ---
 
-static void InitializeTemplate(i::TemplateInfo that, int type,
+static void InitializeTemplate(i::Tagged<i::TemplateInfo> that, int type,
                                bool do_not_cache) {
   that->set_number_of_properties(0);
   that->set_tag(type);
@@ -1201,7 +1201,7 @@ void Template::SetAccessorProperty(v8::Local<v8::Name> name,
 }
 
 // --- F u n c t i o n   T e m p l a t e ---
-static void InitializeFunctionTemplate(i::FunctionTemplateInfo info,
+static void InitializeFunctionTemplate(i::Tagged<i::FunctionTemplateInfo> info,
                                        bool do_not_cache) {
   InitializeTemplate(info, Consts::FUNCTION_TEMPLATE, do_not_cache);
   info->set_flag(0, kRelaxedStore);
@@ -1220,7 +1220,7 @@ Local<ObjectTemplate> ObjectTemplateNew(i::Isolate* i_isolate,
   {
     // Disallow GC until all fields of obj have acceptable types.
     i::DisallowGarbageCollection no_gc;
-    i::ObjectTemplateInfo raw = *obj;
+    i::Tagged<i::ObjectTemplateInfo> raw = *obj;
     InitializeTemplate(raw, Consts::OBJECT_TEMPLATE, do_not_cache);
     raw->set_data(0);
     if (!constructor.IsEmpty()) {
@@ -1287,7 +1287,7 @@ Local<FunctionTemplate> FunctionTemplateNew(
   {
     // Disallow GC until all fields of obj have acceptable types.
     i::DisallowGarbageCollection no_gc;
-    i::FunctionTemplateInfo raw = *obj;
+    i::Tagged<i::FunctionTemplateInfo> raw = *obj;
     InitializeFunctionTemplate(raw, do_not_cache);
     raw->set_length(length);
     raw->set_undetectable(false);
@@ -1476,7 +1476,7 @@ i::Handle<i::AccessorInfo> MakeAccessorInfo(
         i::Handle<i::String>::cast(accessor_name));
   }
   i::DisallowGarbageCollection no_gc;
-  i::AccessorInfo raw_obj = *obj;
+  i::Tagged<i::AccessorInfo> raw_obj = *obj;
   if (data.IsEmpty()) {
     raw_obj->set_data(i::ReadOnlyRoots(i_isolate).undefined_value());
   } else {
@@ -1570,9 +1570,11 @@ namespace {
 // constructor is available we create one.
 i::Handle<i::FunctionTemplateInfo> EnsureConstructor(
     i::Isolate* i_isolate, ObjectTemplate* object_template) {
-  i::Object obj = Utils::OpenDirectHandle(object_template)->constructor();
+  i::Tagged<i::Object> obj =
+      Utils::OpenDirectHandle(object_template)->constructor();
   if (!IsUndefined(obj, i_isolate)) {
-    i::FunctionTemplateInfo info = i::FunctionTemplateInfo::cast(obj);
+    i::Tagged<i::FunctionTemplateInfo> info =
+        i::FunctionTemplateInfo::cast(obj);
     return i::Handle<i::FunctionTemplateInfo>(info, i_isolate);
   }
   Local<FunctionTemplate> templ =
@@ -1600,7 +1602,7 @@ void TemplateSetAccessor(Template* template_obj, v8::Local<Name> name,
                        is_special_data_property, replace_on_access);
   {
     i::DisallowGarbageCollection no_gc;
-    i::AccessorInfo raw = *accessor_info;
+    i::Tagged<i::AccessorInfo> raw = *accessor_info;
     raw->set_initial_property_attributes(
         static_cast<i::PropertyAttributes>(attribute));
     raw->set_getter_side_effect_type(getter_side_effect_type);
@@ -2018,7 +2020,7 @@ Local<Value> UnboundScript::GetScriptName() {
     i::Isolate* i_isolate = i::GetIsolateFromWritableObject(*obj);
     ENTER_V8_NO_SCRIPT_NO_EXCEPTION(i_isolate);
     API_RCS_SCOPE(i_isolate, UnboundScript, GetName);
-    i::Object name = i::Script::cast(obj->script())->name();
+    i::Tagged<i::Object> name = i::Script::cast(obj->script())->name();
     return Utils::ToLocal(i::DirectHandle<i::Object>(name, i_isolate),
                           i_isolate);
   } else {
@@ -2035,7 +2037,7 @@ Local<Value> UnboundScript::GetSourceURL() {
     i::Isolate* i_isolate = i::GetIsolateFromWritableObject(*obj);
     API_RCS_SCOPE(i_isolate, UnboundScript, GetSourceURL);
     ENTER_V8_NO_SCRIPT_NO_EXCEPTION(i_isolate);
-    i::Object url = i::Script::cast(obj->script())->source_url();
+    i::Tagged<i::Object> url = i::Script::cast(obj->script())->source_url();
     return Utils::ToLocal(i::DirectHandle<i::Object>(url, i_isolate),
                           i_isolate);
   } else {
@@ -2052,7 +2054,8 @@ Local<Value> UnboundScript::GetSourceMappingURL() {
     i::Isolate* i_isolate = i::GetIsolateFromWritableObject(*obj);
     API_RCS_SCOPE(i_isolate, UnboundScript, GetSourceMappingURL);
     ENTER_V8_NO_SCRIPT_NO_EXCEPTION(i_isolate);
-    i::Object url = i::Script::cast(obj->script())->source_mapping_url();
+    i::Tagged<i::Object> url =
+        i::Script::cast(obj->script())->source_mapping_url();
     return Utils::ToLocal(i::DirectHandle<i::Object>(url, i_isolate),
                           i_isolate);
   } else {
@@ -2069,7 +2072,7 @@ Local<Value> UnboundModuleScript::GetSourceURL() {
     i::Isolate* i_isolate = i::GetIsolateFromWritableObject(*obj);
     API_RCS_SCOPE(i_isolate, UnboundModuleScript, GetSourceURL);
     ENTER_V8_NO_SCRIPT_NO_EXCEPTION(i_isolate);
-    i::Object url = i::Script::cast(obj->script())->source_url();
+    i::Tagged<i::Object> url = i::Script::cast(obj->script())->source_url();
     return Utils::ToLocal(i::DirectHandle<i::Object>(url, i_isolate),
                           i_isolate);
   } else {
@@ -2086,7 +2089,8 @@ Local<Value> UnboundModuleScript::GetSourceMappingURL() {
     i::Isolate* i_isolate = i::GetIsolateFromWritableObject(*obj);
     API_RCS_SCOPE(i_isolate, UnboundModuleScript, GetSourceMappingURL);
     ENTER_V8_NO_SCRIPT_NO_EXCEPTION(i_isolate);
-    i::Object url = i::Script::cast(obj->script())->source_mapping_url();
+    i::Tagged<i::Object> url =
+        i::Script::cast(obj->script())->source_mapping_url();
     return Utils::ToLocal(i::DirectHandle<i::Object>(url, i_isolate),
                           i_isolate);
   } else {
@@ -2166,7 +2170,7 @@ Local<UnboundScript> Script::GetUnboundScript() {
 Local<Value> Script::GetResourceName() {
   i::DisallowGarbageCollection no_gc;
   i::DirectHandle<i::JSFunction> func = Utils::OpenDirectHandle(this);
-  i::SharedFunctionInfo sfi = func->shared();
+  i::Tagged<i::SharedFunctionInfo> sfi = func->shared();
   CHECK(IsScript(sfi->script()));
   i::DirectHandle<i::Object> name(i::Script::cast(sfi->script())->name(),
                                   func->GetIsolate());
@@ -2177,16 +2181,17 @@ std::vector<int> Script::GetProducedCompileHints() const {
   i::DisallowGarbageCollection no_gc;
   i::DirectHandle<i::JSFunction> func = Utils::OpenDirectHandle(this);
   i::Isolate* i_isolate = func->GetIsolate();
-  i::SharedFunctionInfo sfi = func->shared();
+  i::Tagged<i::SharedFunctionInfo> sfi = func->shared();
   CHECK(IsScript(sfi->script()));
-  i::Script script = i::Script::cast(sfi->script());
-  i::Object maybe_array_list = script->compiled_lazy_function_positions();
+  i::Tagged<i::Script> script = i::Script::cast(sfi->script());
+  i::Tagged<i::Object> maybe_array_list =
+      script->compiled_lazy_function_positions();
   std::vector<int> result;
   if (!IsUndefined(maybe_array_list, i_isolate)) {
-    i::ArrayList array_list = i::ArrayList::cast(maybe_array_list);
+    i::Tagged<i::ArrayList> array_list = i::ArrayList::cast(maybe_array_list);
     result.reserve(array_list->Length());
     for (int i = 0; i < array_list->Length(); ++i) {
-      i::Object item = array_list->Get(i);
+      i::Tagged<i::Object> item = array_list->Get(i);
       CHECK(IsSmi(item));
       result.push_back(i::Smi::ToInt(item));
     }
@@ -2368,7 +2373,7 @@ Local<UnboundModuleScript> Module::GetUnboundModuleScript() {
 }
 
 int Module::ScriptId() const {
-  i::Module self = *Utils::OpenDirectHandle(this);
+  i::Tagged<i::Module> self = *Utils::OpenDirectHandle(this);
   Utils::ApiCheck(i::IsSourceTextModule(self), "v8::Module::ScriptId",
                   "v8::Module::ScriptId must be used on an SourceTextModule");
   DCHECK_NO_SCRIPT_NO_EXCEPTION(self->GetIsolate());
@@ -2379,7 +2384,7 @@ bool Module::IsGraphAsync() const {
   Utils::ApiCheck(
       GetStatus() >= kInstantiated, "v8::Module::IsGraphAsync",
       "v8::Module::IsGraphAsync must be used on an instantiated module");
-  i::Module self = *Utils::OpenDirectHandle(this);
+  i::Tagged<i::Module> self = *Utils::OpenDirectHandle(this);
   auto i_isolate = self->GetIsolate();
   DCHECK_NO_SCRIPT_NO_EXCEPTION(i_isolate);
   return self->IsGraphAsync(i_isolate);
@@ -2993,7 +2998,7 @@ v8::Local<v8::Value> v8::TryCatch::ReThrow() {
 v8::Local<Value> v8::TryCatch::Exception() const {
   if (HasCaught()) {
     // Check for out of memory exception.
-    i::Object exception(reinterpret_cast<i::Address>(exception_));
+    i::Tagged<i::Object> exception(reinterpret_cast<i::Address>(exception_));
     return v8::Utils::ToLocal(i::Handle<i::Object>(exception, i_isolate_));
   } else {
     return v8::Local<Value>();
@@ -3024,7 +3029,7 @@ MaybeLocal<Value> v8::TryCatch::StackTrace(Local<Context> context) const {
 }
 
 v8::Local<v8::Message> v8::TryCatch::Message() const {
-  i::Object message(reinterpret_cast<i::Address>(message_obj_));
+  i::Tagged<i::Object> message(reinterpret_cast<i::Address>(message_obj_));
   DCHECK(IsJSMessageObject(message) || IsTheHole(message, i_isolate_));
   if (HasCaught() && !IsTheHole(message, i_isolate_)) {
     return v8::Utils::MessageToLocal(i::Handle<i::Object>(message, i_isolate_));
@@ -3044,7 +3049,7 @@ void v8::TryCatch::Reset() {
 }
 
 void v8::TryCatch::ResetInternal() {
-  i::Object the_hole = i::ReadOnlyRoots(i_isolate_).the_hole_value();
+  i::Tagged<i::Object> the_hole = i::ReadOnlyRoots(i_isolate_).the_hole_value();
   exception_ = reinterpret_cast<void*>(the_hole.ptr());
   message_obj_ = reinterpret_cast<void*>(the_hole.ptr());
 }
@@ -3671,13 +3676,13 @@ bool Value::FullIsNull() const {
 }
 
 bool Value::IsTrue() const {
-  i::Object object = *Utils::OpenHandle(this);
+  i::Tagged<i::Object> object = *Utils::OpenHandle(this);
   if (i::IsSmi(object)) return false;
   return i::IsTrue(object);
 }
 
 bool Value::IsFalse() const {
-  i::Object object = *Utils::OpenHandle(this);
+  i::Tagged<i::Object> object = *Utils::OpenHandle(this);
   if (i::IsSmi(object)) return false;
   return i::IsFalse(object);
 }
@@ -3699,7 +3704,7 @@ bool Value::IsSymbol() const {
 bool Value::IsArray() const { return IsJSArray(*Utils::OpenHandle(this)); }
 
 bool Value::IsArrayBuffer() const {
-  i::Object obj = *Utils::OpenHandle(this);
+  i::Tagged<i::Object> obj = *Utils::OpenHandle(this);
   if (!IsJSArrayBuffer(obj)) return false;
   return !i::JSArrayBuffer::cast(obj)->is_shared();
 }
@@ -3729,7 +3734,7 @@ bool Value::IsDataView() const {
 }
 
 bool Value::IsSharedArrayBuffer() const {
-  i::Object obj = *Utils::OpenHandle(this);
+  i::Tagged<i::Object> obj = *Utils::OpenHandle(this);
   if (!IsJSArrayBuffer(obj)) return false;
   return i::JSArrayBuffer::cast(obj)->is_shared();
 }
@@ -3777,12 +3782,12 @@ VALUE_IS_SPECIFIC_TYPE(WeakRef, JSWeakRef)
 bool Value::IsBoolean() const { return i::IsBoolean(*Utils::OpenHandle(this)); }
 
 bool Value::IsExternal() const {
-  i::Object obj = *Utils::OpenHandle(this);
+  i::Tagged<i::Object> obj = *Utils::OpenHandle(this);
   return IsJSExternalObject(obj);
 }
 
 bool Value::IsInt32() const {
-  i::Object obj = *Utils::OpenHandle(this);
+  i::Tagged<i::Object> obj = *Utils::OpenHandle(this);
   if (i::IsSmi(obj)) return true;
   if (i::IsNumber(obj)) {
     return i::IsInt32Double(i::Object::Number(obj));
@@ -3811,16 +3816,16 @@ bool Value::IsRegExp() const {
 }
 
 bool Value::IsAsyncFunction() const {
-  i::Object obj = *Utils::OpenHandle(this);
+  i::Tagged<i::Object> obj = *Utils::OpenHandle(this);
   if (!IsJSFunction(obj)) return false;
-  i::JSFunction func = i::JSFunction::cast(obj);
+  i::Tagged<i::JSFunction> func = i::JSFunction::cast(obj);
   return i::IsAsyncFunction(func->shared()->kind());
 }
 
 bool Value::IsGeneratorFunction() const {
-  i::Object obj = *Utils::OpenHandle(this);
+  i::Tagged<i::Object> obj = *Utils::OpenHandle(this);
   if (!IsJSFunction(obj)) return false;
-  i::JSFunction func = i::JSFunction::cast(obj);
+  i::Tagged<i::JSFunction> func = i::JSFunction::cast(obj);
   DCHECK_NO_SCRIPT_NO_EXCEPTION(func->GetIsolate());
   return i::IsGeneratorFunction(func->shared()->kind());
 }
@@ -5252,13 +5257,13 @@ void* v8::Object::GetAlignedPointerFromEmbedderDataInCreationContext(
   // The code below mostly mimics Context::GetAlignedPointerFromEmbedderData()
   // but it doesn't try to expand the EmbedderDataArray instance.
   i::DisallowGarbageCollection no_gc;
-  i::NativeContext native_context =
+  i::Tagged<i::NativeContext> native_context =
       i::NativeContext::cast(maybe_context.value());
   i::Isolate* i_isolate = native_context->GetIsolate();
 
   DCHECK_NO_SCRIPT_NO_EXCEPTION(i_isolate);
   // TODO(ishell): remove cast once embedder_data slot has a proper type.
-  i::EmbedderDataArray data =
+  i::Tagged<i::EmbedderDataArray> data =
       i::EmbedderDataArray::cast(native_context->embedder_data());
   if (V8_LIKELY(static_cast<unsigned>(index) <
                 static_cast<unsigned>(data->length()))) {
@@ -5390,11 +5395,12 @@ MaybeLocal<Object> Function::NewInstanceWithSideEffectType(
   if (should_set_has_no_side_effect) {
     CHECK(IsJSFunction(*self) &&
           i::JSFunction::cast(*self)->shared()->IsApiFunction());
-    i::Object obj =
+    i::Tagged<i::Object> obj =
         i::JSFunction::cast(*self)->shared()->api_func_data()->call_code(
             kAcquireLoad);
     if (i::IsCallHandlerInfo(obj)) {
-      i::CallHandlerInfo handler_info = i::CallHandlerInfo::cast(obj);
+      i::Tagged<i::CallHandlerInfo> handler_info =
+          i::CallHandlerInfo::cast(obj);
       if (handler_info->IsSideEffectCallHandlerInfo()) {
         i_isolate->debug()->IgnoreSideEffectsOnNextCallTo(
             handle(handler_info, i_isolate));
@@ -5547,13 +5553,13 @@ int Function::GetScriptStartPosition() const {
 MaybeLocal<UnboundScript> Function::GetUnboundScript() const {
   i::Handle<i::JSReceiver> self = Utils::OpenHandle(this);
   if (!IsJSFunction(*self)) return MaybeLocal<UnboundScript>();
-  i::SharedFunctionInfo sfi = i::JSFunction::cast(*self)->shared();
+  i::Tagged<i::SharedFunctionInfo> sfi = i::JSFunction::cast(*self)->shared();
   i::Isolate* i_isolate = self->GetIsolate();
   return ToApiHandle<UnboundScript>(i::handle(sfi, i_isolate));
 }
 
 int Function::ScriptId() const {
-  i::JSReceiver self = *Utils::OpenHandle(this);
+  i::Tagged<i::JSReceiver> self = *Utils::OpenHandle(this);
   if (!IsJSFunction(self)) return v8::UnboundScript::kNoScriptId;
   auto func = i::JSFunction::cast(self);
   if (!IsScript(func->shared()->script()))
@@ -5575,7 +5581,7 @@ Local<v8::Value> Function::GetBoundFunction() const {
 bool Function::Experimental_IsNopFunction() const {
   auto self = Utils::OpenHandle(this);
   if (!IsJSFunction(*self)) return false;
-  i::SharedFunctionInfo sfi = i::JSFunction::cast(*self)->shared();
+  i::Tagged<i::SharedFunctionInfo> sfi = i::JSFunction::cast(*self)->shared();
   i::Isolate* i_isolate = self->GetIsolate();
   i::IsCompiledScope is_compiled_scope(sfi->is_compiled_scope(i_isolate));
   if (!is_compiled_scope.is_compiled() &&
@@ -5656,8 +5662,9 @@ class ContainsOnlyOneByteHelper {
   ContainsOnlyOneByteHelper(const ContainsOnlyOneByteHelper&) = delete;
   ContainsOnlyOneByteHelper& operator=(const ContainsOnlyOneByteHelper&) =
       delete;
-  bool Check(i::String string) {
-    i::ConsString cons_string = i::String::VisitFlat(this, string, 0);
+  bool Check(i::Tagged<i::String> string) {
+    i::Tagged<i::ConsString> cons_string =
+        i::String::VisitFlat(this, string, 0);
     if (cons_string.is_null()) return is_one_byte_;
     return CheckCons(cons_string);
   }
@@ -5697,15 +5704,17 @@ class ContainsOnlyOneByteHelper {
   }
 
  private:
-  bool CheckCons(i::ConsString cons_string) {
+  bool CheckCons(i::Tagged<i::ConsString> cons_string) {
     while (true) {
       // Check left side if flat.
-      i::String left = cons_string->first();
-      i::ConsString left_as_cons = i::String::VisitFlat(this, left, 0);
+      i::Tagged<i::String> left = cons_string->first();
+      i::Tagged<i::ConsString> left_as_cons =
+          i::String::VisitFlat(this, left, 0);
       if (!is_one_byte_) return false;
       // Check right side if flat.
-      i::String right = cons_string->second();
-      i::ConsString right_as_cons = i::String::VisitFlat(this, right, 0);
+      i::Tagged<i::String> right = cons_string->second();
+      i::Tagged<i::ConsString> right_as_cons =
+          i::String::VisitFlat(this, right, 0);
       if (!is_one_byte_) return false;
       // Standard recurse/iterate trick.
       if (!left_as_cons.is_null() && !right_as_cons.is_null()) {
@@ -5950,13 +5959,13 @@ int String::Write(Isolate* v8_isolate, uint16_t* buffer, int start, int length,
 
 namespace {
 
-bool HasExternalStringResource(i::String string) {
+bool HasExternalStringResource(i::Tagged<i::String> string) {
   return i::StringShape(string).IsExternal() ||
          string->HasExternalForwardingIndex(kAcquireLoad);
 }
 
 v8::String::ExternalStringResourceBase* GetExternalResourceFromForwardingTable(
-    i::String string, uint32_t raw_hash, bool* is_one_byte) {
+    i::Tagged<i::String> string, uint32_t raw_hash, bool* is_one_byte) {
   DCHECK(i::String::IsExternalForwardingIndex(raw_hash));
   const int index = i::String::ForwardingIndexValueBits::decode(raw_hash);
   i::Isolate* isolate = i::GetIsolateFromWritableObject(string);
@@ -6000,7 +6009,7 @@ bool v8::String::IsExternalOneByte() const {
 void v8::String::VerifyExternalStringResource(
     v8::String::ExternalStringResource* value) const {
   i::DisallowGarbageCollection no_gc;
-  i::String str = *Utils::OpenHandle(this);
+  i::Tagged<i::String> str = *Utils::OpenHandle(this);
   const v8::String::ExternalStringResource* expected;
 
   if (i::IsThinString(str)) {
@@ -6029,7 +6038,7 @@ void v8::String::VerifyExternalStringResource(
 void v8::String::VerifyExternalStringResourceBase(
     v8::String::ExternalStringResourceBase* value, Encoding encoding) const {
   i::DisallowGarbageCollection no_gc;
-  i::String str = *Utils::OpenHandle(this);
+  i::Tagged<i::String> str = *Utils::OpenHandle(this);
   const v8::String::ExternalStringResourceBase* expected;
   Encoding expectedEncoding;
 
@@ -6064,7 +6073,7 @@ void v8::String::VerifyExternalStringResourceBase(
 
 String::ExternalStringResource* String::GetExternalStringResourceSlow() const {
   i::DisallowGarbageCollection no_gc;
-  i::String str = *Utils::OpenHandle(this);
+  i::Tagged<i::String> str = *Utils::OpenHandle(this);
 
   if (i::IsThinString(str)) {
     str = i::ThinString::cast(str)->actual();
@@ -6112,7 +6121,7 @@ String::ExternalStringResourceBase* String::GetExternalStringResourceBaseSlow(
     String::Encoding* encoding_out) const {
   i::DisallowGarbageCollection no_gc;
   ExternalStringResourceBase* resource = nullptr;
-  i::String str = *Utils::OpenHandle(this);
+  i::Tagged<i::String> str = *Utils::OpenHandle(this);
 
   if (i::IsThinString(str)) {
     str = i::ThinString::cast(str)->actual();
@@ -6146,7 +6155,7 @@ String::ExternalStringResourceBase* String::GetExternalStringResourceBaseSlow(
 const v8::String::ExternalOneByteStringResource*
 v8::String::GetExternalOneByteStringResource() const {
   i::DisallowGarbageCollection no_gc;
-  i::String str = *Utils::OpenHandle(this);
+  i::Tagged<i::String> str = *Utils::OpenHandle(this);
   if (i::StringShape(str).IsExternalOneByte()) {
     return i::ExternalOneByteString::cast(str)->resource();
   } else if (i::IsThinString(str)) {
@@ -6194,7 +6203,7 @@ bool Boolean::Value() const {
 }
 
 int64_t Integer::Value() const {
-  i::Object obj = *Utils::OpenHandle(this);
+  i::Tagged<i::Object> obj = *Utils::OpenHandle(this);
   if (i::IsSmi(obj)) {
     return i::Smi::ToInt(obj);
   } else {
@@ -6203,7 +6212,7 @@ int64_t Integer::Value() const {
 }
 
 int32_t Int32::Value() const {
-  i::Object obj = *Utils::OpenHandle(this);
+  i::Tagged<i::Object> obj = *Utils::OpenHandle(this);
   if (i::IsSmi(obj)) {
     return i::Smi::ToInt(obj);
   } else {
@@ -6212,7 +6221,7 @@ int32_t Int32::Value() const {
 }
 
 uint32_t Uint32::Value() const {
-  i::Object obj = *Utils::OpenHandle(this);
+  i::Tagged<i::Object> obj = *Utils::OpenHandle(this);
   if (i::IsSmi(obj)) {
     return i::Smi::ToInt(obj);
   } else {
@@ -6221,7 +6230,7 @@ uint32_t Uint32::Value() const {
 }
 
 int v8::Object::InternalFieldCount() const {
-  i::JSReceiver self = *Utils::OpenHandle(this);
+  i::Tagged<i::JSReceiver> self = *Utils::OpenHandle(this);
   if (!IsJSObject(self)) return 0;
   return i::JSObject::cast(self)->GetEmbedderFieldCount();
 }
@@ -6282,7 +6291,7 @@ void v8::Object::SetAlignedPointerInInternalFields(int argc, int indices[],
 
   i::DisallowGarbageCollection no_gc;
   const char* location = "v8::Object::SetAlignedPointerInInternalFields()";
-  i::JSObject js_obj = i::JSObject::cast(*obj);
+  i::Tagged<i::JSObject> js_obj = i::JSObject::cast(*obj);
   int nof_embedder_fields = js_obj->GetEmbedderFieldCount();
   for (int i = 0; i < argc; i++) {
     int index = indices[i];
@@ -6732,7 +6741,7 @@ void v8::Context::UseDefaultSecurityToken() {
 Local<Value> v8::Context::GetSecurityToken() {
   i::Handle<i::NativeContext> env = Utils::OpenHandle(this);
   i::Isolate* i_isolate = env->GetIsolate();
-  i::Object security_token = env->security_token();
+  i::Tagged<i::Object> security_token = env->security_token();
   i::Handle<i::Object> token_handle(security_token, i_isolate);
   return Utils::ToLocal(token_handle);
 }
@@ -6844,21 +6853,21 @@ class ObjectVisitorDeepFreezer : i::ObjectVisitor {
     return true;
   }
 
-  void VisitPointers(i::HeapObject host, i::ObjectSlot start,
+  void VisitPointers(i::Tagged<i::HeapObject> host, i::ObjectSlot start,
                      i::ObjectSlot end) final {
     VisitPointersImpl(start, end);
   }
-  void VisitPointers(i::HeapObject host, i::MaybeObjectSlot start,
+  void VisitPointers(i::Tagged<i::HeapObject> host, i::MaybeObjectSlot start,
                      i::MaybeObjectSlot end) final {
     VisitPointersImpl(start, end);
   }
-  void VisitMapPointer(i::HeapObject host) final {
+  void VisitMapPointer(i::Tagged<i::HeapObject> host) final {
     VisitPointer(host, host->map_slot());
   }
-  void VisitInstructionStreamPointer(i::Code host,
+  void VisitInstructionStreamPointer(i::Tagged<i::Code> host,
                                      i::InstructionStreamSlot slot) final {}
-  void VisitCustomWeakPointers(i::HeapObject host, i::ObjectSlot start,
-                               i::ObjectSlot end) final {}
+  void VisitCustomWeakPointers(i::Tagged<i::HeapObject> host,
+                               i::ObjectSlot start, i::ObjectSlot end) final {}
 
  private:
   struct ErrorInfo {
@@ -6870,7 +6879,7 @@ class ObjectVisitorDeepFreezer : i::ObjectVisitor {
   void VisitPointersImpl(TSlot start, TSlot end) {
     for (TSlot current = start; current < end; ++current) {
       typename TSlot::TObject object = current.load(isolate_);
-      i::HeapObject heap_object;
+      i::Tagged<i::HeapObject> heap_object;
       if (object.GetHeapObjectIfStrong(&heap_object)) {
         if (!VisitObject(heap_object)) {
           return;
@@ -6894,7 +6903,7 @@ class ObjectVisitorDeepFreezer : i::ObjectVisitor {
     return true;
   }
 
-  bool VisitObject(i::HeapObject obj) {
+  bool VisitObject(i::Tagged<i::HeapObject> obj) {
     DCHECK(!obj.is_null());
     if (error_.has_value()) {
       return false;
@@ -6919,7 +6928,7 @@ class ObjectVisitorDeepFreezer : i::ObjectVisitor {
       // have been instantiated into actual JavaScript objects that can be
       // frozen. If they haven't then we need to save them to instantiate
       // (and recurse) before freezing.
-      i::AccessorPair accessor_pair = i::AccessorPair::cast(obj);
+      i::Tagged<i::AccessorPair> accessor_pair = i::AccessorPair::cast(obj);
       if (i::IsFunctionTemplateInfo(accessor_pair->getter()) ||
           IsFunctionTemplateInfo(accessor_pair->setter())) {
         i::Handle<i::AccessorPair> lazy_accessor_pair(accessor_pair, isolate_);
@@ -6928,8 +6937,8 @@ class ObjectVisitorDeepFreezer : i::ObjectVisitor {
     } else if (i::InstanceTypeChecker::IsContext(obj_type)) {
       // For contexts we need to ensure that all accessible locals are const.
       // If not they could be replaced to bypass freezing.
-      i::ScopeInfo scope_info = i::Context::cast(obj)->scope_info();
-      for (auto it : i::ScopeInfo::IterateLocalNames(&scope_info, no_gc)) {
+      i::Tagged<i::ScopeInfo> scope_info = i::Context::cast(obj)->scope_info();
+      for (auto it : i::ScopeInfo::IterateLocalNames(scope_info, no_gc)) {
         if (scope_info->ContextLocalMode(it->index()) !=
             i::VariableMode::kConst) {
           DCHECK(!error_.has_value());
@@ -7108,7 +7117,7 @@ void Context::AllowCodeGenerationFromStrings(bool allow) {
 }
 
 bool Context::IsCodeGenerationFromStringsAllowed() const {
-  i::NativeContext context = *Utils::OpenHandle(this);
+  i::Tagged<i::NativeContext> context = *Utils::OpenHandle(this);
   return !IsFalse(context->allow_code_gen_from_strings(),
                   context->GetIsolate());
 }
@@ -7190,10 +7199,10 @@ void v8::Context::SetPromiseHooks(Local<Function> init_hook,
 
   i_isolate->SetHasContextPromiseHooks(has_hook);
 
-  context->native_context().set_promise_hook_init_function(*init);
-  context->native_context().set_promise_hook_before_function(*before);
-  context->native_context().set_promise_hook_after_function(*after);
-  context->native_context().set_promise_hook_resolve_function(*resolve);
+  context->native_context()->set_promise_hook_init_function(*init);
+  context->native_context()->set_promise_hook_before_function(*before);
+  context->native_context()->set_promise_hook_after_function(*after);
+  context->native_context()->set_promise_hook_resolve_function(*resolve);
 #else   // V8_ENABLE_JAVASCRIPT_PROMISE_HOOKS
   Utils::ApiCheck(false, "v8::Context::SetPromiseHook",
                   "V8 was compiled without JavaScript Promise hooks");
@@ -7202,7 +7211,7 @@ void v8::Context::SetPromiseHooks(Local<Function> init_hook,
 
 bool Context::HasTemplateLiteralObject(Local<Value> object) {
   i::DisallowGarbageCollection no_gc;
-  i::Object i_object = *Utils::OpenHandle(*object);
+  i::Tagged<i::Object> i_object = *Utils::OpenHandle(*object);
   if (!IsJSArray(i_object)) return false;
   return Utils::OpenHandle(this)->native_context()->HasTemplateLiteralObject(
       i::JSArray::cast(i_object));
@@ -7229,10 +7238,11 @@ metrics::LongTaskStats metrics::LongTaskStats::Get(v8::Isolate* v8_isolate) {
 
 namespace {
 i::Address* GetSerializedDataFromFixedArray(i::Isolate* i_isolate,
-                                            i::FixedArray list, size_t index) {
+                                            i::Tagged<i::FixedArray> list,
+                                            size_t index) {
   if (index < static_cast<size_t>(list->length())) {
     int int_index = static_cast<int>(index);
-    i::Object object = list->get(int_index);
+    i::Tagged<i::Object> object = list->get(int_index);
     if (!IsTheHole(object, i_isolate)) {
       list->set_the_hole(i_isolate, int_index);
       // Shrink the list so that the last element is not the hole (unless it's
@@ -7251,7 +7261,7 @@ i::Address* GetSerializedDataFromFixedArray(i::Isolate* i_isolate,
 i::Address* Context::GetDataFromSnapshotOnce(size_t index) {
   auto context = Utils::OpenHandle(this);
   i::Isolate* i_isolate = context->GetIsolate();
-  i::FixedArray list = context->serialized_objects();
+  i::Tagged<i::FixedArray> list = context->serialized_objects();
   return GetSerializedDataFromFixedArray(i_isolate, list, index);
 }
 
@@ -7345,7 +7355,7 @@ bool FunctionTemplate::IsLeafTemplateForApiObject(
     v8::Local<v8::Value> value) const {
   i::DisallowGarbageCollection no_gc;
 
-  i::Object object = *Utils::OpenHandle(*value);
+  i::Tagged<i::Object> object = *Utils::OpenHandle(*value);
 
   auto self = Utils::OpenHandle(this);
   return self->IsLeafTemplateForApiObject(object);
@@ -7543,7 +7553,7 @@ MaybeLocal<String> v8::String::NewExternalOneByte(
 bool v8::String::MakeExternal(v8::String::ExternalStringResource* resource) {
   i::DisallowGarbageCollection no_gc;
 
-  i::String obj = *Utils::OpenHandle(this);
+  i::Tagged<i::String> obj = *Utils::OpenHandle(this);
 
   if (i::IsThinString(obj)) {
     obj = i::ThinString::cast(obj)->actual();
@@ -7576,7 +7586,7 @@ bool v8::String::MakeExternal(
     v8::String::ExternalOneByteStringResource* resource) {
   i::DisallowGarbageCollection no_gc;
 
-  i::String obj = *Utils::OpenHandle(this);
+  i::Tagged<i::String> obj = *Utils::OpenHandle(this);
 
   if (i::IsThinString(obj)) {
     obj = i::ThinString::cast(obj)->actual();
@@ -7606,7 +7616,7 @@ bool v8::String::MakeExternal(
 }
 
 bool v8::String::CanMakeExternal(Encoding encoding) const {
-  i::String obj = *Utils::OpenHandle(this);
+  i::Tagged<i::String> obj = *Utils::OpenHandle(this);
 
   return obj->SupportsExternalization(encoding);
 }
@@ -7779,8 +7789,9 @@ Local<v8::Value> v8::BooleanObject::New(Isolate* v8_isolate, bool value) {
 }
 
 bool v8::BooleanObject::ValueOf() const {
-  i::Object obj = *Utils::OpenHandle(this);
-  i::JSPrimitiveWrapper js_primitive_wrapper = i::JSPrimitiveWrapper::cast(obj);
+  i::Tagged<i::Object> obj = *Utils::OpenHandle(this);
+  i::Tagged<i::JSPrimitiveWrapper> js_primitive_wrapper =
+      i::JSPrimitiveWrapper::cast(obj);
   i::Isolate* i_isolate = js_primitive_wrapper->GetIsolate();
   API_RCS_SCOPE(i_isolate, BooleanObject, BooleanValue);
   return i::IsTrue(js_primitive_wrapper->value(), i_isolate);
@@ -7984,7 +7995,7 @@ Local<v8::Array> v8::Array::New(Isolate* v8_isolate, Local<Value>* elements,
 
 uint32_t v8::Array::Length() const {
   i::Handle<i::JSArray> obj = Utils::OpenHandle(this);
-  i::Object length = obj->length();
+  i::Tagged<i::Object> length = obj->length();
   if (i::IsSmi(length)) {
     return i::Smi::ToInt(length);
   } else {
@@ -8082,8 +8093,9 @@ enum class SetAsArrayKind {
   kValues = i::JS_SET_VALUE_ITERATOR_TYPE
 };
 
-i::Handle<i::JSArray> MapAsArray(i::Isolate* i_isolate, i::Object table_obj,
-                                 int offset, MapAsArrayKind kind) {
+i::Handle<i::JSArray> MapAsArray(i::Isolate* i_isolate,
+                                 i::Tagged<i::Object> table_obj, int offset,
+                                 MapAsArrayKind kind) {
   i::Factory* factory = i_isolate->factory();
   i::Handle<i::OrderedHashMap> table(i::OrderedHashMap::cast(table_obj),
                                      i_isolate);
@@ -8098,10 +8110,10 @@ i::Handle<i::JSArray> MapAsArray(i::Isolate* i_isolate, i::Object table_obj,
   int result_index = 0;
   {
     i::DisallowGarbageCollection no_gc;
-    i::Hole the_hole = i::ReadOnlyRoots(i_isolate).the_hole_value();
+    i::Tagged<i::Hole> the_hole = i::ReadOnlyRoots(i_isolate).the_hole_value();
     for (int i = offset; i < capacity; ++i) {
       i::InternalIndex entry(i);
-      i::Object key = table->KeyAt(entry);
+      i::Tagged<i::Object> key = table->KeyAt(entry);
       if (key == the_hole) continue;
       if (collect_keys) result->set(result_index++, key);
       if (collect_values) result->set(result_index++, table->ValueAt(entry));
@@ -8188,8 +8200,9 @@ Maybe<bool> Set::Delete(Local<Context> context, Local<Value> key) {
 }
 
 namespace {
-i::Handle<i::JSArray> SetAsArray(i::Isolate* i_isolate, i::Object table_obj,
-                                 int offset, SetAsArrayKind kind) {
+i::Handle<i::JSArray> SetAsArray(i::Isolate* i_isolate,
+                                 i::Tagged<i::Object> table_obj, int offset,
+                                 SetAsArrayKind kind) {
   i::Factory* factory = i_isolate->factory();
   i::Handle<i::OrderedHashSet> table(i::OrderedHashSet::cast(table_obj),
                                      i_isolate);
@@ -8202,10 +8215,10 @@ i::Handle<i::JSArray> SetAsArray(i::Isolate* i_isolate, i::Object table_obj,
   int result_index = 0;
   {
     i::DisallowGarbageCollection no_gc;
-    i::Hole the_hole = i::ReadOnlyRoots(i_isolate).the_hole_value();
+    i::Tagged<i::Hole> the_hole = i::ReadOnlyRoots(i_isolate).the_hole_value();
     for (int i = offset; i < capacity; ++i) {
       i::InternalIndex entry(i);
-      i::Object key = table->KeyAt(entry);
+      i::Tagged<i::Object> key = table->KeyAt(entry);
       if (key == the_hole) continue;
       result->set(result_index++, key);
       if (collect_key_values) result->set(result_index++, key);
@@ -8327,7 +8340,7 @@ MaybeLocal<Promise> Promise::Then(Local<Context> context,
 }
 
 bool Promise::HasHandler() const {
-  i::JSReceiver promise = *Utils::OpenHandle(this);
+  i::Tagged<i::JSReceiver> promise = *Utils::OpenHandle(this);
   i::Isolate* i_isolate = promise->GetIsolate();
   API_RCS_SCOPE(i_isolate, Promise, HasRejectHandler);
   ENTER_V8_NO_SCRIPT_NO_EXCEPTION(i_isolate);
@@ -8741,7 +8754,7 @@ size_t v8::ArrayBufferView::ByteOffset() {
 
 size_t v8::ArrayBufferView::ByteLength() {
   i::DisallowGarbageCollection no_gc;
-  i::JSArrayBufferView obj = *Utils::OpenHandle(this);
+  i::Tagged<i::JSArrayBufferView> obj = *Utils::OpenHandle(this);
   if (obj->WasDetached()) {
     return 0;
   }
@@ -8756,7 +8769,7 @@ size_t v8::ArrayBufferView::ByteLength() {
 
 size_t v8::TypedArray::Length() {
   i::DisallowGarbageCollection no_gc;
-  i::JSTypedArray obj = *Utils::OpenHandle(this);
+  i::Tagged<i::JSTypedArray> obj = *Utils::OpenHandle(this);
   return obj->WasDetached() ? 0 : obj->GetLength();
 }
 
@@ -9092,9 +9105,9 @@ void Isolate::ClearKeptObjects() {
 
 v8::Local<v8::Context> Isolate::GetCurrentContext() {
   i::Isolate* i_isolate = reinterpret_cast<i::Isolate*>(this);
-  i::Context context = i_isolate->context();
+  i::Tagged<i::Context> context = i_isolate->context();
   if (context.is_null()) return Local<Context>();
-  i::NativeContext native_context = context->native_context();
+  i::Tagged<i::NativeContext> native_context = context->native_context();
   return Utils::ToLocal(handle(native_context, i_isolate));
 }
 
@@ -9527,7 +9540,7 @@ Isolate::SafeForTerminationScope::~SafeForTerminationScope() {
 
 i::Address* Isolate::GetDataFromSnapshotOnce(size_t index) {
   i::Isolate* i_isolate = reinterpret_cast<i::Isolate*>(this);
-  i::FixedArray list = i_isolate->heap()->serialized_objects();
+  i::Tagged<i::FixedArray> list = i_isolate->heap()->serialized_objects();
   return GetSerializedDataFromFixedArray(i_isolate, list, index);
 }
 
@@ -9990,7 +10003,7 @@ JSEntryStubs Isolate::GetJSEntryStubs() {
        {i::Builtin::kJSRunMicrotasksEntry,
         &entry_stubs.js_run_microtasks_entry_stub}}};
   for (auto& pair : stubs) {
-    i::Code js_entry = i_isolate->builtins()->code(pair.first);
+    i::Tagged<i::Code> js_entry = i_isolate->builtins()->code(pair.first);
     pair.second->code.start =
         reinterpret_cast<const void*>(js_entry->instruction_start());
     pair.second->code.length_in_bytes = js_entry->instruction_size();
@@ -10134,13 +10147,13 @@ void Isolate::RemoveMessageListeners(MessageCallback that) {
   ENTER_V8_NO_SCRIPT_NO_EXCEPTION(i_isolate);
   i::HandleScope scope(i_isolate);
   i::DisallowGarbageCollection no_gc;
-  i::ArrayList listeners = i_isolate->heap()->message_listeners();
+  i::Tagged<i::ArrayList> listeners = i_isolate->heap()->message_listeners();
   for (int i = 0; i < listeners->Length(); i++) {
     if (i::IsUndefined(listeners->Get(i), i_isolate)) {
       continue;  // skip deleted ones
     }
-    i::FixedArray listener = i::FixedArray::cast(listeners->Get(i));
-    i::Foreign callback_obj = i::Foreign::cast(listener->get(0));
+    i::Tagged<i::FixedArray> listener = i::FixedArray::cast(listeners->Get(i));
+    i::Tagged<i::Foreign> callback_obj = i::Foreign::cast(listener->get(0));
     if (callback_obj->foreign_address() == FUNCTION_ADDR(that)) {
       listeners->Set(i, i::ReadOnlyRoots(i_isolate).undefined_value());
     }
@@ -11244,7 +11257,7 @@ void InvokeAccessorGetterCallback(
     Address arg = i_isolate->isolate_data()->api_callback_thunk_argument();
     // Currently we don't call InterceptorInfo callbacks via CallApiGetter.
     DCHECK(IsAccessorInfo(Object(arg)));
-    AccessorInfo accessor_info = AccessorInfo::cast(Object(arg));
+    Tagged<AccessorInfo> accessor_info = AccessorInfo::cast(Object(arg));
     getter = reinterpret_cast<v8::AccessorNameGetterCallback>(
         accessor_info->getter(i_isolate));
 
@@ -11278,8 +11291,9 @@ inline void InvokeFunctionCallback(
         StackFrameIterator it(i_isolate);
         CHECK(it.frame()->is_api_callback_exit());
         ApiCallbackExitFrame* frame = ApiCallbackExitFrame::cast(it.frame());
-        FunctionTemplateInfo fti = FunctionTemplateInfo::cast(frame->target());
-        CallHandlerInfo call_handler_info =
+        Tagged<FunctionTemplateInfo> fti =
+            FunctionTemplateInfo::cast(frame->target());
+        Tagged<CallHandlerInfo> call_handler_info =
             CallHandlerInfo::cast(fti->call_code(kAcquireLoad));
         if (!i_isolate->debug()->PerformSideEffectCheckForCallback(
                 handle(call_handler_info, i_isolate))) {
diff --git a/src/api/api.h b/src/api/api.h
index f4481584c29..8b67c3e045d 100644
--- a/src/api/api.h
+++ b/src/api/api.h
@@ -56,10 +56,11 @@ class Consts {
 };
 
 template <typename T>
-inline T ToCData(v8::internal::Object obj);
+inline T ToCData(v8::internal::Tagged<v8::internal::Object> obj);
 
 template <>
-inline v8::internal::Address ToCData(v8::internal::Object obj);
+inline v8::internal::Address ToCData(
+    v8::internal::Tagged<v8::internal::Object> obj);
 
 template <typename T>
 inline v8::internal::Handle<v8::internal::Object> FromCData(
@@ -341,20 +342,20 @@ class HandleScopeImplementer {
   inline internal::Address* GetSpareOrNewBlock();
   inline void DeleteExtensions(internal::Address* prev_limit);
 
-  inline void EnterContext(NativeContext context);
+  inline void EnterContext(Tagged<NativeContext> context);
   inline void LeaveContext();
-  inline bool LastEnteredContextWas(NativeContext context);
+  inline bool LastEnteredContextWas(Tagged<NativeContext> context);
   inline size_t EnteredContextCount() const { return entered_contexts_.size(); }
 
-  inline void EnterMicrotaskContext(NativeContext context);
+  inline void EnterMicrotaskContext(Tagged<NativeContext> context);
 
   // Returns the last entered context or an empty handle if no
   // contexts have been entered.
   inline Handle<NativeContext> LastEnteredContext();
   inline Handle<NativeContext> LastEnteredOrMicrotaskContext();
 
-  inline void SaveContext(Context context);
-  inline Context RestoreContext();
+  inline void SaveContext(Tagged<Context> context);
+  inline Tagged<Context> RestoreContext();
   inline bool HasSavedContexts();
 
   inline DetachableVector<Address*>* blocks() { return &blocks_; }
@@ -427,12 +428,12 @@ class HandleScopeImplementer {
 
 const int kHandleBlockSize = v8::internal::KB - 2;  // fit in one page
 
-void HandleScopeImplementer::SaveContext(Context context) {
+void HandleScopeImplementer::SaveContext(Tagged<Context> context) {
   saved_contexts_.push_back(context);
 }
 
-Context HandleScopeImplementer::RestoreContext() {
-  Context last_context = saved_contexts_.back();
+Tagged<Context> HandleScopeImplementer::RestoreContext() {
+  Tagged<Context> last_context = saved_contexts_.back();
   saved_contexts_.pop_back();
   return last_context;
 }
@@ -449,7 +450,8 @@ void HandleScopeImplementer::LeaveContext() {
   is_microtask_context_.pop_back();
 }
 
-bool HandleScopeImplementer::LastEnteredContextWas(NativeContext context) {
+bool HandleScopeImplementer::LastEnteredContextWas(
+    Tagged<NativeContext> context) {
   return !entered_contexts_.empty() && entered_contexts_.back() == context;
 }
 
diff --git a/src/ast/ast-value-factory.cc b/src/ast/ast-value-factory.cc
index 30a3ddb325a..239f0a6e8d3 100644
--- a/src/ast/ast-value-factory.cc
+++ b/src/ast/ast-value-factory.cc
@@ -339,7 +339,8 @@ const AstRawString* AstValueFactory::GetTwoByteStringInternal(
 }
 
 const AstRawString* AstValueFactory::GetString(
-    String literal, const SharedStringAccessGuardIfNeeded& access_guard) {
+    Tagged<String> literal,
+    const SharedStringAccessGuardIfNeeded& access_guard) {
   const AstRawString* result = nullptr;
   DisallowGarbageCollection no_gc;
   String::FlatContent content = literal->GetFlatContent(no_gc, access_guard);
diff --git a/src/ast/ast-value-factory.h b/src/ast/ast-value-factory.h
index bea71bd670e..b7bdc33b160 100644
--- a/src/ast/ast-value-factory.h
+++ b/src/ast/ast-value-factory.h
@@ -361,7 +361,7 @@ class AstValueFactory {
   const AstRawString* GetTwoByteString(base::Vector<const uint16_t> literal) {
     return GetTwoByteStringInternal(literal);
   }
-  const AstRawString* GetString(String literal,
+  const AstRawString* GetString(Tagged<String> literal,
                                 const SharedStringAccessGuardIfNeeded&);
 
   V8_EXPORT_PRIVATE AstConsString* NewConsString();
diff --git a/src/ast/ast.cc b/src/ast/ast.cc
index 8a74081796a..25a9bd3f82e 100644
--- a/src/ast/ast.cc
+++ b/src/ast/ast.cc
@@ -699,7 +699,7 @@ void ArrayLiteralBoilerplateBuilder::BuildBoilerplateDescription(
       // New handle scope here, needs to be after BuildContants().
       typename IsolateT::HandleScopeType scope(isolate);
 
-      Object boilerplate_value = *GetBoilerplateValue(element, isolate);
+      Tagged<Object> boilerplate_value = *GetBoilerplateValue(element, isolate);
       // We shouldn't allocate after creating the boilerplate value.
       DisallowGarbageCollection no_gc;
 
@@ -804,7 +804,7 @@ Handle<TemplateObjectDescription> GetTemplateObject::GetOrBuildDescription(
   bool raw_and_cooked_match = true;
   {
     DisallowGarbageCollection no_gc;
-    FixedArray raw_strings = *raw_strings_handle;
+    Tagged<FixedArray> raw_strings = *raw_strings_handle;
 
     for (int i = 0; i < raw_strings->length(); ++i) {
       if (this->raw_strings()->at(i) != this->cooked_strings()->at(i)) {
@@ -825,7 +825,7 @@ Handle<TemplateObjectDescription> GetTemplateObject::GetOrBuildDescription(
     cooked_strings_handle = isolate->factory()->NewFixedArray(
         this->cooked_strings()->length(), AllocationType::kOld);
     DisallowGarbageCollection no_gc;
-    FixedArray cooked_strings = *cooked_strings_handle;
+    Tagged<FixedArray> cooked_strings = *cooked_strings_handle;
     ReadOnlyRoots roots(isolate);
     for (int i = 0; i < cooked_strings->length(); ++i) {
       if (this->cooked_strings()->at(i) != nullptr) {
@@ -853,7 +853,7 @@ static bool IsCommutativeOperationWithSmiLiteral(Token::Value op) {
 
 // Check for the pattern: x + 1.
 static bool MatchSmiLiteralOperation(Expression* left, Expression* right,
-                                     Expression** expr, Smi* literal) {
+                                     Expression** expr, Tagged<Smi>* literal) {
   if (right->IsSmiLiteral()) {
     *expr = left;
     *literal = right->AsLiteral()->AsSmiLiteral();
@@ -863,7 +863,7 @@ static bool MatchSmiLiteralOperation(Expression* left, Expression* right,
 }
 
 bool BinaryOperation::IsSmiLiteralOperation(Expression** subexpr,
-                                            Smi* literal) {
+                                            Tagged<Smi>* literal) {
   return MatchSmiLiteralOperation(left_, right_, subexpr, literal) ||
          (IsCommutativeOperationWithSmiLiteral(op()) &&
           MatchSmiLiteralOperation(right_, left_, subexpr, literal));
diff --git a/src/ast/ast.h b/src/ast/ast.h
index 49594de1a16..cf9d52eeed6 100644
--- a/src/ast/ast.h
+++ b/src/ast/ast.h
@@ -953,7 +953,7 @@ class Literal final : public Expression {
     return string_;
   }
 
-  Smi AsSmiLiteral() const {
+  Tagged<Smi> AsSmiLiteral() const {
     DCHECK_EQ(kSmi, type());
     return Smi::FromInt(smi_);
   }
@@ -1865,7 +1865,7 @@ class BinaryOperation final : public Expression {
 
   // Returns true if one side is a Smi literal, returning the other side's
   // sub-expression in |subexpr| and the literal Smi in |literal|.
-  bool IsSmiLiteralOperation(Expression** subexpr, Smi* literal);
+  bool IsSmiLiteralOperation(Expression** subexpr, Tagged<Smi>* literal);
 
  private:
   friend class AstNodeFactory;
diff --git a/src/ast/scopes.cc b/src/ast/scopes.cc
index f57874e3816..003bd0f2736 100644
--- a/src/ast/scopes.cc
+++ b/src/ast/scopes.cc
@@ -412,7 +412,7 @@ bool Scope::ContainsAsmModule() const {
 
 template <typename IsolateT>
 Scope* Scope::DeserializeScopeChain(IsolateT* isolate, Zone* zone,
-                                    ScopeInfo scope_info,
+                                    Tagged<ScopeInfo> scope_info,
                                     DeclarationScope* script_scope,
                                     AstValueFactory* ast_value_factory,
                                     DeserializationMode deserialization_mode) {
@@ -502,7 +502,7 @@ Scope* Scope::DeserializeScopeChain(IsolateT* isolate, Zone* zone,
     current_scope = outer_scope;
     if (innermost_scope == nullptr) innermost_scope = current_scope;
     scope_info = scope_info->HasOuterScopeInfo() ? scope_info->OuterScopeInfo()
-                                                 : ScopeInfo();
+                                                 : Tagged<ScopeInfo>();
   }
 
   if (deserialization_mode == DeserializationMode::kIncludingVariables) {
@@ -534,12 +534,12 @@ template EXPORT_TEMPLATE_DEFINE(
 
 template EXPORT_TEMPLATE_DEFINE(V8_EXPORT_PRIVATE)
     Scope* Scope::DeserializeScopeChain(
-        Isolate* isolate, Zone* zone, ScopeInfo scope_info,
+        Isolate* isolate, Zone* zone, Tagged<ScopeInfo> scope_info,
         DeclarationScope* script_scope, AstValueFactory* ast_value_factory,
         DeserializationMode deserialization_mode);
 template EXPORT_TEMPLATE_DEFINE(V8_EXPORT_PRIVATE)
     Scope* Scope::DeserializeScopeChain(
-        LocalIsolate* isolate, Zone* zone, ScopeInfo scope_info,
+        LocalIsolate* isolate, Zone* zone, Tagged<ScopeInfo> scope_info,
         DeclarationScope* script_scope, AstValueFactory* ast_value_factory,
         DeserializationMode deserialization_mode);
 
diff --git a/src/ast/scopes.h b/src/ast/scopes.h
index 90e1c9af954..b4c2e8b2136 100644
--- a/src/ast/scopes.h
+++ b/src/ast/scopes.h
@@ -149,7 +149,7 @@ class V8_EXPORT_PRIVATE Scope : public NON_EXPORTED_BASE(ZoneObject) {
   template <typename IsolateT>
   EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE)
   static Scope* DeserializeScopeChain(IsolateT* isolate, Zone* zone,
-                                      ScopeInfo scope_info,
+                                      Tagged<ScopeInfo> scope_info,
                                       DeclarationScope* script_scope,
                                       AstValueFactory* ast_value_factory,
                                       DeserializationMode deserialization_mode);
diff --git a/src/base/macros.h b/src/base/macros.h
index 25a533c5a4a..f812583633b 100644
--- a/src/base/macros.h
+++ b/src/base/macros.h
@@ -387,6 +387,7 @@ bool is_inbounds(float_t v) {
 #ifdef V8_OS_WIN
 
 // Setup for Windows shared library export.
+#define V8_EXPORT_ENUM
 #ifdef BUILDING_V8_SHARED
 #define V8_EXPORT_PRIVATE __declspec(dllexport)
 #elif USING_V8_SHARED
@@ -401,11 +402,14 @@ bool is_inbounds(float_t v) {
 #if V8_HAS_ATTRIBUTE_VISIBILITY
 #ifdef BUILDING_V8_SHARED
 #define V8_EXPORT_PRIVATE __attribute__((visibility("default")))
+#define V8_EXPORT_ENUM V8_EXPORT_PRIVATE
 #else
 #define V8_EXPORT_PRIVATE
+#define V8_EXPORT_ENUM
 #endif
 #else
 #define V8_EXPORT_PRIVATE
+#define V8_EXPORT_ENUM
 #endif
 
 #endif  // V8_OS_WIN
diff --git a/src/baseline/arm/baseline-assembler-arm-inl.h b/src/baseline/arm/baseline-assembler-arm-inl.h
index 41bafd975ed..e5fb71dd20c 100644
--- a/src/baseline/arm/baseline-assembler-arm-inl.h
+++ b/src/baseline/arm/baseline-assembler-arm-inl.h
@@ -150,7 +150,7 @@ void BaselineAssembler::JumpIfPointer(Condition cc, Register value,
   __ ldr(tmp, operand);
   JumpIf(cc, value, Operand(tmp), target);
 }
-void BaselineAssembler::JumpIfSmi(Condition cc, Register value, Smi smi,
+void BaselineAssembler::JumpIfSmi(Condition cc, Register value, Tagged<Smi> smi,
                                   Label* target, Label::Distance) {
   __ AssertSmi(value);
   JumpIf(cc, value, Operand(smi), target);
@@ -185,7 +185,7 @@ void BaselineAssembler::JumpIfByte(Condition cc, Register value, int32_t byte,
 void BaselineAssembler::Move(interpreter::Register output, Register source) {
   Move(RegisterFrameOperand(output), source);
 }
-void BaselineAssembler::Move(Register output, TaggedIndex value) {
+void BaselineAssembler::Move(Register output, Tagged<TaggedIndex> value) {
   __ mov(output, Operand(value.ptr()));
 }
 void BaselineAssembler::Move(MemOperand output, Register source) {
@@ -345,7 +345,7 @@ void BaselineAssembler::LoadWord8Field(Register output, Register source,
 }
 
 void BaselineAssembler::StoreTaggedSignedField(Register target, int offset,
-                                               Smi value) {
+                                               Tagged<Smi> value) {
   ASM_CODE_COMMENT(masm_);
   ScratchRegisterScope temps(this);
   Register tmp = temps.AcquireScratch();
@@ -488,7 +488,7 @@ void BaselineAssembler::StaModuleVariable(Register context, Register value,
   StoreTaggedFieldWithWriteBarrier(context, Cell::kValueOffset, value);
 }
 
-void BaselineAssembler::AddSmi(Register lhs, Smi rhs) {
+void BaselineAssembler::AddSmi(Register lhs, Tagged<Smi> rhs) {
   __ add(lhs, lhs, Operand(rhs));
 }
 
diff --git a/src/baseline/arm64/baseline-assembler-arm64-inl.h b/src/baseline/arm64/baseline-assembler-arm64-inl.h
index 0d2f15c0271..77d29bb8981 100644
--- a/src/baseline/arm64/baseline-assembler-arm64-inl.h
+++ b/src/baseline/arm64/baseline-assembler-arm64-inl.h
@@ -167,7 +167,7 @@ void BaselineAssembler::JumpIfPointer(Condition cc, Register value,
   __ Ldr(tmp, operand);
   JumpIf(cc, value, tmp, target);
 }
-void BaselineAssembler::JumpIfSmi(Condition cc, Register value, Smi smi,
+void BaselineAssembler::JumpIfSmi(Condition cc, Register value, Tagged<Smi> smi,
                                   Label* target, Label::Distance distance) {
   __ AssertSmi(value);
   __ CompareTaggedAndBranch(value, smi, cc, target);
@@ -203,7 +203,7 @@ void BaselineAssembler::JumpIfByte(Condition cc, Register value, int32_t byte,
 void BaselineAssembler::Move(interpreter::Register output, Register source) {
   Move(RegisterFrameOperand(output), source);
 }
-void BaselineAssembler::Move(Register output, TaggedIndex value) {
+void BaselineAssembler::Move(Register output, Tagged<TaggedIndex> value) {
   __ Mov(output, Immediate(value.ptr()));
 }
 void BaselineAssembler::Move(MemOperand output, Register source) {
@@ -420,7 +420,7 @@ void BaselineAssembler::LoadWord8Field(Register output, Register source,
 }
 
 void BaselineAssembler::StoreTaggedSignedField(Register target, int offset,
-                                               Smi value) {
+                                               Tagged<Smi> value) {
   ASM_CODE_COMMENT(masm_);
   ScratchRegisterScope temps(this);
   Register tmp = temps.AcquireScratch();
@@ -546,7 +546,7 @@ void BaselineAssembler::StaModuleVariable(Register context, Register value,
   StoreTaggedFieldWithWriteBarrier(context, Cell::kValueOffset, value);
 }
 
-void BaselineAssembler::AddSmi(Register lhs, Smi rhs) {
+void BaselineAssembler::AddSmi(Register lhs, Tagged<Smi> rhs) {
   if (SmiValuesAre31Bits()) {
     __ Add(lhs.W(), lhs.W(), Immediate(rhs));
   } else {
diff --git a/src/baseline/baseline-assembler-inl.h b/src/baseline/baseline-assembler-inl.h
index 17b7353d2c6..9fba6a927bd 100644
--- a/src/baseline/baseline-assembler-inl.h
+++ b/src/baseline/baseline-assembler-inl.h
@@ -103,7 +103,7 @@ void BaselineAssembler::Move(Register output, Register source) {
 void BaselineAssembler::Move(Register output, MemOperand operand) {
   __ Move(output, operand);
 }
-void BaselineAssembler::Move(Register output, Smi value) {
+void BaselineAssembler::Move(Register output, Tagged<Smi> value) {
   __ Move(output, value);
 }
 
diff --git a/src/baseline/baseline-assembler.h b/src/baseline/baseline-assembler.h
index 7c8f028f083..41f3c22df39 100644
--- a/src/baseline/baseline-assembler.h
+++ b/src/baseline/baseline-assembler.h
@@ -83,8 +83,8 @@ class BaselineAssembler {
                             Label* target,
                             Label::Distance distance = Label::kFar);
   inline Condition CheckSmi(Register value);
-  inline void JumpIfSmi(Condition cc, Register value, Smi smi, Label* target,
-                        Label::Distance distance = Label::kFar);
+  inline void JumpIfSmi(Condition cc, Register value, Tagged<Smi> smi,
+                        Label* target, Label::Distance distance = Label::kFar);
   inline void JumpIfSmi(Condition cc, Register lhs, Register rhs, Label* target,
                         Label::Distance distance = Label::kFar);
   inline void JumpIfImmediate(Condition cc, Register left, int right,
@@ -105,8 +105,8 @@ class BaselineAssembler {
 
   inline void Move(Register output, Register source);
   inline void Move(Register output, MemOperand operand);
-  inline void Move(Register output, Smi value);
-  inline void Move(Register output, TaggedIndex value);
+  inline void Move(Register output, Tagged<Smi> value);
+  inline void Move(Register output, Tagged<TaggedIndex> value);
   inline void Move(Register output, interpreter::Register source);
   inline void Move(interpreter::Register output, Register source);
   inline void Move(Register output, RootIndex source);
@@ -166,7 +166,8 @@ class BaselineAssembler {
   inline void LoadWord16FieldZeroExtend(Register output, Register source,
                                         int offset);
   inline void LoadWord8Field(Register output, Register source, int offset);
-  inline void StoreTaggedSignedField(Register target, int offset, Smi value);
+  inline void StoreTaggedSignedField(Register target, int offset,
+                                     Tagged<Smi> value);
   inline void StoreTaggedFieldWithWriteBarrier(Register target, int offset,
                                                Register value);
   inline void StoreTaggedFieldNoWriteBarrier(Register target, int offset,
@@ -213,7 +214,7 @@ class BaselineAssembler {
   inline void StaModuleVariable(Register context, Register value,
                                 int cell_index, uint32_t depth);
 
-  inline void AddSmi(Register lhs, Smi rhs);
+  inline void AddSmi(Register lhs, Tagged<Smi> rhs);
   inline void SmiUntag(Register value);
   inline void SmiUntag(Register output, Register value);
 
diff --git a/src/baseline/baseline-batch-compiler.cc b/src/baseline/baseline-batch-compiler.cc
index ef8cbe5bf6c..db634068dd3 100644
--- a/src/baseline/baseline-batch-compiler.cc
+++ b/src/baseline/baseline-batch-compiler.cc
@@ -27,7 +27,7 @@ namespace v8 {
 namespace internal {
 namespace baseline {
 
-static bool CanCompileWithConcurrentBaseline(SharedFunctionInfo shared,
+static bool CanCompileWithConcurrentBaseline(Tagged<SharedFunctionInfo> shared,
                                              Isolate* isolate) {
   return !shared->HasBaselineCode() && CanCompileWithBaseline(isolate, shared);
 }
@@ -35,7 +35,7 @@ static bool CanCompileWithConcurrentBaseline(SharedFunctionInfo shared,
 class BaselineCompilerTask {
  public:
   BaselineCompilerTask(Isolate* isolate, PersistentHandles* handles,
-                       SharedFunctionInfo sfi)
+                       Tagged<SharedFunctionInfo> sfi)
       : shared_function_info_(handles->NewHandle(sfi)),
         bytecode_(handles->NewHandle(sfi->GetBytecodeArray(isolate))) {
     DCHECK(sfi->is_compiled());
@@ -107,11 +107,11 @@ class BaselineBatchCompilerJob {
       MaybeObject maybe_sfi = task_queue->Get(i);
       // TODO(victorgomes): Do I need to clear the value?
       task_queue->Set(i, HeapObjectReference::ClearedValue(isolate));
-      HeapObject obj;
+      Tagged<HeapObject> obj;
       // Skip functions where weak reference is no longer valid.
       if (!maybe_sfi.GetHeapObjectIfWeak(&obj)) continue;
       // Skip functions where the bytecode has been flushed.
-      SharedFunctionInfo shared = SharedFunctionInfo::cast(obj);
+      Tagged<SharedFunctionInfo> shared = SharedFunctionInfo::cast(obj);
       if (!CanCompileWithConcurrentBaseline(shared, isolate)) continue;
       // Skip functions that are already being compiled.
       if (shared->is_sparkplug_compiling()) continue;
@@ -272,7 +272,7 @@ void BaselineBatchCompiler::EnqueueFunction(Handle<JSFunction> function) {
   }
 }
 
-void BaselineBatchCompiler::EnqueueSFI(SharedFunctionInfo shared) {
+void BaselineBatchCompiler::EnqueueSFI(Tagged<SharedFunctionInfo> shared) {
   if (!v8_flags.concurrent_sparkplug || !is_enabled()) return;
   if (ShouldCompileBatch(shared)) {
     CompileBatchConcurrent(shared);
@@ -322,13 +322,15 @@ void BaselineBatchCompiler::CompileBatch(Handle<JSFunction> function) {
   ClearBatch();
 }
 
-void BaselineBatchCompiler::CompileBatchConcurrent(SharedFunctionInfo shared) {
+void BaselineBatchCompiler::CompileBatchConcurrent(
+    Tagged<SharedFunctionInfo> shared) {
   Enqueue(Handle<SharedFunctionInfo>(shared, isolate_));
   concurrent_compiler_->CompileBatch(compilation_queue_, last_index_);
   ClearBatch();
 }
 
-bool BaselineBatchCompiler::ShouldCompileBatch(SharedFunctionInfo shared) {
+bool BaselineBatchCompiler::ShouldCompileBatch(
+    Tagged<SharedFunctionInfo> shared) {
   // Early return if the function is compiled with baseline already or it is not
   // suitable for baseline compilation.
   if (shared->HasBaselineCode()) return false;
@@ -367,7 +369,7 @@ bool BaselineBatchCompiler::ShouldCompileBatch(SharedFunctionInfo shared) {
 }
 
 bool BaselineBatchCompiler::MaybeCompileFunction(MaybeObject maybe_sfi) {
-  HeapObject heapobj;
+  Tagged<HeapObject> heapobj;
   // Skip functions where the weak reference is no longer valid.
   if (!maybe_sfi.GetHeapObjectIfWeak(&heapobj)) return false;
   Handle<SharedFunctionInfo> shared =
diff --git a/src/baseline/baseline-batch-compiler.h b/src/baseline/baseline-batch-compiler.h
index a7915c3128f..9bb30b32c72 100644
--- a/src/baseline/baseline-batch-compiler.h
+++ b/src/baseline/baseline-batch-compiler.h
@@ -25,7 +25,7 @@ class BaselineBatchCompiler {
   ~BaselineBatchCompiler();
   // Enqueues SharedFunctionInfo of |function| for compilation.
   void EnqueueFunction(Handle<JSFunction> function);
-  void EnqueueSFI(SharedFunctionInfo shared);
+  void EnqueueSFI(Tagged<SharedFunctionInfo> shared);
 
   void set_enabled(bool enabled) { enabled_ = enabled; }
   bool is_enabled() { return enabled_; }
@@ -42,13 +42,13 @@ class BaselineBatchCompiler {
 
   // Returns true if the current batch exceeds the threshold and should be
   // compiled.
-  bool ShouldCompileBatch(SharedFunctionInfo shared);
+  bool ShouldCompileBatch(Tagged<SharedFunctionInfo> shared);
 
   // Compiles the current batch.
   void CompileBatch(Handle<JSFunction> function);
 
   // Compiles the current batch concurrently.
-  void CompileBatchConcurrent(SharedFunctionInfo shared);
+  void CompileBatchConcurrent(Tagged<SharedFunctionInfo> shared);
 
   // Resets the current batch.
   void ClearBatch();
diff --git a/src/baseline/baseline-compiler.cc b/src/baseline/baseline-compiler.cc
index b871b258585..1252af976e0 100644
--- a/src/baseline/baseline-compiler.cc
+++ b/src/baseline/baseline-compiler.cc
@@ -79,7 +79,7 @@ namespace detail {
 bool Clobbers(Register target, Register reg) { return target == reg; }
 bool Clobbers(Register target, Handle<Object> handle) { return false; }
 bool Clobbers(Register target, Smi smi) { return false; }
-bool Clobbers(Register target, TaggedIndex index) { return false; }
+bool Clobbers(Register target, Tagged<TaggedIndex> index) { return false; }
 bool Clobbers(Register target, int32_t imm) { return false; }
 bool Clobbers(Register target, RootIndex index) { return false; }
 bool Clobbers(Register target, interpreter::Register reg) { return false; }
@@ -95,9 +95,9 @@ bool MachineTypeMatches(MachineType type, Handle<HeapObject> handle) {
 bool MachineTypeMatches(MachineType type, Smi handle) {
   return type.IsTagged() && !type.IsTaggedPointer();
 }
-bool MachineTypeMatches(MachineType type, TaggedIndex handle) {
-  // TaggedIndex doesn't have a separate type, so check for the same type as for
-  // Smis.
+bool MachineTypeMatches(MachineType type, Tagged<TaggedIndex> handle) {
+  // Tagged<TaggedIndex> doesn't have a separate type, so check for the same
+  // type as for Smis.
   return type.IsTagged() && !type.IsTaggedPointer();
 }
 bool MachineTypeMatches(MachineType type, int32_t imm) {
@@ -356,7 +356,7 @@ MaybeHandle<Code> BaselineCompiler::Build(LocalIsolate* local_isolate) {
   return code_builder.TryBuild();
 }
 
-int BaselineCompiler::EstimateInstructionSize(BytecodeArray bytecode) {
+int BaselineCompiler::EstimateInstructionSize(Tagged<BytecodeArray> bytecode) {
   return bytecode->length() * kAverageBytecodeToInstructionRatio;
 }
 
@@ -390,7 +390,7 @@ Handle<Type> BaselineCompiler::Constant(int operand_index) {
   return Handle<Type>::cast(
       iterator().GetConstantForIndexOperand(operand_index, local_isolate_));
 }
-Smi BaselineCompiler::ConstantSmi(int operand_index) {
+Tagged<Smi> BaselineCompiler::ConstantSmi(int operand_index) {
   return iterator().GetConstantAtIndexAsSmi(operand_index);
 }
 template <typename Type>
@@ -415,22 +415,22 @@ uint32_t BaselineCompiler::Flag16(int operand_index) {
 uint32_t BaselineCompiler::RegisterCount(int operand_index) {
   return iterator().GetRegisterCountOperand(operand_index);
 }
-TaggedIndex BaselineCompiler::IndexAsTagged(int operand_index) {
+Tagged<TaggedIndex> BaselineCompiler::IndexAsTagged(int operand_index) {
   return TaggedIndex::FromIntptr(Index(operand_index));
 }
-TaggedIndex BaselineCompiler::UintAsTagged(int operand_index) {
+Tagged<TaggedIndex> BaselineCompiler::UintAsTagged(int operand_index) {
   return TaggedIndex::FromIntptr(Uint(operand_index));
 }
-Smi BaselineCompiler::IndexAsSmi(int operand_index) {
+Tagged<Smi> BaselineCompiler::IndexAsSmi(int operand_index) {
   return Smi::FromInt(Index(operand_index));
 }
-Smi BaselineCompiler::IntAsSmi(int operand_index) {
+Tagged<Smi> BaselineCompiler::IntAsSmi(int operand_index) {
   return Smi::FromInt(Int(operand_index));
 }
-Smi BaselineCompiler::Flag8AsSmi(int operand_index) {
+Tagged<Smi> BaselineCompiler::Flag8AsSmi(int operand_index) {
   return Smi::FromInt(Flag8(operand_index));
 }
-Smi BaselineCompiler::Flag16AsSmi(int operand_index) {
+Tagged<Smi> BaselineCompiler::Flag16AsSmi(int operand_index) {
   return Smi::FromInt(Flag16(operand_index));
 }
 
diff --git a/src/baseline/baseline-compiler.h b/src/baseline/baseline-compiler.h
index f26ee51f5f3..f5d2c89bf13 100644
--- a/src/baseline/baseline-compiler.h
+++ b/src/baseline/baseline-compiler.h
@@ -59,7 +59,7 @@ class BaselineCompiler {
 
   void GenerateCode();
   MaybeHandle<Code> Build(LocalIsolate* local_isolate);
-  static int EstimateInstructionSize(BytecodeArray bytecode);
+  static int EstimateInstructionSize(Tagged<BytecodeArray> bytecode);
 
  private:
   void Prologue();
@@ -81,7 +81,7 @@ class BaselineCompiler {
   // Constant pool operands.
   template <typename Type>
   Handle<Type> Constant(int operand_index);
-  Smi ConstantSmi(int operand_index);
+  Tagged<Smi> ConstantSmi(int operand_index);
   template <typename Type>
   void LoadConstant(Register output, int operand_index);
 
@@ -92,12 +92,12 @@ class BaselineCompiler {
   uint32_t Flag8(int operand_index);
   uint32_t Flag16(int operand_index);
   uint32_t RegisterCount(int operand_index);
-  TaggedIndex IndexAsTagged(int operand_index);
-  TaggedIndex UintAsTagged(int operand_index);
-  Smi IndexAsSmi(int operand_index);
-  Smi IntAsSmi(int operand_index);
-  Smi Flag8AsSmi(int operand_index);
-  Smi Flag16AsSmi(int operand_index);
+  Tagged<TaggedIndex> IndexAsTagged(int operand_index);
+  Tagged<TaggedIndex> UintAsTagged(int operand_index);
+  Tagged<Smi> IndexAsSmi(int operand_index);
+  Tagged<Smi> IntAsSmi(int operand_index);
+  Tagged<Smi> Flag8AsSmi(int operand_index);
+  Tagged<Smi> Flag16AsSmi(int operand_index);
 
   // Jump helpers.
   Label* NewLabel();
diff --git a/src/baseline/baseline.cc b/src/baseline/baseline.cc
index 71764a09cde..23f53faf1ee 100644
--- a/src/baseline/baseline.cc
+++ b/src/baseline/baseline.cc
@@ -22,7 +22,8 @@
 namespace v8 {
 namespace internal {
 
-bool CanCompileWithBaseline(Isolate* isolate, SharedFunctionInfo shared) {
+bool CanCompileWithBaseline(Isolate* isolate,
+                            Tagged<SharedFunctionInfo> shared) {
   DisallowGarbageCollection no_gc;
 
   // Check that baseline compiler is enabled.
diff --git a/src/baseline/baseline.h b/src/baseline/baseline.h
index 10a6e25e4fb..ab85136d771 100644
--- a/src/baseline/baseline.h
+++ b/src/baseline/baseline.h
@@ -14,7 +14,8 @@ class Code;
 class SharedFunctionInfo;
 class MacroAssembler;
 
-bool CanCompileWithBaseline(Isolate* isolate, SharedFunctionInfo shared);
+bool CanCompileWithBaseline(Isolate* isolate,
+                            Tagged<SharedFunctionInfo> shared);
 
 MaybeHandle<Code> GenerateBaselineCode(Isolate* isolate,
                                        Handle<SharedFunctionInfo> shared);
diff --git a/src/baseline/bytecode-offset-iterator.cc b/src/baseline/bytecode-offset-iterator.cc
index 923fc5b743e..d60559e4a77 100644
--- a/src/baseline/bytecode-offset-iterator.cc
+++ b/src/baseline/bytecode-offset-iterator.cc
@@ -26,8 +26,8 @@ BytecodeOffsetIterator::BytecodeOffsetIterator(Handle<ByteArray> mapping_table,
   Initialize();
 }
 
-BytecodeOffsetIterator::BytecodeOffsetIterator(ByteArray mapping_table,
-                                               BytecodeArray bytecodes)
+BytecodeOffsetIterator::BytecodeOffsetIterator(Tagged<ByteArray> mapping_table,
+                                               Tagged<BytecodeArray> bytecodes)
     : data_start_address_(mapping_table->GetDataStartAddress()),
       data_length_(mapping_table->length()),
       current_index_(0),
diff --git a/src/baseline/bytecode-offset-iterator.h b/src/baseline/bytecode-offset-iterator.h
index 249cd193339..6f30cd9a72d 100644
--- a/src/baseline/bytecode-offset-iterator.h
+++ b/src/baseline/bytecode-offset-iterator.h
@@ -22,8 +22,8 @@ class V8_EXPORT_PRIVATE BytecodeOffsetIterator {
   explicit BytecodeOffsetIterator(Handle<ByteArray> mapping_table,
                                   Handle<BytecodeArray> bytecodes);
   // Non-handlified version for use when no GC can happen.
-  explicit BytecodeOffsetIterator(ByteArray mapping_table,
-                                  BytecodeArray bytecodes);
+  explicit BytecodeOffsetIterator(Tagged<ByteArray> mapping_table,
+                                  Tagged<BytecodeArray> bytecodes);
   ~BytecodeOffsetIterator();
 
   inline void Advance() {
diff --git a/src/baseline/ia32/baseline-assembler-ia32-inl.h b/src/baseline/ia32/baseline-assembler-ia32-inl.h
index f96f2e00a56..d474366ae5a 100644
--- a/src/baseline/ia32/baseline-assembler-ia32-inl.h
+++ b/src/baseline/ia32/baseline-assembler-ia32-inl.h
@@ -158,7 +158,7 @@ void BaselineAssembler::JumpIfPointer(Condition cc, Register value,
                                       Label::Distance distance) {
   JumpIf(cc, value, operand, target, distance);
 }
-void BaselineAssembler::JumpIfSmi(Condition cc, Register value, Smi smi,
+void BaselineAssembler::JumpIfSmi(Condition cc, Register value, Tagged<Smi> smi,
                                   Label* target, Label::Distance distance) {
   if (smi.value() == 0) {
     __ test(value, value);
@@ -194,7 +194,7 @@ void BaselineAssembler::JumpIfByte(Condition cc, Register value, int32_t byte,
 void BaselineAssembler::Move(interpreter::Register output, Register source) {
   return __ mov(RegisterFrameOperand(output), source);
 }
-void BaselineAssembler::Move(Register output, TaggedIndex value) {
+void BaselineAssembler::Move(Register output, Tagged<TaggedIndex> value) {
   __ Move(output, Immediate(value.ptr()));
 }
 void BaselineAssembler::Move(MemOperand output, Register source) {
@@ -221,10 +221,12 @@ inline void PushSingle(MacroAssembler* masm, RootIndex source) {
   masm->PushRoot(source);
 }
 inline void PushSingle(MacroAssembler* masm, Register reg) { masm->Push(reg); }
-inline void PushSingle(MacroAssembler* masm, TaggedIndex value) {
+inline void PushSingle(MacroAssembler* masm, Tagged<TaggedIndex> value) {
   masm->Push(Immediate(value.ptr()));
 }
-inline void PushSingle(MacroAssembler* masm, Smi value) { masm->Push(value); }
+inline void PushSingle(MacroAssembler* masm, Tagged<Smi> value) {
+  masm->Push(value);
+}
 inline void PushSingle(MacroAssembler* masm, Handle<HeapObject> object) {
   masm->Push(object);
 }
@@ -331,7 +333,7 @@ void BaselineAssembler::LoadWord8Field(Register output, Register source,
 }
 
 void BaselineAssembler::StoreTaggedSignedField(Register target, int offset,
-                                               Smi value) {
+                                               Tagged<Smi> value) {
   __ mov(FieldOperand(target, offset), Immediate(value));
 }
 
@@ -458,7 +460,7 @@ void BaselineAssembler::StaModuleVariable(Register context, Register value,
   StoreTaggedFieldWithWriteBarrier(context, Cell::kValueOffset, value);
 }
 
-void BaselineAssembler::AddSmi(Register lhs, Smi rhs) {
+void BaselineAssembler::AddSmi(Register lhs, Tagged<Smi> rhs) {
   if (rhs.value() == 0) return;
   __ add(lhs, Immediate(rhs));
 }
diff --git a/src/baseline/loong64/baseline-assembler-loong64-inl.h b/src/baseline/loong64/baseline-assembler-loong64-inl.h
index a0394194a2a..1dd4a7ef0df 100644
--- a/src/baseline/loong64/baseline-assembler-loong64-inl.h
+++ b/src/baseline/loong64/baseline-assembler-loong64-inl.h
@@ -150,7 +150,7 @@ void BaselineAssembler::JumpIfInstanceType(Condition cc, Register map,
   __ Ld_hu(type, FieldMemOperand(map, Map::kInstanceTypeOffset));
   __ Branch(target, cc, type, Operand(instance_type));
 }
-void BaselineAssembler::JumpIfSmi(Condition cc, Register value, Smi smi,
+void BaselineAssembler::JumpIfSmi(Condition cc, Register value, Tagged<Smi> smi,
                                   Label* target, Label::Distance) {
   __ CompareTaggedAndBranch(target, cc, value, Operand(smi));
 }
@@ -183,7 +183,7 @@ void BaselineAssembler::JumpIfByte(Condition cc, Register value, int32_t byte,
 void BaselineAssembler::Move(interpreter::Register output, Register source) {
   Move(RegisterFrameOperand(output), source);
 }
-void BaselineAssembler::Move(Register output, TaggedIndex value) {
+void BaselineAssembler::Move(Register output, Tagged<TaggedIndex> value) {
   __ li(output, Operand(value.ptr()));
 }
 void BaselineAssembler::Move(MemOperand output, Register source) {
@@ -339,7 +339,7 @@ void BaselineAssembler::LoadWord8Field(Register output, Register source,
   __ Ld_b(output, FieldMemOperand(source, offset));
 }
 void BaselineAssembler::StoreTaggedSignedField(Register target, int offset,
-                                               Smi value) {
+                                               Tagged<Smi> value) {
   ASM_CODE_COMMENT(masm_);
   ScratchRegisterScope temps(this);
   Register scratch = temps.AcquireScratch();
@@ -474,7 +474,7 @@ void BaselineAssembler::StaModuleVariable(Register context, Register value,
   StoreTaggedFieldWithWriteBarrier(context, Cell::kValueOffset, value);
 }
 
-void BaselineAssembler::AddSmi(Register lhs, Smi rhs) {
+void BaselineAssembler::AddSmi(Register lhs, Tagged<Smi> rhs) {
   if (SmiValuesAre31Bits()) {
     __ Add_w(lhs, lhs, Operand(rhs));
   } else {
diff --git a/src/baseline/mips64/baseline-assembler-mips64-inl.h b/src/baseline/mips64/baseline-assembler-mips64-inl.h
index 762ec1745ba..47b8fabaa1d 100644
--- a/src/baseline/mips64/baseline-assembler-mips64-inl.h
+++ b/src/baseline/mips64/baseline-assembler-mips64-inl.h
@@ -142,7 +142,7 @@ void BaselineAssembler::JumpIfPointer(Condition cc, Register value,
   __ Ld(scratch, operand);
   __ Branch(target, cc, value, Operand(scratch));
 }
-void BaselineAssembler::JumpIfSmi(Condition cc, Register value, Smi smi,
+void BaselineAssembler::JumpIfSmi(Condition cc, Register value, Tagged<Smi> smi,
                                   Label* target, Label::Distance) {
   ScratchRegisterScope temps(this);
   Register scratch = temps.AcquireScratch();
@@ -180,7 +180,7 @@ void BaselineAssembler::JumpIfByte(Condition cc, Register value, int32_t byte,
 void BaselineAssembler::Move(interpreter::Register output, Register source) {
   Move(RegisterFrameOperand(output), source);
 }
-void BaselineAssembler::Move(Register output, TaggedIndex value) {
+void BaselineAssembler::Move(Register output, Tagged<TaggedIndex> value) {
   __ li(output, Operand(value.ptr()));
 }
 void BaselineAssembler::Move(MemOperand output, Register source) {
@@ -335,7 +335,7 @@ void BaselineAssembler::LoadWord8Field(Register output, Register source,
   __ Lb(output, FieldMemOperand(source, offset));
 }
 void BaselineAssembler::StoreTaggedSignedField(Register target, int offset,
-                                               Smi value) {
+                                               Tagged<Smi> value) {
   ASM_CODE_COMMENT(masm_);
   ScratchRegisterScope temps(this);
   Register scratch = temps.AcquireScratch();
@@ -472,7 +472,7 @@ void BaselineAssembler::StaModuleVariable(Register context, Register value,
   StoreTaggedFieldWithWriteBarrier(context, Cell::kValueOffset, value);
 }
 
-void BaselineAssembler::AddSmi(Register lhs, Smi rhs) {
+void BaselineAssembler::AddSmi(Register lhs, Tagged<Smi> rhs) {
   __ Daddu(lhs, lhs, Operand(rhs));
 }
 
diff --git a/src/baseline/ppc/baseline-assembler-ppc-inl.h b/src/baseline/ppc/baseline-assembler-ppc-inl.h
index 74a3db7ca3b..639ff8ae1ff 100644
--- a/src/baseline/ppc/baseline-assembler-ppc-inl.h
+++ b/src/baseline/ppc/baseline-assembler-ppc-inl.h
@@ -195,7 +195,7 @@ void BaselineAssembler::JumpIfPointer(Condition cc, Register value,
   JumpIfHelper(masm_, cc, value, tmp, target);
 }
 
-void BaselineAssembler::JumpIfSmi(Condition cc, Register value, Smi smi,
+void BaselineAssembler::JumpIfSmi(Condition cc, Register value, Tagged<Smi> smi,
                                   Label* target, Label::Distance) {
   ASM_CODE_COMMENT(masm_);
   __ AssertSmi(value);
@@ -238,7 +238,7 @@ void BaselineAssembler::Move(interpreter::Register output, Register source) {
   Move(RegisterFrameOperand(output), source);
 }
 
-void BaselineAssembler::Move(Register output, TaggedIndex value) {
+void BaselineAssembler::Move(Register output, Tagged<TaggedIndex> value) {
   ASM_CODE_COMMENT(masm_);
   __ mov(output, Operand(value.ptr()));
 }
@@ -415,7 +415,7 @@ void BaselineAssembler::LoadWord8Field(Register output, Register source,
 }
 
 void BaselineAssembler::StoreTaggedSignedField(Register target, int offset,
-                                               Smi value) {
+                                               Tagged<Smi> value) {
   ASM_CODE_COMMENT(masm_);
   ScratchRegisterScope temps(this);
   Register tmp = temps.AcquireScratch();
@@ -567,7 +567,7 @@ void BaselineAssembler::StaModuleVariable(Register context, Register value,
   StoreTaggedFieldWithWriteBarrier(context, Cell::kValueOffset, value);
 }
 
-void BaselineAssembler::AddSmi(Register lhs, Smi rhs) {
+void BaselineAssembler::AddSmi(Register lhs, Tagged<Smi> rhs) {
   ASM_CODE_COMMENT(masm_);
   if (rhs.value() == 0) return;
   __ LoadSmiLiteral(r0, rhs);
diff --git a/src/baseline/riscv/baseline-assembler-riscv-inl.h b/src/baseline/riscv/baseline-assembler-riscv-inl.h
index 2b7a9603bc7..7f65c785805 100644
--- a/src/baseline/riscv/baseline-assembler-riscv-inl.h
+++ b/src/baseline/riscv/baseline-assembler-riscv-inl.h
@@ -158,7 +158,7 @@ void BaselineAssembler::JumpIfPointer(Condition cc, Register value,
   __ LoadWord(temp, operand);
   __ Branch(target, cc, value, Operand(temp), distance);
 }
-void BaselineAssembler::JumpIfSmi(Condition cc, Register value, Smi smi,
+void BaselineAssembler::JumpIfSmi(Condition cc, Register value, Tagged<Smi> smi,
                                   Label* target, Label::Distance distance) {
   __ CompareTaggedAndBranch(target, cc, value, Operand(smi));
 }
@@ -195,7 +195,7 @@ void BaselineAssembler::JumpIfByte(Condition cc, Register value, int32_t byte,
 void BaselineAssembler::Move(interpreter::Register output, Register source) {
   Move(RegisterFrameOperand(output), source);
 }
-void BaselineAssembler::Move(Register output, TaggedIndex value) {
+void BaselineAssembler::Move(Register output, Tagged<TaggedIndex> value) {
   __ li(output, Operand(value.ptr()));
 }
 void BaselineAssembler::Move(MemOperand output, Register source) {
@@ -341,7 +341,7 @@ void BaselineAssembler::LoadWord8Field(Register output, Register source,
   __ Lb(output, FieldMemOperand(source, offset));
 }
 void BaselineAssembler::StoreTaggedSignedField(Register target, int offset,
-                                               Smi value) {
+                                               Tagged<Smi> value) {
   ASM_CODE_COMMENT(masm_);
   ScratchRegisterScope temps(this);
   Register tmp = temps.AcquireScratch();
@@ -484,7 +484,7 @@ void BaselineAssembler::StaModuleVariable(Register context, Register value,
   StoreTaggedFieldWithWriteBarrier(context, Cell::kValueOffset, value);
 }
 
-void BaselineAssembler::AddSmi(Register lhs, Smi rhs) {
+void BaselineAssembler::AddSmi(Register lhs, Tagged<Smi> rhs) {
   ASM_CODE_COMMENT(masm_);
   if (SmiValuesAre31Bits()) {
     __ Add32(lhs, lhs, Operand(rhs));
diff --git a/src/baseline/s390/baseline-assembler-s390-inl.h b/src/baseline/s390/baseline-assembler-s390-inl.h
index 257426897ee..bd157932db5 100644
--- a/src/baseline/s390/baseline-assembler-s390-inl.h
+++ b/src/baseline/s390/baseline-assembler-s390-inl.h
@@ -194,7 +194,7 @@ void BaselineAssembler::JumpIfPointer(Condition cc, Register value,
   JumpIfHelper(masm_, cc, value, tmp, target);
 }
 
-void BaselineAssembler::JumpIfSmi(Condition cc, Register value, Smi smi,
+void BaselineAssembler::JumpIfSmi(Condition cc, Register value, Tagged<Smi> smi,
                                   Label* target, Label::Distance) {
   ASM_CODE_COMMENT(masm_);
   __ AssertSmi(value);
@@ -255,7 +255,7 @@ void BaselineAssembler::Move(interpreter::Register output, Register source) {
   Move(RegisterFrameOperand(output), source);
 }
 
-void BaselineAssembler::Move(Register output, TaggedIndex value) {
+void BaselineAssembler::Move(Register output, Tagged<TaggedIndex> value) {
   ASM_CODE_COMMENT(masm_);
   __ mov(output, Operand(value.ptr()));
 }
@@ -432,7 +432,7 @@ void BaselineAssembler::LoadWord8Field(Register output, Register source,
 }
 
 void BaselineAssembler::StoreTaggedSignedField(Register target, int offset,
-                                               Smi value) {
+                                               Tagged<Smi> value) {
   ASM_CODE_COMMENT(masm_);
   ScratchRegisterScope temps(this);
   Register tmp = temps.AcquireScratch();
@@ -580,7 +580,7 @@ void BaselineAssembler::StaModuleVariable(Register context, Register value,
   StoreTaggedFieldWithWriteBarrier(context, Cell::kValueOffset, value);
 }
 
-void BaselineAssembler::AddSmi(Register lhs, Smi rhs) {
+void BaselineAssembler::AddSmi(Register lhs, Tagged<Smi> rhs) {
   if (rhs.value() == 0) return;
   __ LoadSmiLiteral(r0, rhs);
   if (SmiValuesAre31Bits()) {
diff --git a/src/baseline/x64/baseline-assembler-x64-inl.h b/src/baseline/x64/baseline-assembler-x64-inl.h
index 1f2217f9178..c80c077b83b 100644
--- a/src/baseline/x64/baseline-assembler-x64-inl.h
+++ b/src/baseline/x64/baseline-assembler-x64-inl.h
@@ -165,7 +165,7 @@ void BaselineAssembler::JumpIfPointer(Condition cc, Register value,
   __ cmpq(value, operand);
   __ j(cc, target, distance);
 }
-void BaselineAssembler::JumpIfSmi(Condition cc, Register lhs, Smi smi,
+void BaselineAssembler::JumpIfSmi(Condition cc, Register lhs, Tagged<Smi> smi,
                                   Label* target, Label::Distance distance) {
   __ SmiCompare(lhs, smi);
   __ j(cc, target, distance);
@@ -205,7 +205,7 @@ void BaselineAssembler::JumpIfByte(Condition cc, Register value, int32_t byte,
 void BaselineAssembler::Move(interpreter::Register output, Register source) {
   return __ movq(RegisterFrameOperand(output), source);
 }
-void BaselineAssembler::Move(Register output, TaggedIndex value) {
+void BaselineAssembler::Move(Register output, Tagged<TaggedIndex> value) {
   __ Move(output, value);
 }
 void BaselineAssembler::Move(MemOperand output, Register source) {
@@ -232,10 +232,12 @@ inline void PushSingle(MacroAssembler* masm, RootIndex source) {
   masm->PushRoot(source);
 }
 inline void PushSingle(MacroAssembler* masm, Register reg) { masm->Push(reg); }
-inline void PushSingle(MacroAssembler* masm, TaggedIndex value) {
+inline void PushSingle(MacroAssembler* masm, Tagged<TaggedIndex> value) {
+  masm->Push(value);
+}
+inline void PushSingle(MacroAssembler* masm, Tagged<Smi> value) {
   masm->Push(value);
 }
-inline void PushSingle(MacroAssembler* masm, Smi value) { masm->Push(value); }
 inline void PushSingle(MacroAssembler* masm, Handle<HeapObject> object) {
   masm->Push(object);
 }
@@ -336,7 +338,7 @@ void BaselineAssembler::LoadWord8Field(Register output, Register source,
   __ movb(output, FieldOperand(source, offset));
 }
 void BaselineAssembler::StoreTaggedSignedField(Register target, int offset,
-                                               Smi value) {
+                                               Tagged<Smi> value) {
   __ StoreTaggedSignedField(FieldOperand(target, offset), value);
 }
 void BaselineAssembler::StoreTaggedFieldWithWriteBarrier(Register target,
@@ -517,7 +519,7 @@ void BaselineAssembler::StaModuleVariable(Register context, Register value,
   StoreTaggedFieldWithWriteBarrier(context, Cell::kValueOffset, value);
 }
 
-void BaselineAssembler::AddSmi(Register lhs, Smi rhs) {
+void BaselineAssembler::AddSmi(Register lhs, Tagged<Smi> rhs) {
   if (rhs.value() == 0) return;
   if (SmiValuesAre31Bits()) {
     __ addl(lhs, Immediate(rhs));
diff --git a/src/builtins/accessors.cc b/src/builtins/accessors.cc
index 7e808d9f182..c0e840b5afd 100644
--- a/src/builtins/accessors.cc
+++ b/src/builtins/accessors.cc
@@ -135,7 +135,7 @@ void Accessors::ArgumentsIteratorGetter(
   i::Isolate* isolate = reinterpret_cast<i::Isolate*>(info.GetIsolate());
   DisallowGarbageCollection no_gc;
   HandleScope scope(isolate);
-  Object result = isolate->native_context()->array_values_iterator();
+  Tagged<Object> result = isolate->native_context()->array_values_iterator();
   info.GetReturnValue().Set(Utils::ToLocal(Handle<Object>(result, isolate)));
 }
 
@@ -154,8 +154,8 @@ void Accessors::ArrayLengthGetter(
   RCS_SCOPE(isolate, RuntimeCallCounterId::kArrayLengthGetter);
   DisallowGarbageCollection no_gc;
   HandleScope scope(isolate);
-  JSArray holder = JSArray::cast(*Utils::OpenHandle(*info.Holder()));
-  Object result = holder->length();
+  Tagged<JSArray> holder = JSArray::cast(*Utils::OpenHandle(*info.Holder()));
+  Tagged<Object> result = holder->length();
   info.GetReturnValue().Set(Utils::ToLocal(Handle<Object>(result, isolate)));
 }
 
@@ -238,7 +238,7 @@ void Accessors::ModuleNamespaceEntryGetter(
     v8::Local<v8::Name> name, const v8::PropertyCallbackInfo<v8::Value>& info) {
   i::Isolate* isolate = reinterpret_cast<i::Isolate*>(info.GetIsolate());
   HandleScope scope(isolate);
-  JSModuleNamespace holder =
+  Tagged<JSModuleNamespace> holder =
       JSModuleNamespace::cast(*Utils::OpenHandle(*info.Holder()));
   Handle<Object> result;
   if (!holder
@@ -291,14 +291,14 @@ void Accessors::StringLengthGetter(
   // v8::Object, but internally we have callbacks on entities which are higher
   // in the hierarchy, in this case for String values.
 
-  Object value = *Utils::OpenHandle(*v8::Local<v8::Value>(info.This()));
+  Tagged<Object> value = *Utils::OpenHandle(*v8::Local<v8::Value>(info.This()));
   if (!IsString(value)) {
     // Not a string value. That means that we either got a String wrapper or
     // a Value with a String wrapper in its prototype chain.
     value =
         JSPrimitiveWrapper::cast(*Utils::OpenHandle(*info.Holder()))->value();
   }
-  Object result = Smi::FromInt(String::cast(value)->length());
+  Tagged<Object> result = Smi::FromInt(String::cast(value)->length());
   info.GetReturnValue().Set(Utils::ToLocal(Handle<Object>(result, isolate)));
 }
 
@@ -482,7 +482,7 @@ Handle<JSObject> GetFrameArguments(Isolate* isolate,
   // Copy the parameters to the arguments object.
   DCHECK(array->length() == length);
   for (int i = 0; i < length; i++) {
-    Object value = frame->GetParameter(i);
+    Tagged<Object> value = frame->GetParameter(i);
     if (IsTheHole(value, isolate)) {
       // Generators currently use holes as dummy arguments when resuming.  We
       // must not leak those.
@@ -557,8 +557,8 @@ Handle<AccessorInfo> Accessors::MakeFunctionArgumentsInfo(Isolate* isolate) {
 // Accessors::FunctionCaller
 //
 
-static inline bool AllowAccessToFunction(Context current_context,
-                                         JSFunction function) {
+static inline bool AllowAccessToFunction(Tagged<Context> current_context,
+                                         Tagged<JSFunction> function) {
   return current_context->HasSameSecurityTokenAs(function->context());
 }
 
diff --git a/src/builtins/arm/builtins-arm.cc b/src/builtins/arm/builtins-arm.cc
index 79b31392e6a..9a465695ca8 100644
--- a/src/builtins/arm/builtins-arm.cc
+++ b/src/builtins/arm/builtins-arm.cc
@@ -1659,7 +1659,7 @@ static void Generate_InterpreterEnterBytecode(MacroAssembler* masm) {
   // Set the return address to the correct point in the interpreter entry
   // trampoline.
   Label builtin_trampoline, trampoline_loaded;
-  Smi interpreter_entry_return_pc_offset(
+  Tagged<Smi> interpreter_entry_return_pc_offset(
       masm->isolate()->heap()->interpreter_entry_return_pc_offset());
   DCHECK_NE(interpreter_entry_return_pc_offset, Smi::zero());
 
diff --git a/src/builtins/arm64/builtins-arm64.cc b/src/builtins/arm64/builtins-arm64.cc
index 53ed795b665..3d7f7b284cf 100644
--- a/src/builtins/arm64/builtins-arm64.cc
+++ b/src/builtins/arm64/builtins-arm64.cc
@@ -1906,7 +1906,7 @@ static void Generate_InterpreterEnterBytecode(MacroAssembler* masm) {
   // We return here after having executed the function in the interpreter.
   // Now jump to the correct point in the interpreter entry trampoline.
   Label builtin_trampoline, trampoline_loaded;
-  Smi interpreter_entry_return_pc_offset(
+  Tagged<Smi> interpreter_entry_return_pc_offset(
       masm->isolate()->heap()->interpreter_entry_return_pc_offset());
   DCHECK_NE(interpreter_entry_return_pc_offset, Smi::zero());
 
diff --git a/src/builtins/builtins-api.cc b/src/builtins/builtins-api.cc
index 1dce546a974..378ff1f66a7 100644
--- a/src/builtins/builtins-api.cc
+++ b/src/builtins/builtins-api.cc
@@ -23,26 +23,28 @@ namespace {
 // Returns the holder JSObject if the function can legally be called with this
 // receiver.  Returns nullptr if the call is illegal.
 // TODO(dcarney): CallOptimization duplicates this logic, merge.
-JSReceiver GetCompatibleReceiver(Isolate* isolate, FunctionTemplateInfo info,
-                                 JSReceiver receiver) {
+Tagged<JSReceiver> GetCompatibleReceiver(Isolate* isolate,
+                                         Tagged<FunctionTemplateInfo> info,
+                                         Tagged<JSReceiver> receiver) {
   RCS_SCOPE(isolate, RuntimeCallCounterId::kGetCompatibleReceiver);
-  Object recv_type = info->signature();
+  Tagged<Object> recv_type = info->signature();
   // No signature, return holder.
   if (!IsFunctionTemplateInfo(recv_type)) return receiver;
   // A Proxy cannot have been created from the signature template.
   if (!IsJSObject(receiver)) return JSReceiver();
 
-  JSObject js_obj_receiver = JSObject::cast(receiver);
-  FunctionTemplateInfo signature = FunctionTemplateInfo::cast(recv_type);
+  Tagged<JSObject> js_obj_receiver = JSObject::cast(receiver);
+  Tagged<FunctionTemplateInfo> signature =
+      FunctionTemplateInfo::cast(recv_type);
 
   // Check the receiver.
   if (signature->IsTemplateFor(js_obj_receiver)) return receiver;
 
   // The JSGlobalProxy might have a hidden prototype.
   if (V8_UNLIKELY(IsJSGlobalProxy(js_obj_receiver))) {
-    HeapObject prototype = js_obj_receiver->map()->prototype();
+    Tagged<HeapObject> prototype = js_obj_receiver->map()->prototype();
     if (!IsNull(prototype, isolate)) {
-      JSObject js_obj_prototype = JSObject::cast(prototype);
+      Tagged<JSObject> js_obj_prototype = JSObject::cast(prototype);
       if (signature->IsTemplateFor(js_obj_prototype)) return js_obj_prototype;
     }
   }
@@ -58,7 +60,7 @@ V8_WARN_UNUSED_RESULT MaybeHandle<Object> HandleApiCallHelper(
     Handle<FunctionTemplateInfo> fun_data, Handle<Object> receiver,
     Address* argv, int argc) {
   Handle<JSReceiver> js_receiver;
-  JSReceiver raw_holder;
+  Tagged<JSReceiver> raw_holder;
   if (is_construct) {
     DCHECK(IsTheHole(*receiver, isolate));
     if (IsUndefined(fun_data->GetInstanceTemplate(), isolate)) {
@@ -101,11 +103,11 @@ V8_WARN_UNUSED_RESULT MaybeHandle<Object> HandleApiCallHelper(
     }
   }
 
-  Object raw_call_data = fun_data->call_code(kAcquireLoad);
+  Tagged<Object> raw_call_data = fun_data->call_code(kAcquireLoad);
   if (!IsUndefined(raw_call_data, isolate)) {
     DCHECK(IsCallHandlerInfo(raw_call_data));
-    CallHandlerInfo call_data = CallHandlerInfo::cast(raw_call_data);
-    Object data_obj = call_data->data();
+    Tagged<CallHandlerInfo> call_data = CallHandlerInfo::cast(raw_call_data);
+    Tagged<Object> data_obj = call_data->data();
 
     FunctionCallbackArguments custom(isolate, data_obj, raw_holder, *new_target,
                                      argv, argc);
@@ -119,7 +121,7 @@ V8_WARN_UNUSED_RESULT MaybeHandle<Object> HandleApiCallHelper(
     // Rebox the result.
     {
       DisallowGarbageCollection no_gc;
-      Object raw_result = *result;
+      Tagged<Object> raw_result = *result;
       DCHECK(IsApiCallResultType(raw_result));
       if (!is_construct || IsJSReceiver(raw_result))
         return handle(raw_result, isolate);
@@ -204,17 +206,17 @@ MaybeHandle<Object> Builtins::InvokeApiFunction(
 // Helper function to handle calls to non-function objects created through the
 // API. The object can be called as either a constructor (using new) or just as
 // a function (without new).
-V8_WARN_UNUSED_RESULT static Object
+V8_WARN_UNUSED_RESULT static Tagged<Object>
 HandleApiCallAsFunctionOrConstructorDelegate(Isolate* isolate,
                                              bool is_construct_call,
                                              BuiltinArguments args) {
   Handle<Object> receiver = args.receiver();
 
   // Get the object called.
-  JSObject obj = JSObject::cast(*receiver);
+  Tagged<JSObject> obj = JSObject::cast(*receiver);
 
   // Set the new target.
-  HeapObject new_target;
+  Tagged<HeapObject> new_target;
   if (is_construct_call) {
     // TODO(adamk): This should be passed through in args instead of
     // being patched in here. We need to set a non-undefined value
@@ -228,15 +230,16 @@ HandleApiCallAsFunctionOrConstructorDelegate(Isolate* isolate,
   // Get the invocation callback from the function descriptor that was
   // used to create the called object.
   DCHECK(obj->map()->is_callable());
-  JSFunction constructor = JSFunction::cast(obj->map()->GetConstructor());
+  Tagged<JSFunction> constructor =
+      JSFunction::cast(obj->map()->GetConstructor());
   DCHECK(constructor->shared()->IsApiFunction());
-  Object handler =
+  Tagged<Object> handler =
       constructor->shared()->api_func_data()->GetInstanceCallHandler();
   DCHECK(!IsUndefined(handler, isolate));
-  CallHandlerInfo call_data = CallHandlerInfo::cast(handler);
+  Tagged<CallHandlerInfo> call_data = CallHandlerInfo::cast(handler);
 
   // Get the data for the call and perform the callback.
-  Object result;
+  Tagged<Object> result;
   {
     HandleScope scope(isolate);
     FunctionCallbackArguments custom(
diff --git a/src/builtins/builtins-array.cc b/src/builtins/builtins-array.cc
index d6fc854064d..408c77bc7ac 100644
--- a/src/builtins/builtins-array.cc
+++ b/src/builtins/builtins-array.cc
@@ -30,27 +30,29 @@ namespace internal {
 namespace {
 
 inline bool IsJSArrayFastElementMovingAllowed(Isolate* isolate,
-                                              JSArray receiver) {
+                                              Tagged<JSArray> receiver) {
   return JSObject::PrototypeHasNoElements(isolate, receiver);
 }
 
-inline bool HasSimpleElements(JSObject current) {
+inline bool HasSimpleElements(Tagged<JSObject> current) {
   return !IsCustomElementsReceiverMap(current->map()) &&
          !current->GetElementsAccessor()->HasAccessors(current);
 }
 
-inline bool HasOnlySimpleReceiverElements(Isolate* isolate, JSObject receiver) {
+inline bool HasOnlySimpleReceiverElements(Isolate* isolate,
+                                          Tagged<JSObject> receiver) {
   // Check that we have no accessors on the receiver's elements.
   if (!HasSimpleElements(receiver)) return false;
   return JSObject::PrototypeHasNoElements(isolate, receiver);
 }
 
-inline bool HasOnlySimpleElements(Isolate* isolate, JSReceiver receiver) {
+inline bool HasOnlySimpleElements(Isolate* isolate,
+                                  Tagged<JSReceiver> receiver) {
   DisallowGarbageCollection no_gc;
   PrototypeIterator iter(isolate, receiver, kStartAtReceiver);
   for (; !iter.IsAtEnd(); iter.Advance()) {
     if (IsJSProxy(iter.GetCurrent())) return false;
-    JSObject current = iter.GetCurrent<JSObject>();
+    Tagged<JSObject> current = iter.GetCurrent<JSObject>();
     if (!HasSimpleElements(current)) return false;
   }
   return true;
@@ -75,7 +77,7 @@ void MatchArrayElementsKindToArguments(Isolate* isolate, Handle<JSArray> array,
     DisallowGarbageCollection no_gc;
     int last_arg_index = std::min(first_arg_index + num_arguments, args_length);
     for (int i = first_arg_index; i < last_arg_index; i++) {
-      Object arg = (*args)[i];
+      Tagged<Object> arg = (*args)[i];
       if (IsHeapObject(arg)) {
         if (IsHeapNumber(arg)) {
           target_kind = PACKED_DOUBLE_ELEMENTS;
@@ -187,10 +189,9 @@ V8_WARN_UNUSED_RESULT MaybeHandle<Object> SetLengthProperty(
       Just(ShouldThrow::kThrowOnError));
 }
 
-V8_WARN_UNUSED_RESULT Object GenericArrayFill(Isolate* isolate,
-                                              Handle<JSReceiver> receiver,
-                                              Handle<Object> value,
-                                              double start, double end) {
+V8_WARN_UNUSED_RESULT Tagged<Object> GenericArrayFill(
+    Isolate* isolate, Handle<JSReceiver> receiver, Handle<Object> value,
+    double start, double end) {
   // 7. Repeat, while k < final.
   while (start < end) {
     // a. Let Pk be ! ToString(k).
@@ -313,8 +314,8 @@ BUILTIN(ArrayPrototypeFill) {
 }
 
 namespace {
-V8_WARN_UNUSED_RESULT Object GenericArrayPush(Isolate* isolate,
-                                              BuiltinArguments* args) {
+V8_WARN_UNUSED_RESULT Tagged<Object> GenericArrayPush(Isolate* isolate,
+                                                      BuiltinArguments* args) {
   // 1. Let O be ? ToObject(this value).
   Handle<JSReceiver> receiver;
   ASSIGN_RETURN_FAILURE_ON_EXCEPTION(
@@ -406,8 +407,8 @@ BUILTIN(ArrayPush) {
 
 namespace {
 
-V8_WARN_UNUSED_RESULT Object GenericArrayPop(Isolate* isolate,
-                                             BuiltinArguments* args) {
+V8_WARN_UNUSED_RESULT Tagged<Object> GenericArrayPop(Isolate* isolate,
+                                                     BuiltinArguments* args) {
   // 1. Let O be ? ToObject(this value).
   Handle<JSReceiver> receiver;
   ASSIGN_RETURN_FAILURE_ON_EXCEPTION(
@@ -524,9 +525,8 @@ V8_WARN_UNUSED_RESULT bool CanUseFastArrayShift(Isolate* isolate,
   return !JSArray::HasReadOnlyLength(array);
 }
 
-V8_WARN_UNUSED_RESULT Object GenericArrayShift(Isolate* isolate,
-                                               Handle<JSReceiver> receiver,
-                                               double length) {
+V8_WARN_UNUSED_RESULT Tagged<Object> GenericArrayShift(
+    Isolate* isolate, Handle<JSReceiver> receiver, double length) {
   // 4. Let first be ? Get(O, "0").
   Handle<Object> first;
   ASSIGN_RETURN_FAILURE_ON_EXCEPTION(isolate, first,
@@ -820,7 +820,7 @@ class ArrayConcatVisitor {
 
   inline void clear_storage() { GlobalHandles::Destroy(storage_.location()); }
 
-  inline void set_storage(FixedArray storage) {
+  inline void set_storage(Tagged<FixedArray> storage) {
     DCHECK(is_fixed_array());
     DCHECK(has_simple_elements());
     storage_ = isolate_->global_handles()->Create(storage);
@@ -872,7 +872,7 @@ uint32_t EstimateElementCount(Isolate* isolate, Handle<JSArray> array) {
       // a 32-bit signed integer.
       DCHECK_GE(static_cast<int32_t>(FixedArray::kMaxLength), 0);
       int fast_length = static_cast<int>(length);
-      FixedArray elements = FixedArray::cast(array->elements());
+      Tagged<FixedArray> elements = FixedArray::cast(array->elements());
       for (int i = 0; i < fast_length; i++) {
         if (!IsTheHole(elements->get(i), isolate)) element_count++;
       }
@@ -888,18 +888,20 @@ uint32_t EstimateElementCount(Isolate* isolate, Handle<JSArray> array) {
         DCHECK_EQ(FixedArray::cast(array->elements())->length(), 0);
         break;
       }
-      FixedDoubleArray elements = FixedDoubleArray::cast(array->elements());
+      Tagged<FixedDoubleArray> elements =
+          FixedDoubleArray::cast(array->elements());
       for (int i = 0; i < fast_length; i++) {
         if (!elements->is_the_hole(i)) element_count++;
       }
       break;
     }
     case DICTIONARY_ELEMENTS: {
-      NumberDictionary dictionary = NumberDictionary::cast(array->elements());
+      Tagged<NumberDictionary> dictionary =
+          NumberDictionary::cast(array->elements());
       ReadOnlyRoots roots(isolate);
       for (InternalIndex i : dictionary->IterateEntries()) {
-        Object key = dictionary->KeyAt(i);
-        if (dictionary.IsKey(roots, key)) {
+        Tagged<Object> key = dictionary->KeyAt(i);
+        if (dictionary->IsKey(roots, key)) {
           element_count++;
         }
       }
@@ -943,7 +945,7 @@ void CollectElementIndices(Isolate* isolate, Handle<JSObject> object,
     case HOLEY_NONEXTENSIBLE_ELEMENTS:
     case HOLEY_ELEMENTS: {
       DisallowGarbageCollection no_gc;
-      FixedArray elements = FixedArray::cast(object->elements());
+      Tagged<FixedArray> elements = FixedArray::cast(object->elements());
       uint32_t length = static_cast<uint32_t>(elements->length());
       if (range < length) length = range;
       for (uint32_t i = 0; i < length; i++) {
@@ -972,12 +974,13 @@ void CollectElementIndices(Isolate* isolate, Handle<JSObject> object,
     }
     case DICTIONARY_ELEMENTS: {
       DisallowGarbageCollection no_gc;
-      NumberDictionary dict = NumberDictionary::cast(object->elements());
+      Tagged<NumberDictionary> dict =
+          NumberDictionary::cast(object->elements());
       uint32_t capacity = dict->Capacity();
       ReadOnlyRoots roots(isolate);
       FOR_WITH_HANDLE_SCOPE(isolate, uint32_t, j = 0, j, j < capacity, j++, {
-        Object k = dict->KeyAt(InternalIndex(j));
-        if (!dict.IsKey(roots, k)) continue;
+        Tagged<Object> k = dict->KeyAt(InternalIndex(j));
+        if (!dict->IsKey(roots, k)) continue;
         DCHECK(IsNumber(k));
         uint32_t index = static_cast<uint32_t>(Object::Number(k));
         if (index < range) {
@@ -1010,8 +1013,8 @@ void CollectElementIndices(Isolate* isolate, Handle<JSObject> object,
     case SLOW_SLOPPY_ARGUMENTS_ELEMENTS: {
       DisallowGarbageCollection no_gc;
       DisableGCMole no_gc_mole;
-      FixedArrayBase elements = object->elements();
-      JSObject raw_object = *object;
+      Tagged<FixedArrayBase> elements = object->elements();
+      Tagged<JSObject> raw_object = *object;
       ElementsAccessor* accessor = object->GetElementsAccessor();
       for (uint32_t i = 0; i < range; i++) {
         if (accessor->HasElement(raw_object, i, elements)) {
@@ -1282,8 +1285,8 @@ static Maybe<bool> IsConcatSpreadable(Isolate* isolate, Handle<Object> obj) {
   return Object::IsArray(obj);
 }
 
-Object Slow_ArrayConcat(BuiltinArguments* args, Handle<Object> species,
-                        Isolate* isolate) {
+Tagged<Object> Slow_ArrayConcat(BuiltinArguments* args, Handle<Object> species,
+                                Isolate* isolate) {
   int argument_count = args->length();
 
   bool is_array_species = *species == isolate->context()->array_function();
@@ -1359,7 +1362,7 @@ Object Slow_ArrayConcat(BuiltinArguments* args, Handle<Object> species,
           j++;
         } else {
           DisallowGarbageCollection no_gc;
-          JSArray array = JSArray::cast(*obj);
+          Tagged<JSArray> array = JSArray::cast(*obj);
           uint32_t length =
               static_cast<uint32_t>(Object::Number(array->length()));
           switch (array->GetElementsKind()) {
@@ -1367,7 +1370,7 @@ Object Slow_ArrayConcat(BuiltinArguments* args, Handle<Object> species,
             case PACKED_DOUBLE_ELEMENTS: {
               // Empty array is FixedArray but not FixedDoubleArray.
               if (length == 0) break;
-              FixedDoubleArray elements =
+              Tagged<FixedDoubleArray> elements =
                   FixedDoubleArray::cast(array->elements());
               for (uint32_t k = 0; k < length; k++) {
                 if (elements->is_the_hole(k)) {
@@ -1387,10 +1390,10 @@ Object Slow_ArrayConcat(BuiltinArguments* args, Handle<Object> species,
             }
             case HOLEY_SMI_ELEMENTS:
             case PACKED_SMI_ELEMENTS: {
-              Object the_hole = ReadOnlyRoots(isolate).the_hole_value();
-              FixedArray elements(FixedArray::cast(array->elements()));
+              Tagged<Object> the_hole = ReadOnlyRoots(isolate).the_hole_value();
+              Tagged<FixedArray> elements(FixedArray::cast(array->elements()));
               for (uint32_t k = 0; k < length; k++) {
-                Object element = elements->get(k);
+                Tagged<Object> element = elements->get(k);
                 if (element == the_hole) {
                   failure = true;
                   break;
@@ -1486,7 +1489,7 @@ Object Slow_ArrayConcat(BuiltinArguments* args, Handle<Object> species,
 
 bool IsSimpleArray(Isolate* isolate, Handle<JSArray> obj) {
   DisallowGarbageCollection no_gc;
-  Map map = obj->map();
+  Tagged<Map> map = obj->map();
   // If there is only the 'length' property we are fine.
   if (map->prototype() ==
           isolate->native_context()->initial_array_prototype() &&
@@ -1516,7 +1519,7 @@ MaybeHandle<JSArray> Fast_ArrayConcat(Isolate* isolate,
     // Iterate through all the arguments performing checks
     // and calculating total length.
     for (int i = 0; i < n_arguments; i++) {
-      Object arg = (*args)[i];
+      Tagged<Object> arg = (*args)[i];
       if (!IsJSArray(arg)) return MaybeHandle<JSArray>();
       if (!HasOnlySimpleReceiverElements(isolate, JSObject::cast(arg))) {
         return MaybeHandle<JSArray>();
diff --git a/src/builtins/builtins-arraybuffer.cc b/src/builtins/builtins-arraybuffer.cc
index 3d2b6c17ab9..08810fd4afd 100644
--- a/src/builtins/builtins-arraybuffer.cc
+++ b/src/builtins/builtins-arraybuffer.cc
@@ -39,9 +39,10 @@ namespace internal {
 
 namespace {
 
-Object ConstructBuffer(Isolate* isolate, Handle<JSFunction> target,
-                       Handle<JSReceiver> new_target, Handle<Object> length,
-                       Handle<Object> max_length, InitializedFlag initialized) {
+Tagged<Object> ConstructBuffer(Isolate* isolate, Handle<JSFunction> target,
+                               Handle<JSReceiver> new_target,
+                               Handle<Object> length, Handle<Object> max_length,
+                               InitializedFlag initialized) {
   SharedFlag shared = *target != target->native_context()->array_buffer_fun()
                           ? SharedFlag::kShared
                           : SharedFlag::kNotShared;
@@ -167,8 +168,8 @@ BUILTIN(ArrayBufferConstructor_DoNotInitialize) {
                          InitializedFlag::kUninitialized);
 }
 
-static Object SliceHelper(BuiltinArguments args, Isolate* isolate,
-                          const char* kMethodName, bool is_shared) {
+static Tagged<Object> SliceHelper(BuiltinArguments args, Isolate* isolate,
+                                  const char* kMethodName, bool is_shared) {
   HandleScope scope(isolate);
   Handle<Object> start = args.at(1);
   Handle<Object> end = args.atOrUndefined(isolate, 2);
@@ -360,8 +361,8 @@ BUILTIN(ArrayBufferPrototypeSlice) {
   return SliceHelper(args, isolate, kMethodName, false);
 }
 
-static Object ResizeHelper(BuiltinArguments args, Isolate* isolate,
-                           const char* kMethodName, bool is_shared) {
+static Tagged<Object> ResizeHelper(BuiltinArguments args, Isolate* isolate,
+                                   const char* kMethodName, bool is_shared) {
   HandleScope scope(isolate);
 
   // 1 Let O be the this value.
@@ -505,10 +506,11 @@ namespace {
 
 enum PreserveResizability { kToFixedLength, kPreserveResizability };
 
-Object ArrayBufferTransfer(Isolate* isolate, Handle<JSArrayBuffer> array_buffer,
-                           Handle<Object> new_length,
-                           PreserveResizability preserve_resizability,
-                           const char* method_name) {
+Tagged<Object> ArrayBufferTransfer(Isolate* isolate,
+                                   Handle<JSArrayBuffer> array_buffer,
+                                   Handle<Object> new_length,
+                                   PreserveResizability preserve_resizability,
+                                   const char* method_name) {
   // 2. If IsSharedArrayBuffer(arrayBuffer) is true, throw a TypeError
   // exception.
   CHECK_SHARED(false, array_buffer, method_name);
diff --git a/src/builtins/builtins-bigint.cc b/src/builtins/builtins-bigint.cc
index 90ae17bad1f..f2265752337 100644
--- a/src/builtins/builtins-bigint.cc
+++ b/src/builtins/builtins-bigint.cc
@@ -83,7 +83,7 @@ MaybeHandle<BigInt> ThisBigIntValue(Isolate* isolate, Handle<Object> value,
   if (IsJSPrimitiveWrapper(*value)) {
     // 2a. Assert: value.[[BigIntData]] is a BigInt value.
     // 2b. Return value.[[BigIntData]].
-    Object data = JSPrimitiveWrapper::cast(*value)->value();
+    Tagged<Object> data = JSPrimitiveWrapper::cast(*value)->value();
     if (IsBigInt(data)) return handle(BigInt::cast(data), isolate);
   }
   // 3. Throw a TypeError exception.
@@ -95,8 +95,8 @@ MaybeHandle<BigInt> ThisBigIntValue(Isolate* isolate, Handle<Object> value,
       BigInt);
 }
 
-Object BigIntToStringImpl(Handle<Object> receiver, Handle<Object> radix,
-                          Isolate* isolate, const char* builtin_name) {
+Tagged<Object> BigIntToStringImpl(Handle<Object> receiver, Handle<Object> radix,
+                                  Isolate* isolate, const char* builtin_name) {
   // 1. Let x be ? thisBigIntValue(this value).
   Handle<BigInt> x;
   ASSIGN_RETURN_FAILURE_ON_EXCEPTION(
diff --git a/src/builtins/builtins-callsite.cc b/src/builtins/builtins-callsite.cc
index 4671c0cabe7..fef73e8d1cc 100644
--- a/src/builtins/builtins-callsite.cc
+++ b/src/builtins/builtins-callsite.cc
@@ -27,12 +27,12 @@ namespace internal {
 
 namespace {
 
-Object PositiveNumberOrNull(int value, Isolate* isolate) {
+Tagged<Object> PositiveNumberOrNull(int value, Isolate* isolate) {
   if (value > 0) return *isolate->factory()->NewNumberFromInt(value);
   return ReadOnlyRoots(isolate).null_value();
 }
 
-bool NativeContextIsForShadowRealm(NativeContext native_context) {
+bool NativeContextIsForShadowRealm(Tagged<NativeContext> native_context) {
   return native_context->scope_info()->scope_type() == SHADOW_REALM_SCOPE;
 }
 
diff --git a/src/builtins/builtins-dataview.cc b/src/builtins/builtins-dataview.cc
index 5c98dae3227..8895327cc16 100644
--- a/src/builtins/builtins-dataview.cc
+++ b/src/builtins/builtins-dataview.cc
@@ -125,7 +125,7 @@ BUILTIN(DataViewConstructor) {
     // passes ObjectVerify, which may for example be triggered when allocating
     // error objects below.
     DisallowGarbageCollection no_gc;
-    JSDataViewOrRabGsabDataView raw = *data_view;
+    Tagged<JSDataViewOrRabGsabDataView> raw = *data_view;
 
     for (int i = 0; i < ArrayBufferView::kEmbedderFieldCount; ++i) {
       // TODO(v8:10391, saelo): Handle external pointers in EmbedderDataSlot
diff --git a/src/builtins/builtins-function.cc b/src/builtins/builtins-function.cc
index a4a2324dad9..751b7609358 100644
--- a/src/builtins/builtins-function.cc
+++ b/src/builtins/builtins-function.cc
@@ -184,7 +184,7 @@ BUILTIN(AsyncGeneratorFunctionConstructor) {
 
 namespace {
 
-Object DoFunctionBind(Isolate* isolate, BuiltinArguments args) {
+Tagged<Object> DoFunctionBind(Isolate* isolate, BuiltinArguments args) {
   HandleScope scope(isolate);
   DCHECK_LE(1, args.length());
   if (!IsCallable(*args.receiver())) {
diff --git a/src/builtins/builtins-intl.cc b/src/builtins/builtins-intl.cc
index 794687ef3f9..a7b300bad92 100644
--- a/src/builtins/builtins-intl.cc
+++ b/src/builtins/builtins-intl.cc
@@ -146,7 +146,7 @@ BUILTIN(DateTimeFormatPrototypeFormatToParts) {
 template <class T, MaybeHandle<T> (*F)(Isolate*, Handle<JSDateTimeFormat>,
                                        Handle<Object>, Handle<Object>,
                                        const char* const)>
-V8_WARN_UNUSED_RESULT Object DateTimeFormatRange(
+V8_WARN_UNUSED_RESULT Tagged<Object> DateTimeFormatRange(
     BuiltinArguments args, Isolate* isolate, const char* const method_name) {
   // 1. Let dtf be this value.
   // 2. Perform ? RequireInternalSlot(dtf, [[InitializedDateTimeFormat]]).
@@ -214,10 +214,10 @@ Handle<JSFunction> CreateBoundFunction(Isolate* isolate,
  * NumberFormatConstrutor
  */
 template <class T>
-Object LegacyFormatConstructor(BuiltinArguments args, Isolate* isolate,
-                               v8::Isolate::UseCounterFeature feature,
-                               Handle<Object> constructor,
-                               const char* method_name) {
+Tagged<Object> LegacyFormatConstructor(BuiltinArguments args, Isolate* isolate,
+                                       v8::Isolate::UseCounterFeature feature,
+                                       Handle<Object> constructor,
+                                       const char* method_name) {
   isolate->CountUsage(feature);
   Handle<JSReceiver> new_target;
   // 1. If NewTarget is undefined, let newTarget be the active
@@ -288,9 +288,9 @@ Object LegacyFormatConstructor(BuiltinArguments args, Isolate* isolate,
  * Segmenter
  */
 template <class T>
-Object DisallowCallConstructor(BuiltinArguments args, Isolate* isolate,
-                               v8::Isolate::UseCounterFeature feature,
-                               const char* method_name) {
+Tagged<Object> DisallowCallConstructor(BuiltinArguments args, Isolate* isolate,
+                                       v8::Isolate::UseCounterFeature feature,
+                                       const char* method_name) {
   isolate->CountUsage(feature);
 
   // 1. If NewTarget is undefined, throw a TypeError exception.
@@ -321,8 +321,9 @@ Object DisallowCallConstructor(BuiltinArguments args, Isolate* isolate,
  * Common code shared by Collator and V8BreakIterator
  */
 template <class T>
-Object CallOrConstructConstructor(BuiltinArguments args, Isolate* isolate,
-                                  const char* method_name) {
+Tagged<Object> CallOrConstructConstructor(BuiltinArguments args,
+                                          Isolate* isolate,
+                                          const char* method_name) {
   Handle<JSReceiver> new_target;
 
   if (IsUndefined(*args.new_target(), isolate)) {
@@ -512,9 +513,8 @@ BUILTIN(NumberFormatInternalFormatNumber) {
 // Common code for NumberFormatPrototypeFormtRange(|ToParts)
 template <class T, MaybeHandle<T> (*F)(Isolate*, Handle<JSNumberFormat>,
                                        Handle<Object>, Handle<Object>)>
-V8_WARN_UNUSED_RESULT Object NumberFormatRange(BuiltinArguments args,
-                                               Isolate* isolate,
-                                               const char* const method_name) {
+V8_WARN_UNUSED_RESULT Tagged<Object> NumberFormatRange(
+    BuiltinArguments args, Isolate* isolate, const char* const method_name) {
   // 1. Let nf be this value.
   // 2. Perform ? RequireInternalSlot(nf, [[InitializedNumberFormat]]).
   CHECK_RECEIVER(JSNumberFormat, nf, method_name);
@@ -965,7 +965,7 @@ BUILTIN(RelativeTimeFormatPrototypeResolvedOptions) {
   return *JSRelativeTimeFormat::ResolvedOptions(isolate, format_holder);
 }
 
-bool IsFastLocale(Object maybe_locale) {
+bool IsFastLocale(Tagged<Object> maybe_locale) {
   DisallowGarbageCollection no_gc;
   if (!IsSeqOneByteString(maybe_locale)) {
     return false;
diff --git a/src/builtins/builtins-object.cc b/src/builtins/builtins-object.cc
index c3f31e313ef..cd8fd50e88c 100644
--- a/src/builtins/builtins-object.cc
+++ b/src/builtins/builtins-object.cc
@@ -59,8 +59,9 @@ BUILTIN(ObjectDefineProperty) {
 namespace {
 
 template <AccessorComponent which_accessor>
-Object ObjectDefineAccessor(Isolate* isolate, Handle<Object> object,
-                            Handle<Object> name, Handle<Object> accessor) {
+Tagged<Object> ObjectDefineAccessor(Isolate* isolate, Handle<Object> object,
+                                    Handle<Object> name,
+                                    Handle<Object> accessor) {
   // 1. Let O be ? ToObject(this value).
   Handle<JSReceiver> receiver;
   ASSIGN_RETURN_FAILURE_ON_EXCEPTION(isolate, receiver,
@@ -100,8 +101,9 @@ Object ObjectDefineAccessor(Isolate* isolate, Handle<Object> object,
   return ReadOnlyRoots(isolate).undefined_value();
 }
 
-Object ObjectLookupAccessor(Isolate* isolate, Handle<Object> object,
-                            Handle<Object> key, AccessorComponent component) {
+Tagged<Object> ObjectLookupAccessor(Isolate* isolate, Handle<Object> object,
+                                    Handle<Object> key,
+                                    AccessorComponent component) {
   ASSIGN_RETURN_FAILURE_ON_EXCEPTION(isolate, object,
                                      Object::ToObject(isolate, object));
   // TODO(jkummerow/verwaest): PropertyKey(..., bool*) performs a
@@ -270,8 +272,8 @@ BUILTIN(ObjectPrototypeSetProto) {
 
 namespace {
 
-Object GetOwnPropertyKeys(Isolate* isolate, BuiltinArguments args,
-                          PropertyFilter filter) {
+Tagged<Object> GetOwnPropertyKeys(Isolate* isolate, BuiltinArguments args,
+                                  PropertyFilter filter) {
   HandleScope scope(isolate);
   Handle<Object> object = args.atOrUndefined(isolate, 1);
   Handle<JSReceiver> receiver;
diff --git a/src/builtins/builtins-sharedarraybuffer.cc b/src/builtins/builtins-sharedarraybuffer.cc
index 159f44b5c2c..9779bec18ac 100644
--- a/src/builtins/builtins-sharedarraybuffer.cc
+++ b/src/builtins/builtins-sharedarraybuffer.cc
@@ -179,9 +179,9 @@ BUILTIN(AtomicsNotify) {
   return FutexEmulation::Wake(array_buffer, wake_addr, c);
 }
 
-Object DoWait(Isolate* isolate, FutexEmulation::WaitMode mode,
-              Handle<Object> array, Handle<Object> index, Handle<Object> value,
-              Handle<Object> timeout) {
+Tagged<Object> DoWait(Isolate* isolate, FutexEmulation::WaitMode mode,
+                      Handle<Object> array, Handle<Object> index,
+                      Handle<Object> value, Handle<Object> timeout) {
   // 1. Let buffer be ? ValidateIntegerTypedArray(typedArray, true).
   Handle<JSTypedArray> sta;
   ASSIGN_RETURN_FAILURE_ON_EXCEPTION(
diff --git a/src/builtins/builtins-symbol.cc b/src/builtins/builtins-symbol.cc
index 5c45af7f663..be8bd19d3b6 100644
--- a/src/builtins/builtins-symbol.cc
+++ b/src/builtins/builtins-symbol.cc
@@ -53,7 +53,7 @@ BUILTIN(SymbolKeyFor) {
   }
   Handle<Symbol> symbol = Handle<Symbol>::cast(obj);
   DisallowGarbageCollection no_gc;
-  Object result;
+  Tagged<Object> result;
   if (symbol->is_in_public_symbol_table()) {
     result = symbol->description();
     DCHECK(IsString(result));
diff --git a/src/builtins/builtins-utils.h b/src/builtins/builtins-utils.h
index c2f9511fb9d..9d9a51f33ba 100644
--- a/src/builtins/builtins-utils.h
+++ b/src/builtins/builtins-utils.h
@@ -26,7 +26,7 @@ class BuiltinArguments : public JavaScriptArguments {
     DCHECK(Object((*at(0)).ptr()).IsObject());
   }
 
-  Object operator[](int index) const {
+  Tagged<Object> operator[](int index) const {
     DCHECK_LT(index, length());
     return Object(*address_of_arg_at(index + kArgsOffset));
   }
@@ -37,7 +37,7 @@ class BuiltinArguments : public JavaScriptArguments {
     return Handle<S>(address_of_arg_at(index + kArgsOffset));
   }
 
-  inline void set_at(int index, Object value) {
+  inline void set_at(int index, Tagged<Object> value) {
     DCHECK_LT(index, length());
     *address_of_arg_at(index + kArgsOffset) = value.ptr();
   }
diff --git a/src/builtins/builtins.cc b/src/builtins/builtins.cc
index bc645b2a899..1085f16075f 100644
--- a/src/builtins/builtins.cc
+++ b/src/builtins/builtins.cc
@@ -179,14 +179,14 @@ FullObjectSlot Builtins::builtin_tier0_slot(Builtin builtin) {
   return FullObjectSlot(location);
 }
 
-void Builtins::set_code(Builtin builtin, Code code) {
+void Builtins::set_code(Builtin builtin, Tagged<Code> code) {
   DCHECK_EQ(builtin, code->builtin_id());
   DCHECK(Internals::HasHeapObjectTag(code.ptr()));
   // The given builtin may be uninitialized thus we cannot check its type here.
   isolate_->builtin_table()[Builtins::ToInt(builtin)] = code.ptr();
 }
 
-Code Builtins::code(Builtin builtin) {
+Tagged<Code> Builtins::code(Builtin builtin) {
   Address ptr = isolate_->builtin_table()[Builtins::ToInt(builtin)];
   return Code::cast(Object(ptr));
 }
@@ -291,7 +291,7 @@ void Builtins::PrintBuiltinCode() {
                      base::CStrVector(v8_flags.print_builtin_code_filter))) {
       CodeTracer::Scope trace_scope(isolate_->GetCodeTracer());
       OFStream os(trace_scope.file());
-      Code builtin_code = code(builtin);
+      Tagged<Code> builtin_code = code(builtin);
       builtin_code->Disassemble(builtin_name, os, isolate_);
       os << "\n";
     }
@@ -305,7 +305,7 @@ void Builtins::PrintBuiltinSize() {
        ++builtin) {
     const char* builtin_name = name(builtin);
     const char* kind = KindNameOf(builtin);
-    Code code = Builtins::code(builtin);
+    Tagged<Code> code = Builtins::code(builtin);
     PrintF(stdout, "%s Builtin, %s, %d\n", kind, builtin_name,
            code->instruction_size());
   }
@@ -318,7 +318,7 @@ Address Builtins::CppEntryOf(Builtin builtin) {
 }
 
 // static
-bool Builtins::IsBuiltin(const Code code) {
+bool Builtins::IsBuiltin(const Tagged<Code> code) {
   return Builtins::IsBuiltinId(code->builtin_id());
 }
 
@@ -334,7 +334,7 @@ bool Builtins::IsBuiltinHandle(Handle<HeapObject> maybe_code,
 }
 
 // static
-bool Builtins::IsIsolateIndependentBuiltin(Code code) {
+bool Builtins::IsIsolateIndependentBuiltin(Tagged<Code> code) {
   Builtin builtin = code->builtin_id();
   return Builtins::IsBuiltinId(builtin) &&
          Builtins::IsIsolateIndependent(builtin);
@@ -397,7 +397,7 @@ Handle<Code> Builtins::CreateInterpreterEntryTrampolineForProfiling(
   DCHECK_NOT_NULL(isolate->embedded_blob_code());
   DCHECK_NE(0, isolate->embedded_blob_code_size());
 
-  Code code = isolate->builtins()->code(
+  Tagged<Code> code = isolate->builtins()->code(
       Builtin::kInterpreterEntryTrampolineForProfiling);
 
   CodeDesc desc;
diff --git a/src/builtins/builtins.h b/src/builtins/builtins.h
index d953321460b..5797cc15cf3 100644
--- a/src/builtins/builtins.h
+++ b/src/builtins/builtins.h
@@ -163,9 +163,9 @@ class Builtins {
   Handle<Code> OrdinaryToPrimitive(OrdinaryToPrimitiveHint hint);
 
   // Used by CreateOffHeapTrampolines in isolate.cc.
-  void set_code(Builtin builtin, Code code);
+  void set_code(Builtin builtin, Tagged<Code> code);
 
-  V8_EXPORT_PRIVATE Code code(Builtin builtin);
+  V8_EXPORT_PRIVATE Tagged<Code> code(Builtin builtin);
   V8_EXPORT_PRIVATE Handle<Code> code_handle(Builtin builtin);
 
   static CallInterfaceDescriptor CallInterfaceDescriptorFor(Builtin builtin);
@@ -193,7 +193,7 @@ class Builtins {
 
   // True, iff the given code object is a builtin. Note that this does not
   // necessarily mean that its kind is InstructionStream::BUILTIN.
-  static bool IsBuiltin(const Code code);
+  static bool IsBuiltin(const Tagged<Code> code);
 
   // As above, but safe to access off the main thread since the check is done
   // by handle location. Similar to Heap::IsRootHandle.
@@ -211,7 +211,7 @@ class Builtins {
   }
 
   // True, iff the given code object is a builtin with off-heap embedded code.
-  static bool IsIsolateIndependentBuiltin(Code code);
+  static bool IsIsolateIndependentBuiltin(Tagged<Code> code);
 
   static void InitializeIsolateDataTables(Isolate* isolate);
 
diff --git a/src/builtins/constants-table-builder.cc b/src/builtins/constants-table-builder.cc
index 8ed0150f1a2..016d417e023 100644
--- a/src/builtins/constants-table-builder.cc
+++ b/src/builtins/constants-table-builder.cc
@@ -115,7 +115,7 @@ void BuiltinsConstantsTableBuilder::Finalize() {
   ConstantsMap::IteratableScope it_scope(&map_);
   for (auto it = it_scope.begin(); it != it_scope.end(); ++it) {
     uint32_t index = *it.entry();
-    Object value = it.key();
+    Tagged<Object> value = it.key();
     if (IsCode(value) && Code::cast(value)->kind() == CodeKind::BUILTIN) {
       // Replace placeholder code objects with the real builtin.
       // See also: SetupIsolateDelegate::PopulateWithPlaceholders.
diff --git a/src/builtins/setup-builtins-internal.cc b/src/builtins/setup-builtins-internal.cc
index dda398f235d..3010364d501 100644
--- a/src/builtins/setup-builtins-internal.cc
+++ b/src/builtins/setup-builtins-internal.cc
@@ -101,9 +101,9 @@ Handle<Code> BuildPlaceholder(Isolate* isolate, Builtin builtin) {
   return scope.CloseAndEscape(code);
 }
 
-Code BuildWithMacroAssembler(Isolate* isolate, Builtin builtin,
-                             MacroAssemblerGenerator generator,
-                             const char* s_name) {
+Tagged<Code> BuildWithMacroAssembler(Isolate* isolate, Builtin builtin,
+                                     MacroAssemblerGenerator generator,
+                                     const char* s_name) {
   HandleScope scope(isolate);
   uint8_t buffer[kBufferSize];
 
@@ -149,8 +149,8 @@ Code BuildWithMacroAssembler(Isolate* isolate, Builtin builtin,
   return *code;
 }
 
-Code BuildAdaptor(Isolate* isolate, Builtin builtin, Address builtin_address,
-                  const char* name) {
+Tagged<Code> BuildAdaptor(Isolate* isolate, Builtin builtin,
+                          Address builtin_address, const char* name) {
   HandleScope scope(isolate);
   uint8_t buffer[kBufferSize];
   MacroAssembler masm(isolate, BuiltinAssemblerOptions(isolate, builtin),
@@ -169,9 +169,9 @@ Code BuildAdaptor(Isolate* isolate, Builtin builtin, Address builtin_address,
 }
 
 // Builder for builtins implemented in TurboFan with JS linkage.
-Code BuildWithCodeStubAssemblerJS(Isolate* isolate, Builtin builtin,
-                                  CodeAssemblerGenerator generator, int argc,
-                                  const char* name) {
+Tagged<Code> BuildWithCodeStubAssemblerJS(Isolate* isolate, Builtin builtin,
+                                          CodeAssemblerGenerator generator,
+                                          int argc, const char* name) {
   HandleScope scope(isolate);
 
   Zone zone(isolate->allocator(), ZONE_NAME, kCompressGraphZone);
@@ -185,10 +185,9 @@ Code BuildWithCodeStubAssemblerJS(Isolate* isolate, Builtin builtin,
 }
 
 // Builder for builtins implemented in TurboFan with CallStub linkage.
-Code BuildWithCodeStubAssemblerCS(Isolate* isolate, Builtin builtin,
-                                  CodeAssemblerGenerator generator,
-                                  CallDescriptors::Key interface_descriptor,
-                                  const char* name) {
+Tagged<Code> BuildWithCodeStubAssemblerCS(
+    Isolate* isolate, Builtin builtin, CodeAssemblerGenerator generator,
+    CallDescriptors::Key interface_descriptor, const char* name) {
   HandleScope scope(isolate);
   Zone zone(isolate->allocator(), ZONE_NAME, kCompressGraphZone);
   // The interface descriptor with given key must be initialized at this point
@@ -209,7 +208,7 @@ Code BuildWithCodeStubAssemblerCS(Isolate* isolate, Builtin builtin,
 
 // static
 void SetupIsolateDelegate::AddBuiltin(Builtins* builtins, Builtin builtin,
-                                      Code code) {
+                                      Tagged<Code> code) {
   DCHECK_EQ(builtin, code->builtin_id());
   builtins->set_code(builtin, code);
 }
@@ -241,28 +240,29 @@ void SetupIsolateDelegate::ReplacePlaceholders(Isolate* isolate) {
   PtrComprCageBase cage_base(isolate);
   for (Builtin builtin = Builtins::kFirst; builtin <= Builtins::kLast;
        ++builtin) {
-    Code code = builtins->code(builtin);
-    InstructionStream istream = code->instruction_stream();
+    Tagged<Code> code = builtins->code(builtin);
+    Tagged<InstructionStream> istream = code->instruction_stream();
     CodePageMemoryModificationScope code_modification_scope(istream);
     bool flush_icache = false;
     for (RelocIterator it(code, kRelocMask); !it.done(); it.next()) {
       RelocInfo* rinfo = it.rinfo();
       if (RelocInfo::IsCodeTargetMode(rinfo->rmode())) {
-        Code target_code = Code::FromTargetAddress(rinfo->target_address());
+        Tagged<Code> target_code =
+            Code::FromTargetAddress(rinfo->target_address());
         DCHECK_IMPLIES(
             RelocInfo::IsRelativeCodeTarget(rinfo->rmode()),
             Builtins::IsIsolateIndependent(target_code->builtin_id()));
         if (!target_code->is_builtin()) continue;
-        Code new_target = builtins->code(target_code->builtin_id());
+        Tagged<Code> new_target = builtins->code(target_code->builtin_id());
         rinfo->set_target_address(istream, new_target->instruction_start(),
                                   UPDATE_WRITE_BARRIER, SKIP_ICACHE_FLUSH);
       } else {
         DCHECK(RelocInfo::IsEmbeddedObjectMode(rinfo->rmode()));
-        Object object = rinfo->target_object(cage_base);
+        Tagged<Object> object = rinfo->target_object(cage_base);
         if (!IsCode(object, cage_base)) continue;
-        Code target = Code::cast(object);
+        Tagged<Code> target = Code::cast(object);
         if (!target->is_builtin()) continue;
-        Code new_target = builtins->code(target->builtin_id());
+        Tagged<Code> new_target = builtins->code(target->builtin_id());
         rinfo->set_target_object(istream, new_target, UPDATE_WRITE_BARRIER,
                                  SKIP_ICACHE_FLUSH);
       }
@@ -277,9 +277,9 @@ void SetupIsolateDelegate::ReplacePlaceholders(Isolate* isolate) {
 
 namespace {
 
-Code GenerateBytecodeHandler(Isolate* isolate, Builtin builtin,
-                             interpreter::OperandScale operand_scale,
-                             interpreter::Bytecode bytecode) {
+Tagged<Code> GenerateBytecodeHandler(Isolate* isolate, Builtin builtin,
+                                     interpreter::OperandScale operand_scale,
+                                     interpreter::Bytecode bytecode) {
   DCHECK(interpreter::Bytecodes::BytecodeHasHandler(bytecode, operand_scale));
   Handle<Code> code = interpreter::GenerateBytecodeHandler(
       isolate, Builtins::name(builtin), bytecode, operand_scale, builtin,
@@ -305,7 +305,7 @@ void SetupIsolateDelegate::SetupBuiltinsInternal(Isolate* isolate) {
   HandleScope scope(isolate);
 
   int index = 0;
-  Code code;
+  Tagged<Code> code;
 #define BUILD_CPP(Name)                                      \
   code = BuildAdaptor(isolate, Builtin::k##Name,             \
                       FUNCTION_ADDR(Builtin_##Name), #Name); \
diff --git a/src/builtins/x64/builtins-x64.cc b/src/builtins/x64/builtins-x64.cc
index e0e898332db..55fa656cb01 100644
--- a/src/builtins/x64/builtins-x64.cc
+++ b/src/builtins/x64/builtins-x64.cc
@@ -1579,7 +1579,7 @@ static void Generate_InterpreterEnterBytecode(MacroAssembler* masm) {
   // Set the return address to the correct point in the interpreter entry
   // trampoline.
   Label builtin_trampoline, trampoline_loaded;
-  Smi interpreter_entry_return_pc_offset(
+  Tagged<Smi> interpreter_entry_return_pc_offset(
       masm->isolate()->heap()->interpreter_entry_return_pc_offset());
   DCHECK_NE(interpreter_entry_return_pc_offset, Smi::zero());
 
diff --git a/src/codegen/arm/assembler-arm-inl.h b/src/codegen/arm/assembler-arm-inl.h
index 5a7e14facd2..2021142b910 100644
--- a/src/codegen/arm/assembler-arm-inl.h
+++ b/src/codegen/arm/assembler-arm-inl.h
@@ -91,7 +91,7 @@ Address RelocInfo::constant_pool_entry_address() {
 
 int RelocInfo::target_address_size() { return kPointerSize; }
 
-HeapObject RelocInfo::target_object(PtrComprCageBase cage_base) {
+Tagged<HeapObject> RelocInfo::target_object(PtrComprCageBase cage_base) {
   DCHECK(IsCodeTarget(rmode_) || IsFullEmbeddedObject(rmode_));
   return HeapObject::cast(
       Object(Assembler::target_address_at(pc_, constant_pool_)));
@@ -106,7 +106,7 @@ Handle<HeapObject> RelocInfo::target_object_handle(Assembler* origin) {
   return origin->relative_code_target_object_handle_at(pc_);
 }
 
-void RelocInfo::set_target_object(HeapObject target,
+void RelocInfo::set_target_object(Tagged<HeapObject> target,
                                   ICacheFlushMode icache_flush_mode) {
   DCHECK(IsCodeTarget(rmode_) || IsFullEmbeddedObject(rmode_));
   Assembler::set_target_address_at(pc_, constant_pool_, target.ptr(),
@@ -167,7 +167,7 @@ Operand::Operand(const ExternalReference& f)
   value_.immediate = static_cast<int32_t>(f.address());
 }
 
-Operand::Operand(Smi value) : rmode_(RelocInfo::NO_INFO) {
+Operand::Operand(Tagged<Smi> value) : rmode_(RelocInfo::NO_INFO) {
   value_.immediate = static_cast<intptr_t>(value.ptr());
 }
 
@@ -187,7 +187,7 @@ void Assembler::emit(Instr x) {
 }
 
 void Assembler::deserialization_set_special_target_at(
-    Address constant_pool_entry, Code code, Address target) {
+    Address constant_pool_entry, Tagged<Code> code, Address target) {
   DCHECK(!Builtins::IsIsolateIndependentBuiltin(code));
   Memory<Address>(constant_pool_entry) = target;
 }
diff --git a/src/codegen/arm/assembler-arm.h b/src/codegen/arm/assembler-arm.h
index 8437ad63f9e..4f7fa673ddb 100644
--- a/src/codegen/arm/assembler-arm.h
+++ b/src/codegen/arm/assembler-arm.h
@@ -92,7 +92,7 @@ class V8_EXPORT_PRIVATE Operand {
   V8_INLINE static Operand Zero();
   V8_INLINE explicit Operand(const ExternalReference& f);
   explicit Operand(Handle<HeapObject> handle);
-  V8_INLINE explicit Operand(Smi value);
+  V8_INLINE explicit Operand(Tagged<Smi> value);
 
   // rm
   V8_INLINE explicit Operand(Register rm);
@@ -371,7 +371,7 @@ class V8_EXPORT_PRIVATE Assembler : public AssemblerBase {
   // This sets the branch destination (which is in the constant pool on ARM).
   // This is for calls and branches within generated code.
   inline static void deserialization_set_special_target_at(
-      Address constant_pool_entry, Code code, Address target);
+      Address constant_pool_entry, Tagged<Code> code, Address target);
 
   // Get the size of the special target encoded at 'location'.
   inline static int deserialization_special_target_size(Address location);
diff --git a/src/codegen/arm/macro-assembler-arm.cc b/src/codegen/arm/macro-assembler-arm.cc
index cfea5fb9b3a..6ab47b8f629 100644
--- a/src/codegen/arm/macro-assembler-arm.cc
+++ b/src/codegen/arm/macro-assembler-arm.cc
@@ -422,14 +422,14 @@ void MacroAssembler::Push(Handle<HeapObject> handle) {
   push(scratch);
 }
 
-void MacroAssembler::Push(Smi smi) {
+void MacroAssembler::Push(Tagged<Smi> smi) {
   UseScratchRegisterScope temps(this);
   Register scratch = temps.Acquire();
   mov(scratch, Operand(smi));
   push(scratch);
 }
 
-void MacroAssembler::Push(TaggedIndex index) {
+void MacroAssembler::Push(Tagged<TaggedIndex> index) {
   // TaggedIndex is the same as Smi for 32 bit archs.
   Push(Smi::FromIntptr(index.value()));
 }
@@ -464,7 +464,9 @@ void MacroAssembler::PushArray(Register array, Register size, Register scratch,
   }
 }
 
-void MacroAssembler::Move(Register dst, Smi smi) { mov(dst, Operand(smi)); }
+void MacroAssembler::Move(Register dst, Tagged<Smi> smi) {
+  mov(dst, Operand(smi));
+}
 
 void MacroAssembler::Move(Register dst, Handle<HeapObject> value) {
   // TODO(jgruber,v8:8887): Also consider a root-relative load when generating
diff --git a/src/codegen/arm/macro-assembler-arm.h b/src/codegen/arm/macro-assembler-arm.h
index 6f23370ee9a..2bf55c94b8f 100644
--- a/src/codegen/arm/macro-assembler-arm.h
+++ b/src/codegen/arm/macro-assembler-arm.h
@@ -94,8 +94,8 @@ class V8_EXPORT_PRIVATE MacroAssembler : public MacroAssemblerBase {
   void Push(Register src) { push(src); }
 
   void Push(Handle<HeapObject> handle);
-  void Push(Smi smi);
-  void Push(TaggedIndex index);
+  void Push(Tagged<Smi> smi);
+  void Push(Tagged<TaggedIndex> index);
 
   // Push two registers.  Pushes leftmost register first (to highest address).
   void Push(Register src1, Register src2, Condition cond = al) {
@@ -501,7 +501,7 @@ class V8_EXPORT_PRIVATE MacroAssembler : public MacroAssemblerBase {
                  NeonMemOperand dst);
 
   // Register move. May do nothing if the registers are identical.
-  void Move(Register dst, Smi smi);
+  void Move(Register dst, Tagged<Smi> smi);
   void Move(Register dst, Handle<HeapObject> value);
   void Move(Register dst, ExternalReference reference);
   void Move(Register dst, Register src, Condition cond = al);
diff --git a/src/codegen/arm64/assembler-arm64-inl.h b/src/codegen/arm64/assembler-arm64-inl.h
index 22af82d3fe0..4fcf4782570 100644
--- a/src/codegen/arm64/assembler-arm64-inl.h
+++ b/src/codegen/arm64/assembler-arm64-inl.h
@@ -213,11 +213,6 @@ struct ImmediateInitializer<Tagged<Smi>> {
   }
 };
 
-template <>
-struct ImmediateInitializer<Smi> : ImmediateInitializer<Tagged<Smi>> {
-  static_assert(kTaggedCanConvertToRawObjects);
-};
-
 template <>
 struct ImmediateInitializer<ExternalReference> {
   static inline RelocInfo::Mode rmode_for(ExternalReference t) {
@@ -557,7 +552,7 @@ int Assembler::deserialization_special_target_size(Address location) {
 }
 
 void Assembler::deserialization_set_special_target_at(Address location,
-                                                      Code code,
+                                                      Tagged<Code> code,
                                                       Address target) {
   Instruction* instr = reinterpret_cast<Instruction*>(location);
   if (instr->IsBranchAndLink() || instr->IsUnconditionalBranch()) {
@@ -664,13 +659,13 @@ Address RelocInfo::constant_pool_entry_address() {
   return Assembler::target_pointer_address_at(pc_);
 }
 
-HeapObject RelocInfo::target_object(PtrComprCageBase cage_base) {
+Tagged<HeapObject> RelocInfo::target_object(PtrComprCageBase cage_base) {
   DCHECK(IsCodeTarget(rmode_) || IsEmbeddedObjectMode(rmode_));
   if (IsCompressedEmbeddedObject(rmode_)) {
     Tagged_t compressed =
         Assembler::target_compressed_address_at(pc_, constant_pool_);
     DCHECK(!HAS_SMI_TAG(compressed));
-    Object obj(
+    Tagged<Object> obj(
         V8HeapCompressionScheme::DecompressTagged(cage_base, compressed));
     // Embedding of compressed InstructionStream objects must not happen when
     // external code space is enabled, because Codes must be used
@@ -693,7 +688,7 @@ Handle<HeapObject> RelocInfo::target_object_handle(Assembler* origin) {
   }
 }
 
-void RelocInfo::set_target_object(HeapObject target,
+void RelocInfo::set_target_object(Tagged<HeapObject> target,
                                   ICacheFlushMode icache_flush_mode) {
   DCHECK(IsCodeTarget(rmode_) || IsEmbeddedObjectMode(rmode_));
   if (IsCompressedEmbeddedObject(rmode_)) {
diff --git a/src/codegen/arm64/assembler-arm64.h b/src/codegen/arm64/assembler-arm64.h
index 69a705e08d9..fea82ed1f7d 100644
--- a/src/codegen/arm64/assembler-arm64.h
+++ b/src/codegen/arm64/assembler-arm64.h
@@ -280,7 +280,7 @@ class V8_EXPORT_PRIVATE Assembler : public AssemblerBase {
   // an immediate branch or the address of an entry in the constant pool.
   // This is for calls and branches within generated code.
   inline static void deserialization_set_special_target_at(Address location,
-                                                           Code code,
+                                                           Tagged<Code> code,
                                                            Address target);
 
   // Get the size of the special target encoded at 'location'.
diff --git a/src/codegen/assembler.h b/src/codegen/assembler.h
index bd4fba814b1..ff7c1d42695 100644
--- a/src/codegen/assembler.h
+++ b/src/codegen/assembler.h
@@ -322,7 +322,7 @@ class V8_EXPORT_PRIVATE AssemblerBase : public Malloced {
 
   // Overwrite a host NaN with a quiet target NaN.  Used by mksnapshot for
   // cross-snapshotting.
-  static void QuietNaN(HeapObject nan) {}
+  static void QuietNaN(Tagged<HeapObject> nan) {}
 
   int pc_offset() const { return static_cast<int>(pc_ - buffer_start_); }
 
diff --git a/src/codegen/code-stub-assembler.cc b/src/codegen/code-stub-assembler.cc
index 411c63f0f0e..f727230b47f 100644
--- a/src/codegen/code-stub-assembler.cc
+++ b/src/codegen/code-stub-assembler.cc
@@ -241,8 +241,8 @@ TNode<Boolean> CodeStubAssembler::SelectBooleanConstant(
 }
 
 TNode<Smi> CodeStubAssembler::SelectSmiConstant(TNode<BoolT> condition,
-                                                Smi true_value,
-                                                Smi false_value) {
+                                                Tagged<Smi> true_value,
+                                                Tagged<Smi> false_value) {
   return SelectConstant<Smi>(condition, SmiConstant(true_value),
                              SmiConstant(false_value));
 }
@@ -314,7 +314,7 @@ TNode<RawPtrT> CodeStubAssembler::IntPtrOrSmiConstant<RawPtrT>(int value) {
 
 bool CodeStubAssembler::TryGetIntPtrOrSmiConstantValue(
     TNode<Smi> maybe_constant, int* value) {
-  Smi smi_constant;
+  Tagged<Smi> smi_constant;
   if (TryToSmiConstant(maybe_constant, &smi_constant)) {
     *value = Smi::ToInt(smi_constant);
     return true;
@@ -2595,7 +2595,7 @@ void CodeStubAssembler::FixedArrayBoundsCheck(TNode<FixedArrayBase> array,
   if (!v8_flags.fixed_array_bounds_checks) return;
   DCHECK(IsAligned(additional_offset, kTaggedSize));
   TNode<Smi> effective_index;
-  Smi constant_index;
+  Tagged<Smi> constant_index;
   bool index_is_constant = TryToSmiConstant(index, &constant_index);
   if (index_is_constant) {
     effective_index = SmiConstant(Smi::ToInt(constant_index) +
@@ -11297,7 +11297,7 @@ TNode<IntPtrT> CodeStubAssembler::ElementOffsetFromIndex(
     TNode<Smi> smi_index_node = ReinterpretCast<Smi>(index_node);
     int const kSmiShiftBits = kSmiShiftSize + kSmiTagSize;
     element_size_shift -= kSmiShiftBits;
-    Smi smi_index;
+    Tagged<Smi> smi_index;
     constant_index = TryToSmiConstant(smi_index_node, &smi_index);
     if (constant_index) {
       index = smi_index.value();
diff --git a/src/codegen/code-stub-assembler.h b/src/codegen/code-stub-assembler.h
index 4cc1c42e255..a42880457b9 100644
--- a/src/codegen/code-stub-assembler.h
+++ b/src/codegen/code-stub-assembler.h
@@ -980,13 +980,13 @@ class V8_EXPORT_PRIVATE CodeStubAssembler
   TNode<IntPtrT> SelectIntPtrConstant(TNode<BoolT> condition, int true_value,
                                       int false_value);
   TNode<Boolean> SelectBooleanConstant(TNode<BoolT> condition);
-  TNode<Smi> SelectSmiConstant(TNode<BoolT> condition, Smi true_value,
-                               Smi false_value);
+  TNode<Smi> SelectSmiConstant(TNode<BoolT> condition, Tagged<Smi> true_value,
+                               Tagged<Smi> false_value);
   TNode<Smi> SelectSmiConstant(TNode<BoolT> condition, int true_value,
-                               Smi false_value) {
+                               Tagged<Smi> false_value) {
     return SelectSmiConstant(condition, Smi::FromInt(true_value), false_value);
   }
-  TNode<Smi> SelectSmiConstant(TNode<BoolT> condition, Smi true_value,
+  TNode<Smi> SelectSmiConstant(TNode<BoolT> condition, Tagged<Smi> true_value,
                                int false_value) {
     return SelectSmiConstant(condition, true_value, Smi::FromInt(false_value));
   }
diff --git a/src/codegen/compilation-cache.cc b/src/codegen/compilation-cache.cc
index 5433de6c58e..f7147eab10c 100644
--- a/src/codegen/compilation-cache.cc
+++ b/src/codegen/compilation-cache.cc
@@ -43,7 +43,7 @@ Handle<CompilationCacheTable> CompilationCacheRegExp::GetTable(int generation) {
     result = CompilationCacheTable::New(isolate(), kInitialCacheSize);
     tables_[generation] = *result;
   } else {
-    CompilationCacheTable table =
+    Tagged<CompilationCacheTable> table =
         CompilationCacheTable::cast(tables_[generation]);
     result = Handle<CompilationCacheTable>(table, isolate());
   }
@@ -65,16 +65,16 @@ void CompilationCacheRegExp::Age() {
 void CompilationCacheScript::Age() {
   DisallowGarbageCollection no_gc;
   if (IsUndefined(table_, isolate())) return;
-  CompilationCacheTable table = CompilationCacheTable::cast(table_);
+  Tagged<CompilationCacheTable> table = CompilationCacheTable::cast(table_);
 
   for (InternalIndex entry : table->IterateEntries()) {
-    Object key;
+    Tagged<Object> key;
     if (!table->ToKey(isolate(), entry, &key)) continue;
     DCHECK(IsWeakFixedArray(key));
 
-    Object value = table->PrimaryValueAt(entry);
+    Tagged<Object> value = table->PrimaryValueAt(entry);
     if (!IsUndefined(value, isolate())) {
-      SharedFunctionInfo info = SharedFunctionInfo::cast(value);
+      Tagged<SharedFunctionInfo> info = SharedFunctionInfo::cast(value);
       // Clear entries after Bytecode was flushed from SFI.
       if (!info->HasBytecodeArray()) {
         table->SetPrimaryValueAt(entry,
@@ -88,10 +88,10 @@ void CompilationCacheScript::Age() {
 void CompilationCacheEval::Age() {
   DisallowGarbageCollection no_gc;
   if (IsUndefined(table_, isolate())) return;
-  CompilationCacheTable table = CompilationCacheTable::cast(table_);
+  Tagged<CompilationCacheTable> table = CompilationCacheTable::cast(table_);
 
   for (InternalIndex entry : table->IterateEntries()) {
-    Object key;
+    Tagged<Object> key;
     if (!table->ToKey(isolate(), entry, &key)) continue;
 
     if (IsNumber(key, isolate())) {
@@ -114,7 +114,7 @@ void CompilationCacheEval::Age() {
     } else {
       DCHECK(IsFixedArray(key));
       // The ageing mechanism for eval caches.
-      SharedFunctionInfo info =
+      Tagged<SharedFunctionInfo> info =
           SharedFunctionInfo::cast(table->PrimaryValueAt(entry));
       // Clear entries after Bytecode was flushed from SFI.
       if (!info->HasBytecodeArray()) {
diff --git a/src/codegen/compiler.cc b/src/codegen/compiler.cc
index 7b50427a909..972898e2a95 100644
--- a/src/codegen/compiler.cc
+++ b/src/codegen/compiler.cc
@@ -1726,42 +1726,44 @@ class MergeAssumptionChecker final : public ObjectVisitor {
   explicit MergeAssumptionChecker(PtrComprCageBase cage_base)
       : cage_base_(cage_base) {}
 
-  void IterateObjects(HeapObject start) {
+  void IterateObjects(Tagged<HeapObject> start) {
     QueueVisit(start, kNormalObject);
     while (to_visit_.size() > 0) {
       std::pair<HeapObject, ObjectKind> pair = to_visit_.top();
       to_visit_.pop();
-      HeapObject current = pair.first;
+      Tagged<HeapObject> current = pair.first;
       // The Script's shared_function_infos list and the constant pools for all
       // BytecodeArrays are expected to contain pointers to SharedFunctionInfos.
       // However, the type of those objects (FixedArray or WeakFixedArray)
       // doesn't have enough information to indicate their usage, so we enqueue
       // those objects here rather than during VisitPointers.
       if (IsScript(current)) {
-        HeapObject sfis = Script::cast(current).shared_function_infos();
+        Tagged<HeapObject> sfis =
+            Script::cast(current)->shared_function_infos();
         QueueVisit(sfis, kScriptSfiList);
       } else if (IsBytecodeArray(current)) {
-        HeapObject constants = BytecodeArray::cast(current).constant_pool();
+        Tagged<HeapObject> constants =
+            BytecodeArray::cast(current)->constant_pool();
         QueueVisit(constants, kConstantPool);
       }
       current_object_kind_ = pair.second;
-      current.IterateBody(cage_base_, this);
-      QueueVisit(current.map(), kNormalObject);
+      current->IterateBody(cage_base_, this);
+      QueueVisit(current->map(), kNormalObject);
     }
   }
 
   // ObjectVisitor implementation:
-  void VisitPointers(HeapObject host, ObjectSlot start,
+  void VisitPointers(Tagged<HeapObject> host, ObjectSlot start,
                      ObjectSlot end) override {
     MaybeObjectSlot maybe_start(start);
     MaybeObjectSlot maybe_end(end);
     VisitPointers(host, maybe_start, maybe_end);
   }
-  void VisitPointers(HeapObject host, MaybeObjectSlot start,
+  void VisitPointers(Tagged<HeapObject> host, MaybeObjectSlot start,
                      MaybeObjectSlot end) override {
     for (MaybeObjectSlot current = start; current != end; ++current) {
       MaybeObject maybe_obj = current.load(cage_base_);
-      HeapObject obj;
+      Tagged<HeapObject> obj;
       bool is_weak = maybe_obj.IsWeak();
       if (maybe_obj.GetHeapObject(&obj)) {
         if (IsSharedFunctionInfo(obj)) {
@@ -1785,14 +1787,16 @@ class MergeAssumptionChecker final : public ObjectVisitor {
   // The object graph for a newly compiled Script shouldn't yet contain any
   // Code. If any of these functions are called, then that would indicate that
   // the graph was not disjoint from the rest of the heap as expected.
-  void VisitInstructionStreamPointer(Code host,
+  void VisitInstructionStreamPointer(Tagged<Code> host,
                                      InstructionStreamSlot slot) override {
     UNREACHABLE();
   }
-  void VisitCodeTarget(InstructionStream host, RelocInfo* rinfo) override {
+  void VisitCodeTarget(Tagged<InstructionStream> host,
+                       RelocInfo* rinfo) override {
     UNREACHABLE();
   }
-  void VisitEmbeddedPointer(InstructionStream host, RelocInfo* rinfo) override {
+  void VisitEmbeddedPointer(Tagged<InstructionStream> host,
+                            RelocInfo* rinfo) override {
     UNREACHABLE();
   }
 
@@ -1805,7 +1809,7 @@ class MergeAssumptionChecker final : public ObjectVisitor {
 
   // If the object hasn't yet been added to the worklist, add it. Subsequent
   // calls with the same object have no effect, even if kind is different.
-  void QueueVisit(HeapObject obj, ObjectKind kind) {
+  void QueueVisit(Tagged<HeapObject> obj, ObjectKind kind) {
     if (visited_.insert(obj).second) {
       to_visit_.push(std::make_pair(obj, kind));
     }
@@ -2002,7 +2006,7 @@ class ConstantPoolPointerForwarder {
     for (int i = 0, length = constant_pool->length(); i < length; ++i) {
       Object obj = constant_pool->get(i);
       if (IsSmi(obj)) continue;
-      HeapObject heap_obj = HeapObject::cast(obj);
+      Tagged<HeapObject> heap_obj = HeapObject::cast(obj);
       if (IsFixedArray(heap_obj, cage_base_)) {
         // Constant pools can have nested fixed arrays, but such relationships
         // are acyclic and never more than a few layers deep, so recursion is
diff --git a/src/codegen/external-reference.cc b/src/codegen/external-reference.cc
index 7637d5f68a9..1163601c26f 100644
--- a/src/codegen/external-reference.cc
+++ b/src/codegen/external-reference.cc
@@ -367,13 +367,13 @@ namespace {
 
 intptr_t DebugBreakAtEntry(Isolate* isolate, Address raw_sfi) {
   DisallowGarbageCollection no_gc;
-  SharedFunctionInfo sfi = SharedFunctionInfo::cast(Object(raw_sfi));
+  Tagged<SharedFunctionInfo> sfi = SharedFunctionInfo::cast(Object(raw_sfi));
   return isolate->debug()->BreakAtEntry(sfi) ? 1 : 0;
 }
 
 Address DebugGetCoverageInfo(Isolate* isolate, Address raw_sfi) {
   DisallowGarbageCollection no_gc;
-  SharedFunctionInfo sfi = SharedFunctionInfo::cast(Object(raw_sfi));
+  Tagged<SharedFunctionInfo> sfi = SharedFunctionInfo::cast(Object(raw_sfi));
   base::Optional<DebugInfo> debug_info = isolate->debug()->TryGetDebugInfo(sfi);
   if (debug_info.has_value() && debug_info->HasCoverageInfo()) {
     return debug_info->coverage_info().ptr();
@@ -796,8 +796,8 @@ namespace {
 static uintptr_t BaselinePCForBytecodeOffset(Address raw_code_obj,
                                              int bytecode_offset,
                                              Address raw_bytecode_array) {
-  Code code_obj = Code::cast(Object(raw_code_obj));
-  BytecodeArray bytecode_array =
+  Tagged<Code> code_obj = Code::cast(Object(raw_code_obj));
+  Tagged<BytecodeArray> bytecode_array =
       BytecodeArray::cast(Object(raw_bytecode_array));
   return code_obj->GetBaselineStartPCForBytecodeOffset(bytecode_offset,
                                                        bytecode_array);
@@ -806,8 +806,8 @@ static uintptr_t BaselinePCForBytecodeOffset(Address raw_code_obj,
 static uintptr_t BaselinePCForNextExecutedBytecode(Address raw_code_obj,
                                                    int bytecode_offset,
                                                    Address raw_bytecode_array) {
-  Code code_obj = Code::cast(Object(raw_code_obj));
-  BytecodeArray bytecode_array =
+  Tagged<Code> code_obj = Code::cast(Object(raw_code_obj));
+  Tagged<BytecodeArray> bytecode_array =
       BytecodeArray::cast(Object(raw_bytecode_array));
   return code_obj->GetBaselinePCForNextExecutedBytecode(bytecode_offset,
                                                         bytecode_array);
@@ -1154,7 +1154,7 @@ Address GetOrCreateHash(Isolate* isolate, Address raw_key) {
 FUNCTION_REFERENCE(get_or_create_hash_raw, GetOrCreateHash)
 
 static Address JSReceiverCreateIdentityHash(Isolate* isolate, Address raw_key) {
-  JSReceiver key = JSReceiver::cast(Object(raw_key));
+  Tagged<JSReceiver> key = JSReceiver::cast(Object(raw_key));
   return JSReceiver::CreateIdentityHash(isolate, key).ptr();
 }
 
@@ -1229,8 +1229,8 @@ FUNCTION_REFERENCE(array_indexof_includes_double, ArrayIndexOfIncludesDouble)
 
 static Address LexicographicCompareWrapper(Isolate* isolate, Address smi_x,
                                            Address smi_y) {
-  Smi x(smi_x);
-  Smi y(smi_y);
+  Tagged<Smi> x(smi_x);
+  Tagged<Smi> y(smi_y);
   return Smi::LexicographicCompare(isolate, x, y);
 }
 
@@ -1314,8 +1314,8 @@ FUNCTION_REFERENCE(check_object_type, CheckObjectType)
 #ifdef V8_INTL_SUPPORT
 
 static Address ConvertOneByteToLower(Address raw_src, Address raw_dst) {
-  String src = String::cast(Object(raw_src));
-  String dst = String::cast(Object(raw_dst));
+  Tagged<String> src = String::cast(Object(raw_src));
+  Tagged<String> dst = String::cast(Object(raw_dst));
   return Intl::ConvertOneByteToLower(src, dst).ptr();
 }
 FUNCTION_REFERENCE(intl_convert_one_byte_to_lower, ConvertOneByteToLower)
@@ -1386,7 +1386,7 @@ ExternalReference ExternalReference::runtime_function_table_address(
 }
 
 static Address InvalidatePrototypeChainsWrapper(Address raw_map) {
-  Map map = Map::cast(Object(raw_map));
+  Tagged<Map> map = Map::cast(Object(raw_map));
   return JSObject::InvalidatePrototypeChains(map).ptr();
 }
 
@@ -1675,7 +1675,7 @@ IF_TSAN(FUNCTION_REFERENCE, tsan_relaxed_load_function_64_bits,
 
 static int EnterMicrotaskContextWrapper(HandleScopeImplementer* hsi,
                                         Address raw_context) {
-  NativeContext context = NativeContext::cast(Object(raw_context));
+  Tagged<NativeContext> context = NativeContext::cast(Object(raw_context));
   hsi->EnterMicrotaskContext(context);
   return 0;
 }
diff --git a/src/codegen/handler-table.cc b/src/codegen/handler-table.cc
index bb8d62db7ff..e8f1e2e8ede 100644
--- a/src/codegen/handler-table.cc
+++ b/src/codegen/handler-table.cc
@@ -19,7 +19,7 @@
 namespace v8 {
 namespace internal {
 
-HandlerTable::HandlerTable(Code code)
+HandlerTable::HandlerTable(Tagged<Code> code)
     : HandlerTable(code->handler_table_address(), code->handler_table_size(),
                    kReturnAddressBasedEncoding) {}
 
@@ -29,10 +29,10 @@ HandlerTable::HandlerTable(const wasm::WasmCode* code)
                    kReturnAddressBasedEncoding) {}
 #endif  // V8_ENABLE_WEBASSEMBLY
 
-HandlerTable::HandlerTable(BytecodeArray bytecode_array)
+HandlerTable::HandlerTable(Tagged<BytecodeArray> bytecode_array)
     : HandlerTable(bytecode_array->handler_table()) {}
 
-HandlerTable::HandlerTable(ByteArray byte_array)
+HandlerTable::HandlerTable(Tagged<ByteArray> byte_array)
     : HandlerTable(reinterpret_cast<Address>(byte_array->GetDataStartAddress()),
                    byte_array->length(), kRangeBasedEncoding) {}
 
diff --git a/src/codegen/handler-table.h b/src/codegen/handler-table.h
index 7a4554796b4..55affff82fa 100644
--- a/src/codegen/handler-table.h
+++ b/src/codegen/handler-table.h
@@ -55,13 +55,13 @@ class V8_EXPORT_PRIVATE HandlerTable {
   enum EncodingMode { kRangeBasedEncoding, kReturnAddressBasedEncoding };
 
   // Constructors for the various encodings.
-  explicit HandlerTable(InstructionStream code);
-  explicit HandlerTable(Code code);
-  explicit HandlerTable(ByteArray byte_array);
+  explicit HandlerTable(Tagged<InstructionStream> code);
+  explicit HandlerTable(Tagged<Code> code);
+  explicit HandlerTable(Tagged<ByteArray> byte_array);
 #if V8_ENABLE_WEBASSEMBLY
   explicit HandlerTable(const wasm::WasmCode* code);
 #endif  // V8_ENABLE_WEBASSEMBLY
-  explicit HandlerTable(BytecodeArray bytecode_array);
+  explicit HandlerTable(Tagged<BytecodeArray> bytecode_array);
   HandlerTable(Address handler_table, int handler_table_size,
                EncodingMode encoding_mode);
 
diff --git a/src/codegen/ia32/assembler-ia32-inl.h b/src/codegen/ia32/assembler-ia32-inl.h
index 92113770f97..359489e858a 100644
--- a/src/codegen/ia32/assembler-ia32-inl.h
+++ b/src/codegen/ia32/assembler-ia32-inl.h
@@ -80,7 +80,7 @@ Address RelocInfo::constant_pool_entry_address() { UNREACHABLE(); }
 
 int RelocInfo::target_address_size() { return Assembler::kSpecialTargetSize; }
 
-HeapObject RelocInfo::target_object(PtrComprCageBase cage_base) {
+Tagged<HeapObject> RelocInfo::target_object(PtrComprCageBase cage_base) {
   DCHECK(IsCodeTarget(rmode_) || IsFullEmbeddedObject(rmode_));
   return HeapObject::cast(Object(ReadUnalignedValue<Address>(pc_)));
 }
@@ -90,7 +90,7 @@ Handle<HeapObject> RelocInfo::target_object_handle(Assembler* origin) {
   return Handle<HeapObject>::cast(ReadUnalignedValue<Handle<Object>>(pc_));
 }
 
-void RelocInfo::set_target_object(HeapObject target,
+void RelocInfo::set_target_object(Tagged<HeapObject> target,
                                   ICacheFlushMode icache_flush_mode) {
   DCHECK(IsCodeTarget(rmode_) || IsFullEmbeddedObject(rmode_));
   WriteUnalignedValue(pc_, target.ptr());
@@ -220,7 +220,7 @@ void Assembler::set_target_address_at(Address pc, Address constant_pool,
 }
 
 void Assembler::deserialization_set_special_target_at(
-    Address instruction_payload, Code code, Address target) {
+    Address instruction_payload, Tagged<Code> code, Address target) {
   set_target_address_at(instruction_payload,
                         !code.is_null() ? code->constant_pool() : kNullAddress,
                         target);
diff --git a/src/codegen/ia32/assembler-ia32.h b/src/codegen/ia32/assembler-ia32.h
index 5d9f17cf6b7..cbb94612582 100644
--- a/src/codegen/ia32/assembler-ia32.h
+++ b/src/codegen/ia32/assembler-ia32.h
@@ -124,7 +124,7 @@ class Immediate {
       : Immediate(ext.address(), RelocInfo::EXTERNAL_REFERENCE) {}
   inline explicit Immediate(Handle<HeapObject> handle)
       : Immediate(handle.address(), RelocInfo::FULL_EMBEDDED_OBJECT) {}
-  inline explicit Immediate(Smi value)
+  inline explicit Immediate(Tagged<Smi> value)
       : Immediate(static_cast<intptr_t>(value.ptr())) {}
 
   static Immediate EmbeddedNumber(double number);  // Smi or HeapNumber.
@@ -423,7 +423,7 @@ class V8_EXPORT_PRIVATE Assembler : public AssemblerBase {
   // This sets the branch destination (which is in the instruction on x86).
   // This is for calls and branches within generated code.
   inline static void deserialization_set_special_target_at(
-      Address instruction_payload, Code code, Address target);
+      Address instruction_payload, Tagged<Code> code, Address target);
 
   // Get the size of the special target encoded at 'instruction_payload'.
   inline static int deserialization_special_target_size(
diff --git a/src/codegen/ia32/macro-assembler-ia32.h b/src/codegen/ia32/macro-assembler-ia32.h
index fa7eec6bfde..8935d82c952 100644
--- a/src/codegen/ia32/macro-assembler-ia32.h
+++ b/src/codegen/ia32/macro-assembler-ia32.h
@@ -129,7 +129,7 @@ class V8_EXPORT_PRIVATE MacroAssembler
     }
   }
   void Move(Register dst, const Immediate& src);
-  void Move(Register dst, Smi src) { Move(dst, Immediate(src)); }
+  void Move(Register dst, Tagged<Smi> src) { Move(dst, Immediate(src)); }
   void Move(Register dst, Handle<HeapObject> src);
   void Move(Register dst, Register src);
   void Move(Register dst, Operand src);
@@ -361,7 +361,7 @@ class V8_EXPORT_PRIVATE MacroAssembler
   void Push(Operand src) { push(src); }
   void Push(Immediate value);
   void Push(Handle<HeapObject> handle) { push(Immediate(handle)); }
-  void Push(Smi smi) { Push(Immediate(smi)); }
+  void Push(Tagged<Smi> smi) { Push(Immediate(smi)); }
   void Push(XMMRegister src, Register scratch) {
     movd(scratch, src);
     push(scratch);
diff --git a/src/codegen/macro-assembler-base.cc b/src/codegen/macro-assembler-base.cc
index 86041441606..2e657dd3b89 100644
--- a/src/codegen/macro-assembler-base.cc
+++ b/src/codegen/macro-assembler-base.cc
@@ -130,7 +130,7 @@ bool MacroAssemblerBase::IsAddressableThroughRootRegister(
 Tagged_t MacroAssemblerBase::ReadOnlyRootPtr(RootIndex index,
                                              Isolate* isolate) {
   DCHECK(CanBeImmediate(index));
-  Object obj = isolate->root(index);
+  Tagged<Object> obj = isolate->root(index);
   CHECK(IsHeapObject(obj));
   return V8HeapCompressionScheme::CompressObject(obj.ptr());
 }
diff --git a/src/codegen/maglev-safepoint-table.cc b/src/codegen/maglev-safepoint-table.cc
index 629ead3b3b2..934cc2a19fe 100644
--- a/src/codegen/maglev-safepoint-table.cc
+++ b/src/codegen/maglev-safepoint-table.cc
@@ -13,14 +13,14 @@ namespace v8 {
 namespace internal {
 
 MaglevSafepointTable::MaglevSafepointTable(Isolate* isolate, Address pc,
-                                           Code code)
+                                           Tagged<Code> code)
     : MaglevSafepointTable(code->InstructionStart(isolate, pc),
                            code->safepoint_table_address()) {
   DCHECK(code->is_maglevved());
 }
 
 MaglevSafepointTable::MaglevSafepointTable(Isolate* isolate, Address pc,
-                                           GcSafeCode code)
+                                           Tagged<GcSafeCode> code)
     : MaglevSafepointTable(code->InstructionStart(isolate, pc),
                            code->safepoint_table_address()) {
   DCHECK(code->is_maglevved());
@@ -83,7 +83,7 @@ MaglevSafepointEntry MaglevSafepointTable::FindEntry(Address pc) const {
 
 // static
 MaglevSafepointEntry MaglevSafepointTable::FindEntry(Isolate* isolate,
-                                                     GcSafeCode code,
+                                                     Tagged<GcSafeCode> code,
                                                      Address pc) {
   MaglevSafepointTable table(isolate, pc, code);
   return table.FindEntry(pc);
diff --git a/src/codegen/maglev-safepoint-table.h b/src/codegen/maglev-safepoint-table.h
index d08231ea578..c134acc3c7f 100644
--- a/src/codegen/maglev-safepoint-table.h
+++ b/src/codegen/maglev-safepoint-table.h
@@ -66,7 +66,8 @@ class MaglevSafepointTable {
  public:
   // The isolate and pc arguments are used for figuring out whether pc
   // belongs to the embedded or un-embedded code blob.
-  explicit MaglevSafepointTable(Isolate* isolate, Address pc, Code code);
+  explicit MaglevSafepointTable(Isolate* isolate, Address pc,
+                                Tagged<Code> code);
   MaglevSafepointTable(const MaglevSafepointTable&) = delete;
   MaglevSafepointTable& operator=(const MaglevSafepointTable&) = delete;
 
@@ -107,13 +108,13 @@ class MaglevSafepointTable {
 
   // Returns the entry for the given pc.
   MaglevSafepointEntry FindEntry(Address pc) const;
-  static MaglevSafepointEntry FindEntry(Isolate* isolate, GcSafeCode code,
-                                        Address pc);
+  static MaglevSafepointEntry FindEntry(Isolate* isolate,
+                                        Tagged<GcSafeCode> code, Address pc);
 
   void Print(std::ostream&) const;
 
  private:
-  MaglevSafepointTable(Isolate* isolate, Address pc, GcSafeCode code);
+  MaglevSafepointTable(Isolate* isolate, Address pc, Tagged<GcSafeCode> code);
 
   // Layout information.
   static constexpr int kLengthOffset = 0;
diff --git a/src/codegen/optimized-compilation-info.cc b/src/codegen/optimized-compilation-info.cc
index 5a7f319e2c9..c2fae3881d7 100644
--- a/src/codegen/optimized-compilation-info.cc
+++ b/src/codegen/optimized-compilation-info.cc
@@ -201,7 +201,7 @@ bool OptimizedCompilationInfo::has_context() const {
   return !closure().is_null();
 }
 
-Context OptimizedCompilationInfo::context() const {
+Tagged<Context> OptimizedCompilationInfo::context() const {
   DCHECK(has_context());
   return closure()->context();
 }
@@ -210,7 +210,7 @@ bool OptimizedCompilationInfo::has_native_context() const {
   return !closure().is_null() && !closure()->native_context().is_null();
 }
 
-NativeContext OptimizedCompilationInfo::native_context() const {
+Tagged<NativeContext> OptimizedCompilationInfo::native_context() const {
   DCHECK(has_native_context());
   return closure()->native_context();
 }
@@ -219,7 +219,7 @@ bool OptimizedCompilationInfo::has_global_object() const {
   return has_native_context();
 }
 
-JSGlobalObject OptimizedCompilationInfo::global_object() const {
+Tagged<JSGlobalObject> OptimizedCompilationInfo::global_object() const {
   DCHECK(has_global_object());
   return native_context()->global_object();
 }
diff --git a/src/codegen/optimized-compilation-info.h b/src/codegen/optimized-compilation-info.h
index b5c4aac5503..54d13d67224 100644
--- a/src/codegen/optimized-compilation-info.h
+++ b/src/codegen/optimized-compilation-info.h
@@ -141,13 +141,13 @@ class V8_EXPORT_PRIVATE OptimizedCompilationInfo final {
 #endif  // V8_ENABLE_WEBASSEMBLY
 
   bool has_context() const;
-  Context context() const;
+  Tagged<Context> context() const;
 
   bool has_native_context() const;
-  NativeContext native_context() const;
+  Tagged<NativeContext> native_context() const;
 
   bool has_global_object() const;
-  JSGlobalObject global_object() const;
+  Tagged<JSGlobalObject> global_object() const;
 
   // Accessors for the different compilation modes.
   bool IsOptimizing() const {
@@ -171,12 +171,6 @@ class V8_EXPORT_PRIVATE OptimizedCompilationInfo final {
     DCHECK_NOT_NULL(canonical_handles_);
   }
 
-  template <typename T>
-  Handle<T> CanonicalHandle(T object, Isolate* isolate) {
-    static_assert(kTaggedCanConvertToRawObjects);
-    return CanonicalHandle(Tagged<T>(object), isolate);
-  }
-
   template <typename T>
   Handle<T> CanonicalHandle(Tagged<T> object, Isolate* isolate) {
     DCHECK_NOT_NULL(canonical_handles_);
diff --git a/src/codegen/pending-optimization-table.cc b/src/codegen/pending-optimization-table.cc
index fe7913ab0bd..c0f0b8d6916 100644
--- a/src/codegen/pending-optimization-table.cc
+++ b/src/codegen/pending-optimization-table.cc
@@ -36,7 +36,7 @@ void ManualOptimizationTable::MarkFunctionForManualOptimization(
 }
 
 void ManualOptimizationTable::CheckMarkedForManualOptimization(
-    Isolate* isolate, JSFunction function) {
+    Isolate* isolate, Tagged<JSFunction> function) {
   if (!IsMarkedForManualOptimization(isolate, function)) {
     PrintF("Error: Function ");
     ShortPrint(function);
@@ -50,7 +50,7 @@ void ManualOptimizationTable::CheckMarkedForManualOptimization(
 }
 
 bool ManualOptimizationTable::IsMarkedForManualOptimization(
-    Isolate* isolate, JSFunction function) {
+    Isolate* isolate, Tagged<JSFunction> function) {
   DCHECK(v8_flags.testing_d8_test_runner || v8_flags.allow_natives_syntax);
 
   Handle<Object> table = handle(
diff --git a/src/codegen/pending-optimization-table.h b/src/codegen/pending-optimization-table.h
index 459f9638745..2c739564164 100644
--- a/src/codegen/pending-optimization-table.h
+++ b/src/codegen/pending-optimization-table.h
@@ -31,12 +31,12 @@ class ManualOptimizationTable {
   // via the intrinsics. This will check whether
   // MarkFunctionForManualOptimization was called with this function.
   static void CheckMarkedForManualOptimization(Isolate* isolate,
-                                               JSFunction function);
+                                               Tagged<JSFunction> function);
 
   // Returns true if MarkFunctionForManualOptimization was called with this
   // function.
   static bool IsMarkedForManualOptimization(Isolate* isolate,
-                                            JSFunction function);
+                                            Tagged<JSFunction> function);
 };
 
 }  // namespace internal
diff --git a/src/codegen/reloc-info-inl.h b/src/codegen/reloc-info-inl.h
index 70fe88f41a8..62424d739f6 100644
--- a/src/codegen/reloc-info-inl.h
+++ b/src/codegen/reloc-info-inl.h
@@ -12,7 +12,8 @@
 namespace v8 {
 namespace internal {
 
-void RelocInfo::set_target_object(InstructionStream host, HeapObject target,
+void RelocInfo::set_target_object(Tagged<InstructionStream> host,
+                                  Tagged<HeapObject> target,
                                   WriteBarrierMode write_barrier_mode,
                                   ICacheFlushMode icache_flush_mode) {
   set_target_object(target, icache_flush_mode);
diff --git a/src/codegen/reloc-info.cc b/src/codegen/reloc-info.cc
index 38116194551..c09dd0765ac 100644
--- a/src/codegen/reloc-info.cc
+++ b/src/codegen/reloc-info.cc
@@ -178,22 +178,23 @@ void RelocIterator::next() {
   done_ = true;
 }
 
-RelocIterator::RelocIterator(Code code, int mode_mask)
+RelocIterator::RelocIterator(Tagged<Code> code, int mode_mask)
     : RelocIterator(
           code->instruction_start(), code->constant_pool(),
           code->instruction_stream()->relocation_info()->GetDataEndAddress(),
           code->instruction_stream()->relocation_info()->GetDataStartAddress(),
           mode_mask) {}
 
-RelocIterator::RelocIterator(InstructionStream istream, Address constant_pool,
-                             int mode_mask)
+RelocIterator::RelocIterator(Tagged<InstructionStream> istream,
+                             Address constant_pool, int mode_mask)
     : RelocIterator(istream->instruction_start(), constant_pool,
                     istream->relocation_info()->GetDataEndAddress(),
                     istream->relocation_info()->GetDataStartAddress(),
                     mode_mask) {}
 
-RelocIterator::RelocIterator(Code code, InstructionStream instruction_stream,
-                             ByteArray relocation_info, int mode_mask)
+RelocIterator::RelocIterator(Tagged<Code> code,
+                             Tagged<InstructionStream> instruction_stream,
+                             Tagged<ByteArray> relocation_info, int mode_mask)
     : RelocIterator(instruction_stream->instruction_start(),
                     code->constant_pool(instruction_stream),
                     relocation_info->GetDataEndAddress(),
@@ -205,7 +206,7 @@ RelocIterator::RelocIterator(const CodeReference code_reference)
                     code_reference.relocation_end(),
                     code_reference.relocation_start(), kAllModesMask) {}
 
-RelocIterator::RelocIterator(EmbeddedData* embedded_data, Code code,
+RelocIterator::RelocIterator(EmbeddedData* embedded_data, Tagged<Code> code,
                              int mode_mask)
     : RelocIterator(embedded_data->InstructionStartOf(code->builtin_id()),
                     code->constant_pool(), code->relocation_end(),
@@ -279,12 +280,13 @@ void RelocInfo::set_target_address(Address target,
                                    icache_flush_mode);
 }
 
-void RelocInfo::set_target_address(InstructionStream host, Address target,
+void RelocInfo::set_target_address(Tagged<InstructionStream> host,
+                                   Address target,
                                    WriteBarrierMode write_barrier_mode,
                                    ICacheFlushMode icache_flush_mode) {
   set_target_address(target, icache_flush_mode);
   if (IsCodeTargetMode(rmode_) && !v8_flags.disable_write_barriers) {
-    InstructionStream target_code =
+    Tagged<InstructionStream> target_code =
         InstructionStream::FromTargetAddress(target);
     WriteBarrierForCode(host, this, target_code, write_barrier_mode);
   }
@@ -387,7 +389,7 @@ void RelocInfo::Print(Isolate* isolate, std::ostream& os) {
        << ")";
   } else if (IsCodeTargetMode(rmode_)) {
     const Address code_target = target_address();
-    Code target_code = Code::FromTargetAddress(code_target);
+    Tagged<Code> target_code = Code::FromTargetAddress(code_target);
     os << " (" << CodeKindToString(target_code->kind());
     if (Builtins::IsBuiltin(target_code)) {
       os << " " << Builtins::name(target_code->builtin_id());
@@ -426,8 +428,10 @@ void RelocInfo::Verify(Isolate* isolate) {
       Address addr = target_address();
       CHECK_NE(addr, kNullAddress);
       // Check that we can find the right code object.
-      InstructionStream code = InstructionStream::FromTargetAddress(addr);
-      Code lookup_result = isolate->heap()->FindCodeForInnerPointer(addr);
+      Tagged<InstructionStream> code =
+          InstructionStream::FromTargetAddress(addr);
+      Tagged<Code> lookup_result =
+          isolate->heap()->FindCodeForInnerPointer(addr);
       CHECK_EQ(code.address(), lookup_result->instruction_stream().address());
       break;
     }
@@ -435,7 +439,7 @@ void RelocInfo::Verify(Isolate* isolate) {
     case INTERNAL_REFERENCE_ENCODED: {
       Address target = target_internal_reference();
       Address pc = target_internal_reference_address();
-      Code lookup_result = isolate->heap()->FindCodeForInnerPointer(pc);
+      Tagged<Code> lookup_result = isolate->heap()->FindCodeForInnerPointer(pc);
       CHECK_GE(target, lookup_result->instruction_start());
       CHECK_LT(target, lookup_result->instruction_end());
       break;
diff --git a/src/codegen/reloc-info.h b/src/codegen/reloc-info.h
index c1de7771caf..5da3a1c2dc0 100644
--- a/src/codegen/reloc-info.h
+++ b/src/codegen/reloc-info.h
@@ -300,7 +300,7 @@ class RelocInfo {
       Address, ICacheFlushMode icache_flush_mode = FLUSH_ICACHE_IF_NEEDED);
 
   void set_target_address(
-      InstructionStream host, Address target,
+      Tagged<InstructionStream> host, Address target,
       WriteBarrierMode write_barrier_mode = UPDATE_WRITE_BARRIER,
       ICacheFlushMode icache_flush_mode = FLUSH_ICACHE_IF_NEEDED);
   // Use this overload only when an InstructionStream host is not available.
@@ -315,17 +315,17 @@ class RelocInfo {
   // can only be called if IsCodeTarget(rmode_)
   V8_INLINE Address target_address();
   // Cage base value is used for decompressing compressed embedded references.
-  V8_INLINE HeapObject target_object(PtrComprCageBase cage_base);
+  V8_INLINE Tagged<HeapObject> target_object(PtrComprCageBase cage_base);
 
   V8_INLINE Handle<HeapObject> target_object_handle(Assembler* origin);
 
   V8_INLINE void set_target_object(
-      InstructionStream host, HeapObject target,
+      Tagged<InstructionStream> host, Tagged<HeapObject> target,
       WriteBarrierMode write_barrier_mode = UPDATE_WRITE_BARRIER,
       ICacheFlushMode icache_flush_mode = FLUSH_ICACHE_IF_NEEDED);
   // Use this overload only when an InstructionStream host is not available.
   V8_INLINE void set_target_object(
-      HeapObject target,
+      Tagged<HeapObject> target,
       ICacheFlushMode icache_flush_mode = FLUSH_ICACHE_IF_NEEDED);
 
   // Decodes builtin ID encoded as a PC-relative offset. This encoding is used
@@ -376,7 +376,7 @@ class RelocInfo {
   V8_INLINE void WipeOut();
 
   template <typename ObjectVisitor>
-  void Visit(InstructionStream host, ObjectVisitor* visitor) {
+  void Visit(Tagged<InstructionStream> host, ObjectVisitor* visitor) {
     Mode mode = rmode();
     if (IsEmbeddedObjectMode(mode)) {
       visitor->VisitEmbeddedPointer(host, this);
@@ -489,12 +489,12 @@ class V8_EXPORT_PRIVATE RelocIterator {
 
  public:
   // Prefer using this ctor when possible:
-  explicit RelocIterator(Code code, int mode_mask = kAllModesMask);
+  explicit RelocIterator(Tagged<Code> code, int mode_mask = kAllModesMask);
 
   // Relocation during InstructionStream initialization. Pass the instruction
   // stream directly since we want to avoid reading from the Code object for
   // CFI. I.e. the istream lives in trusted memory but the code object doesn't.
-  RelocIterator(InstructionStream istream, Address constant_pool,
+  RelocIterator(Tagged<InstructionStream> istream, Address constant_pool,
                 int mode_mask);
 
   // For when GC may be in progress and thus pointers on the Code object may be
@@ -502,8 +502,9 @@ class V8_EXPORT_PRIVATE RelocIterator {
   // or we're operating on fake objects for some reason. Then, we pass relevant
   // objects explicitly. Note they must all refer to the same underlying
   // {Code,IStream} composite object.
-  explicit RelocIterator(Code code, InstructionStream instruction_stream,
-                         ByteArray relocation_info, int mode_mask);
+  explicit RelocIterator(Tagged<Code> code,
+                         Tagged<InstructionStream> instruction_stream,
+                         Tagged<ByteArray> relocation_info, int mode_mask);
   // For Wasm.
   explicit RelocIterator(base::Vector<uint8_t> instructions,
                          base::Vector<const uint8_t> reloc_info,
@@ -511,7 +512,8 @@ class V8_EXPORT_PRIVATE RelocIterator {
   // For the disassembler.
   explicit RelocIterator(const CodeReference code_reference);
   // For FinalizeEmbeddedCodeTargets when creating embedded builtins.
-  explicit RelocIterator(EmbeddedData* embedded_data, Code code, int mode_mask);
+  explicit RelocIterator(EmbeddedData* embedded_data, Tagged<Code> code,
+                         int mode_mask);
 
   RelocIterator(RelocIterator&&) V8_NOEXCEPT = default;
   RelocIterator(const RelocIterator&) = delete;
diff --git a/src/codegen/safepoint-table.cc b/src/codegen/safepoint-table.cc
index d2d57cbe517..a3dda0f2330 100644
--- a/src/codegen/safepoint-table.cc
+++ b/src/codegen/safepoint-table.cc
@@ -20,11 +20,12 @@
 namespace v8 {
 namespace internal {
 
-SafepointTable::SafepointTable(Isolate* isolate, Address pc, Code code)
+SafepointTable::SafepointTable(Isolate* isolate, Address pc, Tagged<Code> code)
     : SafepointTable(code->InstructionStart(isolate, pc),
                      code->safepoint_table_address()) {}
 
-SafepointTable::SafepointTable(Isolate* isolate, Address pc, GcSafeCode code)
+SafepointTable::SafepointTable(Isolate* isolate, Address pc,
+                               Tagged<GcSafeCode> code)
     : SafepointTable(code->InstructionStart(isolate, pc),
                      code->safepoint_table_address()) {}
 
@@ -78,8 +79,8 @@ SafepointEntry SafepointTable::FindEntry(Address pc) const {
 }
 
 // static
-SafepointEntry SafepointTable::FindEntry(Isolate* isolate, GcSafeCode code,
-                                         Address pc) {
+SafepointEntry SafepointTable::FindEntry(Isolate* isolate,
+                                         Tagged<GcSafeCode> code, Address pc) {
   SafepointTable table(isolate, pc, code);
   return table.FindEntry(pc);
 }
diff --git a/src/codegen/safepoint-table.h b/src/codegen/safepoint-table.h
index 3676ae1c117..0db461f2f13 100644
--- a/src/codegen/safepoint-table.h
+++ b/src/codegen/safepoint-table.h
@@ -62,8 +62,9 @@ class SafepointTable {
  public:
   // The isolate and pc arguments are used for figuring out whether pc
   // belongs to the embedded or un-embedded code blob.
-  explicit SafepointTable(Isolate* isolate, Address pc, InstructionStream code);
-  explicit SafepointTable(Isolate* isolate, Address pc, Code code);
+  explicit SafepointTable(Isolate* isolate, Address pc,
+                          Tagged<InstructionStream> code);
+  explicit SafepointTable(Isolate* isolate, Address pc, Tagged<Code> code);
 #if V8_ENABLE_WEBASSEMBLY
   explicit SafepointTable(const wasm::WasmCode* code);
 #endif  // V8_ENABLE_WEBASSEMBLY
@@ -115,13 +116,13 @@ class SafepointTable {
 
   // Returns the entry for the given pc.
   SafepointEntry FindEntry(Address pc) const;
-  static SafepointEntry FindEntry(Isolate* isolate, GcSafeCode code,
+  static SafepointEntry FindEntry(Isolate* isolate, Tagged<GcSafeCode> code,
                                   Address pc);
 
   void Print(std::ostream&) const;
 
  private:
-  SafepointTable(Isolate* isolate, Address pc, GcSafeCode code);
+  SafepointTable(Isolate* isolate, Address pc, Tagged<GcSafeCode> code);
 
   // Layout information.
   static constexpr int kLengthOffset = 0;
diff --git a/src/codegen/source-position-table.cc b/src/codegen/source-position-table.cc
index b4465b42b2b..7e4cfa815a8 100644
--- a/src/codegen/source-position-table.cc
+++ b/src/codegen/source-position-table.cc
@@ -118,7 +118,7 @@ void DecodeEntry(base::Vector<const uint8_t> bytes, int* index,
   entry->source_position = DecodeInt<int64_t>(bytes, index);
 }
 
-base::Vector<const uint8_t> VectorFromByteArray(ByteArray byte_array) {
+base::Vector<const uint8_t> VectorFromByteArray(Tagged<ByteArray> byte_array) {
   return base::Vector<const uint8_t>(byte_array->GetDataStartAddress(),
                                      byte_array->length());
 }
@@ -230,7 +230,7 @@ void SourcePositionTableIterator::Initialize() {
 }
 
 SourcePositionTableIterator::SourcePositionTableIterator(
-    ByteArray byte_array, IterationFilter iteration_filter,
+    Tagged<ByteArray> byte_array, IterationFilter iteration_filter,
     FunctionEntryFilter function_entry_filter)
     : raw_table_(VectorFromByteArray(byte_array)),
       iteration_filter_(iteration_filter),
diff --git a/src/codegen/source-position-table.h b/src/codegen/source-position-table.h
index d00d226edef..4d8e6de0a05 100644
--- a/src/codegen/source-position-table.h
+++ b/src/codegen/source-position-table.h
@@ -105,7 +105,8 @@ class V8_EXPORT_PRIVATE SourcePositionTableIterator {
   // allocation during its lifetime. This is useful if there is no handle
   // scope around.
   explicit SourcePositionTableIterator(
-      ByteArray byte_array, IterationFilter iteration_filter = kJavaScriptOnly,
+      Tagged<ByteArray> byte_array,
+      IterationFilter iteration_filter = kJavaScriptOnly,
       FunctionEntryFilter function_entry_filter = kSkipFunctionEntry);
 
   // Handle-safe iterator based on an a vector located outside the garbage
diff --git a/src/codegen/source-position.cc b/src/codegen/source-position.cc
index 49f13e5125f..eb433fdae3a 100644
--- a/src/codegen/source-position.cc
+++ b/src/codegen/source-position.cc
@@ -61,9 +61,9 @@ std::vector<SourcePositionInfo> SourcePosition::InliningStack(
   return stack;
 }
 
-std::vector<SourcePositionInfo> SourcePosition::InliningStack(Isolate* isolate,
-                                                              Code code) const {
-  DeoptimizationData deopt_data =
+std::vector<SourcePositionInfo> SourcePosition::InliningStack(
+    Isolate* isolate, Tagged<Code> code) const {
+  Tagged<DeoptimizationData> deopt_data =
       DeoptimizationData::cast(code->deoptimization_data());
   SourcePosition pos = *this;
   std::vector<SourcePositionInfo> stack;
@@ -82,9 +82,9 @@ std::vector<SourcePositionInfo> SourcePosition::InliningStack(Isolate* isolate,
 }
 
 SourcePositionInfo SourcePosition::FirstInfo(Isolate* isolate,
-                                             Code code) const {
+                                             Tagged<Code> code) const {
   DisallowGarbageCollection no_gc;
-  DeoptimizationData deopt_data =
+  Tagged<DeoptimizationData> deopt_data =
       DeoptimizationData::cast(code->deoptimization_data());
   SourcePosition pos = *this;
   if (pos.isInlined()) {
@@ -100,11 +100,11 @@ SourcePositionInfo SourcePosition::FirstInfo(Isolate* isolate,
 }
 
 void SourcePosition::Print(std::ostream& out,
-                           SharedFunctionInfo function) const {
+                           Tagged<SharedFunctionInfo> function) const {
   Script::PositionInfo pos;
-  Object source_name;
+  Tagged<Object> source_name;
   if (IsScript(function->script())) {
-    Script script = Script::cast(function->script());
+    Tagged<Script> script = Script::cast(function->script());
     source_name = script->name();
     script->GetPositionInfo(ScriptOffset(), &pos);
   }
@@ -130,11 +130,11 @@ void SourcePosition::PrintJson(std::ostream& out) const {
   }
 }
 
-void SourcePosition::Print(std::ostream& out, Code code) const {
-  DeoptimizationData deopt_data =
+void SourcePosition::Print(std::ostream& out, Tagged<Code> code) const {
+  Tagged<DeoptimizationData> deopt_data =
       DeoptimizationData::cast(code->deoptimization_data());
   if (!isInlined()) {
-    SharedFunctionInfo function(
+    Tagged<SharedFunctionInfo> function(
         SharedFunctionInfo::cast(deopt_data->SharedFunctionInfo()));
     Print(out, function);
   } else {
@@ -142,7 +142,7 @@ void SourcePosition::Print(std::ostream& out, Code code) const {
     if (inl.inlined_function_id == -1) {
       out << *this;
     } else {
-      SharedFunctionInfo function =
+      Tagged<SharedFunctionInfo> function =
           deopt_data->GetInlinedFunction(inl.inlined_function_id);
       Print(out, function);
     }
@@ -157,7 +157,7 @@ SourcePositionInfo::SourcePositionInfo(Isolate* isolate, SourcePosition pos,
   {
     DisallowGarbageCollection no_gc;
     if (sfi.is_null()) return;
-    Object maybe_script = sfi->script();
+    Tagged<Object> maybe_script = sfi->script();
     if (!IsScript(maybe_script)) return;
     script = handle(Script::cast(maybe_script), isolate);
   }
diff --git a/src/codegen/source-position.h b/src/codegen/source-position.h
index 5d6b1a93f76..74bd2cfa8c0 100644
--- a/src/codegen/source-position.h
+++ b/src/codegen/source-position.h
@@ -80,12 +80,12 @@ class SourcePosition final {
 
   // Assumes that the code object is optimized.
   std::vector<SourcePositionInfo> InliningStack(Isolate* isolate,
-                                                Code code) const;
+                                                Tagged<Code> code) const;
   std::vector<SourcePositionInfo> InliningStack(
       Isolate* isolate, OptimizedCompilationInfo* cinfo) const;
-  SourcePositionInfo FirstInfo(Isolate* isolate, Code code) const;
+  SourcePositionInfo FirstInfo(Isolate* isolate, Tagged<Code> code) const;
 
-  void Print(std::ostream& out, Code code) const;
+  void Print(std::ostream& out, Tagged<Code> code) const;
   void PrintJson(std::ostream& out) const;
 
   int ScriptOffset() const {
@@ -140,7 +140,7 @@ class SourcePosition final {
     SetInliningId(inlining_id);
   }
 
-  void Print(std::ostream& out, SharedFunctionInfo function) const;
+  void Print(std::ostream& out, Tagged<SharedFunctionInfo> function) const;
 
   using IsExternalField = base::BitField64<bool, 0, 1>;
 
diff --git a/src/codegen/x64/assembler-x64-inl.h b/src/codegen/x64/assembler-x64-inl.h
index ed43f096bf4..5d196ac4152 100644
--- a/src/codegen/x64/assembler-x64-inl.h
+++ b/src/codegen/x64/assembler-x64-inl.h
@@ -216,7 +216,7 @@ void Assembler::deserialization_set_target_internal_reference_at(
 }
 
 void Assembler::deserialization_set_special_target_at(
-    Address instruction_payload, Code code, Address target) {
+    Address instruction_payload, Tagged<Code> code, Address target) {
   set_target_address_at(instruction_payload,
                         !code.is_null() ? code->constant_pool() : kNullAddress,
                         target);
@@ -280,12 +280,12 @@ int RelocInfo::target_address_size() {
   }
 }
 
-HeapObject RelocInfo::target_object(PtrComprCageBase cage_base) {
+Tagged<HeapObject> RelocInfo::target_object(PtrComprCageBase cage_base) {
   DCHECK(IsCodeTarget(rmode_) || IsEmbeddedObjectMode(rmode_));
   if (IsCompressedEmbeddedObject(rmode_)) {
     Tagged_t compressed = ReadUnalignedValue<Tagged_t>(pc_);
     DCHECK(!HAS_SMI_TAG(compressed));
-    Object obj(
+    Tagged<Object> obj(
         V8HeapCompressionScheme::DecompressTagged(cage_base, compressed));
     // Embedding of compressed InstructionStream objects must not happen when
     // external code space is enabled, because Codes must be used
@@ -335,7 +335,7 @@ Address RelocInfo::target_internal_reference_address() {
   return pc_;
 }
 
-void RelocInfo::set_target_object(HeapObject target,
+void RelocInfo::set_target_object(Tagged<HeapObject> target,
                                   ICacheFlushMode icache_flush_mode) {
   DCHECK(IsCodeTarget(rmode_) || IsEmbeddedObjectMode(rmode_));
   if (IsCompressedEmbeddedObject(rmode_)) {
diff --git a/src/codegen/x64/assembler-x64.h b/src/codegen/x64/assembler-x64.h
index ea36beafe46..6c65ea6303a 100644
--- a/src/codegen/x64/assembler-x64.h
+++ b/src/codegen/x64/assembler-x64.h
@@ -127,7 +127,7 @@ class Immediate {
   explicit constexpr Immediate(int32_t value) : value_(value) {}
   explicit constexpr Immediate(int32_t value, RelocInfo::Mode rmode)
       : value_(value), rmode_(rmode) {}
-  explicit Immediate(Smi value)
+  explicit Immediate(Tagged<Smi> value)
       : value_(static_cast<int32_t>(static_cast<intptr_t>(value.ptr()))) {
     DCHECK(SmiValuesAre31Bits());  // Only available for 31-bit SMI.
   }
@@ -534,7 +534,7 @@ class V8_EXPORT_PRIVATE Assembler : public AssemblerBase {
   // This sets the branch destination (which is in the instruction on x64).
   // This is for calls and branches within generated code.
   inline static void deserialization_set_special_target_at(
-      Address instruction_payload, Code code, Address target);
+      Address instruction_payload, Tagged<Code> code, Address target);
 
   // Get the size of the special target encoded at 'instruction_payload'.
   inline static int deserialization_special_target_size(
diff --git a/src/codegen/x64/macro-assembler-x64.cc b/src/codegen/x64/macro-assembler-x64.cc
index d67ebd80e99..04922494c78 100644
--- a/src/codegen/x64/macro-assembler-x64.cc
+++ b/src/codegen/x64/macro-assembler-x64.cc
@@ -313,7 +313,7 @@ void MacroAssembler::StoreTaggedField(Operand dst_field_operand,
 }
 
 void MacroAssembler::StoreTaggedSignedField(Operand dst_field_operand,
-                                            Smi value) {
+                                            Tagged<Smi> value) {
   if (SmiValuesAre32Bits()) {
     Move(kScratchRegister, value);
     movq(dst_field_operand, kScratchRegister);
@@ -1564,7 +1564,7 @@ void MacroAssembler::S256Select(YMMRegister dst, YMMRegister mask,
 // ----------------------------------------------------------------------------
 // Smi tagging, untagging and tag detection.
 
-Register MacroAssembler::GetSmiConstant(Smi source) {
+Register MacroAssembler::GetSmiConstant(Tagged<Smi> source) {
   Move(kScratchRegister, source);
   return kScratchRegister;
 }
@@ -1909,12 +1909,12 @@ void MacroAssembler::SmiCompare(Register smi1, Register smi2) {
   cmp_tagged(smi1, smi2);
 }
 
-void MacroAssembler::SmiCompare(Register dst, Smi src) {
+void MacroAssembler::SmiCompare(Register dst, Tagged<Smi> src) {
   AssertSmi(dst);
   Cmp(dst, src);
 }
 
-void MacroAssembler::Cmp(Register dst, Smi src) {
+void MacroAssembler::Cmp(Register dst, Tagged<Smi> src) {
   if (src.value() == 0) {
     test_tagged(dst, dst);
   } else if (COMPRESS_POINTERS_BOOL) {
@@ -1938,7 +1938,7 @@ void MacroAssembler::SmiCompare(Operand dst, Register src) {
   cmp_tagged(dst, src);
 }
 
-void MacroAssembler::SmiCompare(Operand dst, Smi src) {
+void MacroAssembler::SmiCompare(Operand dst, Tagged<Smi> src) {
   AssertSmi(dst);
   if (SmiValuesAre32Bits()) {
     cmpl(Operand(dst, kSmiShift / kBitsPerByte), Immediate(src.value()));
@@ -1948,7 +1948,7 @@ void MacroAssembler::SmiCompare(Operand dst, Smi src) {
   }
 }
 
-void MacroAssembler::Cmp(Operand dst, Smi src) {
+void MacroAssembler::Cmp(Operand dst, Tagged<Smi> src) {
   // The Operand cannot use the smi register.
   Register smi_reg = GetSmiConstant(src);
   DCHECK(!dst.AddressUsesRegister(smi_reg));
@@ -1985,7 +1985,7 @@ void MacroAssembler::JumpIfNotSmi(Operand src, Label* on_not_smi,
   j(NegateCondition(smi), on_not_smi, near_jump);
 }
 
-void MacroAssembler::SmiAddConstant(Operand dst, Smi constant) {
+void MacroAssembler::SmiAddConstant(Operand dst, Tagged<Smi> constant) {
   if (constant.value() != 0) {
     if (SmiValuesAre32Bits()) {
       addl(Operand(dst, kSmiShift / kBitsPerByte), Immediate(constant.value()));
@@ -2056,7 +2056,7 @@ void MacroAssembler::Switch(Register scratch, Register reg, int case_value_base,
   bind(&fallthrough);
 }
 
-void MacroAssembler::Push(Smi source) {
+void MacroAssembler::Push(Tagged<Smi> source) {
   intptr_t smi = static_cast<intptr_t>(source.ptr());
   if (is_int32(smi)) {
     Push(Immediate(static_cast<int32_t>(smi)));
@@ -2077,7 +2077,7 @@ void MacroAssembler::Push(Smi source) {
 
 // ----------------------------------------------------------------------------
 
-void MacroAssembler::Move(Register dst, Smi source) {
+void MacroAssembler::Move(Register dst, Tagged<Smi> source) {
   static_assert(kSmiTag == 0);
   int value = source.value();
   if (value == 0) {
diff --git a/src/codegen/x64/macro-assembler-x64.h b/src/codegen/x64/macro-assembler-x64.h
index fd13aa99842..a1bf09bc096 100644
--- a/src/codegen/x64/macro-assembler-x64.h
+++ b/src/codegen/x64/macro-assembler-x64.h
@@ -87,8 +87,8 @@ class V8_EXPORT_PRIVATE MacroAssembler
   void Push(Register src);
   void Push(Operand src);
   void Push(Immediate value);
-  void Push(Smi smi);
-  void Push(TaggedIndex index) {
+  void Push(Tagged<Smi> smi);
+  void Push(Tagged<TaggedIndex> index) {
     Push(Immediate(static_cast<uint32_t>(index.ptr())));
   }
   void Push(Handle<HeapObject> source);
@@ -210,8 +210,8 @@ class V8_EXPORT_PRIVATE MacroAssembler
   void Popcntq(Register dst, Register src);
   void Popcntq(Register dst, Operand src);
 
-  void Cmp(Register dst, Smi src);
-  void Cmp(Operand dst, Smi src);
+  void Cmp(Register dst, Tagged<Smi> src);
+  void Cmp(Operand dst, Tagged<Smi> src);
   void Cmp(Register dst, int32_t src);
 
   void CmpTagged(const Register& src1, const Register& src2) {
@@ -280,10 +280,10 @@ class V8_EXPORT_PRIVATE MacroAssembler
   // Simple comparison of smis.  Both sides must be known smis to use these,
   // otherwise use Cmp.
   void SmiCompare(Register smi1, Register smi2);
-  void SmiCompare(Register dst, Smi src);
+  void SmiCompare(Register dst, Tagged<Smi> src);
   void SmiCompare(Register dst, Operand src);
   void SmiCompare(Operand dst, Register src);
-  void SmiCompare(Operand dst, Smi src);
+  void SmiCompare(Operand dst, Tagged<Smi> src);
 
   // Functions performing a check on a known or potential smi. Returns
   // a condition that is satisfied if the check is successful.
@@ -319,7 +319,7 @@ class V8_EXPORT_PRIVATE MacroAssembler
 
   // Add an integer constant to a tagged smi, giving a tagged smi as result.
   // No overflow testing on the result is done.
-  void SmiAddConstant(Operand dst, Smi constant);
+  void SmiAddConstant(Operand dst, Tagged<Smi> constant);
 
   // Specialized operations
 
@@ -372,16 +372,20 @@ class V8_EXPORT_PRIVATE MacroAssembler
     }
   }
   void Move(Operand dst, intptr_t x);
-  void Move(Register dst, Smi source);
+  void Move(Register dst, Tagged<Smi> source);
 
-  void Move(Operand dst, Smi source) {
+  void Move(Operand dst, Tagged<Smi> source) {
     Register constant = GetSmiConstant(source);
     movq(dst, constant);
   }
 
-  void Move(Register dst, TaggedIndex source) { Move(dst, source.ptr()); }
+  void Move(Register dst, Tagged<TaggedIndex> source) {
+    Move(dst, source.ptr());
+  }
 
-  void Move(Operand dst, TaggedIndex source) { Move(dst, source.ptr()); }
+  void Move(Operand dst, Tagged<TaggedIndex> source) {
+    Move(dst, source.ptr());
+  }
 
   void Move(Register dst, ExternalReference ext);
 
@@ -677,7 +681,7 @@ class V8_EXPORT_PRIVATE MacroAssembler
   // location.
   void StoreTaggedField(Operand dst_field_operand, Immediate immediate);
   void StoreTaggedField(Operand dst_field_operand, Register value);
-  void StoreTaggedSignedField(Operand dst_field_operand, Smi value);
+  void StoreTaggedSignedField(Operand dst_field_operand, Tagged<Smi> value);
   void AtomicStoreTaggedField(Operand dst_field_operand, Register value);
 
   // The following macros work even when pointer compression is not enabled.
@@ -1014,7 +1018,7 @@ class V8_EXPORT_PRIVATE MacroAssembler
 
   // Returns a register holding the smi value. The register MUST NOT be
   // modified. It may be the "smi 1 constant" register.
-  Register GetSmiConstant(Smi value);
+  Register GetSmiConstant(Tagged<Smi> value);
 
   // Drops arguments assuming that the return address was already popped.
   void DropArguments(Register count, ArgumentsCountType type = kCountIsInteger,
diff --git a/src/common/globals.h b/src/common/globals.h
index 55ddf3f023b..1ca88205099 100644
--- a/src/common/globals.h
+++ b/src/common/globals.h
@@ -900,7 +900,7 @@ static const intptr_t kPageAlignmentMask = (intptr_t{1} << kPageSizeBits) - 1;
 // If looking only at the top 32 bits, the QNaN mask is bits 19 to 30.
 constexpr uint32_t kQuietNaNHighBitsMask = 0xfff << (51 - 32);
 
-enum class HeapObjectReferenceType {
+enum class V8_EXPORT_ENUM HeapObjectReferenceType {
   WEAK,
   STRONG,
 };
diff --git a/src/common/ptr-compr-inl.h b/src/common/ptr-compr-inl.h
index 07e61a7973c..abba4502a29 100644
--- a/src/common/ptr-compr-inl.h
+++ b/src/common/ptr-compr-inl.h
@@ -271,7 +271,7 @@ V8_INLINE PtrComprCageBase GetPtrComprCageBase() { return PtrComprCageBase(); }
 
 #endif  // V8_COMPRESS_POINTERS
 
-V8_INLINE PtrComprCageBase GetPtrComprCageBase(HeapObject object) {
+V8_INLINE PtrComprCageBase GetPtrComprCageBase(Tagged<HeapObject> object) {
   return GetPtrComprCageBaseFromOnHeapAddress(object.ptr());
 }
 
diff --git a/src/compiler-dispatcher/lazy-compile-dispatcher.cc b/src/compiler-dispatcher/lazy-compile-dispatcher.cc
index 518bcdfb834..b69ad1ec126 100644
--- a/src/compiler-dispatcher/lazy-compile-dispatcher.cc
+++ b/src/compiler-dispatcher/lazy-compile-dispatcher.cc
@@ -94,7 +94,7 @@ namespace {
 void SetUncompiledDataJobPointer(LocalIsolate* isolate,
                                  Handle<SharedFunctionInfo> shared_info,
                                  Address job_address) {
-  UncompiledData uncompiled_data = shared_info->uncompiled_data();
+  Tagged<UncompiledData> uncompiled_data = shared_info->uncompiled_data();
   switch (uncompiled_data->map(isolate)->instance_type()) {
     // The easy cases -- we already have a job slot, so can write into it and
     // return.
@@ -183,7 +183,7 @@ void LazyCompileDispatcher::Enqueue(
 bool LazyCompileDispatcher::IsEnqueued(
     Handle<SharedFunctionInfo> function) const {
   Job* job = nullptr;
-  Object function_data = function->function_data(kAcquireLoad);
+  Tagged<Object> function_data = function->function_data(kAcquireLoad);
   if (IsUncompiledDataWithPreparseDataAndJob(function_data)) {
     job = reinterpret_cast<Job*>(
         UncompiledDataWithPreparseDataAndJob::cast(function_data)->job());
@@ -369,7 +369,7 @@ void LazyCompileDispatcher::AbortAll() {
 
 LazyCompileDispatcher::Job* LazyCompileDispatcher::GetJobFor(
     Handle<SharedFunctionInfo> shared, const base::MutexGuard&) const {
-  Object function_data = shared->function_data(kAcquireLoad);
+  Tagged<Object> function_data = shared->function_data(kAcquireLoad);
   if (IsUncompiledDataWithPreparseDataAndJob(function_data)) {
     return reinterpret_cast<Job*>(
         UncompiledDataWithPreparseDataAndJob::cast(function_data)->job());
diff --git a/src/compiler/access-info.cc b/src/compiler/access-info.cc
index 63287dbd1a4..852131a5a54 100644
--- a/src/compiler/access-info.cc
+++ b/src/compiler/access-info.cc
@@ -683,20 +683,21 @@ bool AccessInfoFactory::TryLoadPropertyDetails(
 
     Handle<JSObject> holder = maybe_holder->object();
     if (V8_ENABLE_SWISS_NAME_DICTIONARY_BOOL) {
-      SwissNameDictionary dict = holder->property_dictionary_swiss();
+      Tagged<SwissNameDictionary> dict = holder->property_dictionary_swiss();
       *index_out = dict->FindEntry(isolate(), name.object());
       if (index_out->is_found()) {
         *details_out = dict->DetailsAt(*index_out);
       }
     } else {
-      NameDictionary dict = holder->property_dictionary();
+      Tagged<NameDictionary> dict = holder->property_dictionary();
       *index_out = dict->FindEntry(isolate(), name.object());
       if (index_out->is_found()) {
         *details_out = dict->DetailsAt(*index_out);
       }
     }
   } else {
-    DescriptorArray descriptors = *map.instance_descriptors(broker()).object();
+    Tagged<DescriptorArray> descriptors =
+        *map.instance_descriptors(broker()).object();
     *index_out = descriptors->Search(*name.object(), *map.object(), true);
     if (index_out->is_found()) {
       *details_out = descriptors->GetDetails(*index_out);
@@ -1083,7 +1084,7 @@ PropertyAccessInfo AccessInfoFactory::LookupTransition(
     MapRef map, NameRef name, OptionalJSObjectRef holder,
     PropertyAttributes attrs) const {
   // Check if the {map} has a data transition with the given {name}.
-  Map transition =
+  Tagged<Map> transition =
       TransitionsAccessor(isolate(), *map.object(), true)
           .SearchTransition(*name.object(), PropertyKind::kData, attrs);
   if (transition.is_null()) return Invalid();
diff --git a/src/compiler/backend/code-generator.cc b/src/compiler/backend/code-generator.cc
index 868bdde9198..77511bd3514 100644
--- a/src/compiler/backend/code-generator.cc
+++ b/src/compiler/backend/code-generator.cc
@@ -1263,7 +1263,7 @@ void CodeGenerator::AddTranslationForOperand(Instruction* instr,
           // When pointers are 4 bytes, we can use int32 constants to represent
           // Smis.
           DCHECK_EQ(4, kSystemPointerSize);
-          Smi smi(static_cast<Address>(constant.ToInt32()));
+          Tagged<Smi> smi(static_cast<Address>(constant.ToInt32()));
           DCHECK(IsSmi(smi));
           literal = DeoptimizationLiteral(static_cast<double>(smi.value()));
         } else if (type.representation() == MachineRepresentation::kBit) {
@@ -1307,7 +1307,7 @@ void CodeGenerator::AddTranslationForOperand(Instruction* instr,
           // When pointers are 8 bytes, we can use int64 constants to represent
           // Smis.
           DCHECK_EQ(MachineRepresentation::kTagged, type.representation());
-          Smi smi(static_cast<Address>(constant.ToInt64()));
+          Tagged<Smi> smi(static_cast<Address>(constant.ToInt64()));
           DCHECK(IsSmi(smi));
           literal = DeoptimizationLiteral(static_cast<double>(smi.value()));
         }
diff --git a/src/compiler/backend/instruction-selector.cc b/src/compiler/backend/instruction-selector.cc
index 905ac4b6a8a..ce06089ec0a 100644
--- a/src/compiler/backend/instruction-selector.cc
+++ b/src/compiler/backend/instruction-selector.cc
@@ -42,10 +42,10 @@ const turboshaft::EffectDimensions::Bits kTurboshaftEffectLevelMask =
     turboshaft::OpEffects().CanReadMemory().produces.bits();
 }
 
-Smi NumberConstantToSmi(Node* node) {
+Tagged<Smi> NumberConstantToSmi(Node* node) {
   DCHECK_EQ(node->opcode(), IrOpcode::kNumberConstant);
   const double d = OpParameter<double>(node->op());
-  Smi smi = Smi::FromInt(static_cast<int32_t>(d));
+  Tagged<Smi> smi = Smi::FromInt(static_cast<int32_t>(d));
   CHECK_EQ(smi.value(), d);
   return smi;
 }
@@ -589,7 +589,7 @@ InstructionOperand OperandForDeopt(Isolate* isolate,
       case Kind::kNumber:
         if (rep == MachineRepresentation::kWord32) {
           const double d = constant->number();
-          Smi smi = Smi::FromInt(static_cast<int32_t>(d));
+          Tagged<Smi> smi = Smi::FromInt(static_cast<int32_t>(d));
           CHECK_EQ(smi.value(), d);
           return g->UseImmediate(static_cast<int32_t>(smi.ptr()));
         }
@@ -649,7 +649,7 @@ InstructionOperand OperandForDeopt(Isolate* isolate,
       return g->UseImmediate(input);
     case IrOpcode::kNumberConstant:
       if (rep == MachineRepresentation::kWord32) {
-        Smi smi = NumberConstantToSmi(input);
+        Tagged<Smi> smi = NumberConstantToSmi(input);
         return g->UseImmediate(static_cast<int32_t>(smi.ptr()));
       } else {
         return g->UseImmediate(input);
@@ -2375,35 +2375,34 @@ void InstructionSelectorT<Adapter>::VisitTailCall(node_t node) {
       opcode = kArchTailCallAddress;
       break;
 #if V8_ENABLE_WEBASSEMBLY
-      case CallDescriptor::kCallWasmFunction:
-        DCHECK(!caller->IsJSFunctionCall());
-        opcode = kArchTailCallWasm;
-        break;
+    case CallDescriptor::kCallWasmFunction:
+      DCHECK(!caller->IsJSFunctionCall());
+      opcode = kArchTailCallWasm;
+      break;
 #endif  // V8_ENABLE_WEBASSEMBLY
-      default:
-        UNREACHABLE();
+    default:
+      UNREACHABLE();
   }
-    opcode = EncodeCallDescriptorFlags(opcode, callee->flags());
-
-    Emit(kArchPrepareTailCall, g.NoOutput());
-
-    // Add an immediate operand that represents the offset to the first slot
-    // that is unused with respect to the stack pointer that has been updated
-    // for the tail call instruction. Backends that pad arguments can write the
-    // padding value at this offset from the stack.
-    const int optional_padding_offset =
-        callee->GetOffsetToFirstUnusedStackSlot() - 1;
-    buffer.instruction_args.push_back(g.TempImmediate(optional_padding_offset));
-
-    const int first_unused_slot_offset =
-        kReturnAddressStackSlotCount + stack_param_delta;
-    buffer.instruction_args.push_back(
-        g.TempImmediate(first_unused_slot_offset));
-
-    // Emit the tailcall instruction.
-    Emit(opcode, 0, nullptr, buffer.instruction_args.size(),
-         &buffer.instruction_args.front(), temps.size(),
-         temps.empty() ? nullptr : &temps.front());
+  opcode = EncodeCallDescriptorFlags(opcode, callee->flags());
+
+  Emit(kArchPrepareTailCall, g.NoOutput());
+
+  // Add an immediate operand that represents the offset to the first slot
+  // that is unused with respect to the stack pointer that has been updated
+  // for the tail call instruction. Backends that pad arguments can write the
+  // padding value at this offset from the stack.
+  const int optional_padding_offset =
+      callee->GetOffsetToFirstUnusedStackSlot() - 1;
+  buffer.instruction_args.push_back(g.TempImmediate(optional_padding_offset));
+
+  const int first_unused_slot_offset =
+      kReturnAddressStackSlotCount + stack_param_delta;
+  buffer.instruction_args.push_back(g.TempImmediate(first_unused_slot_offset));
+
+  // Emit the tailcall instruction.
+  Emit(opcode, 0, nullptr, buffer.instruction_args.size(),
+       &buffer.instruction_args.front(), temps.size(),
+       temps.empty() ? nullptr : &temps.front());
 }
 
 template <typename Adapter>
diff --git a/src/compiler/code-assembler.cc b/src/compiler/code-assembler.cc
index 4eb9cb0a70b..ead1ec634df 100644
--- a/src/compiler/code-assembler.cc
+++ b/src/compiler/code-assembler.cc
@@ -273,7 +273,7 @@ TNode<Number> CodeAssembler::NumberConstant(double value) {
   }
 }
 
-TNode<Smi> CodeAssembler::SmiConstant(Smi value) {
+TNode<Smi> CodeAssembler::SmiConstant(Tagged<Smi> value) {
   return UncheckedCast<Smi>(BitcastWordToTaggedSigned(
       IntPtrConstant(static_cast<intptr_t>(value.ptr()))));
 }
@@ -347,7 +347,7 @@ bool CodeAssembler::TryToInt64Constant(TNode<IntegralT> node,
   return m.HasResolvedValue();
 }
 
-bool CodeAssembler::TryToSmiConstant(TNode<Smi> tnode, Smi* out_value) {
+bool CodeAssembler::TryToSmiConstant(TNode<Smi> tnode, Tagged<Smi>* out_value) {
   Node* node = tnode;
   if (node->opcode() == IrOpcode::kBitcastWordToTaggedSigned) {
     node = node->InputAt(0);
@@ -355,7 +355,8 @@ bool CodeAssembler::TryToSmiConstant(TNode<Smi> tnode, Smi* out_value) {
   return TryToSmiConstant(ReinterpretCast<IntPtrT>(tnode), out_value);
 }
 
-bool CodeAssembler::TryToSmiConstant(TNode<IntegralT> node, Smi* out_value) {
+bool CodeAssembler::TryToSmiConstant(TNode<IntegralT> node,
+                                     Tagged<Smi>* out_value) {
   IntPtrMatcher m(node);
   if (m.HasResolvedValue()) {
     intptr_t value = m.ResolvedValue();
diff --git a/src/compiler/code-assembler.h b/src/compiler/code-assembler.h
index d4749d1d203..75a9f322405 100644
--- a/src/compiler/code-assembler.h
+++ b/src/compiler/code-assembler.h
@@ -545,7 +545,7 @@ class V8_EXPORT_PRIVATE CodeAssembler {
         IntPtrConstant(base::bit_cast<intptr_t>(value)));
   }
   TNode<Number> NumberConstant(double value);
-  TNode<Smi> SmiConstant(Smi value);
+  TNode<Smi> SmiConstant(Tagged<Smi> value);
   TNode<Smi> SmiConstant(int value);
   template <typename E,
             typename = typename std::enable_if<std::is_enum<E>::value>::type>
@@ -582,8 +582,8 @@ class V8_EXPORT_PRIVATE CodeAssembler {
   bool TryToInt64Constant(TNode<IntegralT> node, int64_t* out_value);
   bool TryToIntPtrConstant(TNode<IntegralT> node, intptr_t* out_value);
   bool TryToIntPtrConstant(TNode<Smi> tnode, intptr_t* out_value);
-  bool TryToSmiConstant(TNode<IntegralT> node, Smi* out_value);
-  bool TryToSmiConstant(TNode<Smi> node, Smi* out_value);
+  bool TryToSmiConstant(TNode<IntegralT> node, Tagged<Smi>* out_value);
+  bool TryToSmiConstant(TNode<Smi> node, Tagged<Smi>* out_value);
 
   bool IsUndefinedConstant(TNode<Object> node);
   bool IsNullConstant(TNode<Object> node);
diff --git a/src/compiler/compilation-dependencies.cc b/src/compiler/compilation-dependencies.cc
index 9499f9ab30a..4832cceb612 100644
--- a/src/compiler/compilation-dependencies.cc
+++ b/src/compiler/compilation-dependencies.cc
@@ -350,7 +350,7 @@ class ConstantInDictionaryPrototypeChainDependency final
     Isolate* isolate = broker->isolate();
 
     Handle<Object> holder;
-    HeapObject prototype = receiver_map_.object()->prototype();
+    Tagged<HeapObject> prototype = receiver_map_.object()->prototype();
 
     enum class ValidationResult { kFoundCorrect, kFoundIncorrect, kNotFound };
     auto try_load = [&](auto dictionary) -> ValidationResult {
@@ -365,8 +365,8 @@ class ConstantInDictionaryPrototypeChainDependency final
         return ValidationResult::kFoundIncorrect;
       }
 
-      Object dictionary_value = dictionary->ValueAt(entry);
-      Object value;
+      Tagged<Object> dictionary_value = dictionary->ValueAt(entry);
+      Tagged<Object> value;
       // We must be able to detect the case that the property |property_name_|
       // of |holder_| was originally a plain function |constant_| (when creating
       // this dependency) and has since become an accessor whose getter is
@@ -395,7 +395,7 @@ class ConstantInDictionaryPrototypeChainDependency final
       // We only care about JSObjects because that's the only type of holder
       // (and types of prototypes on the chain to the holder) that
       // AccessInfoFactory::ComputePropertyAccessInfo allows.
-      JSObject object = JSObject::cast(prototype);
+      Tagged<JSObject> object = JSObject::cast(prototype);
 
       // We only support dictionary mode prototypes on the chain for this kind
       // of dependency.
@@ -459,8 +459,8 @@ class OwnConstantDataPropertyDependency final : public CompilationDependency {
       return false;
     }
     DisallowGarbageCollection no_heap_allocation;
-    Object current_value = holder_.object()->RawFastPropertyAt(index_);
-    Object used_value = *value_.object();
+    Tagged<Object> current_value = holder_.object()->RawFastPropertyAt(index_);
+    Tagged<Object> used_value = *value_.object();
     if (representation_.IsDouble()) {
       // Compare doubles by bit pattern.
       if (!IsHeapNumber(current_value) || !IsHeapNumber(used_value) ||
@@ -530,7 +530,7 @@ class OwnConstantDictionaryPropertyDependency final
       return false;
     }
 
-    base::Optional<Object> maybe_value = JSObject::DictionaryPropertyAt(
+    base::Optional<Tagged<Object>> maybe_value = JSObject::DictionaryPropertyAt(
         holder_.object(), index_, broker->isolate()->heap());
 
     if (!maybe_value) {
@@ -893,7 +893,7 @@ class ObjectSlotValueDependency final : public CompilationDependency {
 
   bool IsValid(JSHeapBroker* broker) const override {
     PtrComprCageBase cage_base = GetPtrComprCageBase(*object_);
-    Object current_value =
+    Tagged<Object> current_value =
         offset_ == HeapObject::kMapOffset
             ? object_->map()
             : TaggedField<Object>::Relaxed_Load(cage_base, *object_, offset_);
@@ -967,8 +967,8 @@ class OwnConstantElementDependency final : public CompilationDependency {
 
   bool IsValid(JSHeapBroker* broker) const override {
     DisallowGarbageCollection no_gc;
-    JSObject holder = *holder_.object();
-    base::Optional<Object> maybe_element =
+    Tagged<JSObject> holder = *holder_.object();
+    base::Optional<Tagged<Object>> maybe_element =
         holder_.GetOwnConstantElementFromHeap(
             broker, holder->elements(), holder->GetElementsKind(), index_);
     if (!maybe_element.has_value()) return false;
diff --git a/src/compiler/graph-visualizer.cc b/src/compiler/graph-visualizer.cc
index 0eb8b2cd420..3d5b33657d3 100644
--- a/src/compiler/graph-visualizer.cc
+++ b/src/compiler/graph-visualizer.cc
@@ -93,7 +93,7 @@ void JsonPrintFunctionSource(std::ostream& os, int source_id,
   int end = 0;
   if (!script.is_null() && !IsUndefined(*script, isolate) &&
       !shared.is_null()) {
-    Object source_name = script->name();
+    Tagged<Object> source_name = script->name();
     os << ", \"sourceName\": \"";
     if (IsString(source_name)) {
       std::ostringstream escaped_name;
@@ -115,7 +115,7 @@ void JsonPrintFunctionSource(std::ostream& os, int source_id,
         }
 #if V8_ENABLE_WEBASSEMBLY
       } else if (shared->HasWasmExportedFunctionData()) {
-        WasmExportedFunctionData function_data =
+        Tagged<WasmExportedFunctionData> function_data =
             shared->wasm_exported_function_data();
         Handle<WasmInstanceObject> instance(function_data->instance(), isolate);
         const wasm::WasmModule* module = instance->module();
@@ -331,9 +331,10 @@ std::unique_ptr<char[]> GetVisualizerLogFileName(OptimizedCompilationInfo* info,
   bool source_available = false;
   if (v8_flags.trace_file_names && info->has_shared_info() &&
       IsScript(info->shared_info()->script())) {
-    Object source_name = Script::cast(info->shared_info()->script())->name();
+    Tagged<Object> source_name =
+        Script::cast(info->shared_info()->script())->name();
     if (IsString(source_name)) {
-      String str = String::cast(source_name);
+      Tagged<String> str = String::cast(source_name);
       if (str->length() > 0) {
         SNPrintF(source_file, "%s", str->ToCString().get());
         std::replace(source_file.begin(),
diff --git a/src/compiler/heap-refs.cc b/src/compiler/heap-refs.cc
index 9b8ad368056..c7d323f37dc 100644
--- a/src/compiler/heap-refs.cc
+++ b/src/compiler/heap-refs.cc
@@ -284,7 +284,7 @@ OptionalObjectRef GetOwnFastDataPropertyFromHeap(JSHeapBroker* broker,
                                                  JSObjectRef holder,
                                                  Representation representation,
                                                  FieldIndex field_index) {
-  base::Optional<Object> constant;
+  base::Optional<Tagged<Object>> constant;
   {
     DisallowGarbageCollection no_gc;
     PtrComprCageBase cage_base = broker->cage_base();
@@ -309,7 +309,7 @@ OptionalObjectRef GetOwnFastDataPropertyFromHeap(JSHeapBroker* broker,
         return {};
       }
     } else {
-      Object raw_properties_or_hash =
+      Tagged<Object> raw_properties_or_hash =
           holder.object()->raw_properties_or_hash(cage_base, kRelaxedLoad);
       // Ensure that the object is safe to inspect.
       if (broker->ObjectMayBeUninitialized(raw_properties_or_hash)) {
@@ -377,8 +377,9 @@ OptionalObjectRef GetOwnDictionaryPropertyFromHeap(JSHeapBroker* broker,
     DisallowGarbageCollection no_gc;
     // DictionaryPropertyAt will check that we are within the bounds of the
     // object.
-    base::Optional<Object> maybe_constant = JSObject::DictionaryPropertyAt(
-        receiver, dict_index, broker->isolate()->heap());
+    base::Optional<Tagged<Object>> maybe_constant =
+        JSObject::DictionaryPropertyAt(receiver, dict_index,
+                                       broker->isolate()->heap());
     DCHECK_IMPLIES(broker->IsMainThread(), maybe_constant);
     if (!maybe_constant) return {};
     constant = broker->CanonicalPersistentHandle(maybe_constant.value());
@@ -577,7 +578,7 @@ int InstanceSizeWithMinSlack(JSHeapBroker* broker, MapRef map) {
 
     static constexpr bool kConcurrentAccess = true;
     TransitionsAccessor(broker->isolate(), *map.object(), kConcurrentAccess)
-        .TraverseTransitionTree([&](Map m) {
+        .TraverseTransitionTree([&](Tagged<Map> m) {
           maps.push_back(broker->CanonicalPersistentHandle(m));
         });
   }
@@ -768,7 +769,7 @@ base::Optional<bool> HeapObjectData::TryGetBooleanValue(
 base::Optional<bool> HeapObjectData::TryGetBooleanValueImpl(
     JSHeapBroker* broker) const {
   DisallowGarbageCollection no_gc;
-  Object o = *object();
+  Tagged<Object> o = *object();
   Isolate* isolate = broker->isolate();
   const InstanceType t = GetMapInstanceType();
   if (IsTrue(o, isolate)) {
@@ -1238,7 +1239,7 @@ OptionalObjectRef JSObjectRef::RawInobjectPropertyAt(JSHeapBroker* broker,
       return {};
     }
 
-    base::Optional<Object> maybe_value =
+    base::Optional<Tagged<Object>> maybe_value =
         object()->RawInobjectPropertyAt(cage_base, current_map, index);
     if (!maybe_value.has_value()) {
       TRACE_BROKER_MISSING(broker,
@@ -1299,7 +1300,7 @@ MapRef MapRef::FindFieldOwner(JSHeapBroker* broker,
 
 OptionalObjectRef StringRef::GetCharAsStringOrUndefined(JSHeapBroker* broker,
                                                         uint32_t index) const {
-  String maybe_char;
+  Tagged<String> maybe_char;
   auto result = ConcurrentLookupIterator::TryGetOwnChar(
       &maybe_char, broker->isolate(), broker->local_isolate(), *object(),
       index);
@@ -1565,7 +1566,7 @@ int RegExpBoilerplateDescriptionRef::flags() const { return object()->flags(); }
 
 OptionalCallHandlerInfoRef FunctionTemplateInfoRef::call_code(
     JSHeapBroker* broker) const {
-  HeapObject call_code = object()->call_code(kAcquireLoad);
+  Tagged<HeapObject> call_code = object()->call_code(kAcquireLoad);
   if (i::IsUndefined(call_code)) return base::nullopt;
   return TryMakeRef(broker, CallHandlerInfo::cast(call_code));
 }
@@ -1592,7 +1593,7 @@ HolderLookupResult FunctionTemplateInfoRef::LookupHolderOfExpectedType(
   Handle<FunctionTemplateInfo> expected_receiver_type;
   {
     DisallowGarbageCollection no_gc;
-    HeapObject signature = object()->signature();
+    Tagged<HeapObject> signature = object()->signature();
     if (i::IsUndefined(signature)) {
       return HolderLookupResult(CallOptimization::kHolderIsReceiver);
     }
@@ -1903,7 +1904,7 @@ bool ObjectRef::should_access_heap() const {
 OptionalObjectRef JSObjectRef::GetOwnConstantElement(
     JSHeapBroker* broker, FixedArrayBaseRef elements_ref, uint32_t index,
     CompilationDependencies* dependencies) const {
-  base::Optional<Object> maybe_element = GetOwnConstantElementFromHeap(
+  base::Optional<Tagged<Object>> maybe_element = GetOwnConstantElementFromHeap(
       broker, *elements_ref.object(), map(broker).elements_kind(), index);
   if (!maybe_element.has_value()) return {};
 
@@ -1914,9 +1915,9 @@ OptionalObjectRef JSObjectRef::GetOwnConstantElement(
   return result;
 }
 
-base::Optional<Object> JSObjectRef::GetOwnConstantElementFromHeap(
-    JSHeapBroker* broker, FixedArrayBase elements, ElementsKind elements_kind,
-    uint32_t index) const {
+base::Optional<Tagged<Object>> JSObjectRef::GetOwnConstantElementFromHeap(
+    JSHeapBroker* broker, Tagged<FixedArrayBase> elements,
+    ElementsKind elements_kind, uint32_t index) const {
   DCHECK_LE(index, JSObject::kMaxElementIndex);
 
   Handle<JSObject> holder = object();
@@ -1932,7 +1933,7 @@ base::Optional<Object> JSObjectRef::GetOwnConstantElementFromHeap(
   // - The release-load of that map ensures we read the newest value
   //   of `length` below.
   if (i::IsJSArray(*holder)) {
-    Object array_length_obj =
+    Tagged<Object> array_length_obj =
         JSArray::cast(*holder)->length(broker->isolate(), kRelaxedLoad);
     if (!i::IsSmi(array_length_obj)) {
       // Can't safely read into HeapNumber objects without atomic semantics
@@ -1947,7 +1948,7 @@ base::Optional<Object> JSObjectRef::GetOwnConstantElementFromHeap(
     if (index >= array_length) return {};
   }
 
-  Object maybe_element;
+  Tagged<Object> maybe_element;
   auto result = ConcurrentLookupIterator::TryGetOwnConstantElement(
       &maybe_element, broker->isolate(), broker->local_isolate(), *holder,
       elements, elements_kind, index);
@@ -2030,9 +2031,10 @@ OptionalObjectRef JSArrayRef::GetOwnCowElement(JSHeapBroker* broker,
   // Likewise we only deal with smi lengths.
   if (!length_ref->IsSmi()) return {};
 
-  base::Optional<Object> result = ConcurrentLookupIterator::TryGetOwnCowElement(
-      broker->isolate(), *elements_ref.AsFixedArray().object(), elements_kind,
-      length_ref->AsSmi(), index);
+  base::Optional<Tagged<Object>> result =
+      ConcurrentLookupIterator::TryGetOwnCowElement(
+          broker->isolate(), *elements_ref.AsFixedArray().object(),
+          elements_kind, length_ref->AsSmi(), index);
   if (!result.has_value()) return {};
 
   return TryMakeRef(broker, result.value());
@@ -2126,7 +2128,7 @@ NameRef DescriptorArrayRef::GetPropertyKey(
 
 OptionalObjectRef DescriptorArrayRef::GetStrongValue(
     JSHeapBroker* broker, InternalIndex descriptor_index) const {
-  HeapObject heap_object;
+  Tagged<HeapObject> heap_object;
   if (!object()
            ->GetValue(descriptor_index)
            .GetHeapObjectIfStrong(&heap_object)) {
@@ -2316,7 +2318,7 @@ bool NativeContextRef::GlobalIsDetached(JSHeapBroker* broker) const {
 
 OptionalPropertyCellRef JSGlobalObjectRef::GetPropertyCell(JSHeapBroker* broker,
                                                            NameRef name) const {
-  base::Optional<PropertyCell> maybe_cell =
+  base::Optional<Tagged<PropertyCell>> maybe_cell =
       ConcurrentLookupIterator::TryGetPropertyCell(
           broker->isolate(), broker->local_isolate_or_isolate(),
           broker->target_native_context().global_object(broker).object(),
diff --git a/src/compiler/heap-refs.h b/src/compiler/heap-refs.h
index 77a2b0040b9..248efcfeec1 100644
--- a/src/compiler/heap-refs.h
+++ b/src/compiler/heap-refs.h
@@ -536,9 +536,9 @@ class JSObjectRef : public JSReceiverRef {
   // The direct-read implementation of the above, extracted into a helper since
   // it's also called from compilation-dependency validation. This helper is
   // guaranteed to not create new Ref instances.
-  base::Optional<Object> GetOwnConstantElementFromHeap(
-      JSHeapBroker* broker, FixedArrayBase elements, ElementsKind elements_kind,
-      uint32_t index) const;
+  base::Optional<Tagged<Object>> GetOwnConstantElementFromHeap(
+      JSHeapBroker* broker, Tagged<FixedArrayBase> elements,
+      ElementsKind elements_kind, uint32_t index) const;
 
   // Return the value of the property identified by the field {index}
   // if {index} is known to be an own data property of the object.
diff --git a/src/compiler/js-heap-broker.cc b/src/compiler/js-heap-broker.cc
index 7f7f5cb8ed8..5b6c6915b64 100644
--- a/src/compiler/js-heap-broker.cc
+++ b/src/compiler/js-heap-broker.cc
@@ -121,11 +121,13 @@ void JSHeapBroker::CollectArrayAndObjectPrototypes() {
   CHECK_EQ(mode(), kSerializing);
   CHECK(array_and_object_prototypes_.empty());
 
-  Object maybe_context = isolate()->heap()->native_contexts_list();
+  Tagged<Object> maybe_context = isolate()->heap()->native_contexts_list();
   while (!IsUndefined(maybe_context, isolate())) {
-    Context context = Context::cast(maybe_context);
-    Object array_prot = context->get(Context::INITIAL_ARRAY_PROTOTYPE_INDEX);
-    Object object_prot = context->get(Context::INITIAL_OBJECT_PROTOTYPE_INDEX);
+    Tagged<Context> context = Context::cast(maybe_context);
+    Tagged<Object> array_prot =
+        context->get(Context::INITIAL_ARRAY_PROTOTYPE_INDEX);
+    Tagged<Object> object_prot =
+        context->get(Context::INITIAL_OBJECT_PROTOTYPE_INDEX);
     array_and_object_prototypes_.emplace(
         CanonicalPersistentHandle(JSObject::cast(array_prot)));
     array_and_object_prototypes_.emplace(
@@ -166,7 +168,7 @@ bool JSHeapBroker::IsArrayOrObjectPrototype(Handle<JSObject> object) const {
          array_and_object_prototypes_.end();
 }
 
-ObjectData* JSHeapBroker::TryGetOrCreateData(Object object,
+ObjectData* JSHeapBroker::TryGetOrCreateData(Tagged<Object> object,
                                              GetOrCreateDataFlags flags) {
   return TryGetOrCreateData(CanonicalPersistentHandle(object), flags);
 }
@@ -178,7 +180,7 @@ ObjectData* JSHeapBroker::GetOrCreateData(Handle<Object> object,
   return return_value;
 }
 
-ObjectData* JSHeapBroker::GetOrCreateData(Object object,
+ObjectData* JSHeapBroker::GetOrCreateData(Tagged<Object> object,
                                           GetOrCreateDataFlags flags) {
   return GetOrCreateData(CanonicalPersistentHandle(object), flags);
 }
@@ -195,12 +197,12 @@ bool JSHeapBroker::ObjectMayBeUninitialized(Handle<Object> object) const {
   return ObjectMayBeUninitialized(*object);
 }
 
-bool JSHeapBroker::ObjectMayBeUninitialized(Object object) const {
+bool JSHeapBroker::ObjectMayBeUninitialized(Tagged<Object> object) const {
   if (!IsHeapObject(object)) return false;
   return ObjectMayBeUninitialized(HeapObject::cast(object));
 }
 
-bool JSHeapBroker::ObjectMayBeUninitialized(HeapObject object) const {
+bool JSHeapBroker::ObjectMayBeUninitialized(Tagged<HeapObject> object) const {
   return !IsMainThread() && isolate()->heap()->IsPendingAllocation(object);
 }
 
@@ -637,7 +639,7 @@ ProcessedFeedback const& JSHeapBroker::ReadFeedbackForArrayOrObjectLiteral(
   FeedbackNexus nexus(source.vector, source.slot, feedback_nexus_config());
   if (nexus.IsUninitialized()) return NewInsufficientFeedback(nexus.kind());
 
-  HeapObject object;
+  Tagged<HeapObject> object;
   if (!nexus.GetFeedback()->GetHeapObject(&object)) {
     return NewInsufficientFeedback(nexus.kind());
   }
@@ -652,7 +654,7 @@ ProcessedFeedback const& JSHeapBroker::ReadFeedbackForRegExpLiteral(
   FeedbackNexus nexus(source.vector, source.slot, feedback_nexus_config());
   if (nexus.IsUninitialized()) return NewInsufficientFeedback(nexus.kind());
 
-  HeapObject object;
+  Tagged<HeapObject> object;
   if (!nexus.GetFeedback()->GetHeapObject(&object)) {
     return NewInsufficientFeedback(nexus.kind());
   }
@@ -667,7 +669,7 @@ ProcessedFeedback const& JSHeapBroker::ReadFeedbackForTemplateObject(
   FeedbackNexus nexus(source.vector, source.slot, feedback_nexus_config());
   if (nexus.IsUninitialized()) return NewInsufficientFeedback(nexus.kind());
 
-  HeapObject object;
+  Tagged<HeapObject> object;
   if (!nexus.GetFeedback()->GetHeapObject(&object)) {
     return NewInsufficientFeedback(nexus.kind());
   }
@@ -684,7 +686,7 @@ ProcessedFeedback const& JSHeapBroker::ReadFeedbackForCall(
   OptionalHeapObjectRef target_ref;
   {
     MaybeObject maybe_target = nexus.GetFeedback();
-    HeapObject target_object;
+    Tagged<HeapObject> target_object;
     if (maybe_target->GetHeapObject(&target_object)) {
       target_ref = TryMakeRef(this, target_object);
     }
@@ -822,7 +824,7 @@ ElementAccessFeedback const& JSHeapBroker::ProcessFeedbackMapsForElementAccess(
 
   // Separate the actual receiver maps and the possible transition sources.
   for (MapRef map : maps) {
-    Map transition_target;
+    Tagged<Map> transition_target;
 
     // Don't generate elements kind transitions from stable maps.
     if (!map.is_stable()) {
@@ -876,7 +878,7 @@ void ElementAccessFeedback::AddGroup(TransitionGroup&& group) {
 }
 
 OptionalNameRef JSHeapBroker::GetNameFeedback(FeedbackNexus const& nexus) {
-  Name raw_name = nexus.GetName();
+  Tagged<Name> raw_name = nexus.GetName();
   if (raw_name.is_null()) return base::nullopt;
   return MakeRefAssumeMemoryFence(this, raw_name);
 }
diff --git a/src/compiler/js-heap-broker.h b/src/compiler/js-heap-broker.h
index fa5dbf14736..0a09e9378ee 100644
--- a/src/compiler/js-heap-broker.h
+++ b/src/compiler/js-heap-broker.h
@@ -181,18 +181,19 @@ class V8_EXPORT_PRIVATE JSHeapBroker {
 #endif  // DEBUG
 
   // Returns the handle from root index table for read only heap objects.
-  Handle<Object> GetRootHandle(Object object);
+  Handle<Object> GetRootHandle(Tagged<Object> object);
 
   // Never returns nullptr.
   ObjectData* GetOrCreateData(Handle<Object> object,
                               GetOrCreateDataFlags flags = {});
-  ObjectData* GetOrCreateData(Object object, GetOrCreateDataFlags flags = {});
+  ObjectData* GetOrCreateData(Tagged<Object> object,
+                              GetOrCreateDataFlags flags = {});
 
   // Gets data only if we have it. However, thin wrappers will be created for
   // smis, read-only objects and never-serialized objects.
   ObjectData* TryGetOrCreateData(Handle<Object> object,
                                  GetOrCreateDataFlags flags = {});
-  ObjectData* TryGetOrCreateData(Object object,
+  ObjectData* TryGetOrCreateData(Tagged<Object> object,
                                  GetOrCreateDataFlags flags = {});
 
   // Check if {object} is any native context's %ArrayPrototype% or
@@ -291,7 +292,7 @@ class V8_EXPORT_PRIVATE JSHeapBroker {
       }
     }
 
-    Object obj(address);
+    Tagged<Object> obj(address);
     auto find_result = canonical_handles_->FindOrInsert(obj);
     if (find_result.already_exists) return Handle<T>(*find_result.entry);
 
@@ -306,12 +307,6 @@ class V8_EXPORT_PRIVATE JSHeapBroker {
     return Handle<T>(*find_result.entry);
   }
 
-  template <typename T>
-  Handle<T> CanonicalPersistentHandle(T object) {
-    static_assert(kTaggedCanConvertToRawObjects);
-    return CanonicalPersistentHandle<T>(Tagged<T>(object));
-  }
-
   template <typename T>
   Handle<T> CanonicalPersistentHandle(Handle<T> object) {
     if (object.is_null()) return object;  // Can't deref a null handle.
@@ -378,8 +373,8 @@ class V8_EXPORT_PRIVATE JSHeapBroker {
   // thus safe to read from a memory safety perspective. The converse does not
   // necessarily hold.
   bool ObjectMayBeUninitialized(Handle<Object> object) const;
-  bool ObjectMayBeUninitialized(Object object) const;
-  bool ObjectMayBeUninitialized(HeapObject object) const;
+  bool ObjectMayBeUninitialized(Tagged<Object> object) const;
+  bool ObjectMayBeUninitialized(Tagged<HeapObject> object) const;
 
   void set_dependencies(CompilationDependencies* dependencies) {
     DCHECK_NOT_NULL(dependencies);
diff --git a/src/compiler/linkage.cc b/src/compiler/linkage.cc
index 1a80184149d..b3fdf3f1cb0 100644
--- a/src/compiler/linkage.cc
+++ b/src/compiler/linkage.cc
@@ -250,7 +250,7 @@ CallDescriptor* Linkage::ComputeIncoming(Zone* zone,
   if (!info->closure().is_null()) {
     // If we are compiling a JS function, use a JS call descriptor,
     // plus the receiver.
-    SharedFunctionInfo shared = info->closure()->shared();
+    Tagged<SharedFunctionInfo> shared = info->closure()->shared();
     return GetJSCallDescriptor(
         zone, info->is_osr(),
         shared->internal_formal_parameter_count_with_receiver(),
diff --git a/src/compiler/property-access-builder.cc b/src/compiler/property-access-builder.cc
index 5248d52e69e..185b338628e 100644
--- a/src/compiler/property-access-builder.cc
+++ b/src/compiler/property-access-builder.cc
@@ -163,7 +163,7 @@ base::Optional<Node*> PropertyAccessBuilder::FoldLoadDictPrototypeConstant(
     if (!IsJSReceiverMap(*map_handle)) {
       // Perform the implicit ToObject for primitives here.
       // Implemented according to ES6 section 7.3.2 GetV (V, P).
-      JSFunction constructor =
+      Tagged<JSFunction> constructor =
           Map::GetConstructorFunction(
               *map_handle, *broker()->target_native_context().object())
               .value();
diff --git a/src/compiler/turboshaft/assembler.h b/src/compiler/turboshaft/assembler.h
index 3a700fe26a6..af04ace287f 100644
--- a/src/compiler/turboshaft/assembler.h
+++ b/src/compiler/turboshaft/assembler.h
@@ -37,6 +37,7 @@
 #include "src/logging/runtime-call-stats.h"
 #include "src/objects/heap-number.h"
 #include "src/objects/oddball.h"
+#include "src/objects/tagged.h"
 #include "src/objects/turbofan-types.h"
 
 #ifdef V8_ENABLE_WEBASSEMBLY
@@ -1383,7 +1384,7 @@ class AssemblerOpInterface {
     return WordConstant(static_cast<uint64_t>(value),
                         WordRepresentation::PointerSized());
   }
-  V<Object> SmiConstant(Smi value) {
+  V<Object> SmiConstant(i::Tagged<Smi> value) {
     return V<Smi>::Cast(UintPtrConstant(value.ptr()));
   }
   V<Float32> Float32Constant(float value) {
diff --git a/src/compiler/turboshaft/assert-types-reducer.h b/src/compiler/turboshaft/assert-types-reducer.h
index 0d3845bf230..5f38e451238 100644
--- a/src/compiler/turboshaft/assert-types-reducer.h
+++ b/src/compiler/turboshaft/assert-types-reducer.h
@@ -38,7 +38,9 @@ class AssertTypesReducer
 
   using Adapter = UniformReducerAdapter<AssertTypesReducer, Next>;
 
-  Smi NoContextConstant() { return Smi::FromInt(Context::kNoContext); }
+  i::Tagged<Smi> NoContextConstant() {
+    return Smi::FromInt(Context::kNoContext);
+  }
 
   template <typename Op, typename Continuation>
   OpIndex ReduceInputGraphOperation(OpIndex ig_index, const Op& operation) {
@@ -75,7 +77,7 @@ class AssertTypesReducer
         [this](Builtin builtin, OpIndex original_value,
                base::SmallVector<OpIndex, 6> actual_value_indices,
                const Type& type) {
-          Smi op_id = Smi::FromInt(original_value.id());
+          i::Tagged<Smi> op_id = Smi::FromInt(original_value.id());
           // Add expected type and operation id.
           Handle<TurboshaftType> expected_type = type.AllocateOnHeap(factory());
           actual_value_indices.push_back(__ HeapConstant(expected_type));
diff --git a/src/compiler/wasm-compiler.cc b/src/compiler/wasm-compiler.cc
index 262fae707c5..a1dd77d2cb5 100644
--- a/src/compiler/wasm-compiler.cc
+++ b/src/compiler/wasm-compiler.cc
@@ -8116,8 +8116,8 @@ class WasmWrapperGraphBuilder : public WasmGraphBuilder {
           BuildReceiverNode(callable_node, native_context, undefined_node);
     }
 
-    SharedFunctionInfo shared = target->shared();
-    FunctionTemplateInfo api_func_data = shared->api_func_data();
+    Tagged<SharedFunctionInfo> shared = target->shared();
+    Tagged<FunctionTemplateInfo> api_func_data = shared->api_func_data();
     const Address c_address = api_func_data->GetCFunction(0);
     const v8::CFunctionInfo* c_signature = api_func_data->GetCSignature(0);
 
diff --git a/src/date/date.h b/src/date/date.h
index 467ba8802df..b2facb39504 100644
--- a/src/date/date.h
+++ b/src/date/date.h
@@ -156,7 +156,7 @@ class V8_EXPORT_PRIVATE DateCache {
   // We increment the stamp each time when the timezone information changes.
   // JSDate objects perform stamp check and invalidate their caches if
   // their saved stamp is not equal to the current stamp.
-  Smi stamp() { return stamp_; }
+  Tagged<Smi> stamp() { return stamp_; }
   void* stamp_address() { return &stamp_; }
 
   // These functions are virtual so that we can override them when testing.
diff --git a/src/debug/debug-coverage.cc b/src/debug/debug-coverage.cc
index 82f1f48013e..e28ae287cb0 100644
--- a/src/debug/debug-coverage.cc
+++ b/src/debug/debug-coverage.cc
@@ -23,7 +23,7 @@ class SharedToCounterMap
                                        base::DefaultAllocationPolicy> {
  public:
   using Entry = base::TemplateHashMapEntry<SharedFunctionInfo, uint32_t>;
-  inline void Add(SharedFunctionInfo key, uint32_t count) {
+  inline void Add(Tagged<SharedFunctionInfo> key, uint32_t count) {
     Entry* entry = LookupOrInsert(key, Hash(key), []() { return 0; });
     uint32_t old_count = entry->value;
     if (UINT32_MAX - count < old_count) {
@@ -33,14 +33,14 @@ class SharedToCounterMap
     }
   }
 
-  inline uint32_t Get(SharedFunctionInfo key) {
+  inline uint32_t Get(Tagged<SharedFunctionInfo> key) {
     Entry* entry = Lookup(key, Hash(key));
     if (entry == nullptr) return 0;
     return entry->value;
   }
 
  private:
-  static uint32_t Hash(SharedFunctionInfo key) {
+  static uint32_t Hash(Tagged<SharedFunctionInfo> key) {
     return static_cast<uint32_t>(key.ptr());
   }
 
@@ -48,7 +48,7 @@ class SharedToCounterMap
 };
 
 namespace {
-int StartPosition(SharedFunctionInfo info) {
+int StartPosition(Tagged<SharedFunctionInfo> info) {
   int start = info->function_token_position();
   if (start == kNoSourcePosition) start = info->StartPosition();
   return start;
@@ -66,11 +66,11 @@ void SortBlockData(std::vector<CoverageBlock>& v) {
   std::sort(v.begin(), v.end(), CompareCoverageBlock);
 }
 
-std::vector<CoverageBlock> GetSortedBlockData(Isolate* isolate,
-                                              SharedFunctionInfo shared) {
+std::vector<CoverageBlock> GetSortedBlockData(
+    Isolate* isolate, Tagged<SharedFunctionInfo> shared) {
   DCHECK(shared->HasCoverageInfo(isolate));
 
-  CoverageInfo coverage_info =
+  Tagged<CoverageInfo> coverage_info =
       CoverageInfo::cast(shared->GetDebugInfo(isolate)->coverage_info());
 
   std::vector<CoverageBlock> result;
@@ -379,10 +379,10 @@ void ClampToBinary(CoverageFunction* function) {
   }
 }
 
-void ResetAllBlockCounts(Isolate* isolate, SharedFunctionInfo shared) {
+void ResetAllBlockCounts(Isolate* isolate, Tagged<SharedFunctionInfo> shared) {
   DCHECK(shared->HasCoverageInfo(isolate));
 
-  CoverageInfo coverage_info =
+  Tagged<CoverageInfo> coverage_info =
       CoverageInfo::cast(shared->GetDebugInfo(isolate)->coverage_info());
 
   for (int i = 0; i < coverage_info->slot_count(); i++) {
@@ -411,7 +411,7 @@ bool IsBinaryMode(debug::CoverageMode mode) {
 }
 
 void CollectBlockCoverageInternal(Isolate* isolate, CoverageFunction* function,
-                                  SharedFunctionInfo info,
+                                  Tagged<SharedFunctionInfo> info,
                                   debug::CoverageMode mode) {
   DCHECK(IsBlockMode(mode));
 
@@ -469,7 +469,8 @@ void CollectBlockCoverageInternal(Isolate* isolate, CoverageFunction* function,
 }
 
 void CollectBlockCoverage(Isolate* isolate, CoverageFunction* function,
-                          SharedFunctionInfo info, debug::CoverageMode mode) {
+                          Tagged<SharedFunctionInfo> info,
+                          debug::CoverageMode mode) {
   CollectBlockCoverageInternal(isolate, function, info, mode);
 
   // Reset all counters on the DebugInfo to zero.
@@ -477,7 +478,8 @@ void CollectBlockCoverage(Isolate* isolate, CoverageFunction* function,
 }
 
 void PrintBlockCoverage(const CoverageFunction* function,
-                        SharedFunctionInfo info, bool has_nonempty_source_range,
+                        Tagged<SharedFunctionInfo> info,
+                        bool has_nonempty_source_range,
                         bool function_is_relevant) {
   DCHECK(v8_flags.trace_block_coverage);
   std::unique_ptr<char[]> function_name =
@@ -512,8 +514,8 @@ void CollectAndMaybeResetCounts(Isolate* isolate,
       Handle<ArrayList> list = Handle<ArrayList>::cast(
           isolate->factory()->feedback_vectors_for_profiling_tools());
       for (int i = 0; i < list->Length(); i++) {
-        FeedbackVector vector = FeedbackVector::cast(list->Get(i));
-        SharedFunctionInfo shared = vector->shared_function_info();
+        Tagged<FeedbackVector> vector = FeedbackVector::cast(list->Get(i));
+        Tagged<SharedFunctionInfo> shared = vector->shared_function_info();
         DCHECK(shared->IsSubjectToDebugging());
         uint32_t count = static_cast<uint32_t>(vector->invocation_count());
         if (reset_count) vector->clear_invocation_count(kRelaxedStore);
@@ -527,11 +529,11 @@ void CollectAndMaybeResetCounts(Isolate* isolate,
       DCHECK_EQ(v8::debug::CoverageMode::kBestEffort, coverage_mode);
       AllowGarbageCollection allow_gc;
       HeapObjectIterator heap_iterator(isolate->heap());
-      for (HeapObject current_obj = heap_iterator.Next();
+      for (Tagged<HeapObject> current_obj = heap_iterator.Next();
            !current_obj.is_null(); current_obj = heap_iterator.Next()) {
         if (!IsJSFunction(current_obj)) continue;
-        JSFunction func = JSFunction::cast(current_obj);
-        SharedFunctionInfo shared = func->shared();
+        Tagged<JSFunction> func = JSFunction::cast(current_obj);
+        Tagged<SharedFunctionInfo> shared = func->shared();
         if (!shared->IsSubjectToDebugging()) continue;
         if (!(func->has_feedback_vector() ||
               func->has_closure_feedback_cell_array())) {
@@ -556,7 +558,7 @@ void CollectAndMaybeResetCounts(Isolate* isolate,
       // vector wasn't allocated yet and the function's interrupt budget wasn't
       // updated (i.e. it didn't execute return / jump).
       for (JavaScriptStackFrameIterator it(isolate); !it.done(); it.Advance()) {
-        SharedFunctionInfo shared = it.frame()->function()->shared();
+        Tagged<SharedFunctionInfo> shared = it.frame()->function()->shared();
         if (counter_map->Get(shared) != 0) continue;
         counter_map->Add(shared, 1);
       }
@@ -630,7 +632,7 @@ std::unique_ptr<Coverage> Coverage::Collect(
 
   std::vector<Handle<Script>> scripts;
   Script::Iterator scriptIt(isolate);
-  for (Script script = scriptIt.Next(); !script.is_null();
+  for (Tagged<Script> script = scriptIt.Next(); !script.is_null();
        script = scriptIt.Next()) {
     if (script->IsUserJavaScript()) scripts.push_back(handle(script, isolate));
   }
@@ -645,7 +647,7 @@ std::unique_ptr<Coverage> Coverage::Collect(
     {
       // Sort functions by start position, from outer to inner functions.
       SharedFunctionInfo::ScriptIterator infos(isolate, *script);
-      for (SharedFunctionInfo info = infos.Next(); !info.is_null();
+      for (Tagged<SharedFunctionInfo> info = infos.Next(); !info.is_null();
            info = infos.Next()) {
         sorted.emplace_back(handle(info, isolate), counter_map.Get(info));
       }
@@ -780,10 +782,10 @@ void Coverage::SelectMode(Isolate* isolate, debug::CoverageMode mode) {
       std::vector<Handle<JSFunction>> funcs_needing_feedback_vector;
       {
         HeapObjectIterator heap_iterator(isolate->heap());
-        for (HeapObject o = heap_iterator.Next(); !o.is_null();
+        for (Tagged<HeapObject> o = heap_iterator.Next(); !o.is_null();
              o = heap_iterator.Next()) {
           if (IsJSFunction(o)) {
-            JSFunction func = JSFunction::cast(o);
+            Tagged<JSFunction> func = JSFunction::cast(o);
             if (func->has_closure_feedback_cell_array()) {
               funcs_needing_feedback_vector.push_back(
                   Handle<JSFunction>(func, isolate));
@@ -792,7 +794,7 @@ void Coverage::SelectMode(Isolate* isolate, debug::CoverageMode mode) {
             // If collecting binary coverage, reset
             // SFI::has_reported_binary_coverage to avoid optimizing / inlining
             // functions before they have reported coverage.
-            SharedFunctionInfo shared = SharedFunctionInfo::cast(o);
+            Tagged<SharedFunctionInfo> shared = SharedFunctionInfo::cast(o);
             shared->set_has_reported_binary_coverage(false);
           } else if (IsFeedbackVector(o)) {
             // In any case, clear any collected invocation counts.
diff --git a/src/debug/debug-frames.cc b/src/debug/debug-frames.cc
index 47ce904261c..30e09932bb1 100644
--- a/src/debug/debug-frames.cc
+++ b/src/debug/debug-frames.cc
@@ -97,9 +97,8 @@ bool FrameInspector::ParameterIsShadowedByContextLocal(
   return info->ContextSlotIndex(parameter_name) != -1;
 }
 
-RedirectActiveFunctions::RedirectActiveFunctions(Isolate* isolate,
-                                                 SharedFunctionInfo shared,
-                                                 Mode mode)
+RedirectActiveFunctions::RedirectActiveFunctions(
+    Isolate* isolate, Tagged<SharedFunctionInfo> shared, Mode mode)
     : shared_(shared), mode_(mode) {
   DCHECK(shared->HasBytecodeArray());
   DCHECK_IMPLIES(mode == Mode::kUseDebugBytecode,
@@ -111,12 +110,12 @@ void RedirectActiveFunctions::VisitThread(Isolate* isolate,
   for (JavaScriptStackFrameIterator it(isolate, top); !it.done();
        it.Advance()) {
     JavaScriptFrame* frame = it.frame();
-    JSFunction function = frame->function();
+    Tagged<JSFunction> function = frame->function();
     if (!frame->is_interpreted()) continue;
     if (function->shared() != shared_) continue;
     InterpretedFrame* interpreted_frame =
         reinterpret_cast<InterpretedFrame*>(frame);
-    BytecodeArray bytecode =
+    Tagged<BytecodeArray> bytecode =
         mode_ == Mode::kUseDebugBytecode
             ? shared_->GetDebugInfo(isolate)->DebugBytecodeArray()
             : shared_->GetBytecodeArray(isolate);
diff --git a/src/debug/debug-frames.h b/src/debug/debug-frames.h
index c11c5fa46b2..b8e99e7fb44 100644
--- a/src/debug/debug-frames.h
+++ b/src/debug/debug-frames.h
@@ -70,7 +70,7 @@ class RedirectActiveFunctions : public ThreadVisitor {
     kUseDebugBytecode,
   };
 
-  RedirectActiveFunctions(Isolate* isolate, SharedFunctionInfo shared,
+  RedirectActiveFunctions(Isolate* isolate, Tagged<SharedFunctionInfo> shared,
                           Mode mode);
 
   void VisitThread(Isolate* isolate, ThreadLocalTop* top) override;
diff --git a/src/debug/debug-interface.cc b/src/debug/debug-interface.cc
index b433a7dc1d4..fb3b6b69e4b 100644
--- a/src/debug/debug-interface.cc
+++ b/src/debug/debug-interface.cc
@@ -42,7 +42,7 @@ void SetContextId(Local<Context> context, int id) {
 int GetContextId(Local<Context> context) {
   auto v8_context = Utils::OpenHandle(*context);
   DCHECK_NO_SCRIPT_NO_EXCEPTION(v8_context->GetIsolate());
-  i::Object value = v8_context->debug_context_id();
+  i::Tagged<i::Object> value = v8_context->debug_context_id();
   return (IsSmi(value)) ? i::Smi::ToInt(value) : 0;
 }
 
@@ -563,7 +563,7 @@ MaybeLocal<String> Script::GetSha256Hash() const {
 
 Maybe<int> Script::ContextId() const {
   i::Handle<i::Script> script = Utils::OpenHandle(this);
-  i::Object value = script->context_data();
+  i::Tagged<i::Object> value = script->context_data();
   if (IsSmi(value)) return Just(i::Smi::ToInt(value));
   return Nothing<int>();
 }
@@ -759,7 +759,8 @@ bool Script::SetInstrumentationBreakpoint(BreakpointId* id) const {
   }
 #endif  // V8_ENABLE_WEBASSEMBLY
   i::SharedFunctionInfo::ScriptIterator it(isolate, *script);
-  for (i::SharedFunctionInfo sfi = it.Next(); !sfi.is_null(); sfi = it.Next()) {
+  for (i::Tagged<i::SharedFunctionInfo> sfi = it.Next(); !sfi.is_null();
+       sfi = it.Next()) {
     if (sfi->is_toplevel()) {
       return isolate->debug()->SetBreakpointForFunction(
           handle(sfi, isolate), isolate->factory()->empty_string(), id,
@@ -952,7 +953,7 @@ void GetLoadedScripts(Isolate* v8_isolate,
   {
     i::DisallowGarbageCollection no_gc;
     i::Script::Iterator iterator(isolate);
-    for (i::Script script = iterator.Next(); !script.is_null();
+    for (i::Tagged<i::Script> script = iterator.Next(); !script.is_null();
          script = iterator.Next()) {
 #if V8_ENABLE_WEBASSEMBLY
       if (script->type() != i::Script::Type::kNormal &&
@@ -960,7 +961,7 @@ void GetLoadedScripts(Isolate* v8_isolate,
         continue;
       }
 #else
-      if (script.type() != i::Script::Type::kNormal) continue;
+      if (script->type() != i::Script::Type::kNormal) continue;
 #endif  // V8_ENABLE_WEBASSEMBLY
       if (!script->HasValidSource()) continue;
       i::HandleScope handle_scope(isolate);
@@ -1018,7 +1019,7 @@ void ResetBlackboxedStateCache(Isolate* v8_isolate, Local<Script> script) {
   i::DisallowGarbageCollection no_gc;
   i::SharedFunctionInfo::ScriptIterator iter(isolate,
                                              *Utils::OpenHandle(*script));
-  for (i::SharedFunctionInfo info = iter.Next(); !info.is_null();
+  for (i::Tagged<i::SharedFunctionInfo> info = iter.Next(); !info.is_null();
        info = iter.Next()) {
     if (auto debug_info = isolate->debug()->TryGetDebugInfo(info)) {
       debug_info->set_computed_debug_is_blackboxed(false);
@@ -1121,7 +1122,7 @@ v8::Local<v8::Message> CreateMessageFromException(
 
 MaybeLocal<Script> GeneratorObject::Script() {
   i::Handle<i::JSGeneratorObject> obj = Utils::OpenHandle(this);
-  i::Object maybe_script = obj->function()->shared()->script();
+  i::Tagged<i::Object> maybe_script = obj->function()->shared()->script();
   if (!IsScript(maybe_script)) return {};
   i::Handle<i::Script> script(i::Script::cast(maybe_script), obj->GetIsolate());
   return ToApiHandle<v8::debug::Script>(script);
@@ -1135,7 +1136,7 @@ Local<Function> GeneratorObject::Function() {
 Location GeneratorObject::SuspendedLocation() {
   i::Handle<i::JSGeneratorObject> obj = Utils::OpenHandle(this);
   CHECK(obj->is_suspended());
-  i::Object maybe_script = obj->function()->shared()->script();
+  i::Tagged<i::Object> maybe_script = obj->function()->shared()->script();
   if (!IsScript(maybe_script)) return Location();
   i::Isolate* isolate = obj->GetIsolate();
   i::Handle<i::Script> script(i::Script::cast(maybe_script), isolate);
diff --git a/src/debug/debug-scopes.cc b/src/debug/debug-scopes.cc
index de475ebbf55..1d499d558b7 100644
--- a/src/debug/debug-scopes.cc
+++ b/src/debug/debug-scopes.cc
@@ -49,7 +49,7 @@ Handle<Object> ScopeIterator::GetFunctionDebugName() const {
 
   if (!IsNativeContext(*context_)) {
     DisallowGarbageCollection no_gc;
-    ScopeInfo closure_info = context_->closure_context()->scope_info();
+    Tagged<ScopeInfo> closure_info = context_->closure_context()->scope_info();
     Handle<String> debug_name(closure_info->FunctionDebugName(), isolate_);
     if (debug_name->length() > 0) return debug_name;
   }
@@ -225,7 +225,8 @@ void ScopeIterator::TryParseAndRetrieveScopes(ReparseStrategy strategy) {
   }
 
   if (strategy == ReparseStrategy::kScriptIfNeeded) {
-    Object maybe_block_list = isolate_->LocalsBlockListCacheGet(scope_info);
+    Tagged<Object> maybe_block_list =
+        isolate_->LocalsBlockListCacheGet(scope_info);
     calculate_blocklists_ = IsTheHole(maybe_block_list);
     strategy = calculate_blocklists_ ? ReparseStrategy::kScriptIfNeeded
                                      : ReparseStrategy::kFunctionLiteral;
@@ -319,9 +320,9 @@ void ScopeIterator::TryParseAndRetrieveScopes(ReparseStrategy strategy) {
 
 void ScopeIterator::UnwrapEvaluationContext() {
   if (!context_->IsDebugEvaluateContext()) return;
-  Context current = *context_;
+  Tagged<Context> current = *context_;
   do {
-    Object wrapped = current->get(Context::WRAPPED_CONTEXT_INDEX);
+    Tagged<Object> wrapped = current->get(Context::WRAPPED_CONTEXT_INDEX);
     if (IsContext(wrapped)) {
       current = Context::cast(wrapped);
     } else {
@@ -795,7 +796,7 @@ void ScopeIterator::VisitModuleScope(const Visitor& visitor) const {
     int index;
     Handle<String> name;
     {
-      String raw_name;
+      Tagged<String> raw_name;
       scope_info->ModuleVariable(i, &raw_name, &index);
       if (ScopeInfo::VariableIsSynthetic(raw_name)) continue;
       name = handle(raw_name, isolate_);
@@ -875,7 +876,7 @@ bool ScopeIterator::VisitLocals(const Visitor& visitor, Mode mode,
         if (frame_inspector_ == nullptr) {
           // Get the variable from the suspended generator.
           DCHECK(!generator_.is_null());
-          FixedArray parameters_and_registers =
+          Tagged<FixedArray> parameters_and_registers =
               generator_->parameters_and_registers();
           DCHECK_LT(index, parameters_and_registers->length());
           value = handle(parameters_and_registers->get(index), isolate_);
@@ -891,7 +892,7 @@ bool ScopeIterator::VisitLocals(const Visitor& visitor, Mode mode,
         if (frame_inspector_ == nullptr) {
           // Get the variable from the suspended generator.
           DCHECK(!generator_.is_null());
-          FixedArray parameters_and_registers =
+          Tagged<FixedArray> parameters_and_registers =
               generator_->parameters_and_registers();
           int parameter_count =
               function_->shared()->scope_info()->ParameterCount();
@@ -1250,9 +1251,9 @@ Handle<ScopeInfo> LocalBlocklistsCollector::FindScopeInfoForScope(
     Scope* scope) const {
   DisallowGarbageCollection no_gc;
   SharedFunctionInfo::ScriptIterator iterator(isolate_, *script_);
-  for (SharedFunctionInfo info = iterator.Next(); !info.is_null();
+  for (Tagged<SharedFunctionInfo> info = iterator.Next(); !info.is_null();
        info = iterator.Next()) {
-    ScopeInfo scope_info = info->scope_info();
+    Tagged<ScopeInfo> scope_info = info->scope_info();
     if (info->is_compiled() && !scope_info.is_null() &&
         scope->start_position() == info->StartPosition() &&
         scope->end_position() == info->EndPosition() &&
diff --git a/src/debug/debug-wasm-objects.cc b/src/debug/debug-wasm-objects.cc
index 678caf30954..580caf4d2ce 100644
--- a/src/debug/debug-wasm-objects.cc
+++ b/src/debug/debug-wasm-objects.cc
@@ -921,7 +921,8 @@ Handle<WasmValueObject> WasmValueObject::New(
     case wasm::kRef: {
       Handle<Object> ref = value.to_ref();
       if (IsWasmStruct(*ref)) {
-        WasmTypeInfo type_info = ref->GetHeapObject()->map()->wasm_type_info();
+        Tagged<WasmTypeInfo> type_info =
+            ref->GetHeapObject()->map()->wasm_type_info();
         wasm::ValueType type = wasm::ValueType::FromIndex(
             wasm::ValueKind::kRef, type_info->type_index());
         // The cast is safe; structs always have the instance defined.
@@ -931,7 +932,8 @@ Handle<WasmValueObject> WasmValueObject::New(
         t = GetRefTypeName(isolate, type, module->native_module());
         v = StructProxy::Create(isolate, Handle<WasmStruct>::cast(ref), module);
       } else if (IsWasmArray(*ref)) {
-        WasmTypeInfo type_info = ref->GetHeapObject()->map()->wasm_type_info();
+        Tagged<WasmTypeInfo> type_info =
+            ref->GetHeapObject()->map()->wasm_type_info();
         wasm::ValueType type = wasm::ValueType::FromIndex(
             wasm::ValueKind::kRef, type_info->type_index());
         // The cast is safe; arrays always have the instance defined.
diff --git a/src/debug/debug.cc b/src/debug/debug.cc
index 82856692c59..da98ec885f3 100644
--- a/src/debug/debug.cc
+++ b/src/debug/debug.cc
@@ -174,13 +174,14 @@ void BreakLocation::AllAtCurrentStatement(
   }
 }
 
-JSGeneratorObject BreakLocation::GetGeneratorObjectForSuspendedFrame(
+Tagged<JSGeneratorObject> BreakLocation::GetGeneratorObjectForSuspendedFrame(
     JavaScriptFrame* frame) const {
   DCHECK(IsSuspend());
   DCHECK_GE(generator_obj_reg_index_, 0);
 
-  Object generator_obj = UnoptimizedFrame::cast(frame)->ReadInterpreterRegister(
-      generator_obj_reg_index_);
+  Tagged<Object> generator_obj =
+      UnoptimizedFrame::cast(frame)->ReadInterpreterRegister(
+          generator_obj_reg_index_);
 
   return JSGeneratorObject::cast(generator_obj);
 }
@@ -291,7 +292,7 @@ void BreakIterator::Next() {
 }
 
 DebugBreakType BreakIterator::GetDebugBreakType() {
-  BytecodeArray bytecode_array = debug_info_->OriginalBytecodeArray();
+  Tagged<BytecodeArray> bytecode_array = debug_info_->OriginalBytecodeArray();
   interpreter::Bytecode bytecode =
       interpreter::Bytecodes::FromByte(bytecode_array->get(code_offset()));
 
@@ -340,8 +341,8 @@ void BreakIterator::ClearDebugBreak() {
   DebugBreakType debug_break_type = GetDebugBreakType();
   if (debug_break_type == DEBUGGER_STATEMENT) return;
   DCHECK(debug_break_type >= DEBUG_BREAK_SLOT);
-  BytecodeArray bytecode_array = debug_info_->DebugBytecodeArray();
-  BytecodeArray original = debug_info_->OriginalBytecodeArray();
+  Tagged<BytecodeArray> bytecode_array = debug_info_->DebugBytecodeArray();
+  Tagged<BytecodeArray> original = debug_info_->OriginalBytecodeArray();
   bytecode_array->set(code_offset(), original->get(code_offset()));
 }
 
@@ -357,7 +358,7 @@ BreakLocation BreakIterator::GetBreakLocation() {
     // index that holds the generator object by reading it directly off the
     // bytecode array, and we'll read the actual generator object off the
     // interpreter stack frame in GetGeneratorObjectForSuspendedFrame.
-    BytecodeArray bytecode_array = debug_info_->OriginalBytecodeArray();
+    Tagged<BytecodeArray> bytecode_array = debug_info_->OriginalBytecodeArray();
     interpreter::BytecodeArrayIterator iterator(
         handle(bytecode_array, isolate()), code_offset());
 
@@ -458,7 +459,8 @@ void Debug::Iterate(RootVisitor* v, ThreadLocal* thread_local_data) {
                       FullObjectSlot(&thread_local_data->promise_stack_));
 }
 
-void DebugInfoCollection::Insert(SharedFunctionInfo sfi, DebugInfo debug_info) {
+void DebugInfoCollection::Insert(Tagged<SharedFunctionInfo> sfi,
+                                 Tagged<DebugInfo> debug_info) {
   DisallowGarbageCollection no_gc;
   base::SharedMutexGuard<base::kExclusive> mutex_guard(
       isolate_->shared_function_info_access());
@@ -473,7 +475,7 @@ void DebugInfoCollection::Insert(SharedFunctionInfo sfi, DebugInfo debug_info) {
   DCHECK_EQ(list_.size(), map_.size());
 }
 
-bool DebugInfoCollection::Contains(SharedFunctionInfo sfi) const {
+bool DebugInfoCollection::Contains(Tagged<SharedFunctionInfo> sfi) const {
   auto it = map_.find(sfi->unique_id());
   if (it == map_.end()) return false;
   DCHECK_EQ(DebugInfo::cast(Object(*it->second))->shared(), sfi);
@@ -481,18 +483,18 @@ bool DebugInfoCollection::Contains(SharedFunctionInfo sfi) const {
 }
 
 base::Optional<DebugInfo> DebugInfoCollection::Find(
-    SharedFunctionInfo sfi) const {
+    Tagged<SharedFunctionInfo> sfi) const {
   auto it = map_.find(sfi->unique_id());
   if (it == map_.end()) return {};
-  DebugInfo di = DebugInfo::cast(Object(*it->second));
+  Tagged<DebugInfo> di = DebugInfo::cast(Object(*it->second));
   DCHECK_EQ(di->shared(), sfi);
   return di;
 }
 
-void DebugInfoCollection::DeleteSlow(SharedFunctionInfo sfi) {
+void DebugInfoCollection::DeleteSlow(Tagged<SharedFunctionInfo> sfi) {
   DebugInfoCollection::Iterator it(this);
   for (; it.HasNext(); it.Advance()) {
-    DebugInfo debug_info = it.Next();
+    Tagged<DebugInfo> debug_info = it.Next();
     if (debug_info->shared() != sfi) continue;
     it.DeleteNext();
     return;
@@ -500,7 +502,7 @@ void DebugInfoCollection::DeleteSlow(SharedFunctionInfo sfi) {
   UNREACHABLE();
 }
 
-DebugInfo DebugInfoCollection::EntryAsDebugInfo(size_t index) const {
+Tagged<DebugInfo> DebugInfoCollection::EntryAsDebugInfo(size_t index) const {
   DCHECK_LT(index, list_.size());
   return DebugInfo::cast(Object(*list_[index]));
 }
@@ -509,8 +511,8 @@ void DebugInfoCollection::DeleteIndex(size_t index) {
   base::SharedMutexGuard<base::kExclusive> mutex_guard(
       isolate_->shared_function_info_access());
 
-  DebugInfo debug_info = EntryAsDebugInfo(index);
-  SharedFunctionInfo sfi = debug_info->shared();
+  Tagged<DebugInfo> debug_info = EntryAsDebugInfo(index);
+  Tagged<SharedFunctionInfo> sfi = debug_info->shared();
   DCHECK(Contains(sfi));
 
   auto it = map_.find(sfi->unique_id());
@@ -919,10 +921,10 @@ void Debug::ApplyBreakPoints(Handle<DebugInfo> debug_info) {
     debug_info->SetBreakAtEntry();
   } else {
     if (!debug_info->HasInstrumentedBytecodeArray()) return;
-    FixedArray break_points = debug_info->break_points();
+    Tagged<FixedArray> break_points = debug_info->break_points();
     for (int i = 0; i < break_points->length(); i++) {
       if (IsUndefined(break_points->get(i), isolate_)) continue;
-      BreakPointInfo info = BreakPointInfo::cast(break_points->get(i));
+      Tagged<BreakPointInfo> info = BreakPointInfo::cast(break_points->get(i));
       if (info->GetBreakPointCount(isolate_) == 0) continue;
       DCHECK(debug_info->HasInstrumentedBytecodeArray());
       BreakIterator it(debug_info);
@@ -1055,7 +1057,7 @@ void Debug::RecordWasmScriptWithBreakpoints(Handle<Script> script) {
     DisallowGarbageCollection no_gc;
     for (int idx = wasm_scripts_with_break_points_->length() - 1; idx >= 0;
          --idx) {
-      HeapObject wasm_script;
+      Tagged<HeapObject> wasm_script;
       if (wasm_scripts_with_break_points_->Get(idx).GetHeapObject(
               &wasm_script) &&
           wasm_script == *script) {
@@ -1087,10 +1089,10 @@ void Debug::ClearAllBreakPoints() {
     DisallowGarbageCollection no_gc;
     for (int idx = wasm_scripts_with_break_points_->length() - 1; idx >= 0;
          --idx) {
-      HeapObject raw_wasm_script;
+      Tagged<HeapObject> raw_wasm_script;
       if (wasm_scripts_with_break_points_->Get(idx).GetHeapObject(
               &raw_wasm_script)) {
-        Script wasm_script = Script::cast(raw_wasm_script);
+        Tagged<Script> wasm_script = Script::cast(raw_wasm_script);
         WasmScript::ClearAllBreakpoints(wasm_script);
         wasm_script->wasm_native_module()->GetDebugInfo()->RemoveIsolate(
             isolate_);
@@ -1484,7 +1486,7 @@ Handle<Object> Debug::GetSourceBreakLocations(
   int count = 0;
   for (int i = 0; i < debug_info->break_points()->length(); ++i) {
     if (!IsUndefined(debug_info->break_points()->get(i), isolate)) {
-      BreakPointInfo break_point_info =
+      Tagged<BreakPointInfo> break_point_info =
           BreakPointInfo::cast(debug_info->break_points()->get(i));
       int break_points = break_point_info->GetBreakPointCount(isolate);
       if (break_points == 0) continue;
@@ -1535,7 +1537,7 @@ void Debug::ClearOneShot() {
 namespace {
 class DiscardBaselineCodeVisitor : public ThreadVisitor {
  public:
-  explicit DiscardBaselineCodeVisitor(SharedFunctionInfo shared)
+  explicit DiscardBaselineCodeVisitor(Tagged<SharedFunctionInfo> shared)
       : shared_(shared) {}
   DiscardBaselineCodeVisitor() : shared_(SharedFunctionInfo()) {}
 
@@ -1589,7 +1591,7 @@ class DiscardBaselineCodeVisitor : public ThreadVisitor {
 };
 }  // namespace
 
-void Debug::DiscardBaselineCode(SharedFunctionInfo shared) {
+void Debug::DiscardBaselineCode(Tagged<SharedFunctionInfo> shared) {
   RCS_SCOPE(isolate_, RuntimeCallCounterId::kDebugger);
   DCHECK(shared->HasBaselineCode());
   DiscardBaselineCodeVisitor visitor(shared);
@@ -1599,10 +1601,10 @@ void Debug::DiscardBaselineCode(SharedFunctionInfo shared) {
   HeapObjectIterator iterator(isolate_->heap());
   auto trampoline = BUILTIN_CODE(isolate_, InterpreterEntryTrampoline);
   shared->FlushBaselineCode();
-  for (HeapObject obj = iterator.Next(); !obj.is_null();
+  for (Tagged<HeapObject> obj = iterator.Next(); !obj.is_null();
        obj = iterator.Next()) {
     if (IsJSFunction(obj)) {
-      JSFunction fun = JSFunction::cast(obj);
+      Tagged<JSFunction> fun = JSFunction::cast(obj);
       if (fun->shared() == shared && fun->ActiveTierIsBaseline()) {
         fun->set_code(*trampoline);
       }
@@ -1617,15 +1619,15 @@ void Debug::DiscardAllBaselineCode() {
   HeapObjectIterator iterator(isolate_->heap());
   auto trampoline = BUILTIN_CODE(isolate_, InterpreterEntryTrampoline);
   isolate_->thread_manager()->IterateArchivedThreads(&visitor);
-  for (HeapObject obj = iterator.Next(); !obj.is_null();
+  for (Tagged<HeapObject> obj = iterator.Next(); !obj.is_null();
        obj = iterator.Next()) {
     if (IsJSFunction(obj)) {
-      JSFunction fun = JSFunction::cast(obj);
+      Tagged<JSFunction> fun = JSFunction::cast(obj);
       if (fun->ActiveTierIsBaseline()) {
         fun->set_code(*trampoline);
       }
     } else if (IsSharedFunctionInfo(obj)) {
-      SharedFunctionInfo shared = SharedFunctionInfo::cast(obj);
+      Tagged<SharedFunctionInfo> shared = SharedFunctionInfo::cast(obj);
       if (shared->HasBaselineCode()) {
         shared->FlushBaselineCode();
       }
@@ -1686,7 +1688,8 @@ void Debug::PrepareFunctionForDebugExecution(
 
 namespace {
 
-bool IsJSFunctionAndNeedsTrampoline(Isolate* isolate, Object maybe_function) {
+bool IsJSFunctionAndNeedsTrampoline(Isolate* isolate,
+                                    Tagged<Object> maybe_function) {
   if (!IsJSFunction(maybe_function)) return false;
   base::Optional<DebugInfo> debug_info = isolate->debug()->TryGetDebugInfo(
       JSFunction::cast(maybe_function)->shared());
@@ -1707,7 +1710,7 @@ void Debug::InstallDebugBreakTrampoline() {
 
   DebugInfoCollection::Iterator it(&debug_infos_);
   for (; it.HasNext(); it.Advance()) {
-    DebugInfo debug_info = it.Next();
+    Tagged<DebugInfo> debug_info = it.Next();
     if (debug_info->CanBreakAtEntry()) {
       needs_to_use_trampoline = true;
       if (debug_info->shared()->IsApiFunction()) {
@@ -1730,29 +1733,29 @@ void Debug::InstallDebugBreakTrampoline() {
     std::set<AccessorPair> recorded;
     HeapObjectIterator iterator(isolate_->heap());
     DisallowGarbageCollection no_gc;
-    for (HeapObject obj = iterator.Next(); !obj.is_null();
+    for (Tagged<HeapObject> obj = iterator.Next(); !obj.is_null();
          obj = iterator.Next()) {
       if (needs_to_clear_ic && IsFeedbackVector(obj)) {
         FeedbackVector::cast(obj)->ClearSlots(isolate_);
         continue;
       } else if (IsJSFunctionAndNeedsTrampoline(isolate_, obj)) {
-        JSFunction fun = JSFunction::cast(obj);
+        Tagged<JSFunction> fun = JSFunction::cast(obj);
         if (!fun->is_compiled()) {
           needs_compile.push_back(handle(fun, isolate_));
         } else {
           fun->set_code(*trampoline);
         }
       } else if (IsJSObject(obj)) {
-        JSObject object = JSObject::cast(obj);
-        DescriptorArray descriptors =
+        Tagged<JSObject> object = JSObject::cast(obj);
+        Tagged<DescriptorArray> descriptors =
             object->map()->instance_descriptors(kRelaxedLoad);
 
         for (InternalIndex i : object->map()->IterateOwnDescriptors()) {
           if (descriptors->GetDetails(i).kind() == PropertyKind::kAccessor) {
-            Object value = descriptors->GetStrongValue(i);
+            Tagged<Object> value = descriptors->GetStrongValue(i);
             if (!IsAccessorPair(value)) continue;
 
-            AccessorPair accessor_pair = AccessorPair::cast(value);
+            Tagged<AccessorPair> accessor_pair = AccessorPair::cast(value);
             if (!IsFunctionTemplateInfo(accessor_pair->getter()) &&
                 !IsFunctionTemplateInfo(accessor_pair->setter())) {
               continue;
@@ -1874,8 +1877,8 @@ class SharedFunctionInfoFinder {
       : current_start_position_(kNoSourcePosition),
         target_position_(target_position) {}
 
-  void NewCandidate(SharedFunctionInfo shared,
-                    JSFunction closure = JSFunction()) {
+  void NewCandidate(Tagged<SharedFunctionInfo> shared,
+                    Tagged<JSFunction> closure = JSFunction()) {
     if (!shared->IsSubjectToDebugging()) return;
     int start_position = shared->function_token_position();
     if (start_position == kNoSourcePosition) {
@@ -1914,9 +1917,9 @@ class SharedFunctionInfoFinder {
     current_candidate_closure_ = closure;
   }
 
-  SharedFunctionInfo Result() { return current_candidate_; }
+  Tagged<SharedFunctionInfo> Result() { return current_candidate_; }
 
-  JSFunction ResultClosure() { return current_candidate_closure_; }
+  Tagged<JSFunction> ResultClosure() { return current_candidate_closure_; }
 
  private:
   SharedFunctionInfo current_candidate_;
@@ -1927,12 +1930,11 @@ class SharedFunctionInfoFinder {
 };
 
 namespace {
-SharedFunctionInfo FindSharedFunctionInfoCandidate(int position,
-                                                   Handle<Script> script,
-                                                   Isolate* isolate) {
+Tagged<SharedFunctionInfo> FindSharedFunctionInfoCandidate(
+    int position, Handle<Script> script, Isolate* isolate) {
   SharedFunctionInfoFinder finder(position);
   SharedFunctionInfo::ScriptIterator iterator(isolate, *script);
-  for (SharedFunctionInfo info = iterator.Next(); !info.is_null();
+  for (Tagged<SharedFunctionInfo> info = iterator.Next(); !info.is_null();
        info = iterator.Next()) {
     finder.NewCandidate(info);
   }
@@ -1990,7 +1992,7 @@ bool Debug::FindSharedFunctionInfosIntersectingRange(
     {
       DisallowGarbageCollection no_gc;
       SharedFunctionInfo::ScriptIterator iterator(isolate_, *script);
-      for (SharedFunctionInfo info = iterator.Next(); !info.is_null();
+      for (Tagged<SharedFunctionInfo> info = iterator.Next(); !info.is_null();
            info = iterator.Next()) {
         if (info->EndPosition() < start_position ||
             info->StartPosition() >= end_position) {
@@ -2009,7 +2011,7 @@ bool Debug::FindSharedFunctionInfosIntersectingRange(
       DCHECK_LE(script->shared_function_info_count(),
                 script->shared_function_infos()->length());
       MaybeObject maybeToplevel = script->shared_function_infos()->Get(0);
-      HeapObject heap_object;
+      Tagged<HeapObject> heap_object;
       const bool topLevelInfoExists =
           maybeToplevel->GetHeapObject(&heap_object) &&
           !IsUndefined(heap_object);
@@ -2062,7 +2064,7 @@ Handle<Object> Debug::FindInnermostContainingFunctionInfo(Handle<Script> script,
     // If there is no shared function info for this script at all, there is
     // no point in looking for it by walking the heap.
 
-    SharedFunctionInfo shared;
+    Tagged<SharedFunctionInfo> shared;
     IsCompiledScope is_compiled_scope;
     {
       shared = FindSharedFunctionInfoCandidate(position, script, isolate_);
@@ -2236,7 +2238,7 @@ Handle<FixedArray> Debug::GetLoadedScripts() {
   int length = 0;
   {
     Script::Iterator iterator(isolate_);
-    for (Script script = iterator.Next(); !script.is_null();
+    for (Tagged<Script> script = iterator.Next(); !script.is_null();
          script = iterator.Next()) {
       if (script->HasValidSource()) results->set(length++, script);
     }
@@ -2244,36 +2246,37 @@ Handle<FixedArray> Debug::GetLoadedScripts() {
   return FixedArray::ShrinkOrEmpty(isolate_, results, length);
 }
 
-base::Optional<DebugInfo> Debug::TryGetDebugInfo(SharedFunctionInfo sfi) {
+base::Optional<DebugInfo> Debug::TryGetDebugInfo(
+    Tagged<SharedFunctionInfo> sfi) {
   return debug_infos_.Find(sfi);
 }
 
-bool Debug::HasDebugInfo(SharedFunctionInfo sfi) {
+bool Debug::HasDebugInfo(Tagged<SharedFunctionInfo> sfi) {
   return TryGetDebugInfo(sfi).has_value();
 }
 
-bool Debug::HasCoverageInfo(SharedFunctionInfo sfi) {
+bool Debug::HasCoverageInfo(Tagged<SharedFunctionInfo> sfi) {
   if (base::Optional<DebugInfo> debug_info = TryGetDebugInfo(sfi)) {
     return debug_info->HasCoverageInfo();
   }
   return false;
 }
 
-bool Debug::HasBreakInfo(SharedFunctionInfo sfi) {
+bool Debug::HasBreakInfo(Tagged<SharedFunctionInfo> sfi) {
   if (base::Optional<DebugInfo> debug_info = TryGetDebugInfo(sfi)) {
     return debug_info->HasBreakInfo();
   }
   return false;
 }
 
-bool Debug::BreakAtEntry(SharedFunctionInfo sfi) {
+bool Debug::BreakAtEntry(Tagged<SharedFunctionInfo> sfi) {
   if (base::Optional<DebugInfo> debug_info = TryGetDebugInfo(sfi)) {
     return debug_info->BreakAtEntry();
   }
   return false;
 }
 
-base::Optional<Object> Debug::OnThrow(Handle<Object> exception) {
+base::Optional<Tagged<Object>> Debug::OnThrow(Handle<Object> exception) {
   RCS_SCOPE(isolate_, RuntimeCallCounterId::kDebugger);
   if (in_debug_scope() || ignore_events()) return {};
   // Temporarily clear any scheduled_exception to allow evaluating
@@ -2440,7 +2443,7 @@ void Debug::OnDebugBreak(Handle<FixedArray> break_points_hit,
   std::vector<int> inspector_break_points_hit;
   // This array contains breakpoints installed using JS debug API.
   for (int i = 0; i < break_points_hit->length(); ++i) {
-    BreakPoint break_point = BreakPoint::cast(break_points_hit->get(i));
+    Tagged<BreakPoint> break_point = BreakPoint::cast(break_points_hit->get(i));
     inspector_break_points_hit.push_back(break_point->id());
   }
   {
@@ -3073,8 +3076,8 @@ bool Debug::PerformSideEffectCheckAtBytecode(InterpretedFrame* frame) {
   using interpreter::Bytecode;
 
   DCHECK_EQ(isolate_->debug_execution_mode(), DebugInfo::kSideEffects);
-  SharedFunctionInfo shared = frame->function()->shared();
-  BytecodeArray bytecode_array = shared->GetBytecodeArray(isolate_);
+  Tagged<SharedFunctionInfo> shared = frame->function()->shared();
+  Tagged<BytecodeArray> bytecode_array = shared->GetBytecodeArray(isolate_);
   int offset = frame->GetBytecodeOffset();
   interpreter::BytecodeArrayIterator bytecode_iterator(
       handle(bytecode_array, isolate_), offset);
diff --git a/src/debug/debug.h b/src/debug/debug.h
index 04d52948719..a96f057c51e 100644
--- a/src/debug/debug.h
+++ b/src/debug/debug.h
@@ -91,7 +91,7 @@ class BreakLocation {
 
   debug::BreakLocationType type() const;
 
-  JSGeneratorObject GetGeneratorObjectForSuspendedFrame(
+  Tagged<JSGeneratorObject> GetGeneratorObjectForSuspendedFrame(
       JavaScriptFrame* frame) const;
 
  private:
@@ -188,12 +188,12 @@ class DebugInfoCollection final {
  public:
   explicit DebugInfoCollection(Isolate* isolate) : isolate_(isolate) {}
 
-  void Insert(SharedFunctionInfo sfi, DebugInfo debug_info);
+  void Insert(Tagged<SharedFunctionInfo> sfi, Tagged<DebugInfo> debug_info);
 
-  bool Contains(SharedFunctionInfo sfi) const;
-  base::Optional<DebugInfo> Find(SharedFunctionInfo sfi) const;
+  bool Contains(Tagged<SharedFunctionInfo> sfi) const;
+  base::Optional<DebugInfo> Find(Tagged<SharedFunctionInfo> sfi) const;
 
-  void DeleteSlow(SharedFunctionInfo sfi);
+  void DeleteSlow(Tagged<SharedFunctionInfo> sfi);
 
   size_t Size() const { return list_.size(); }
 
@@ -206,7 +206,7 @@ class DebugInfoCollection final {
       return index_ < static_cast<int>(collection_->list_.size());
     }
 
-    DebugInfo Next() const {
+    Tagged<DebugInfo> Next() const {
       DCHECK_GE(index_, 0);
       if (!HasNext()) return {};
       return collection_->EntryAsDebugInfo(index_);
@@ -231,7 +231,7 @@ class DebugInfoCollection final {
   };
 
  private:
-  V8_EXPORT_PRIVATE DebugInfo EntryAsDebugInfo(size_t index) const;
+  V8_EXPORT_PRIVATE Tagged<DebugInfo> EntryAsDebugInfo(size_t index) const;
   void DeleteIndex(size_t index);
 
   Isolate* const isolate_;
@@ -256,7 +256,7 @@ class V8_EXPORT_PRIVATE Debug {
                     debug::BreakReasons break_reasons = {});
   debug::DebugDelegate::ActionAfterInstrumentation OnInstrumentationBreak();
 
-  base::Optional<Object> OnThrow(Handle<Object> exception)
+  base::Optional<Tagged<Object>> OnThrow(Handle<Object> exception)
       V8_WARN_UNUSED_RESULT;
   void OnPromiseReject(Handle<Object> promise, Handle<Object> value);
   void OnCompileError(Handle<Script> script);
@@ -273,11 +273,11 @@ class V8_EXPORT_PRIVATE Debug {
   Handle<FixedArray> GetLoadedScripts();
 
   // DebugInfo accessors.
-  base::Optional<DebugInfo> TryGetDebugInfo(SharedFunctionInfo sfi);
-  bool HasDebugInfo(SharedFunctionInfo sfi);
-  bool HasCoverageInfo(SharedFunctionInfo sfi);
-  bool HasBreakInfo(SharedFunctionInfo sfi);
-  bool BreakAtEntry(SharedFunctionInfo sfi);
+  base::Optional<DebugInfo> TryGetDebugInfo(Tagged<SharedFunctionInfo> sfi);
+  bool HasDebugInfo(Tagged<SharedFunctionInfo> sfi);
+  bool HasCoverageInfo(Tagged<SharedFunctionInfo> sfi);
+  bool HasBreakInfo(Tagged<SharedFunctionInfo> sfi);
+  bool BreakAtEntry(Tagged<SharedFunctionInfo> sfi);
 
   // Break point handling.
   enum BreakPointKind { kRegular, kInstrumentation };
@@ -322,7 +322,7 @@ class V8_EXPORT_PRIVATE Debug {
   void SetBreakOnNextFunctionCall();
   void ClearBreakOnNextFunctionCall();
 
-  void DiscardBaselineCode(SharedFunctionInfo shared);
+  void DiscardBaselineCode(Tagged<SharedFunctionInfo> shared);
   void DiscardAllBaselineCode();
 
   void DeoptimizeFunction(Handle<SharedFunctionInfo> shared);
@@ -427,8 +427,10 @@ class V8_EXPORT_PRIVATE Debug {
   StackFrameId break_frame_id() { return thread_local_.break_frame_id_; }
 
   Handle<Object> return_value_handle();
-  Object return_value() { return thread_local_.return_value_; }
-  void set_return_value(Object value) { thread_local_.return_value_ = value; }
+  Tagged<Object> return_value() { return thread_local_.return_value_; }
+  void set_return_value(Tagged<Object> value) {
+    thread_local_.return_value_ = value;
+  }
 
   // Support for embedding into generated code.
   Address is_active_address() { return reinterpret_cast<Address>(&is_active_); }
diff --git a/src/debug/liveedit.cc b/src/debug/liveedit.cc
index 902358c5884..21b780b3e9d 100644
--- a/src/debug/liveedit.cc
+++ b/src/debug/liveedit.cc
@@ -569,12 +569,12 @@ class FunctionDataMap : public ThreadVisitor {
     map_.emplace(GetFuncId(script_id, literal), FunctionData{literal});
   }
 
-  bool Lookup(SharedFunctionInfo sfi, FunctionData** data) {
+  bool Lookup(Tagged<SharedFunctionInfo> sfi, FunctionData** data) {
     int start_position = sfi->StartPosition();
     if (!IsScript(sfi->script()) || start_position == -1) {
       return false;
     }
-    Script script = Script::cast(sfi->script());
+    Tagged<Script> script = Script::cast(sfi->script());
     return Lookup(GetFuncId(script->id(), sfi), data);
   }
 
@@ -587,23 +587,23 @@ class FunctionDataMap : public ThreadVisitor {
     {
       HeapObjectIterator iterator(isolate->heap(),
                                   HeapObjectIterator::kFilterUnreachable);
-      for (HeapObject obj = iterator.Next(); !obj.is_null();
+      for (Tagged<HeapObject> obj = iterator.Next(); !obj.is_null();
            obj = iterator.Next()) {
         if (IsSharedFunctionInfo(obj)) {
-          SharedFunctionInfo sfi = SharedFunctionInfo::cast(obj);
+          Tagged<SharedFunctionInfo> sfi = SharedFunctionInfo::cast(obj);
           FunctionData* data = nullptr;
           if (!Lookup(sfi, &data)) continue;
           data->shared = handle(sfi, isolate);
         } else if (IsJSFunction(obj)) {
-          JSFunction js_function = JSFunction::cast(obj);
-          SharedFunctionInfo sfi = js_function->shared();
+          Tagged<JSFunction> js_function = JSFunction::cast(obj);
+          Tagged<SharedFunctionInfo> sfi = js_function->shared();
           FunctionData* data = nullptr;
           if (!Lookup(sfi, &data)) continue;
           data->js_functions.emplace_back(js_function, isolate);
         } else if (IsJSGeneratorObject(obj)) {
-          JSGeneratorObject gen = JSGeneratorObject::cast(obj);
+          Tagged<JSGeneratorObject> gen = JSGeneratorObject::cast(obj);
           if (gen->is_closed()) continue;
-          SharedFunctionInfo sfi = gen->function()->shared();
+          Tagged<SharedFunctionInfo> sfi = gen->function()->shared();
           FunctionData* data = nullptr;
           if (!Lookup(sfi, &data)) continue;
           data->running_generators.emplace_back(gen, isolate);
@@ -635,7 +635,7 @@ class FunctionDataMap : public ThreadVisitor {
     return FuncId(script_id, start_position);
   }
 
-  FuncId GetFuncId(int script_id, SharedFunctionInfo sfi) {
+  FuncId GetFuncId(int script_id, Tagged<SharedFunctionInfo> sfi) {
     DCHECK_EQ(script_id, Script::cast(sfi->script())->id());
     int start_position = sfi->StartPosition();
     DCHECK_NE(start_position, -1);
@@ -760,14 +760,15 @@ void UpdatePositions(Isolate* isolate, Handle<SharedFunctionInfo> sfi,
 }
 
 #ifdef DEBUG
-ScopeInfo FindOuterScopeInfoFromScriptSfi(Isolate* isolate,
-                                          Handle<Script> script) {
+Tagged<ScopeInfo> FindOuterScopeInfoFromScriptSfi(Isolate* isolate,
+                                                  Handle<Script> script) {
   // We take some SFI from the script and walk outwards until we find the
   // EVAL_SCOPE. Then we do the same search as `DetermineOuterScopeInfo` and
   // check that we found the same ScopeInfo.
   SharedFunctionInfo::ScriptIterator it(isolate, *script);
-  ScopeInfo other_scope_info;
-  for (SharedFunctionInfo sfi = it.Next(); !sfi.is_null(); sfi = it.Next()) {
+  Tagged<ScopeInfo> other_scope_info;
+  for (Tagged<SharedFunctionInfo> sfi = it.Next(); !sfi.is_null();
+       sfi = it.Next()) {
     if (!sfi->scope_info()->IsEmpty()) {
       other_scope_info = sfi->scope_info();
       break;
@@ -804,12 +805,12 @@ MaybeHandle<ScopeInfo> DetermineOuterScopeInfo(Isolate* isolate,
                                                Handle<Script> script) {
   if (!script->has_eval_from_shared()) return kNullMaybeHandle;
   DCHECK_EQ(script->compilation_type(), Script::CompilationType::kEval);
-  ScopeInfo scope_info = script->eval_from_shared()->scope_info();
+  Tagged<ScopeInfo> scope_info = script->eval_from_shared()->scope_info();
   // Sloppy eval compiles use the ScopeInfo of the context. Let's find it.
   while (!scope_info->IsEmpty()) {
     if (scope_info->HasContext()) {
 #ifdef DEBUG
-      ScopeInfo other_scope_info =
+      Tagged<ScopeInfo> other_scope_info =
           FindOuterScopeInfoFromScriptSfi(isolate, script);
       DCHECK_IMPLIES(!other_scope_info.is_null(),
                      scope_info == other_scope_info);
@@ -946,7 +947,8 @@ void LiveEdit::PatchScript(Isolate* isolate, Handle<Script> script,
     }
 
     if (!sfi->HasBytecodeArray()) continue;
-    FixedArray constants = sfi->GetBytecodeArray(isolate)->constant_pool();
+    Tagged<FixedArray> constants =
+        sfi->GetBytecodeArray(isolate)->constant_pool();
     for (int i = 0; i < constants->length(); ++i) {
       if (!IsSharedFunctionInfo(constants->get(i))) continue;
       data = nullptr;
@@ -994,12 +996,14 @@ void LiveEdit::PatchScript(Isolate* isolate, Handle<Script> script,
     }
   }
   SharedFunctionInfo::ScriptIterator it(isolate, *new_script);
-  for (SharedFunctionInfo sfi = it.Next(); !sfi.is_null(); sfi = it.Next()) {
+  for (Tagged<SharedFunctionInfo> sfi = it.Next(); !sfi.is_null();
+       sfi = it.Next()) {
     if (!sfi->HasBytecodeArray()) continue;
-    FixedArray constants = sfi->GetBytecodeArray(isolate)->constant_pool();
+    Tagged<FixedArray> constants =
+        sfi->GetBytecodeArray(isolate)->constant_pool();
     for (int i = 0; i < constants->length(); ++i) {
       if (!IsSharedFunctionInfo(constants->get(i))) continue;
-      SharedFunctionInfo inner_sfi =
+      Tagged<SharedFunctionInfo> inner_sfi =
           SharedFunctionInfo::cast(constants->get(i));
       // See if there is a mapping from this function's start position to a
       // unchanged function's id.
@@ -1009,7 +1013,7 @@ void LiveEdit::PatchScript(Isolate* isolate, Handle<Script> script,
 
       // Grab that function id from the new script's SFI list, which should have
       // already been updated in in the unchanged pass.
-      SharedFunctionInfo old_unchanged_inner_sfi =
+      Tagged<SharedFunctionInfo> old_unchanged_inner_sfi =
           SharedFunctionInfo::cast(new_script->shared_function_infos()
                                        ->Get(unchanged_it->second)
                                        ->GetHeapObject());
@@ -1030,7 +1034,7 @@ void LiveEdit::PatchScript(Isolate* isolate, Handle<Script> script,
 
     SharedFunctionInfo::ScriptIterator script_it(isolate, *new_script);
     std::set<int> start_positions;
-    for (SharedFunctionInfo sfi = script_it.Next(); !sfi.is_null();
+    for (Tagged<SharedFunctionInfo> sfi = script_it.Next(); !sfi.is_null();
          sfi = script_it.Next()) {
       DCHECK_EQ(sfi->script(), *new_script);
       DCHECK_EQ(sfi->function_literal_id(), script_it.CurrentIndex());
@@ -1046,10 +1050,11 @@ void LiveEdit::PatchScript(Isolate* isolate, Handle<Script> script,
       // Check that all the functions in this function's constant pool are also
       // on the new script, and that their id matches their index in the new
       // scripts function list.
-      FixedArray constants = sfi->GetBytecodeArray(isolate)->constant_pool();
+      Tagged<FixedArray> constants =
+          sfi->GetBytecodeArray(isolate)->constant_pool();
       for (int i = 0; i < constants->length(); ++i) {
         if (!IsSharedFunctionInfo(constants->get(i))) continue;
-        SharedFunctionInfo inner_sfi =
+        Tagged<SharedFunctionInfo> inner_sfi =
             SharedFunctionInfo::cast(constants->get(i));
         DCHECK_EQ(inner_sfi->script(), *new_script);
         DCHECK_EQ(inner_sfi, new_script->shared_function_infos()
diff --git a/src/deoptimizer/deoptimizer.cc b/src/deoptimizer/deoptimizer.cc
index 70b42838a6b..c69878d4ed4 100644
--- a/src/deoptimizer/deoptimizer.cc
+++ b/src/deoptimizer/deoptimizer.cc
@@ -44,7 +44,7 @@ class DeoptimizableCodeIterator {
   DeoptimizableCodeIterator(const DeoptimizableCodeIterator&) = delete;
   DeoptimizableCodeIterator& operator=(const DeoptimizableCodeIterator&) =
       delete;
-  Code Next();
+  Tagged<Code> Next();
 
  private:
   Isolate* const isolate_;
@@ -65,9 +65,9 @@ DeoptimizableCodeIterator::DeoptimizableCodeIterator(Isolate* isolate)
           isolate->heap()->code_space()->GetObjectIterator(isolate->heap())),
       state_(kIteratingCodeSpace) {}
 
-Code DeoptimizableCodeIterator::Next() {
+Tagged<Code> DeoptimizableCodeIterator::Next() {
   while (true) {
-    HeapObject object = object_iterator_->Next();
+    Tagged<HeapObject> object = object_iterator_->Next();
     if (object.is_null()) {
       // No objects left in the current iterator, try to move to the next space
       // based on the state.
@@ -92,8 +92,8 @@ Code DeoptimizableCodeIterator::Next() {
           return Code();
       }
     }
-    InstructionStream istream = InstructionStream::cast(object);
-    Code code;
+    Tagged<InstructionStream> istream = InstructionStream::cast(object);
+    Tagged<Code> code;
     if (!istream->TryGetCode(&code, kAcquireLoad)) continue;
     if (!CodeKindCanDeoptimize(code->kind())) continue;
     return code;
@@ -125,7 +125,7 @@ class FrameWriter {
     }
   }
 
-  void PushRawObject(Object obj, const char* debug_hint) {
+  void PushRawObject(Tagged<Object> obj, const char* debug_hint) {
     intptr_t value = obj.ptr();
     PushValue(value);
     if (trace_scope_ != nullptr) {
@@ -162,7 +162,7 @@ class FrameWriter {
 
   void PushTranslatedValue(const TranslatedFrame::iterator& iterator,
                            const char* debug_hint = "") {
-    Object obj = iterator->GetRawValue();
+    Tagged<Object> obj = iterator->GetRawValue();
     PushRawObject(obj, debug_hint);
     if (trace_scope_ != nullptr) {
       PrintF(trace_scope_->file(), " (input #%d)\n", iterator.input_index());
@@ -222,7 +222,7 @@ class FrameWriter {
 #endif
   }
 
-  void DebugPrintOutputObject(Object obj, unsigned output_offset,
+  void DebugPrintOutputObject(Tagged<Object> obj, unsigned output_offset,
                               const char* debug_hint = "") {
     if (trace_scope_ != nullptr) {
       PrintF(trace_scope_->file(), "    " V8PRIxPTR_FMT ": [top + %3d] <- ",
@@ -248,7 +248,7 @@ class FrameWriter {
 Deoptimizer* Deoptimizer::New(Address raw_function, DeoptimizeKind kind,
                               Address from, int fp_to_sp_delta,
                               Isolate* isolate) {
-  JSFunction function = JSFunction::cast(Object(raw_function));
+  Tagged<JSFunction> function = JSFunction::cast(Object(raw_function));
   Deoptimizer* deoptimizer =
       new Deoptimizer(isolate, function, kind, from, fp_to_sp_delta);
   isolate->set_current_deoptimizer(deoptimizer);
@@ -297,7 +297,7 @@ DeoptimizedFrameInfo* Deoptimizer::DebuggerInspectableFrame(
 namespace {
 class ActivationsFinder : public ThreadVisitor {
  public:
-  ActivationsFinder(GcSafeCode topmost_optimized_code,
+  ActivationsFinder(Tagged<GcSafeCode> topmost_optimized_code,
                     bool safe_to_deopt_topmost_optimized_code) {
 #ifdef DEBUG
     topmost_ = topmost_optimized_code;
@@ -311,7 +311,7 @@ class ActivationsFinder : public ThreadVisitor {
   void VisitThread(Isolate* isolate, ThreadLocalTop* top) override {
     for (StackFrameIterator it(isolate, top); !it.done(); it.Advance()) {
       if (it.frame()->is_optimized()) {
-        GcSafeCode code = it.frame()->GcSafeLookupCode();
+        Tagged<GcSafeCode> code = it.frame()->GcSafeLookupCode();
         if (CodeKindCanDeoptimize(code->kind()) &&
             code->marked_for_deoptimization()) {
           // Obtain the trampoline to the deoptimizer call.
@@ -351,7 +351,7 @@ class ActivationsFinder : public ThreadVisitor {
 void Deoptimizer::DeoptimizeMarkedCode(Isolate* isolate) {
   DisallowGarbageCollection no_gc;
 
-  GcSafeCode topmost_optimized_code;
+  Tagged<GcSafeCode> topmost_optimized_code;
   bool safe_to_deopt_topmost_optimized_code = false;
 #ifdef DEBUG
   // Make sure all activations of optimized code can deopt at their current PC.
@@ -360,8 +360,8 @@ void Deoptimizer::DeoptimizeMarkedCode(Isolate* isolate) {
   for (StackFrameIterator it(isolate, isolate->thread_local_top()); !it.done();
        it.Advance()) {
     if (it.frame()->is_optimized()) {
-      GcSafeCode code = it.frame()->GcSafeLookupCode();
-      JSFunction function =
+      Tagged<GcSafeCode> code = it.frame()->GcSafeLookupCode();
+      Tagged<JSFunction> function =
           static_cast<OptimizedFrame*>(it.frame())->function();
       TraceFoundActivation(isolate, function);
       bool safe_if_deopt_triggered;
@@ -407,7 +407,7 @@ void Deoptimizer::DeoptimizeAll(Isolate* isolate) {
   // Mark all code, then deoptimize.
   {
     DeoptimizableCodeIterator it(isolate);
-    for (Code code = it.Next(); !code.is_null(); code = it.Next()) {
+    for (Tagged<Code> code = it.Next(); !code.is_null(); code = it.Next()) {
       code->set_marked_for_deoptimization(true);
     }
   }
@@ -416,7 +416,8 @@ void Deoptimizer::DeoptimizeAll(Isolate* isolate) {
 }
 
 // static
-void Deoptimizer::DeoptimizeFunction(JSFunction function, Code code) {
+void Deoptimizer::DeoptimizeFunction(Tagged<JSFunction> function,
+                                     Tagged<Code> code) {
   Isolate* isolate = function->GetIsolate();
   RCS_SCOPE(isolate, RuntimeCallCounterId::kDeoptimizeCode);
   TimerEventScope<TimerEventDeoptimizeCode> timer(isolate);
@@ -452,7 +453,7 @@ void Deoptimizer::DeoptimizeAllOptimizedCodeWithFunction(
   bool any_marked = false;
   {
     DeoptimizableCodeIterator it(isolate);
-    for (Code code = it.Next(); !code.is_null(); code = it.Next()) {
+    for (Tagged<Code> code = it.Next(); !code.is_null(); code = it.Next()) {
       if (code->Inlines(*function)) {
         code->set_marked_for_deoptimization(true);
         any_marked = true;
@@ -477,7 +478,7 @@ const char* Deoptimizer::MessageFor(DeoptimizeKind kind) {
   }
 }
 
-Deoptimizer::Deoptimizer(Isolate* isolate, JSFunction function,
+Deoptimizer::Deoptimizer(Isolate* isolate, Tagged<JSFunction> function,
                          DeoptimizeKind kind, Address from, int fp_to_sp_delta)
     : isolate_(isolate),
       function_(function),
@@ -539,7 +540,7 @@ Deoptimizer::Deoptimizer(Isolate* isolate, JSFunction function,
   // Calculate the deopt exit index from return address.
   DCHECK_GT(kEagerDeoptExitSize, 0);
   DCHECK_GT(kLazyDeoptExitSize, 0);
-  DeoptimizationData deopt_data =
+  Tagged<DeoptimizationData> deopt_data =
       DeoptimizationData::cast(compiled_code_->deoptimization_data());
   Address deopt_start = compiled_code_->instruction_start() +
                         deopt_data->DeoptExitStart().value();
@@ -669,15 +670,16 @@ void Deoptimizer::TraceDeoptEnd(double deopt_duration) {
 }
 
 // static
-void Deoptimizer::TraceMarkForDeoptimization(Isolate* isolate, Code code,
+void Deoptimizer::TraceMarkForDeoptimization(Isolate* isolate,
+                                             Tagged<Code> code,
                                              const char* reason) {
   if (!v8_flags.trace_deopt && !v8_flags.log_deopt) return;
 
   DisallowGarbageCollection no_gc;
-  Object maybe_data = code->deoptimization_data();
+  Tagged<Object> maybe_data = code->deoptimization_data();
   if (maybe_data == ReadOnlyRoots(isolate).empty_fixed_array()) return;
 
-  DeoptimizationData deopt_data = DeoptimizationData::cast(maybe_data);
+  Tagged<DeoptimizationData> deopt_data = DeoptimizationData::cast(maybe_data);
   CodeTracer::Scope scope(isolate->GetCodeTracer());
   if (v8_flags.trace_deopt) {
     PrintF(scope.file(), "[marking dependent code ");
@@ -702,9 +704,8 @@ void Deoptimizer::TraceMarkForDeoptimization(Isolate* isolate, Code code,
 }
 
 // static
-void Deoptimizer::TraceEvictFromOptimizedCodeCache(Isolate* isolate,
-                                                   SharedFunctionInfo sfi,
-                                                   const char* reason) {
+void Deoptimizer::TraceEvictFromOptimizedCodeCache(
+    Isolate* isolate, Tagged<SharedFunctionInfo> sfi, const char* reason) {
   if (!v8_flags.trace_deopt_verbose) return;
 
   DisallowGarbageCollection no_gc;
@@ -718,7 +719,8 @@ void Deoptimizer::TraceEvictFromOptimizedCodeCache(Isolate* isolate,
 
 #ifdef DEBUG
 // static
-void Deoptimizer::TraceFoundActivation(Isolate* isolate, JSFunction function) {
+void Deoptimizer::TraceFoundActivation(Isolate* isolate,
+                                       Tagged<JSFunction> function) {
   if (!v8_flags.trace_deopt_verbose) return;
   CodeTracer::Scope scope(isolate->GetCodeTracer());
   PrintF(scope.file(), "[deoptimizer found activation of function: ");
@@ -747,7 +749,7 @@ void Deoptimizer::DoComputeOutputFrames() {
 
   // Determine basic deoptimization information.  The optimized frame is
   // described by the input data.
-  DeoptimizationData input_data =
+  Tagged<DeoptimizationData> input_data =
       DeoptimizationData::cast(compiled_code_->deoptimization_data());
 
   {
@@ -924,7 +926,7 @@ void Deoptimizer::DoComputeOutputFrames() {
 
 // static
 bool Deoptimizer::DeoptExitIsInsideOsrLoop(Isolate* isolate,
-                                           JSFunction function,
+                                           Tagged<JSFunction> function,
                                            BytecodeOffset deopt_exit_offset,
                                            BytecodeOffset osr_offset) {
   DisallowGarbageCollection no_gc;
@@ -982,7 +984,7 @@ Builtin DispatchBuiltinFor(bool deopt_to_baseline, bool advance_bc,
 void Deoptimizer::DoComputeUnoptimizedFrame(TranslatedFrame* translated_frame,
                                             int frame_index,
                                             bool goto_catch_handler) {
-  SharedFunctionInfo shared = translated_frame->raw_shared_info();
+  Tagged<SharedFunctionInfo> shared = translated_frame->raw_shared_info();
   TranslatedFrame::iterator value_iterator = translated_frame->begin();
   const bool is_bottommost = (0 == frame_index);
   const bool is_topmost = (output_count_ - 1 == frame_index);
@@ -1009,7 +1011,7 @@ void Deoptimizer::DoComputeUnoptimizedFrame(TranslatedFrame* translated_frame,
 
   TranslatedFrame::iterator function_iterator = value_iterator++;
 
-  BytecodeArray bytecode_array;
+  Tagged<BytecodeArray> bytecode_array;
   base::Optional<DebugInfo> debug_info = shared->TryGetDebugInfo(isolate());
   if (debug_info.has_value() && debug_info->HasBreakInfo()) {
     bytecode_array = debug_info->DebugBytecodeArray();
@@ -1041,7 +1043,7 @@ void Deoptimizer::DoComputeUnoptimizedFrame(TranslatedFrame* translated_frame,
   const bool deopt_to_baseline =
       shared->HasBaselineCode() && v8_flags.deopt_to_baseline;
   const bool restart_frame = goto_catch_handler && is_restart_frame();
-  Code dispatch_builtin = builtins->code(
+  Tagged<Code> dispatch_builtin = builtins->code(
       DispatchBuiltinFor(deopt_to_baseline, advance_bc, restart_frame));
 
   if (verbose_tracing_enabled()) {
@@ -1140,7 +1142,7 @@ void Deoptimizer::DoComputeUnoptimizedFrame(TranslatedFrame* translated_frame,
     }
   }
   // Read the context from the translations.
-  Object context = context_pos->GetRawValue();
+  Tagged<Object> context = context_pos->GetRawValue();
   output_frame->SetContext(static_cast<intptr_t>(context.ptr()));
   frame_writer.PushTranslatedValue(context_pos, "context");
 
@@ -1166,7 +1168,7 @@ void Deoptimizer::DoComputeUnoptimizedFrame(TranslatedFrame* translated_frame,
   // The bytecode offset was mentioned explicitly in the BEGIN_FRAME.
   const int raw_bytecode_offset =
       BytecodeArray::kHeaderSize - kHeapObjectTag + bytecode_offset;
-  Smi smi_bytecode_offset = Smi::FromInt(raw_bytecode_offset);
+  Tagged<Smi> smi_bytecode_offset = Smi::FromInt(raw_bytecode_offset);
   frame_writer.PushRawObject(smi_bytecode_offset, "bytecode offset\n");
 
   if (verbose_tracing_enabled()) {
@@ -1282,7 +1284,7 @@ void Deoptimizer::DoComputeUnoptimizedFrame(TranslatedFrame* translated_frame,
     Register context_reg = JavaScriptFrame::context_register();
     output_frame->SetRegister(context_reg.code(), context_value);
     // Set the continuation for the topmost frame.
-    Code continuation = builtins->code(Builtin::kNotifyDeoptimized);
+    Tagged<Code> continuation = builtins->code(Builtin::kNotifyDeoptimized);
     output_frame->SetContinuation(
         static_cast<intptr_t>(continuation->instruction_start()));
   }
@@ -1460,7 +1462,7 @@ void Deoptimizer::DoComputeConstructCreateStubFrame(
   CHECK_EQ(0u, frame_writer.top_offset());
 
   // Compute this frame's PC.
-  Code construct_stub =
+  Tagged<Code> construct_stub =
       isolate_->builtins()->code(Builtin::kJSConstructStubGeneric);
   Address start = construct_stub->instruction_start();
   const int pc_offset =
@@ -1497,7 +1499,8 @@ void Deoptimizer::DoComputeConstructCreateStubFrame(
 
     // Set the continuation for the topmost frame.
     DCHECK_EQ(DeoptimizeKind::kLazy, deopt_kind_);
-    Code continuation = isolate_->builtins()->code(Builtin::kNotifyDeoptimized);
+    Tagged<Code> continuation =
+        isolate_->builtins()->code(Builtin::kNotifyDeoptimized);
     output_frame->SetContinuation(
         static_cast<intptr_t>(continuation->instruction_start()));
   }
@@ -1589,7 +1592,7 @@ void Deoptimizer::DoComputeConstructInvokeStubFrame(
   CHECK_EQ(0u, frame_writer.top_offset());
 
   // Compute this frame's PC.
-  Code construct_stub = isolate_->builtins()->code(
+  Tagged<Code> construct_stub = isolate_->builtins()->code(
       Builtin::kInterpreterPushArgsThenFastConstructFunction);
   Address start = construct_stub->instruction_start();
   const int pc_offset =
@@ -1626,7 +1629,8 @@ void Deoptimizer::DoComputeConstructInvokeStubFrame(
 
     // Set the continuation for the topmost frame.
     DCHECK_EQ(DeoptimizeKind::kLazy, deopt_kind_);
-    Code continuation = isolate_->builtins()->code(Builtin::kNotifyDeoptimized);
+    Tagged<Code> continuation =
+        isolate_->builtins()->code(Builtin::kNotifyDeoptimized);
     output_frame->SetContinuation(
         static_cast<intptr_t>(continuation->instruction_start()));
   }
@@ -1928,7 +1932,7 @@ void Deoptimizer::DoComputeBuiltinContinuation(
   // sure that it's harvested from the translation and copied into the register
   // set (it was automatically added at the end of the FrameState by the
   // instruction selector).
-  Object context = value_iterator->GetRawValue();
+  Tagged<Object> context = value_iterator->GetRawValue();
   const intptr_t value = context.ptr();
   TranslatedFrame::iterator context_register_value = value_iterator++;
   register_values[kContextRegister.code()] = context_register_value;
@@ -2050,7 +2054,7 @@ void Deoptimizer::DoComputeBuiltinContinuation(
   // For JSToWasmBuiltinContinuations use ContinueToCodeStubBuiltin, and not
   // ContinueToCodeStubBuiltinWithResult because we don't want to overwrite the
   // return value that we have already set.
-  Code continue_to_builtin =
+  Tagged<Code> continue_to_builtin =
       isolate()->builtins()->code(TrampolineForBuiltinContinuation(
           mode, frame_info.frame_has_result_stack_slot() &&
                     !is_js_to_wasm_builtin_continuation));
@@ -2067,7 +2071,8 @@ void Deoptimizer::DoComputeBuiltinContinuation(
         static_cast<intptr_t>(continue_to_builtin->instruction_start()));
   }
 
-  Code continuation = isolate()->builtins()->code(Builtin::kNotifyDeoptimized);
+  Tagged<Code> continuation =
+      isolate()->builtins()->code(Builtin::kNotifyDeoptimized);
   output_frame->SetContinuation(
       static_cast<intptr_t>(continuation->instruction_start()));
 }
@@ -2113,7 +2118,7 @@ void Deoptimizer::MaterializeHeapObjects() {
 }
 
 void Deoptimizer::QueueValueForMaterialization(
-    Address output_address, Object obj,
+    Address output_address, Tagged<Object> obj,
     const TranslatedFrame::iterator& iterator) {
   if (obj == ReadOnlyRoots(isolate_).arguments_marker()) {
     values_to_materialize_.push_back({output_address, iterator});
@@ -2136,10 +2141,10 @@ namespace {
 // points to immediately after the deopt call).
 //
 // See also the Deoptimizer constructor.
-Address GetDeoptCallPCFromReturnPC(Address return_pc, Code code) {
+Address GetDeoptCallPCFromReturnPC(Address return_pc, Tagged<Code> code) {
   DCHECK_GT(Deoptimizer::kEagerDeoptExitSize, 0);
   DCHECK_GT(Deoptimizer::kLazyDeoptExitSize, 0);
-  DeoptimizationData deopt_data =
+  Tagged<DeoptimizationData> deopt_data =
       DeoptimizationData::cast(code->deoptimization_data());
   Address deopt_start =
       code->instruction_start() + deopt_data->DeoptExitStart().value();
@@ -2197,12 +2202,14 @@ unsigned Deoptimizer::ComputeInputFrameSize() const {
 }
 
 // static
-unsigned Deoptimizer::ComputeIncomingArgumentSize(SharedFunctionInfo shared) {
+unsigned Deoptimizer::ComputeIncomingArgumentSize(
+    Tagged<SharedFunctionInfo> shared) {
   int parameter_slots = shared->internal_formal_parameter_count_with_receiver();
   return parameter_slots * kSystemPointerSize;
 }
 
-Deoptimizer::DeoptInfo Deoptimizer::GetDeoptInfo(Code code, Address pc) {
+Deoptimizer::DeoptInfo Deoptimizer::GetDeoptInfo(Tagged<Code> code,
+                                                 Address pc) {
   CHECK(code->instruction_start() <= pc && pc <= code->instruction_end());
   SourcePosition last_position = SourcePosition::Unknown();
   DeoptimizeReason last_reason = DeoptimizeReason::kUnknown;
diff --git a/src/deoptimizer/deoptimizer.h b/src/deoptimizer/deoptimizer.h
index f40721ecf72..f37506b7558 100644
--- a/src/deoptimizer/deoptimizer.h
+++ b/src/deoptimizer/deoptimizer.h
@@ -50,10 +50,11 @@ class Deoptimizer : public Malloced {
   //    for (;;) {
   //    }  // OSR is triggered on this backedge.
   //  }  // This is the outermost loop containing the osr'd loop.
-  static bool DeoptExitIsInsideOsrLoop(Isolate* isolate, JSFunction function,
+  static bool DeoptExitIsInsideOsrLoop(Isolate* isolate,
+                                       Tagged<JSFunction> function,
                                        BytecodeOffset deopt_exit_offset,
                                        BytecodeOffset osr_offset);
-  static DeoptInfo GetDeoptInfo(Code code, Address from);
+  static DeoptInfo GetDeoptInfo(Tagged<Code> code, Address from);
   DeoptInfo GetDeoptInfo() const {
     return Deoptimizer::GetDeoptInfo(compiled_code_, from_);
   }
@@ -85,7 +86,8 @@ class Deoptimizer : public Malloced {
   // again and any activations of the optimized code will get deoptimized when
   // execution returns. If {code} is specified then the given code is targeted
   // instead of the function code (e.g. OSR code not installed on function).
-  static void DeoptimizeFunction(JSFunction function, Code code = {});
+  static void DeoptimizeFunction(Tagged<JSFunction> function,
+                                 Tagged<Code> code = {});
 
   // Deoptimize all code in the given isolate.
   V8_EXPORT_PRIVATE static void DeoptimizeAll(Isolate* isolate);
@@ -140,18 +142,18 @@ class Deoptimizer : public Malloced {
   V8_EXPORT_PRIVATE static const int kLazyDeoptExitSize;
 
   // Tracing.
-  static void TraceMarkForDeoptimization(Isolate* isolate, Code code,
+  static void TraceMarkForDeoptimization(Isolate* isolate, Tagged<Code> code,
                                          const char* reason);
   static void TraceEvictFromOptimizedCodeCache(Isolate* isolate,
-                                               SharedFunctionInfo sfi,
+                                               Tagged<SharedFunctionInfo> sfi,
                                                const char* reason);
 
  private:
-  void QueueValueForMaterialization(Address output_address, Object obj,
+  void QueueValueForMaterialization(Address output_address, Tagged<Object> obj,
                                     const TranslatedFrame::iterator& iterator);
 
-  Deoptimizer(Isolate* isolate, JSFunction function, DeoptimizeKind kind,
-              Address from, int fp_to_sp_delta);
+  Deoptimizer(Isolate* isolate, Tagged<JSFunction> function,
+              DeoptimizeKind kind, Address from, int fp_to_sp_delta);
   void DeleteFrameDescriptions();
 
   void DoComputeOutputFrames();
@@ -179,7 +181,8 @@ class Deoptimizer : public Malloced {
   unsigned ComputeInputFrameAboveFpFixedSize() const;
   unsigned ComputeInputFrameSize() const;
 
-  static unsigned ComputeIncomingArgumentSize(SharedFunctionInfo shared);
+  static unsigned ComputeIncomingArgumentSize(
+      Tagged<SharedFunctionInfo> shared);
 
   // Tracing.
   bool tracing_enabled() const { return trace_scope_ != nullptr; }
@@ -193,7 +196,8 @@ class Deoptimizer : public Malloced {
   void TraceDeoptBegin(int optimization_id, BytecodeOffset bytecode_offset);
   void TraceDeoptEnd(double deopt_duration);
 #ifdef DEBUG
-  static void TraceFoundActivation(Isolate* isolate, JSFunction function);
+  static void TraceFoundActivation(Isolate* isolate,
+                                   Tagged<JSFunction> function);
 #endif
   static void TraceDeoptAll(Isolate* isolate);
 
diff --git a/src/deoptimizer/materialized-object-store.cc b/src/deoptimizer/materialized-object-store.cc
index 722b6163472..bfcbcbfdf4f 100644
--- a/src/deoptimizer/materialized-object-store.cc
+++ b/src/deoptimizer/materialized-object-store.cc
@@ -40,7 +40,7 @@ bool MaterializedObjectStore::Remove(Address fp) {
   int index = static_cast<int>(std::distance(frame_fps_.begin(), it));
 
   frame_fps_.erase(it);
-  FixedArray array = isolate()->heap()->materialized_objects();
+  Tagged<FixedArray> array = isolate()->heap()->materialized_objects();
 
   CHECK_LT(index, array->length());
   int fps_size = static_cast<int>(frame_fps_.size());
@@ -79,7 +79,8 @@ Handle<FixedArray> MaterializedObjectStore::EnsureStackEntries(int length) {
   for (int i = 0; i < array->length(); i++) {
     new_array->set(i, array->get(i));
   }
-  HeapObject undefined_value = ReadOnlyRoots(isolate()).undefined_value();
+  Tagged<HeapObject> undefined_value =
+      ReadOnlyRoots(isolate()).undefined_value();
   for (int i = array->length(); i < length; i++) {
     new_array->set(i, undefined_value);
   }
diff --git a/src/deoptimizer/translated-state.cc b/src/deoptimizer/translated-state.cc
index df1325a3d34..fc7c1b94009 100644
--- a/src/deoptimizer/translated-state.cc
+++ b/src/deoptimizer/translated-state.cc
@@ -36,7 +36,7 @@ namespace internal {
 void DeoptimizationFrameTranslationPrintSingleOpcode(
     std::ostream& os, TranslationOpcode opcode,
     DeoptimizationFrameTranslation::Iterator& iterator,
-    DeoptimizationLiteralArray literal_array) {
+    Tagged<DeoptimizationLiteralArray> literal_array) {
   disasm::NameConverter converter;
   switch (opcode) {
     case TranslationOpcode::BEGIN_WITH_FEEDBACK:
@@ -428,7 +428,7 @@ TranslatedValue TranslatedValue::NewBool(TranslatedState* container,
 
 // static
 TranslatedValue TranslatedValue::NewTagged(TranslatedState* container,
-                                           Object literal) {
+                                           Tagged<Object> literal) {
   TranslatedValue slot(container, kTagged);
   slot.raw_literal_ = literal;
   return slot;
@@ -441,7 +441,7 @@ TranslatedValue TranslatedValue::NewInvalid(TranslatedState* container) {
 
 Isolate* TranslatedValue::isolate() const { return container_->isolate(); }
 
-Object TranslatedValue::raw_literal() const {
+Tagged<Object> TranslatedValue::raw_literal() const {
   DCHECK_EQ(kTagged, kind());
   return raw_literal_;
 }
@@ -486,7 +486,7 @@ int TranslatedValue::object_index() const {
   return materialization_info_.id_;
 }
 
-Object TranslatedValue::GetRawValue() const {
+Tagged<Object> TranslatedValue::GetRawValue() const {
   // If we have a value, return it.
   if (materialization_state() == kFinished) {
     int smi;
@@ -770,8 +770,8 @@ void TranslatedValue::Handlify() {
 }
 
 TranslatedFrame TranslatedFrame::UnoptimizedFrame(
-    BytecodeOffset bytecode_offset, SharedFunctionInfo shared_info, int height,
-    int return_value_offset, int return_value_count) {
+    BytecodeOffset bytecode_offset, Tagged<SharedFunctionInfo> shared_info,
+    int height, int return_value_offset, int return_value_count) {
   TranslatedFrame frame(kUnoptimizedFunction, shared_info, height,
                         return_value_offset, return_value_count);
   frame.bytecode_offset_ = bytecode_offset;
@@ -779,22 +779,22 @@ TranslatedFrame TranslatedFrame::UnoptimizedFrame(
 }
 
 TranslatedFrame TranslatedFrame::InlinedExtraArguments(
-    SharedFunctionInfo shared_info, int height) {
+    Tagged<SharedFunctionInfo> shared_info, int height) {
   return TranslatedFrame(kInlinedExtraArguments, shared_info, height);
 }
 
 TranslatedFrame TranslatedFrame::ConstructCreateStubFrame(
-    SharedFunctionInfo shared_info, int height) {
+    Tagged<SharedFunctionInfo> shared_info, int height) {
   return TranslatedFrame(kConstructCreateStub, shared_info, height);
 }
 
 TranslatedFrame TranslatedFrame::ConstructInvokeStubFrame(
-    SharedFunctionInfo shared_info) {
+    Tagged<SharedFunctionInfo> shared_info) {
   return TranslatedFrame(kConstructInvokeStub, shared_info, 0);
 }
 
 TranslatedFrame TranslatedFrame::BuiltinContinuationFrame(
-    BytecodeOffset bytecode_offset, SharedFunctionInfo shared_info,
+    BytecodeOffset bytecode_offset, Tagged<SharedFunctionInfo> shared_info,
     int height) {
   TranslatedFrame frame(kBuiltinContinuation, shared_info, height);
   frame.bytecode_offset_ = bytecode_offset;
@@ -803,7 +803,7 @@ TranslatedFrame TranslatedFrame::BuiltinContinuationFrame(
 
 #if V8_ENABLE_WEBASSEMBLY
 TranslatedFrame TranslatedFrame::WasmInlinedIntoJSFrame(
-    BytecodeOffset bytecode_offset, SharedFunctionInfo shared_info,
+    BytecodeOffset bytecode_offset, Tagged<SharedFunctionInfo> shared_info,
     int height) {
   TranslatedFrame frame(kWasmInlinedIntoJS, shared_info, height);
   frame.bytecode_offset_ = bytecode_offset;
@@ -811,8 +811,8 @@ TranslatedFrame TranslatedFrame::WasmInlinedIntoJSFrame(
 }
 
 TranslatedFrame TranslatedFrame::JSToWasmBuiltinContinuationFrame(
-    BytecodeOffset bytecode_offset, SharedFunctionInfo shared_info, int height,
-    base::Optional<wasm::ValueKind> return_kind) {
+    BytecodeOffset bytecode_offset, Tagged<SharedFunctionInfo> shared_info,
+    int height, base::Optional<wasm::ValueKind> return_kind) {
   TranslatedFrame frame(kJSToWasmBuiltinContinuation, shared_info, height);
   frame.bytecode_offset_ = bytecode_offset;
   frame.return_kind_ = return_kind;
@@ -821,7 +821,7 @@ TranslatedFrame TranslatedFrame::JSToWasmBuiltinContinuationFrame(
 #endif  // V8_ENABLE_WEBASSEMBLY
 
 TranslatedFrame TranslatedFrame::JavaScriptBuiltinContinuationFrame(
-    BytecodeOffset bytecode_offset, SharedFunctionInfo shared_info,
+    BytecodeOffset bytecode_offset, Tagged<SharedFunctionInfo> shared_info,
     int height) {
   TranslatedFrame frame(kJavaScriptBuiltinContinuation, shared_info, height);
   frame.bytecode_offset_ = bytecode_offset;
@@ -829,7 +829,7 @@ TranslatedFrame TranslatedFrame::JavaScriptBuiltinContinuationFrame(
 }
 
 TranslatedFrame TranslatedFrame::JavaScriptBuiltinContinuationWithCatchFrame(
-    BytecodeOffset bytecode_offset, SharedFunctionInfo shared_info,
+    BytecodeOffset bytecode_offset, Tagged<SharedFunctionInfo> shared_info,
     int height) {
   TranslatedFrame frame(kJavaScriptBuiltinContinuationWithCatch, shared_info,
                         height);
@@ -891,13 +891,14 @@ void TranslatedFrame::Handlify(Isolate* isolate) {
 
 TranslatedFrame TranslatedState::CreateNextTranslatedFrame(
     DeoptimizationFrameTranslation::Iterator* iterator,
-    DeoptimizationLiteralArray literal_array, Address fp, FILE* trace_file) {
+    Tagged<DeoptimizationLiteralArray> literal_array, Address fp,
+    FILE* trace_file) {
   TranslationOpcode opcode = iterator->NextOpcode();
   switch (opcode) {
     case TranslationOpcode::INTERPRETED_FRAME_WITH_RETURN:
     case TranslationOpcode::INTERPRETED_FRAME_WITHOUT_RETURN: {
       BytecodeOffset bytecode_offset = BytecodeOffset(iterator->NextOperand());
-      SharedFunctionInfo shared_info =
+      Tagged<SharedFunctionInfo> shared_info =
           SharedFunctionInfo::cast(literal_array->get(iterator->NextOperand()));
       int height = iterator->NextOperand();
       int return_value_offset = 0;
@@ -923,7 +924,7 @@ TranslatedFrame TranslatedState::CreateNextTranslatedFrame(
     }
 
     case TranslationOpcode::INLINED_EXTRA_ARGUMENTS: {
-      SharedFunctionInfo shared_info =
+      Tagged<SharedFunctionInfo> shared_info =
           SharedFunctionInfo::cast(literal_array->get(iterator->NextOperand()));
       int height = iterator->NextOperand();
       if (trace_file != nullptr) {
@@ -935,7 +936,7 @@ TranslatedFrame TranslatedState::CreateNextTranslatedFrame(
     }
 
     case TranslationOpcode::CONSTRUCT_CREATE_STUB_FRAME: {
-      SharedFunctionInfo shared_info =
+      Tagged<SharedFunctionInfo> shared_info =
           SharedFunctionInfo::cast(literal_array->get(iterator->NextOperand()));
       int height = iterator->NextOperand();
       if (trace_file != nullptr) {
@@ -949,7 +950,7 @@ TranslatedFrame TranslatedState::CreateNextTranslatedFrame(
     }
 
     case TranslationOpcode::CONSTRUCT_INVOKE_STUB_FRAME: {
-      SharedFunctionInfo shared_info =
+      Tagged<SharedFunctionInfo> shared_info =
           SharedFunctionInfo::cast(literal_array->get(iterator->NextOperand()));
       if (trace_file != nullptr) {
         std::unique_ptr<char[]> name = shared_info->DebugNameCStr();
@@ -962,7 +963,7 @@ TranslatedFrame TranslatedState::CreateNextTranslatedFrame(
 
     case TranslationOpcode::BUILTIN_CONTINUATION_FRAME: {
       BytecodeOffset bytecode_offset = BytecodeOffset(iterator->NextOperand());
-      SharedFunctionInfo shared_info =
+      Tagged<SharedFunctionInfo> shared_info =
           SharedFunctionInfo::cast(literal_array->get(iterator->NextOperand()));
       int height = iterator->NextOperand();
       if (trace_file != nullptr) {
@@ -979,7 +980,7 @@ TranslatedFrame TranslatedState::CreateNextTranslatedFrame(
 #if V8_ENABLE_WEBASSEMBLY
     case TranslationOpcode::WASM_INLINED_INTO_JS_FRAME: {
       BytecodeOffset bailout_id = BytecodeOffset(iterator->NextOperand());
-      SharedFunctionInfo shared_info =
+      Tagged<SharedFunctionInfo> shared_info =
           SharedFunctionInfo::cast(literal_array->get(iterator->NextOperand()));
       int height = iterator->NextOperand();
       if (trace_file != nullptr) {
@@ -995,7 +996,7 @@ TranslatedFrame TranslatedState::CreateNextTranslatedFrame(
 
     case TranslationOpcode::JS_TO_WASM_BUILTIN_CONTINUATION_FRAME: {
       BytecodeOffset bailout_id = BytecodeOffset(iterator->NextOperand());
-      SharedFunctionInfo shared_info =
+      Tagged<SharedFunctionInfo> shared_info =
           SharedFunctionInfo::cast(literal_array->get(iterator->NextOperand()));
       int height = iterator->NextOperand();
       int return_kind_code = iterator->NextOperand();
@@ -1019,7 +1020,7 @@ TranslatedFrame TranslatedState::CreateNextTranslatedFrame(
 
     case TranslationOpcode::JAVA_SCRIPT_BUILTIN_CONTINUATION_FRAME: {
       BytecodeOffset bytecode_offset = BytecodeOffset(iterator->NextOperand());
-      SharedFunctionInfo shared_info =
+      Tagged<SharedFunctionInfo> shared_info =
           SharedFunctionInfo::cast(literal_array->get(iterator->NextOperand()));
       int height = iterator->NextOperand();
       if (trace_file != nullptr) {
@@ -1035,7 +1036,7 @@ TranslatedFrame TranslatedState::CreateNextTranslatedFrame(
 
     case TranslationOpcode::JAVA_SCRIPT_BUILTIN_CONTINUATION_WITH_CATCH_FRAME: {
       BytecodeOffset bytecode_offset = BytecodeOffset(iterator->NextOperand());
-      SharedFunctionInfo shared_info =
+      Tagged<SharedFunctionInfo> shared_info =
           SharedFunctionInfo::cast(literal_array->get(iterator->NextOperand()));
       int height = iterator->NextOperand();
       if (trace_file != nullptr) {
@@ -1165,7 +1166,7 @@ void TranslatedState::CreateArgumentsElementsTranslatedValues(
 // DeoptimizationFrameTranslation::Iterator.
 int TranslatedState::CreateNextTranslatedValue(
     int frame_index, DeoptimizationFrameTranslation::Iterator* iterator,
-    DeoptimizationLiteralArray literal_array, Address fp,
+    Tagged<DeoptimizationLiteralArray> literal_array, Address fp,
     RegisterValues* registers, FILE* trace_file) {
   disasm::NameConverter converter;
 
@@ -1630,7 +1631,7 @@ TranslatedState::TranslatedState(const JavaScriptFrame* frame)
 void TranslatedState::Init(Isolate* isolate, Address input_frame_pointer,
                            Address stack_frame_pointer,
                            DeoptimizationFrameTranslation::Iterator* iterator,
-                           DeoptimizationLiteralArray literal_array,
+                           Tagged<DeoptimizationLiteralArray> literal_array,
                            RegisterValues* registers, FILE* trace_file,
                            int formal_parameter_count,
                            int actual_argument_count) {
@@ -2503,7 +2504,7 @@ bool TranslatedState::DoUpdateFeedback() {
 
 void TranslatedState::ReadUpdateFeedback(
     DeoptimizationFrameTranslation::Iterator* iterator,
-    DeoptimizationLiteralArray literal_array, FILE* trace_file) {
+    Tagged<DeoptimizationLiteralArray> literal_array, FILE* trace_file) {
   CHECK_EQ(TranslationOpcode::UPDATE_FEEDBACK, iterator->NextOpcode());
   feedback_vector_ =
       FeedbackVector::cast(literal_array->get(iterator->NextOperand()));
diff --git a/src/deoptimizer/translated-state.h b/src/deoptimizer/translated-state.h
index 9745edae880..34ed3584535 100644
--- a/src/deoptimizer/translated-state.h
+++ b/src/deoptimizer/translated-state.h
@@ -31,7 +31,7 @@ class TranslatedState;
 void DeoptimizationFrameTranslationPrintSingleOpcode(
     std::ostream& os, TranslationOpcode opcode,
     DeoptimizationFrameTranslation::Iterator& iterator,
-    DeoptimizationLiteralArray literal_array);
+    Tagged<DeoptimizationLiteralArray> literal_array);
 
 // The Translated{Value,Frame,State} class hierarchy are a set of utility
 // functions to work with the combination of translations (built from a
@@ -50,7 +50,7 @@ class TranslatedValue {
   // Allocation-free getter of the value.
   // Returns ReadOnlyRoots::arguments_marker() if allocation would be necessary
   // to get the value. In the case of numbers, returns a Smi if possible.
-  Object GetRawValue() const;
+  Tagged<Object> GetRawValue() const;
 
   // Convenience wrapper around GetRawValue (checked).
   int GetSmiValue() const;
@@ -120,7 +120,8 @@ class TranslatedValue {
                                            uint64_t value);
   static TranslatedValue NewUInt32(TranslatedState* container, uint32_t value);
   static TranslatedValue NewBool(TranslatedState* container, uint32_t value);
-  static TranslatedValue NewTagged(TranslatedState* container, Object literal);
+  static TranslatedValue NewTagged(TranslatedState* container,
+                                   Tagged<Object> literal);
   static TranslatedValue NewInvalid(TranslatedState* container);
 
   Isolate* isolate() const;
@@ -154,7 +155,7 @@ class TranslatedValue {
 
   union {
     // kind kTagged. After handlification it is always nullptr.
-    Object raw_literal_;
+    Tagged<Object> raw_literal_;
     // kind is kUInt32 or kBoolBit.
     uint32_t uint32_value_;
     // kind is kInt32.
@@ -172,7 +173,7 @@ class TranslatedValue {
   };
 
   // Checked accessors for the union members.
-  Object raw_literal() const;
+  Tagged<Object> raw_literal() const;
   int32_t int32_value() const;
   int64_t int64_value() const;
   uint32_t uint32_value() const;
@@ -216,7 +217,7 @@ class TranslatedFrame {
   int return_value_offset() const { return return_value_offset_; }
   int return_value_count() const { return return_value_count_; }
 
-  SharedFunctionInfo raw_shared_info() const {
+  Tagged<SharedFunctionInfo> raw_shared_info() const {
     CHECK(!raw_shared_info_.is_null());
     return raw_shared_info_;
   }
@@ -282,32 +283,34 @@ class TranslatedFrame {
   friend class Deoptimizer;
 
   // Constructor static methods.
-  static TranslatedFrame UnoptimizedFrame(BytecodeOffset bytecode_offset,
-                                          SharedFunctionInfo shared_info,
-                                          int height, int return_value_offset,
-                                          int return_value_count);
+  static TranslatedFrame UnoptimizedFrame(
+      BytecodeOffset bytecode_offset, Tagged<SharedFunctionInfo> shared_info,
+      int height, int return_value_offset, int return_value_count);
   static TranslatedFrame AccessorFrame(Kind kind,
-                                       SharedFunctionInfo shared_info);
-  static TranslatedFrame InlinedExtraArguments(SharedFunctionInfo shared_info,
-                                               int height);
+                                       Tagged<SharedFunctionInfo> shared_info);
+  static TranslatedFrame InlinedExtraArguments(
+      Tagged<SharedFunctionInfo> shared_info, int height);
   static TranslatedFrame ConstructCreateStubFrame(
-      SharedFunctionInfo shared_info, int height);
+      Tagged<SharedFunctionInfo> shared_info, int height);
   static TranslatedFrame ConstructInvokeStubFrame(
-      SharedFunctionInfo shared_info);
+      Tagged<SharedFunctionInfo> shared_info);
   static TranslatedFrame BuiltinContinuationFrame(
-      BytecodeOffset bailout_id, SharedFunctionInfo shared_info, int height);
+      BytecodeOffset bailout_id, Tagged<SharedFunctionInfo> shared_info,
+      int height);
 #if V8_ENABLE_WEBASSEMBLY
-  static TranslatedFrame WasmInlinedIntoJSFrame(BytecodeOffset bailout_id,
-                                                SharedFunctionInfo shared_info,
-                                                int height);
+  static TranslatedFrame WasmInlinedIntoJSFrame(
+      BytecodeOffset bailout_id, Tagged<SharedFunctionInfo> shared_info,
+      int height);
   static TranslatedFrame JSToWasmBuiltinContinuationFrame(
-      BytecodeOffset bailout_id, SharedFunctionInfo shared_info, int height,
-      base::Optional<wasm::ValueKind> return_type);
+      BytecodeOffset bailout_id, Tagged<SharedFunctionInfo> shared_info,
+      int height, base::Optional<wasm::ValueKind> return_type);
 #endif  // V8_ENABLE_WEBASSEMBLY
   static TranslatedFrame JavaScriptBuiltinContinuationFrame(
-      BytecodeOffset bailout_id, SharedFunctionInfo shared_info, int height);
+      BytecodeOffset bailout_id, Tagged<SharedFunctionInfo> shared_info,
+      int height);
   static TranslatedFrame JavaScriptBuiltinContinuationWithCatchFrame(
-      BytecodeOffset bailout_id, SharedFunctionInfo shared_info, int height);
+      BytecodeOffset bailout_id, Tagged<SharedFunctionInfo> shared_info,
+      int height);
   static TranslatedFrame InvalidFrame() {
     return TranslatedFrame(kInvalid, SharedFunctionInfo());
   }
@@ -315,7 +318,7 @@ class TranslatedFrame {
   static void AdvanceIterator(std::deque<TranslatedValue>::iterator* iter);
 
   TranslatedFrame(Kind kind,
-                  SharedFunctionInfo shared_info = SharedFunctionInfo(),
+                  Tagged<SharedFunctionInfo> shared_info = SharedFunctionInfo(),
                   int height = 0, int return_value_offset = 0,
                   int return_value_count = 0)
       : kind_(kind),
@@ -399,8 +402,9 @@ class TranslatedState {
   void Init(Isolate* isolate, Address input_frame_pointer,
             Address stack_frame_pointer,
             DeoptimizationFrameTranslation::Iterator* iterator,
-            DeoptimizationLiteralArray literal_array, RegisterValues* registers,
-            FILE* trace_file, int parameter_count, int actual_argument_count);
+            Tagged<DeoptimizationLiteralArray> literal_array,
+            RegisterValues* registers, FILE* trace_file, int parameter_count,
+            int actual_argument_count);
 
   void VerifyMaterializedObjects();
   bool DoUpdateFeedback();
@@ -416,10 +420,11 @@ class TranslatedState {
 
   TranslatedFrame CreateNextTranslatedFrame(
       DeoptimizationFrameTranslation::Iterator* iterator,
-      DeoptimizationLiteralArray literal_array, Address fp, FILE* trace_file);
+      Tagged<DeoptimizationLiteralArray> literal_array, Address fp,
+      FILE* trace_file);
   int CreateNextTranslatedValue(
       int frame_index, DeoptimizationFrameTranslation::Iterator* iterator,
-      DeoptimizationLiteralArray literal_array, Address fp,
+      Tagged<DeoptimizationLiteralArray> literal_array, Address fp,
       RegisterValues* registers, FILE* trace_file);
   Address DecompressIfNeeded(intptr_t value);
   void CreateArgumentsElementsTranslatedValues(int frame_index,
@@ -456,7 +461,7 @@ class TranslatedState {
       Handle<Map> map, const DisallowGarbageCollection& no_gc);
 
   void ReadUpdateFeedback(DeoptimizationFrameTranslation::Iterator* iterator,
-                          DeoptimizationLiteralArray literal_array,
+                          Tagged<DeoptimizationLiteralArray> literal_array,
                           FILE* trace_file);
 
   TranslatedValue* ResolveCapturedObject(TranslatedValue* slot);
diff --git a/src/diagnostics/basic-block-profiler.cc b/src/diagnostics/basic-block-profiler.cc
index 21ba7f76597..ba1a6e15169 100644
--- a/src/diagnostics/basic-block-profiler.cc
+++ b/src/diagnostics/basic-block-profiler.cc
@@ -77,12 +77,12 @@ BasicBlockProfilerData::BasicBlockProfilerData(
 }
 
 BasicBlockProfilerData::BasicBlockProfilerData(
-    OnHeapBasicBlockProfilerData js_heap_data) {
+    Tagged<OnHeapBasicBlockProfilerData> js_heap_data) {
   CopyFromJSHeap(js_heap_data);
 }
 
 void BasicBlockProfilerData::CopyFromJSHeap(
-    OnHeapBasicBlockProfilerData js_heap_data) {
+    Tagged<OnHeapBasicBlockProfilerData> js_heap_data) {
   function_name_ = js_heap_data->name()->ToCString().get();
   schedule_ = js_heap_data->schedule()->ToCString().get();
   code_ = js_heap_data->code()->ToCString().get();
@@ -94,7 +94,8 @@ void BasicBlockProfilerData::CopyFromJSHeap(
   for (int i = 0; i < block_ids.length() / kBlockIdSlotSize; ++i) {
     block_ids_.push_back(block_ids.get(i));
   }
-  PodArray<std::pair<int32_t, int32_t>> branches = js_heap_data->branches();
+  Tagged<PodArray<std::pair<int32_t, int32_t>>> branches =
+      js_heap_data->branches();
   for (int i = 0; i < branches->length(); ++i) {
     branches_.push_back(branches->get(i));
   }
@@ -200,7 +201,7 @@ void BasicBlockProfiler::Log(Isolate* isolate, std::ostream& os) {
 
 std::vector<bool> BasicBlockProfiler::GetCoverageBitmap(Isolate* isolate) {
   DisallowGarbageCollection no_gc;
-  ArrayList list(isolate->heap()->basic_block_profiling_data());
+  Tagged<ArrayList> list(isolate->heap()->basic_block_profiling_data());
   std::vector<bool> out;
   int list_length = list->Length();
   for (int i = 0; i < list_length; ++i) {
diff --git a/src/diagnostics/basic-block-profiler.h b/src/diagnostics/basic-block-profiler.h
index d634c4da915..4b266dbd3b5 100644
--- a/src/diagnostics/basic-block-profiler.h
+++ b/src/diagnostics/basic-block-profiler.h
@@ -27,7 +27,7 @@ class BasicBlockProfilerData {
   V8_EXPORT_PRIVATE BasicBlockProfilerData(
       Handle<OnHeapBasicBlockProfilerData> js_heap_data, Isolate* isolate);
   V8_EXPORT_PRIVATE BasicBlockProfilerData(
-      OnHeapBasicBlockProfilerData js_heap_data);
+      Tagged<OnHeapBasicBlockProfilerData> js_heap_data);
 
   BasicBlockProfilerData(const BasicBlockProfilerData&) = delete;
   BasicBlockProfilerData& operator=(const BasicBlockProfilerData&) = delete;
@@ -59,7 +59,7 @@ class BasicBlockProfilerData {
 
   V8_EXPORT_PRIVATE void ResetCounts();
 
-  void CopyFromJSHeap(OnHeapBasicBlockProfilerData js_heap_data);
+  void CopyFromJSHeap(Tagged<OnHeapBasicBlockProfilerData> js_heap_data);
 
   // These vectors are indexed by reverse post-order block number.
   std::vector<int32_t> block_ids_;
diff --git a/src/diagnostics/disassembler.cc b/src/diagnostics/disassembler.cc
index fcd1c4ee6f8..6656cf4fc5f 100644
--- a/src/diagnostics/disassembler.cc
+++ b/src/diagnostics/disassembler.cc
@@ -257,7 +257,7 @@ static void PrintRelocInfo(std::ostringstream& out, Isolate* isolate,
     out << "    ;; external reference (" << reference_name << ")";
   } else if (RelocInfo::IsCodeTargetMode(rmode)) {
     out << "    ;; code:";
-    Code code =
+    Tagged<Code> code =
         isolate->heap()->FindCodeForInnerPointer(relocinfo->target_address());
     CodeKind kind = code->kind();
     if (code->is_builtin()) {
diff --git a/src/diagnostics/objects-debug.cc b/src/diagnostics/objects-debug.cc
index 0a32bd83906..2b108b85e7e 100644
--- a/src/diagnostics/objects-debug.cc
+++ b/src/diagnostics/objects-debug.cc
@@ -141,7 +141,7 @@ void Object::ObjectVerify(Tagged<Object> obj, Isolate* isolate) {
   CHECK(!IsConstructor(obj, cage_base) || IsCallable(obj, cage_base));
 }
 
-void Object::VerifyPointer(Isolate* isolate, Object p) {
+void Object::VerifyPointer(Isolate* isolate, Tagged<Object> p) {
   if (IsHeapObject(p)) {
     HeapObject::VerifyHeapPointer(isolate, p);
   } else {
@@ -149,7 +149,7 @@ void Object::VerifyPointer(Isolate* isolate, Object p) {
   }
 }
 
-void Object::VerifyAnyTagged(Isolate* isolate, Object p) {
+void Object::VerifyAnyTagged(Isolate* isolate, Tagged<Object> p) {
   if (IsHeapObject(p)) {
     if (V8_EXTERNAL_CODE_SPACE_BOOL) {
       CHECK(IsValidHeapObject(isolate->heap(), HeapObject::cast(p)));
@@ -162,7 +162,7 @@ void Object::VerifyAnyTagged(Isolate* isolate, Object p) {
 }
 
 void MaybeObject::VerifyMaybeObjectPointer(Isolate* isolate, MaybeObject p) {
-  HeapObject heap_object;
+  Tagged<HeapObject> heap_object;
   if (p->GetHeapObject(&heap_object)) {
     HeapObject::VerifyHeapPointer(isolate, heap_object);
   } else {
@@ -336,7 +336,7 @@ void HeapObject::HeapObjectVerify(Isolate* isolate) {
 }
 
 // static
-void HeapObject::VerifyHeapPointer(Isolate* isolate, Object p) {
+void HeapObject::VerifyHeapPointer(Isolate* isolate, Tagged<Object> p) {
   CHECK(IsHeapObject(p));
   // If you crashed here and {isolate->is_shared()}, there is a bug causing the
   // host of {p} to point to a non-shared object.
@@ -345,7 +345,7 @@ void HeapObject::VerifyHeapPointer(Isolate* isolate, Object p) {
 }
 
 // static
-void HeapObject::VerifyCodePointer(Isolate* isolate, Object p) {
+void HeapObject::VerifyCodePointer(Isolate* isolate, Tagged<Object> p) {
   CHECK(IsHeapObject(p));
   CHECK(IsValidCodeObject(isolate->heap(), HeapObject::cast(p)));
   PtrComprCageBase cage_base(isolate);
@@ -385,7 +385,7 @@ bool JSObject::ElementsAreSafeToExamine(PtrComprCageBase cage_base) const {
 
 namespace {
 
-void VerifyJSObjectElements(Isolate* isolate, JSObject object) {
+void VerifyJSObjectElements(Isolate* isolate, Tagged<JSObject> object) {
   // Only TypedArrays can have these specialized elements.
   if (IsJSTypedArray(object)) {
     // TODO(bmeurer,v8:4153): Fix CreateTypedArray to either not instantiate
@@ -407,17 +407,17 @@ void VerifyJSObjectElements(Isolate* isolate, JSObject object) {
     return;
   }
 
-  FixedArray elements = FixedArray::cast(object->elements());
+  Tagged<FixedArray> elements = FixedArray::cast(object->elements());
   if (object->HasSmiElements()) {
     // We might have a partially initialized backing store, in which case we
     // allow the hole + smi values.
     for (int i = 0; i < elements->length(); i++) {
-      Object value = elements->get(i);
+      Tagged<Object> value = elements->get(i);
       CHECK(IsSmi(value) || IsTheHole(value, isolate));
     }
   } else if (object->HasObjectElements()) {
     for (int i = 0; i < elements->length(); i++) {
-      Object element = elements->get(i);
+      Tagged<Object> element = elements->get(i);
       CHECK(!HasWeakHeapObjectTag(element));
     }
   }
@@ -444,7 +444,7 @@ void JSObject::JSObjectVerify(Isolate* isolate) {
       int delta = actual_unused_property_fields - map()->UnusedPropertyFields();
       CHECK_EQ(0, delta % JSObject::kFieldsAdded);
     }
-    DescriptorArray descriptors = map()->instance_descriptors(isolate);
+    Tagged<DescriptorArray> descriptors = map()->instance_descriptors(isolate);
     bool is_transitionable_fast_elements_kind =
         IsTransitionableFastElementsKind(map()->elements_kind());
 
@@ -457,18 +457,18 @@ void JSObject::JSObjectVerify(Isolate* isolate) {
         if (COMPRESS_POINTERS_BOOL && index.is_inobject()) {
           VerifyObjectField(isolate, index.offset());
         }
-        Object value = RawFastPropertyAt(index);
+        Tagged<Object> value = RawFastPropertyAt(index);
         if (r.IsDouble()) DCHECK(IsHeapNumber(value));
         if (IsUninitialized(value, isolate)) continue;
         if (r.IsSmi()) DCHECK(IsSmi(value));
         if (r.IsHeapObject()) DCHECK(IsHeapObject(value));
-        FieldType field_type = descriptors->GetFieldType(i);
+        Tagged<FieldType> field_type = descriptors->GetFieldType(i);
         bool type_is_none = IsNone(field_type);
         bool type_is_any = IsAny(field_type);
         if (r.IsNone()) {
           CHECK(type_is_none);
         } else if (!type_is_any && !(type_is_none && r.IsHeapObject())) {
-          CHECK(!field_type.NowStable() || field_type.NowContains(value));
+          CHECK(!field_type->NowStable() || field_type->NowContains(value));
         }
         CHECK_IMPLIES(is_transitionable_fast_elements_kind,
                       Map::IsMostGeneralFieldType(r, field_type));
@@ -476,9 +476,9 @@ void JSObject::JSObjectVerify(Isolate* isolate) {
     }
 
     if (map()->EnumLength() != kInvalidEnumCacheSentinel) {
-      EnumCache enum_cache = descriptors->enum_cache();
-      FixedArray keys = enum_cache->keys();
-      FixedArray indices = enum_cache->indices();
+      Tagged<EnumCache> enum_cache = descriptors->enum_cache();
+      Tagged<FixedArray> keys = enum_cache->keys();
+      Tagged<FixedArray> indices = enum_cache->indices();
       CHECK_LE(map()->EnumLength(), keys->length());
       CHECK_IMPLIES(indices != ReadOnlyRoots(isolate).empty_fixed_array(),
                     keys->length() == indices->length());
@@ -521,9 +521,9 @@ void Map::MapVerify(Isolate* isolate) {
                instance_descriptors(isolate)->number_of_descriptors());
     } else {
       // If there is a parent map it must be non-stable.
-      Map parent = Map::cast(GetBackPointer());
+      Tagged<Map> parent = Map::cast(GetBackPointer());
       CHECK(!parent->is_stable());
-      DescriptorArray descriptors = instance_descriptors(isolate);
+      Tagged<DescriptorArray> descriptors = instance_descriptors(isolate);
       if (descriptors == parent->instance_descriptors(isolate)) {
         if (NumberOfOwnDescriptors() == parent->NumberOfOwnDescriptors() + 1) {
           // Descriptors sharing through property transitions takes over
@@ -579,7 +579,7 @@ void Map::MapVerify(Isolate* isolate) {
       } else {
         CHECK(Object::InSharedHeap(*this));
         CHECK(IsUndefined(GetBackPointer(), isolate));
-        Object maybe_cell = prototype_validity_cell(kRelaxedLoad);
+        Tagged<Object> maybe_cell = prototype_validity_cell(kRelaxedLoad);
         if (IsCell(maybe_cell)) CHECK(Object::InSharedHeap(maybe_cell));
         CHECK(!is_extensible());
         CHECK(!is_prototype_map());
@@ -593,12 +593,12 @@ void Map::MapVerify(Isolate* isolate) {
 
     // Check constuctor value in JSFunction's maps.
     if (IsJSFunctionMap(*this) && !IsMap(constructor_or_back_pointer())) {
-      Object maybe_constructor = constructor_or_back_pointer();
+      Tagged<Object> maybe_constructor = constructor_or_back_pointer();
       // Constructor field might still contain a tuple if this map used to
       // have non-instance prototype earlier.
       CHECK_IMPLIES(has_non_instance_prototype(), IsTuple2(maybe_constructor));
       if (IsTuple2(maybe_constructor)) {
-        Tuple2 tuple = Tuple2::cast(maybe_constructor);
+        Tagged<Tuple2> tuple = Tuple2::cast(maybe_constructor);
         // Unwrap the {constructor, non-instance_prototype} pair.
         maybe_constructor = tuple->value1();
         CHECK(!IsJSReceiver(tuple->value2()));
@@ -615,7 +615,7 @@ void Map::MapVerify(Isolate* isolate) {
     CHECK(!has_named_interceptor());
     CHECK(!is_dictionary_map());
     CHECK(!is_access_check_needed());
-    DescriptorArray const descriptors = instance_descriptors(isolate);
+    Tagged<DescriptorArray> const descriptors = instance_descriptors(isolate);
     for (InternalIndex i : IterateOwnDescriptors()) {
       CHECK(!descriptors->GetKey(i)->IsInteresting(isolate));
     }
@@ -650,7 +650,7 @@ void EmbedderDataArray::EmbedderDataArrayVerify(Isolate* isolate) {
   EmbedderDataSlot start(*this, 0);
   EmbedderDataSlot end(*this, length());
   for (EmbedderDataSlot slot = start; slot < end; ++slot) {
-    Object e = slot.load_tagged();
+    Tagged<Object> e = slot.load_tagged();
     Object::VerifyPointer(isolate, e);
   }
 }
@@ -692,7 +692,7 @@ void PropertyArray::PropertyArrayVerify(Isolate* isolate) {
   // There are no empty PropertyArrays.
   CHECK_LT(0, length());
   for (int i = 0; i < length(); i++) {
-    Object e = get(i);
+    Tagged<Object> e = get(i);
     Object::VerifyPointer(isolate, e);
   }
 }
@@ -758,7 +758,8 @@ void DescriptorArray::DescriptorArrayVerify(Isolate* isolate) {
     int expected_field_index = 0;
     for (InternalIndex descriptor :
          InternalIndex::Range(number_of_descriptors())) {
-      Object key = *(GetDescriptorSlot(descriptor.as_int()) + kEntryKeyIndex);
+      Tagged<Object> key =
+          *(GetDescriptorSlot(descriptor.as_int()) + kEntryKeyIndex);
       // number_of_descriptors() may be out of sync with the actual descriptors
       // written during descriptor array construction.
       if (IsUndefined(key, isolate)) continue;
@@ -767,7 +768,7 @@ void DescriptorArray::DescriptorArrayVerify(Isolate* isolate) {
         CHECK_NE(details.attributes() & DONT_ENUM, 0);
       }
       MaybeObject value = GetValue(descriptor);
-      HeapObject heap_object;
+      Tagged<HeapObject> heap_object;
       if (details.location() == PropertyLocation::kField) {
         CHECK_EQ(details.field_index(), expected_field_index);
         CHECK(value == MaybeObject::FromObject(FieldType::None()) ||
@@ -790,13 +791,13 @@ void TransitionArray::TransitionArrayVerify(Isolate* isolate) {
 
 namespace {
 void SloppyArgumentsElementsVerify(Isolate* isolate,
-                                   SloppyArgumentsElements elements,
-                                   JSObject holder) {
+                                   Tagged<SloppyArgumentsElements> elements,
+                                   Tagged<JSObject> holder) {
   elements->SloppyArgumentsElementsVerify(isolate);
   ElementsKind kind = holder->GetElementsKind();
   bool is_fast = kind == FAST_SLOPPY_ARGUMENTS_ELEMENTS;
-  Context context_object = elements->context();
-  FixedArray arg_elements = elements->arguments();
+  Tagged<Context> context_object = elements->context();
+  Tagged<FixedArray> arg_elements = elements->arguments();
   if (arg_elements->length() == 0) {
     CHECK(arg_elements == ReadOnlyRoots(isolate).empty_fixed_array());
     return;
@@ -812,7 +813,7 @@ void SloppyArgumentsElementsVerify(Isolate* isolate,
   for (int i = 0; i < nofMappedParameters; i++) {
     // Verify that each context-mapped argument is either the hole or a valid
     // Smi within context length range.
-    Object mapped = elements->mapped_entries(i, kRelaxedLoad);
+    Tagged<Object> mapped = elements->mapped_entries(i, kRelaxedLoad);
     if (IsTheHole(mapped, isolate)) {
       // Slow sloppy arguments can be holey.
       if (!is_fast) continue;
@@ -825,7 +826,7 @@ void SloppyArgumentsElementsVerify(Isolate* isolate,
     nofMappedParameters++;
     CHECK_LE(maxMappedIndex, mappedIndex);
     maxMappedIndex = mappedIndex;
-    Object value = context_object->get(mappedIndex);
+    Tagged<Object> value = context_object->get(mappedIndex);
     CHECK(IsObject(value));
     // None of the context-mapped entries should exist in the arguments
     // elements.
@@ -999,7 +1000,8 @@ void SharedFunctionInfo::SharedFunctionInfoVerify(LocalIsolate* isolate) {
 
 namespace {
 
-bool ShouldVerifySharedFunctionInfoFunctionIndex(SharedFunctionInfo sfi) {
+bool ShouldVerifySharedFunctionInfoFunctionIndex(
+    Tagged<SharedFunctionInfo> sfi) {
   if (!sfi->HasBuiltinId()) return true;
   switch (sfi->builtin_id()) {
     case Builtin::kPromiseCapabilityDefaultReject:
@@ -1015,7 +1017,7 @@ bool ShouldVerifySharedFunctionInfoFunctionIndex(SharedFunctionInfo sfi) {
 }  // namespace
 
 void SharedFunctionInfo::SharedFunctionInfoVerify(ReadOnlyRoots roots) {
-  Object value = name_or_scope_info(kAcquireLoad);
+  Tagged<Object> value = name_or_scope_info(kAcquireLoad);
   if (IsScopeInfo(value)) {
     CHECK(!ScopeInfo::cast(value)->IsEmpty());
     CHECK_NE(value, roots.empty_scope_info());
@@ -1033,7 +1035,7 @@ void SharedFunctionInfo::SharedFunctionInfoVerify(ReadOnlyRoots roots) {
         HasUncompiledDataWithoutPreparseData());
 
   {
-    HeapObject script = this->script(kAcquireLoad);
+    Tagged<HeapObject> script = this->script(kAcquireLoad);
     CHECK(IsUndefined(script, roots) || IsScript(script));
   }
 
@@ -1051,7 +1053,7 @@ void SharedFunctionInfo::SharedFunctionInfoVerify(ReadOnlyRoots roots) {
     CHECK_EQ(expected_map_index, function_map_index());
   }
 
-  ScopeInfo info = EarlyScopeInfo(kAcquireLoad);
+  Tagged<ScopeInfo> info = EarlyScopeInfo(kAcquireLoad);
   if (!info->IsEmpty()) {
     CHECK(kind() == info->function_kind());
     CHECK_EQ(internal::IsModule(kind()), info->scope_type() == MODULE_SCOPE);
@@ -1093,17 +1095,17 @@ void Oddball::OddballVerify(Isolate* isolate) {
   CHECK(IsOddball(*this, isolate));
 
   Heap* heap = isolate->heap();
-  Object string = to_string();
+  Tagged<Object> string = to_string();
   VerifyPointer(isolate, string);
   CHECK(IsString(string));
-  Object type = type_of();
+  Tagged<Object> type = type_of();
   VerifyPointer(isolate, type);
   CHECK(IsString(type));
-  Object kind_value = TaggedField<Object>::load(*this, kKindOffset);
+  Tagged<Object> kind_value = TaggedField<Object>::load(*this, kKindOffset);
   VerifyPointer(isolate, kind_value);
   CHECK(IsSmi(kind_value));
 
-  Object number = to_number();
+  Tagged<Object> number = to_number();
   VerifyPointer(isolate, number);
   CHECK(IsSmi(number) || IsHeapNumber(number));
   if (IsHeapObject(number)) {
@@ -1170,7 +1172,7 @@ void PropertyCell::PropertyCellVerify(Isolate* isolate) {
 void Code::CodeVerify(Isolate* isolate) {
   CHECK(IsCode(*this));
   if (has_instruction_stream()) {
-    InstructionStream istream = instruction_stream();
+    Tagged<InstructionStream> istream = instruction_stream();
     CHECK_EQ(istream->code(kAcquireLoad), *this);
     CHECK_EQ(safepoint_table_offset(), 0);
     CHECK_LE(safepoint_table_offset(), handler_table_offset());
@@ -1193,12 +1195,12 @@ void Code::CodeVerify(Isolate* isolate) {
       // InstructionStream::OffHeapInstructionStart()).  So, do a reverse
       // Code object lookup via instruction_start value to ensure it
       // corresponds to this current Code object.
-      Code lookup_result =
+      Tagged<Code> lookup_result =
           isolate->heap()->FindCodeForInnerPointer(instruction_start());
       CHECK_EQ(lookup_result, *this);
     }
 #else
-    CHECK_EQ(istream.instruction_start(), instruction_start());
+    CHECK_EQ(istream->instruction_start(), instruction_start());
 #endif  // V8_COMPRESS_POINTERS && V8_SHORT_BUILTIN_CALLS
   }
 
@@ -1210,7 +1212,7 @@ void Code::CodeVerify(Isolate* isolate) {
 }
 
 void InstructionStream::InstructionStreamVerify(Isolate* isolate) {
-  Code code;
+  Tagged<Code> code;
   if (!TryGetCode(&code, kAcquireLoad)) return;
   CHECK(
       IsAligned(code->instruction_size(),
@@ -1270,7 +1272,7 @@ void JSArray::JSArrayVerify(Isolate* isolate) {
       CHECK(Object::ToArrayLength(length(), &array_length));
     }
     if (array_length != 0) {
-      NumberDictionary dict = NumberDictionary::cast(elements());
+      Tagged<NumberDictionary> dict = NumberDictionary::cast(elements());
       // The dictionary can never have more elements than the array length + 1.
       // If the backing store grows the verification might be triggered with
       // the old length in place.
@@ -1312,7 +1314,7 @@ USE_TORQUE_VERIFIER(JSWrappedFunction)
 
 namespace {
 
-void VerifyElementIsShared(Object element) {
+void VerifyElementIsShared(Tagged<Object> element) {
   // Exception for ThinStrings:
   // When storing a ThinString in a shared object, we want to store the actual
   // string, which is shared when sharing the string table.
@@ -1336,9 +1338,10 @@ void JSSharedStruct::JSSharedStructVerify(Isolate* isolate) {
   CHECK(HasFastProperties());
   // Shared structs can only point to primitives or other shared HeapObjects,
   // even internally.
-  Map struct_map = map();
+  Tagged<Map> struct_map = map();
   CHECK(Object::InSharedHeap(property_array()));
-  DescriptorArray descriptors = struct_map->instance_descriptors(isolate);
+  Tagged<DescriptorArray> descriptors =
+      struct_map->instance_descriptors(isolate);
   for (InternalIndex i : struct_map->IterateOwnDescriptors()) {
     PropertyDetails details = descriptors->GetDetails(i);
     CHECK_EQ(PropertyKind::kData, details.kind());
@@ -1367,10 +1370,10 @@ void JSSharedArray::JSSharedArrayVerify(Isolate* isolate) {
   CHECK(HasFastProperties());
   // Shared arrays can only point to primitives or other shared HeapObjects,
   // even internally.
-  FixedArray storage = FixedArray::cast(elements());
+  Tagged<FixedArray> storage = FixedArray::cast(elements());
   uint32_t length = storage->length();
   for (uint32_t j = 0; j < length; j++) {
-    Object element_value = storage->get(j);
+    Tagged<Object> element_value = storage->get(j);
     VerifyElementIsShared(element_value);
   }
 }
@@ -1516,7 +1519,7 @@ void SmallOrderedHashTable<Derived>::SmallOrderedHashTableVerify(
 
   for (int entry = 0; entry < NumberOfElements(); entry++) {
     for (int offset = 0; offset < Derived::kEntrySize; offset++) {
-      Object val = GetDataEntry(entry, offset);
+      Tagged<Object> val = GetDataEntry(entry, offset);
       VerifyPointer(isolate, val);
     }
   }
@@ -1524,7 +1527,7 @@ void SmallOrderedHashTable<Derived>::SmallOrderedHashTableVerify(
   for (int entry = NumberOfElements() + NumberOfDeletedElements();
        entry < Capacity(); entry++) {
     for (int offset = 0; offset < Derived::kEntrySize; offset++) {
-      Object val = GetDataEntry(entry, offset);
+      Tagged<Object> val = GetDataEntry(entry, offset);
       CHECK(IsTheHole(val, isolate));
     }
   }
@@ -1537,7 +1540,7 @@ void SmallOrderedHashMap::SmallOrderedHashMapVerify(Isolate* isolate) {
   for (int entry = NumberOfElements(); entry < NumberOfDeletedElements();
        entry++) {
     for (int offset = 0; offset < kEntrySize; offset++) {
-      Object val = GetDataEntry(entry, offset);
+      Tagged<Object> val = GetDataEntry(entry, offset);
       CHECK(IsTheHole(val, isolate));
     }
   }
@@ -1550,7 +1553,7 @@ void SmallOrderedHashSet::SmallOrderedHashSetVerify(Isolate* isolate) {
   for (int entry = NumberOfElements(); entry < NumberOfDeletedElements();
        entry++) {
     for (int offset = 0; offset < kEntrySize; offset++) {
-      Object val = GetDataEntry(entry, offset);
+      Tagged<Object> val = GetDataEntry(entry, offset);
       CHECK(IsTheHole(val, isolate));
     }
   }
@@ -1564,7 +1567,7 @@ void SmallOrderedNameDictionary::SmallOrderedNameDictionaryVerify(
   for (int entry = NumberOfElements(); entry < NumberOfDeletedElements();
        entry++) {
     for (int offset = 0; offset < kEntrySize; offset++) {
-      Object val = GetDataEntry(entry, offset);
+      Tagged<Object> val = GetDataEntry(entry, offset);
       CHECK(IsTheHole(val, isolate) ||
             (PropertyDetails::Empty().AsSmi() == Smi::cast(val)));
     }
@@ -1590,13 +1593,13 @@ void SwissNameDictionary::SwissNameDictionaryVerify(Isolate* isolate,
     ctrl_t ctrl = GetCtrl(i);
 
     if (IsFull(ctrl) || slow_checks) {
-      Object key = KeyAt(i);
-      Object value = ValueAtRaw(i);
+      Tagged<Object> key = KeyAt(i);
+      Tagged<Object> value = ValueAtRaw(i);
 
       if (IsFull(ctrl)) {
         ++seen_present;
 
-        Name name = Name::cast(key);
+        Tagged<Name> name = Name::cast(key);
         if (slow_checks) {
           CHECK_EQ(swiss_table::H2(name->hash()), ctrl);
         }
@@ -1650,18 +1653,20 @@ void JSRegExp::JSRegExpVerify(Isolate* isolate) {
   TorqueGeneratedClassVerifiers::JSRegExpVerify(*this, isolate);
   switch (type_tag()) {
     case JSRegExp::ATOM: {
-      FixedArray arr = FixedArray::cast(data());
+      Tagged<FixedArray> arr = FixedArray::cast(data());
       CHECK(IsString(arr->get(JSRegExp::kAtomPatternIndex)));
       break;
     }
     case JSRegExp::EXPERIMENTAL: {
-      FixedArray arr = FixedArray::cast(data());
-      Smi uninitialized = Smi::FromInt(JSRegExp::kUninitializedValue);
+      Tagged<FixedArray> arr = FixedArray::cast(data());
+      Tagged<Smi> uninitialized = Smi::FromInt(JSRegExp::kUninitializedValue);
 
-      Object latin1_code = arr->get(JSRegExp::kIrregexpLatin1CodeIndex);
-      Object uc16_code = arr->get(JSRegExp::kIrregexpUC16CodeIndex);
-      Object latin1_bytecode = arr->get(JSRegExp::kIrregexpLatin1BytecodeIndex);
-      Object uc16_bytecode = arr->get(JSRegExp::kIrregexpUC16BytecodeIndex);
+      Tagged<Object> latin1_code = arr->get(JSRegExp::kIrregexpLatin1CodeIndex);
+      Tagged<Object> uc16_code = arr->get(JSRegExp::kIrregexpUC16CodeIndex);
+      Tagged<Object> latin1_bytecode =
+          arr->get(JSRegExp::kIrregexpLatin1BytecodeIndex);
+      Tagged<Object> uc16_bytecode =
+          arr->get(JSRegExp::kIrregexpUC16BytecodeIndex);
 
       bool is_compiled = IsCode(latin1_code);
       if (is_compiled) {
@@ -1691,27 +1696,29 @@ void JSRegExp::JSRegExpVerify(Isolate* isolate) {
     case JSRegExp::IRREGEXP: {
       bool can_be_interpreted = RegExp::CanGenerateBytecode();
 
-      FixedArray arr = FixedArray::cast(data());
-      Object one_byte_data = arr->get(JSRegExp::kIrregexpLatin1CodeIndex);
+      Tagged<FixedArray> arr = FixedArray::cast(data());
+      Tagged<Object> one_byte_data =
+          arr->get(JSRegExp::kIrregexpLatin1CodeIndex);
       // Smi : Not compiled yet (-1).
       // InstructionStream: Compiled irregexp code or trampoline to the
       // interpreter.
       CHECK((IsSmi(one_byte_data) &&
              Smi::ToInt(one_byte_data) == JSRegExp::kUninitializedValue) ||
             IsCode(one_byte_data));
-      Object uc16_data = arr->get(JSRegExp::kIrregexpUC16CodeIndex);
+      Tagged<Object> uc16_data = arr->get(JSRegExp::kIrregexpUC16CodeIndex);
       CHECK((IsSmi(uc16_data) &&
              Smi::ToInt(uc16_data) == JSRegExp::kUninitializedValue) ||
             IsCode(uc16_data));
 
-      Object one_byte_bytecode =
+      Tagged<Object> one_byte_bytecode =
           arr->get(JSRegExp::kIrregexpLatin1BytecodeIndex);
       // Smi : Not compiled yet (-1).
       // ByteArray: Bytecode to interpret regexp.
       CHECK((IsSmi(one_byte_bytecode) &&
              Smi::ToInt(one_byte_bytecode) == JSRegExp::kUninitializedValue) ||
             (can_be_interpreted && IsByteArray(one_byte_bytecode)));
-      Object uc16_bytecode = arr->get(JSRegExp::kIrregexpUC16BytecodeIndex);
+      Tagged<Object> uc16_bytecode =
+          arr->get(JSRegExp::kIrregexpUC16BytecodeIndex);
       CHECK((IsSmi(uc16_bytecode) &&
              Smi::ToInt(uc16_bytecode) == JSRegExp::kUninitializedValue) ||
             (can_be_interpreted && IsByteArray(uc16_bytecode)));
@@ -1882,7 +1889,7 @@ void PrototypeInfo::PrototypeInfoVerify(Isolate* isolate) {
   }
 }
 
-void PrototypeUsers::Verify(WeakArrayList array) {
+void PrototypeUsers::Verify(Tagged<WeakArrayList> array) {
   if (array->length() == 0) {
     // Allow empty & uninitialized lists.
     return;
@@ -1901,7 +1908,7 @@ void PrototypeUsers::Verify(WeakArrayList array) {
   // slots.
   int weak_maps_count = 0;
   for (int i = kFirstIndex; i < array->length(); ++i) {
-    HeapObject heap_object;
+    Tagged<HeapObject> heap_object;
     MaybeObject object = array->Get(i);
     if ((object->GetHeapObjectIfWeak(&heap_object) && IsMap(heap_object)) ||
         object->IsCleared()) {
@@ -2038,7 +2045,7 @@ void Script::ScriptVerify(Isolate* isolate) {
 #endif  // V8_ENABLE_WEBASSEMBLY
   for (int i = 0; i < shared_function_info_count(); ++i) {
     MaybeObject maybe_object = shared_function_infos()->Get(i);
-    HeapObject heap_object;
+    Tagged<HeapObject> heap_object;
     CHECK(maybe_object->IsWeak() || maybe_object->IsCleared() ||
           (maybe_object->GetHeapObjectIfStrong(&heap_object) &&
            IsUndefined(heap_object, isolate)));
@@ -2050,7 +2057,7 @@ void NormalizedMapCache::NormalizedMapCacheVerify(Isolate* isolate) {
   if (v8_flags.enable_slow_asserts) {
     for (int i = 0; i < length(); i++) {
       MaybeObject e = WeakFixedArray::Get(i);
-      HeapObject heap_object;
+      Tagged<HeapObject> heap_object;
       if (e->GetHeapObjectIfWeak(&heap_object)) {
         Map::cast(heap_object)->DictionaryMapVerify(isolate);
       } else {
@@ -2067,7 +2074,7 @@ void PreparseData::PreparseDataVerify(Isolate* isolate) {
   CHECK_LE(0, children_length());
 
   for (int i = 0; i < children_length(); ++i) {
-    Object child = get_child_raw(i);
+    Tagged<Object> child = get_child_raw(i);
     CHECK(IsNull(child) || IsPreparseData(child));
     VerifyPointer(isolate, child);
   }
@@ -2115,10 +2122,10 @@ class StringTableVerifier : public RootVisitor {
                          OffHeapObjectSlot end) override {
     // Visit all HeapObject pointers in [start, end).
     for (OffHeapObjectSlot p = start; p < end; ++p) {
-      Object o = p.load(isolate_);
+      Tagged<Object> o = p.load(isolate_);
       DCHECK(!HasWeakHeapObjectTag(o));
       if (IsHeapObject(o)) {
-        HeapObject object = HeapObject::cast(o);
+        Tagged<HeapObject> object = HeapObject::cast(o);
         // Check that the string is actually internalized.
         CHECK(IsInternalizedString(object));
       }
@@ -2149,18 +2156,18 @@ void JSObject::IncrementSpillStatistics(Isolate* isolate,
     info->number_of_fast_used_fields_ += map()->NextFreePropertyIndex();
     info->number_of_fast_unused_fields_ += map()->UnusedPropertyFields();
   } else if (IsJSGlobalObject(*this)) {
-    GlobalDictionary dict =
+    Tagged<GlobalDictionary> dict =
         JSGlobalObject::cast(*this)->global_dictionary(kAcquireLoad);
     info->number_of_slow_used_properties_ += dict->NumberOfElements();
     info->number_of_slow_unused_properties_ +=
         dict->Capacity() - dict->NumberOfElements();
   } else if (V8_ENABLE_SWISS_NAME_DICTIONARY_BOOL) {
-    SwissNameDictionary dict = property_dictionary_swiss();
+    Tagged<SwissNameDictionary> dict = property_dictionary_swiss();
     info->number_of_slow_used_properties_ += dict->NumberOfElements();
     info->number_of_slow_unused_properties_ +=
         dict->Capacity() - dict->NumberOfElements();
   } else {
-    NameDictionary dict = property_dictionary();
+    Tagged<NameDictionary> dict = property_dictionary();
     info->number_of_slow_used_properties_ += dict->NumberOfElements();
     info->number_of_slow_unused_properties_ +=
         dict->Capacity() - dict->NumberOfElements();
@@ -2183,7 +2190,7 @@ void JSObject::IncrementSpillStatistics(Isolate* isolate,
     case SHARED_ARRAY_ELEMENTS: {
       info->number_of_objects_with_fast_elements_++;
       int holes = 0;
-      FixedArray e = FixedArray::cast(elements());
+      Tagged<FixedArray> e = FixedArray::cast(elements());
       int len = e->length();
       for (int i = 0; i < len; i++) {
         if (IsTheHole(e->get(i), isolate)) holes++;
@@ -2200,13 +2207,13 @@ void JSObject::IncrementSpillStatistics(Isolate* isolate,
 #undef TYPED_ARRAY_CASE
       {
         info->number_of_objects_with_fast_elements_++;
-        FixedArrayBase e = FixedArrayBase::cast(elements());
+        Tagged<FixedArrayBase> e = FixedArrayBase::cast(elements());
         info->number_of_fast_used_elements_ += e->length();
         break;
       }
     case DICTIONARY_ELEMENTS:
     case SLOW_STRING_WRAPPER_ELEMENTS: {
-      NumberDictionary dict = element_dictionary();
+      Tagged<NumberDictionary> dict = element_dictionary();
       info->number_of_slow_used_elements_ += dict->NumberOfElements();
       info->number_of_slow_unused_elements_ +=
           dict->Capacity() - dict->NumberOfElements();
@@ -2257,10 +2264,10 @@ void JSObject::SpillInformation::Print() {
 }
 
 bool DescriptorArray::IsSortedNoDuplicates() {
-  Name current_key;
+  Tagged<Name> current_key;
   uint32_t current = 0;
   for (int i = 0; i < number_of_descriptors(); i++) {
-    Name key = GetSortedKey(i);
+    Tagged<Name> key = GetSortedKey(i);
     uint32_t hash;
     const bool has_hash = key->TryGetHash(&hash);
     CHECK(has_hash);
@@ -2279,13 +2286,13 @@ bool DescriptorArray::IsSortedNoDuplicates() {
 }
 
 bool TransitionArray::IsSortedNoDuplicates() {
-  Name prev_key;
+  Tagged<Name> prev_key;
   PropertyKind prev_kind = PropertyKind::kData;
   PropertyAttributes prev_attributes = NONE;
   uint32_t prev_hash = 0;
 
   for (int i = 0; i < number_of_transitions(); i++) {
-    Name key = GetSortedKey(i);
+    Tagged<Name> key = GetSortedKey(i);
     uint32_t hash;
     const bool has_hash = key->TryGetHash(&hash);
     CHECK(has_hash);
@@ -2293,7 +2300,7 @@ bool TransitionArray::IsSortedNoDuplicates() {
     PropertyAttributes attributes = NONE;
     if (!TransitionsAccessor::IsSpecialTransition(key->GetReadOnlyRoots(),
                                                   key)) {
-      Map target = GetTarget(i);
+      Tagged<Map> target = GetTarget(i);
       PropertyDetails details =
           TransitionsAccessor::GetTargetDetails(key, target);
       kind = details.kind();
@@ -2323,14 +2330,15 @@ bool TransitionsAccessor::IsSortedNoDuplicates() {
   return transitions()->IsSortedNoDuplicates();
 }
 
-static bool CheckOneBackPointer(Map current_map, Object target) {
+static bool CheckOneBackPointer(Tagged<Map> current_map,
+                                Tagged<Object> target) {
   return !IsMap(target) || Map::cast(target)->GetBackPointer() == current_map;
 }
 
 bool TransitionsAccessor::IsConsistentWithBackPointers() {
   int num_transitions = NumberOfTransitions();
   for (int i = 0; i < num_transitions; i++) {
-    Map target = GetTarget(i);
+    Tagged<Map> target = GetTarget(i);
     if (!CheckOneBackPointer(map_, target)) return false;
   }
   return true;
diff --git a/src/diagnostics/objects-printer.cc b/src/diagnostics/objects-printer.cc
index f30b257aeca..0bf0872eef4 100644
--- a/src/diagnostics/objects-printer.cc
+++ b/src/diagnostics/objects-printer.cc
@@ -38,7 +38,7 @@ constexpr char kUnavailableString[] = "unavailable";
 
 #ifdef OBJECT_PRINT
 
-void Print(Object obj) {
+void Print(Tagged<Object> obj) {
   // Output into debugger's command window if a debugger is attached.
   DbgStdoutStream dbg_os;
   Print(obj, dbg_os);
@@ -49,7 +49,7 @@ void Print(Object obj) {
   os << std::flush;
 }
 
-void Print(Object obj, std::ostream& os) {
+void Print(Tagged<Object> obj, std::ostream& os) {
   if (IsSmi(obj)) {
     os << "Smi: " << std::hex << "0x" << Smi::ToInt(obj);
     os << std::dec << " (" << Smi::ToInt(obj) << ")\n";
@@ -60,8 +60,8 @@ void Print(Object obj, std::ostream& os) {
 
 namespace {
 
-void PrintHeapObjectHeaderWithoutMap(HeapObject object, std::ostream& os,
-                                     const char* id) {
+void PrintHeapObjectHeaderWithoutMap(Tagged<HeapObject> object,
+                                     std::ostream& os, const char* id) {
   PtrComprCageBase cage_base = GetPtrComprCageBase();
   os << reinterpret_cast<void*>(object.ptr()) << ": [";
   if (id != nullptr) {
@@ -92,7 +92,7 @@ void PrintDictionaryContents(std::ostream& os, Tagged<T> dict) {
   HandleScope scope(isolate);
 #endif
   for (InternalIndex i : dict->IterateEntries()) {
-    Object k;
+    Tagged<Object> k;
     if (!dict->ToKey(roots, i, &k)) continue;
     os << "\n   ";
     if (IsString(k)) {
@@ -315,7 +315,7 @@ void FreeSpace::FreeSpacePrint(std::ostream& os) {
 
 bool JSObject::PrintProperties(std::ostream& os) {
   if (HasFastProperties()) {
-    DescriptorArray descs = map()->instance_descriptors(GetIsolate());
+    Tagged<DescriptorArray> descs = map()->instance_descriptors(GetIsolate());
     int nof_inobject_properties = map()->GetInObjectProperties();
     for (InternalIndex i : map()->IterateOwnDescriptors()) {
       os << "\n    ";
@@ -361,27 +361,27 @@ bool JSObject::PrintProperties(std::ostream& os) {
 namespace {
 
 template <class T>
-bool IsTheHoleAt(T array, int index) {
+bool IsTheHoleAt(Tagged<T> array, int index) {
   return false;
 }
 
 template <>
-bool IsTheHoleAt(FixedDoubleArray array, int index) {
+bool IsTheHoleAt(Tagged<FixedDoubleArray> array, int index) {
   return array->is_the_hole(index);
 }
 
 template <class T>
-double GetScalarElement(T array, int index) {
+double GetScalarElement(Tagged<T> array, int index) {
   if (IsTheHoleAt(array, index)) {
     return std::numeric_limits<double>::quiet_NaN();
   }
-  return array.get_scalar(index);
+  return array->get_scalar(index);
 }
 
 template <class T>
-void DoPrintElements(std::ostream& os, Object object, int length) {
+void DoPrintElements(std::ostream& os, Tagged<Object> object, int length) {
   const bool print_the_hole = std::is_same<T, FixedDoubleArray>::value;
-  T array = T::cast(object);
+  Tagged<T> array = T::cast(object);
   if (length == 0) return;
   int previous_index = 0;
   double previous_value = GetScalarElement(array, 0);
@@ -442,16 +442,16 @@ void PrintTypedArrayElements(std::ostream& os, const ElementType* data_ptr,
 }
 
 template <typename T>
-void PrintFixedArrayElements(std::ostream& os, T array) {
+void PrintFixedArrayElements(std::ostream& os, Tagged<T> array) {
   // Print in array notation for non-sparse arrays.
-  if (array.length() == 0) return;
-  Object previous_value = array.get(0);
-  Object value;
+  if (array->length() == 0) return;
+  Tagged<Object> previous_value = array->get(0);
+  Tagged<Object> value;
   int previous_index = 0;
   int i;
-  for (i = 1; i <= array.length(); i++) {
-    if (i < array.length()) value = array.get(i);
-    if (previous_value == value && i != array.length()) {
+  for (i = 1; i <= array->length(); i++) {
+    if (i < array->length()) value = array->get(i);
+    if (previous_value == value && i != array->length()) {
       continue;
     }
     os << "\n";
@@ -466,7 +466,8 @@ void PrintFixedArrayElements(std::ostream& os, T array) {
   }
 }
 
-void PrintDictionaryElements(std::ostream& os, FixedArrayBase elements) {
+void PrintDictionaryElements(std::ostream& os,
+                             Tagged<FixedArrayBase> elements) {
   // Print some internal fields
   Tagged<NumberDictionary> dict = NumberDictionary::cast(elements);
   if (dict->requires_slow_elements()) {
@@ -478,13 +479,13 @@ void PrintDictionaryElements(std::ostream& os, FixedArrayBase elements) {
 }
 
 void PrintSloppyArgumentElements(std::ostream& os, ElementsKind kind,
-                                 SloppyArgumentsElements elements) {
-  FixedArray arguments_store = elements->arguments();
+                                 Tagged<SloppyArgumentsElements> elements) {
+  Tagged<FixedArray> arguments_store = elements->arguments();
   os << "\n    0: context: " << Brief(elements->context())
      << "\n    1: arguments_store: " << Brief(arguments_store)
      << "\n    parameter to context slot map:";
   for (int i = 0; i < elements->length(); i++) {
-    Object mapped_entry = elements->mapped_entries(i, kRelaxedLoad);
+    Tagged<Object> mapped_entry = elements->mapped_entries(i, kRelaxedLoad);
     os << "\n    " << i << ": param(" << i << "): " << Brief(mapped_entry);
     if (IsTheHole(mapped_entry)) {
       os << " in the arguments_store[" << i << "]";
@@ -507,7 +508,7 @@ void PrintSloppyArgumentElements(std::ostream& os, ElementsKind kind,
 void PrintEmbedderData(Isolate* isolate, std::ostream& os,
                        EmbedderDataSlot slot) {
   DisallowGarbageCollection no_gc;
-  Object value = slot.load_tagged();
+  Tagged<Object> value = slot.load_tagged();
   os << Brief(value);
   void* raw_pointer;
   if (slot.ToAlignedPointer(isolate, &raw_pointer)) {
@@ -574,7 +575,7 @@ void JSObject::PrintElements(std::ostream& os) {
   os << "\n }\n";
 }
 
-static void JSObjectPrintHeader(std::ostream& os, JSObject obj,
+static void JSObjectPrintHeader(std::ostream& os, Tagged<JSObject> obj,
                                 const char* id) {
   Isolate* isolate = obj->GetIsolate();
   obj->PrintHeader(os, id);
@@ -592,7 +593,7 @@ static void JSObjectPrintHeader(std::ostream& os, JSObject obj,
      << ElementsKindToString(obj->map()->elements_kind());
   if (obj->elements()->IsCowArray()) os << " (COW)";
   os << "]";
-  Object hash = Object::GetHash(obj);
+  Tagged<Object> hash = Object::GetHash(obj);
   if (IsSmi(hash)) {
     os << "\n - hash: " << Brief(hash);
   }
@@ -601,10 +602,10 @@ static void JSObjectPrintHeader(std::ostream& os, JSObject obj,
   }
 }
 
-static void JSObjectPrintBody(std::ostream& os, JSObject obj,
+static void JSObjectPrintBody(std::ostream& os, Tagged<JSObject> obj,
                               bool print_elements = true) {
   os << "\n - properties: ";
-  Object properties_or_hash = obj->raw_properties_or_hash(kRelaxedLoad);
+  Tagged<Object> properties_or_hash = obj->raw_properties_or_hash(kRelaxedLoad);
   if (!IsSmi(properties_or_hash)) {
     os << Brief(properties_or_hash);
   }
@@ -670,12 +671,12 @@ void JSGeneratorObject::JSGeneratorObjectPrint(std::ostream& os) {
   if (is_suspended()) os << " (suspended)";
   if (is_suspended()) {
     DisallowGarbageCollection no_gc;
-    SharedFunctionInfo fun_info = function()->shared();
+    Tagged<SharedFunctionInfo> fun_info = function()->shared();
     if (fun_info->HasSourceCode()) {
-      Script script = Script::cast(fun_info->script());
-      String script_name = IsString(script->name())
-                               ? Tagged<String>::cast(script->name())
-                               : GetReadOnlyRoots().empty_string();
+      Tagged<Script> script = Script::cast(fun_info->script());
+      Tagged<String> script_name = IsString(script->name())
+                                       ? Tagged<String>::cast(script->name())
+                                       : GetReadOnlyRoots().empty_string();
 
       os << "\n - source position: ";
       // Can't collect source positions here if not available as that would
@@ -773,7 +774,7 @@ void DescriptorArray::DescriptorArrayPrint(std::ostream& os) {
 }
 
 namespace {
-void PrintFixedArrayWithHeader(std::ostream& os, FixedArray array,
+void PrintFixedArrayWithHeader(std::ostream& os, Tagged<FixedArray> array,
                                const char* type) {
   array->PrintHeader(os, type);
   os << "\n - length: " << array->length();
@@ -872,7 +873,7 @@ void AccessorInfo::AccessorInfoPrint(std::ostream& os) {
 }
 
 namespace {
-void PrintContextWithHeader(std::ostream& os, Context context,
+void PrintContextWithHeader(std::ostream& os, Tagged<Context> context,
                             const char* type) {
   context->PrintHeader(os, type);
   os << "\n - type: " << context->map()->instance_type();
@@ -917,7 +918,7 @@ void PrintTableContentsGeneric(std::ostream& os, T dict,
   ReadOnlyRoots roots = dict.GetReadOnlyRoots();
 
   for (InternalIndex i : dict.IterateEntries()) {
-    Object k;
+    Tagged<Object> k;
     if (!dict.ToKey(roots, i, &k)) continue;
     os << "\n   " << std::setw(12) << i.as_int() << ": ";
     if (IsString(k)) {
@@ -932,7 +933,7 @@ void PrintTableContentsGeneric(std::ostream& os, T dict,
   }
 }
 
-void PrintNameDictionaryFlags(std::ostream& os, NameDictionary dict) {
+void PrintNameDictionaryFlags(std::ostream& os, Tagged<NameDictionary> dict) {
   if (dict->may_have_interesting_properties()) {
     os << "\n - may_have_interesting_properties";
   }
@@ -983,7 +984,7 @@ void PrintOrderedHashTableHeaderAndBuckets(std::ostream& os, T table,
 
   os << "\n - buckets: {";
   for (int bucket = 0; bucket < table.NumberOfBuckets(); bucket++) {
-    Object entry = table.get(T::HashTableStartIndex() + bucket);
+    Tagged<Object> entry = table.get(T::HashTableStartIndex() + bucket);
     DCHECK(IsSmi(entry));
     os << "\n   " << std::setw(12) << bucket << ": " << Brief(entry);
   }
@@ -1117,10 +1118,10 @@ void SwissNameDictionary::SwissNameDictionaryPrint(std::ostream& os) {
 
   os << "\n - data table (omitting slots where key is the hole): {";
   for (int bucket = 0; bucket < this->Capacity(); ++bucket) {
-    Object k;
+    Tagged<Object> k;
     if (!this->ToKey(this->GetReadOnlyRoots(), bucket, &k)) continue;
 
-    Object value = this->ValueAtRaw(bucket);
+    Tagged<Object> value = this->ValueAtRaw(bucket);
     PropertyDetails details = this->DetailsAt(bucket);
     os << "\n   " << std::setw(12) << std::dec << bucket << ": ";
     if (IsString(k)) {
@@ -1139,7 +1140,7 @@ void PropertyArray::PropertyArrayPrint(std::ostream& os) {
   PrintHeader(os, "PropertyArray");
   os << "\n - length: " << length();
   os << "\n - hash: " << Hash();
-  PrintFixedArrayElements(os, *this);
+  PrintFixedArrayElements(os, Tagged(*this));
   os << "\n";
 }
 
@@ -1325,14 +1326,14 @@ void FeedbackNexus::Print(std::ostream& os) {
       os << InlineCacheState2String(ic_state());
       if (ic_state() == InlineCacheState::MONOMORPHIC) {
         os << "\n   " << Brief(GetFeedback()) << ": ";
-        Object handler = GetFeedbackExtra().GetHeapObjectOrSmi();
+        Tagged<Object> handler = GetFeedbackExtra().GetHeapObjectOrSmi();
         if (IsWeakFixedArray(handler)) {
           handler = WeakFixedArray::cast(handler)->Get(0).GetHeapObjectOrSmi();
         }
         LoadHandler::PrintHandler(handler, os);
       } else if (ic_state() == InlineCacheState::POLYMORPHIC) {
-        HeapObject feedback = GetFeedback().GetHeapObject();
-        WeakFixedArray array;
+        Tagged<HeapObject> feedback = GetFeedback().GetHeapObject();
+        Tagged<WeakFixedArray> array;
         if (IsName(feedback)) {
           os << " with name " << Brief(feedback);
           array = WeakFixedArray::cast(GetFeedbackExtra().GetHeapObject());
@@ -1352,13 +1353,13 @@ void FeedbackNexus::Print(std::ostream& os) {
     case FeedbackSlotKind::kSetKeyedStrict: {
       os << InlineCacheState2String(ic_state());
       if (ic_state() == InlineCacheState::MONOMORPHIC) {
-        HeapObject feedback = GetFeedback().GetHeapObject();
+        Tagged<HeapObject> feedback = GetFeedback().GetHeapObject();
         if (IsName(feedback)) {
           os << " with name " << Brief(feedback);
-          WeakFixedArray array =
+          Tagged<WeakFixedArray> array =
               WeakFixedArray::cast(GetFeedbackExtra().GetHeapObject());
           os << "\n   " << Brief(array->Get(0)) << ": ";
-          Object handler = array->Get(1).GetHeapObjectOrSmi();
+          Tagged<Object> handler = array->Get(1).GetHeapObjectOrSmi();
           StoreHandler::PrintHandler(handler, os);
         } else {
           os << "\n   " << Brief(feedback) << ": ";
@@ -1366,8 +1367,8 @@ void FeedbackNexus::Print(std::ostream& os) {
                                      os);
         }
       } else if (ic_state() == InlineCacheState::POLYMORPHIC) {
-        HeapObject feedback = GetFeedback().GetHeapObject();
-        WeakFixedArray array;
+        Tagged<HeapObject> feedback = GetFeedback().GetHeapObject();
+        Tagged<WeakFixedArray> array;
         if (IsName(feedback)) {
           os << " with name " << Brief(feedback);
           array = WeakFixedArray::cast(GetFeedbackExtra().GetHeapObject());
@@ -1409,7 +1410,7 @@ void FeedbackNexus::Print(std::ostream& os) {
 void Oddball::OddballPrint(std::ostream& os) {
   PrintHeapObjectHeaderWithoutMap(*this, os, "Oddball");
   os << ": ";
-  String s = to_string();
+  Tagged<String> s = to_string();
   os << s->PrefixForDebugPrint();
   s->PrintUC16(os);
   os << s->SuffixForDebugPrint();
@@ -1570,13 +1571,13 @@ void JSFinalizationRegistry::JSFinalizationRegistryPrint(std::ostream& os) {
   os << "\n - native_context: " << Brief(native_context());
   os << "\n - cleanup: " << Brief(cleanup());
   os << "\n - active_cells: " << Brief(active_cells());
-  Object active_cell = active_cells();
+  Tagged<Object> active_cell = active_cells();
   while (IsWeakCell(active_cell)) {
     os << "\n   - " << Brief(active_cell);
     active_cell = WeakCell::cast(active_cell)->next();
   }
   os << "\n - cleared_cells: " << Brief(cleared_cells());
-  Object cleared_cell = cleared_cells();
+  Tagged<Object> cleared_cell = cleared_cells();
   while (IsWeakCell(cleared_cell)) {
     os << "\n   - " << Brief(cleared_cell);
     cleared_cell = WeakCell::cast(cleared_cell)->next();
@@ -1795,12 +1796,12 @@ void JSFunction::JSFunctionPrint(std::ostream& os) {
   }
 #if V8_ENABLE_WEBASSEMBLY
   if (WasmExportedFunction::IsWasmExportedFunction(*this)) {
-    WasmExportedFunction function = WasmExportedFunction::cast(*this);
+    Tagged<WasmExportedFunction> function = WasmExportedFunction::cast(*this);
     os << "\n - Wasm instance: " << Brief(function->instance());
     os << "\n - Wasm function index: " << function->function_index();
   }
   if (WasmJSFunction::IsWasmJSFunction(*this)) {
-    WasmJSFunction function = WasmJSFunction::cast(*this);
+    Tagged<WasmJSFunction> function = WasmJSFunction::cast(*this);
     os << "\n - Wasm wrapper around: " << Brief(function->GetCallable());
   }
 #endif  // V8_ENABLE_WEBASSEMBLY
@@ -1822,7 +1823,7 @@ void JSFunction::JSFunctionPrint(std::ostream& os) {
 void SharedFunctionInfo::PrintSourceCode(std::ostream& os) {
   if (HasSourceCode()) {
     os << "\n - source code: ";
-    String source = String::cast(Script::cast(script())->source());
+    Tagged<String> source = String::cast(Script::cast(script())->source());
     int start = StartPosition();
     int length = EndPosition() - start;
     std::unique_ptr<char[]> source_string = source->ToCString(
@@ -1957,7 +1958,7 @@ void Code::CodePrint(std::ostream& os, const char* name, Address current_pc) {
 
   // Then, InstructionStream:
   if (has_instruction_stream()) {
-    InstructionStream istream = instruction_stream();
+    Tagged<InstructionStream> istream = instruction_stream();
     os << "\n - instruction_stream.relocation_info: "
        << Brief(istream->relocation_info());
     os << "\n - instruction_stream.body_size: " << istream->body_size();
@@ -1997,7 +1998,7 @@ void AsyncGeneratorRequest::AsyncGeneratorRequestPrint(std::ostream& os) {
   os << "\n";
 }
 
-static void PrintModuleFields(Module module, std::ostream& os) {
+static void PrintModuleFields(Tagged<Module> module, std::ostream& os) {
   os << "\n - exports: " << Brief(module->exports());
   os << "\n - status: " << module->status();
   os << "\n - exception: " << Brief(module->exception());
@@ -2017,7 +2018,7 @@ void SourceTextModule::SourceTextModulePrint(std::ostream& os) {
   PrintHeader(os, "SourceTextModule");
   PrintModuleFields(*this, os);
   os << "\n - sfi/code/info: " << Brief(code());
-  Script script = GetScript();
+  Tagged<Script> script = GetScript();
   os << "\n - script: " << Brief(script);
   os << "\n - origin: " << Brief(script->GetNameOrSourceURL());
   os << "\n - requested_modules: " << Brief(requested_modules());
@@ -2691,13 +2692,13 @@ void JSSegments::JSSegmentsPrint(std::ostream& os) {
 #endif  // V8_INTL_SUPPORT
 
 namespace {
-void PrintScopeInfoList(ScopeInfo scope_info, std::ostream& os,
+void PrintScopeInfoList(Tagged<ScopeInfo> scope_info, std::ostream& os,
                         const char* list_name, int length) {
   DisallowGarbageCollection no_gc;
   if (length <= 0) return;
   os << "\n - " << list_name;
   os << " {\n";
-  for (auto it : ScopeInfo::IterateLocalNames(&scope_info, no_gc)) {
+  for (auto it : ScopeInfo::IterateLocalNames(scope_info, no_gc)) {
     os << "    - " << it->index() << ": " << it->name() << "\n";
   }
   os << "  }";
@@ -2812,7 +2813,7 @@ void HeapObject::HeapObjectShortPrint(std::ostream& os) {
   switch (map(cage_base)->instance_type()) {
     case MAP_TYPE: {
       os << "<Map";
-      Map mapInstance = Map::cast(*this);
+      Tagged<Map> mapInstance = Map::cast(*this);
       if (mapInstance->instance_size() != kVariableSizeSentinel) {
         os << "[" << mapInstance->instance_size() << "]";
       }
@@ -2954,14 +2955,14 @@ void HeapObject::HeapObjectShortPrint(std::ostream& os) {
       break;
 
     case PREPARSE_DATA_TYPE: {
-      PreparseData data = PreparseData::cast(*this);
+      Tagged<PreparseData> data = PreparseData::cast(*this);
       os << "<PreparseData[data=" << data->data_length()
          << " children=" << data->children_length() << "]>";
       break;
     }
 
     case UNCOMPILED_DATA_WITHOUT_PREPARSE_DATA_TYPE: {
-      UncompiledDataWithoutPreparseData data =
+      Tagged<UncompiledDataWithoutPreparseData> data =
           UncompiledDataWithoutPreparseData::cast(*this);
       os << "<UncompiledDataWithoutPreparseData (" << data->start_position()
          << ", " << data->end_position() << ")]>";
@@ -2969,7 +2970,7 @@ void HeapObject::HeapObjectShortPrint(std::ostream& os) {
     }
 
     case UNCOMPILED_DATA_WITH_PREPARSE_DATA_TYPE: {
-      UncompiledDataWithPreparseData data =
+      Tagged<UncompiledDataWithPreparseData> data =
           UncompiledDataWithPreparseData::cast(*this);
       os << "<UncompiledDataWithPreparseData (" << data->start_position()
          << ", " << data->end_position()
@@ -2978,7 +2979,7 @@ void HeapObject::HeapObjectShortPrint(std::ostream& os) {
     }
 
     case SHARED_FUNCTION_INFO_TYPE: {
-      SharedFunctionInfo shared = SharedFunctionInfo::cast(*this);
+      Tagged<SharedFunctionInfo> shared = SharedFunctionInfo::cast(*this);
       std::unique_ptr<char[]> debug_name = shared->DebugNameCStr();
       if (debug_name[0] != '\0') {
         os << "<SharedFunctionInfo " << debug_name.get() << ">";
@@ -3005,14 +3006,14 @@ void HeapObject::HeapObjectShortPrint(std::ostream& os) {
       break;
     }
     case SCOPE_INFO_TYPE: {
-      ScopeInfo scope = ScopeInfo::cast(*this);
+      Tagged<ScopeInfo> scope = ScopeInfo::cast(*this);
       os << "<ScopeInfo";
       if (!scope->IsEmpty()) os << " " << scope->scope_type();
       os << ">";
       break;
     }
     case CODE_TYPE: {
-      Code code = Code::cast(*this);
+      Tagged<Code> code = Code::cast(*this);
       os << "<Code " << CodeKindToString(code->kind());
       if (code->is_builtin()) {
         os << " " << Builtins::name(code->builtin_id());
@@ -3031,8 +3032,8 @@ void HeapObject::HeapObjectShortPrint(std::ostream& os) {
       UNREACHABLE();
     }
     case INSTRUCTION_STREAM_TYPE: {
-      InstructionStream istream = InstructionStream::cast(*this);
-      Code code = istream->code(kAcquireLoad);
+      Tagged<InstructionStream> istream = InstructionStream::cast(*this);
+      Tagged<Code> code = istream->code(kAcquireLoad);
       os << "<InstructionStream " << CodeKindToString(code->kind());
       if (code->is_builtin()) {
         os << " " << Builtins::name(code->builtin_id());
@@ -3057,7 +3058,7 @@ void HeapObject::HeapObjectShortPrint(std::ostream& os) {
       break;
     }
     case SYMBOL_TYPE: {
-      Symbol symbol = Symbol::cast(*this);
+      Tagged<Symbol> symbol = Symbol::cast(*this);
       symbol->SymbolShortPrint(os);
       break;
     }
@@ -3089,7 +3090,7 @@ void HeapObject::HeapObjectShortPrint(std::ostream& os) {
       break;
     }
     case PROPERTY_CELL_TYPE: {
-      PropertyCell cell = PropertyCell::cast(*this);
+      Tagged<PropertyCell> cell = PropertyCell::cast(*this);
       os << "<PropertyCell name=";
       ShortPrint(cell->name(), os);
       os << " value=";
@@ -3101,7 +3102,7 @@ void HeapObject::HeapObjectShortPrint(std::ostream& os) {
       break;
     }
     case ACCESSOR_INFO_TYPE: {
-      AccessorInfo info = AccessorInfo::cast(*this);
+      Tagged<AccessorInfo> info = AccessorInfo::cast(*this);
       os << "<AccessorInfo ";
       os << "name= " << Brief(info->name());
       os << ", data= " << Brief(info->data());
@@ -3109,7 +3110,7 @@ void HeapObject::HeapObjectShortPrint(std::ostream& os) {
       break;
     }
     case CALL_HANDLER_INFO_TYPE: {
-      CallHandlerInfo info = CallHandlerInfo::cast(*this);
+      Tagged<CallHandlerInfo> info = CallHandlerInfo::cast(*this);
       os << "<CallHandlerInfo ";
       Isolate* isolate;
       if (GetIsolateFromHeapObject(*this, &isolate)) {
@@ -3160,7 +3161,7 @@ void Name::NameShortPrint() {
     PrintF("%s", String::cast(*this)->ToCString().get());
   } else {
     DCHECK(IsSymbol(*this));
-    Symbol s = Symbol::cast(*this);
+    Tagged<Symbol> s = Symbol::cast(*this);
     if (IsUndefined(s->description())) {
       PrintF("#<%s>", s->PrivateSymbolToName());
     } else {
@@ -3175,7 +3176,7 @@ int Name::NameShortPrint(base::Vector<char> str) {
     return SNPrintF(str, "%s", String::cast(*this)->ToCString().get());
   } else {
     DCHECK(IsSymbol(*this));
-    Symbol s = Symbol::cast(*this);
+    Tagged<Symbol> s = Symbol::cast(*this);
     if (IsUndefined(s->description())) {
       return SNPrintF(str, "#<%s>", s->PrivateSymbolToName());
     } else {
@@ -3189,7 +3190,7 @@ void Symbol::SymbolShortPrint(std::ostream& os) {
   os << "<Symbol:";
   if (!IsUndefined(description())) {
     os << " ";
-    String description_as_string = String::cast(description());
+    Tagged<String> description_as_string = String::cast(description());
     description_as_string->PrintUC16(os, 0, description_as_string->length());
   } else {
     os << " (" << PrivateSymbolToName() << ")";
@@ -3266,8 +3267,8 @@ void Map::MapPrint(std::ostream& os) {
     int nof_transitions = transitions.NumberOfTransitions();
     if (nof_transitions > 0) {
       os << "\n - transitions #" << nof_transitions << ": ";
-      HeapObject heap_object;
-      Smi smi;
+      Tagged<HeapObject> heap_object;
+      Tagged<Smi> smi;
       if (raw_transitions()->ToSmi(&smi)) {
         os << Brief(smi);
       } else if (raw_transitions()->GetHeapObject(&heap_object)) {
@@ -3292,7 +3293,7 @@ void Map::MapPrint(std::ostream& os) {
 
 void DescriptorArray::PrintDescriptors(std::ostream& os) {
   for (InternalIndex i : InternalIndex::Range(number_of_descriptors())) {
-    Name key = GetKey(i);
+    Tagged<Name> key = GetKey(i);
     os << "\n  [" << i.as_int() << "]: ";
 #ifdef OBJECT_PRINT
     key->NamePrint(os);
@@ -3313,15 +3314,15 @@ void DescriptorArray::PrintDescriptorDetails(std::ostream& os,
   os << " @ ";
   switch (details.location()) {
     case PropertyLocation::kField: {
-      FieldType field_type = GetFieldType(descriptor);
-      field_type.PrintTo(os);
+      Tagged<FieldType> field_type = GetFieldType(descriptor);
+      field_type->PrintTo(os);
       break;
     }
     case PropertyLocation::kDescriptor:
-      Object value = GetStrongValue(descriptor);
+      Tagged<Object> value = GetStrongValue(descriptor);
       os << Brief(value);
       if (IsAccessorPair(value)) {
-        AccessorPair pair = AccessorPair::cast(value);
+        Tagged<AccessorPair> pair = AccessorPair::cast(value);
         os << "(get: " << Brief(pair->getter())
            << ", set: " << Brief(pair->setter()) << ")";
       }
@@ -3345,8 +3346,8 @@ char* String::ToAsciiArray() {
 }
 
 // static
-void TransitionsAccessor::PrintOneTransition(std::ostream& os, Name key,
-                                             Map target) {
+void TransitionsAccessor::PrintOneTransition(std::ostream& os, Tagged<Name> key,
+                                             Tagged<Map> target) {
   os << "\n     ";
 #ifdef OBJECT_PRINT
   key->NamePrint(os);
@@ -3370,7 +3371,7 @@ void TransitionsAccessor::PrintOneTransition(std::ostream& os, Name key,
     DCHECK(!IsSpecialTransition(roots, key));
     os << "(transition to ";
     InternalIndex descriptor = target->LastAdded();
-    DescriptorArray descriptors = target->instance_descriptors();
+    Tagged<DescriptorArray> descriptors = target->instance_descriptors();
     descriptors->PrintDescriptorDetails(os, descriptor,
                                         PropertyDetails::kForTransitions);
     os << ")";
@@ -3382,8 +3383,8 @@ void TransitionArray::PrintInternal(std::ostream& os) {
   int num_transitions = number_of_transitions();
   os << "Transition array #" << num_transitions << ":";
   for (int i = 0; i < num_transitions; i++) {
-    Name key = GetKey(i);
-    Map target = GetTarget(i);
+    Tagged<Name> key = GetKey(i);
+    Tagged<Map> target = GetTarget(i);
     TransitionsAccessor::PrintOneTransition(os, key, target);
   }
   os << "\n" << std::flush;
@@ -3396,8 +3397,9 @@ void TransitionsAccessor::PrintTransitions(std::ostream& os) {
     case kMigrationTarget:
       return;
     case kWeakRef: {
-      Map target = Map::cast(raw_transitions_->GetHeapObjectAssumeWeak());
-      Name key = GetSimpleTransitionKey(target);
+      Tagged<Map> target =
+          Map::cast(raw_transitions_->GetHeapObjectAssumeWeak());
+      Tagged<Name> key = GetSimpleTransitionKey(target);
       PrintOneTransition(os, key, target);
       break;
     }
@@ -3420,8 +3422,8 @@ void TransitionsAccessor::PrintTransitionTree(
   int num_transitions = NumberOfTransitions();
   if (num_transitions == 0) return;
   for (int i = 0; i < num_transitions; i++) {
-    Name key = GetKey(i);
-    Map target = GetTarget(i);
+    Tagged<Name> key = GetKey(i);
+    Tagged<Map> target = GetTarget(i);
     os << std::endl
        << "  " << level << "/" << i << ":" << std::setw(level * 2 + 2) << " ";
     std::stringstream ss;
@@ -3448,7 +3450,8 @@ void TransitionsAccessor::PrintTransitionTree(
       DCHECK(!IsSpecialTransition(ReadOnlyRoots(isolate_), key));
       os << "to ";
       InternalIndex descriptor = target->LastAdded();
-      DescriptorArray descriptors = target->instance_descriptors(isolate_);
+      Tagged<DescriptorArray> descriptors =
+          target->instance_descriptors(isolate_);
       descriptors->PrintDescriptorDetails(os, descriptor,
                                           PropertyDetails::kForTransitions);
     }
@@ -3470,7 +3473,7 @@ void JSObject::PrintTransitions(std::ostream& os) {
 
 namespace {
 
-inline i::Object GetObjectFromRaw(void* object) {
+inline i::Tagged<i::Object> GetObjectFromRaw(void* object) {
   i::Address object_ptr = reinterpret_cast<i::Address>(object);
 #ifdef V8_COMPRESS_POINTERS
   if (RoundDown<i::kPtrComprCageBaseAlignment>(object_ptr) == i::kNullAddress) {
@@ -3489,7 +3492,8 @@ inline i::Object GetObjectFromRaw(void* object) {
 // The following functions are used by our gdb macros.
 //
 V8_DONT_STRIP_SYMBOL
-V8_EXPORT_PRIVATE extern i::Object _v8_internal_Get_Object(void* object) {
+V8_EXPORT_PRIVATE extern i::Tagged<i::Object> _v8_internal_Get_Object(
+    void* object) {
   return GetObjectFromRaw(object);
 }
 
@@ -3592,12 +3596,12 @@ V8_EXPORT_PRIVATE extern void _v8_internal_Print_StackTrace() {
 
 V8_DONT_STRIP_SYMBOL
 V8_EXPORT_PRIVATE extern void _v8_internal_Print_TransitionTree(void* object) {
-  i::Object o(GetObjectFromRaw(object));
+  i::Tagged<i::Object> o(GetObjectFromRaw(object));
   if (!IsMap(o)) {
     printf("Please provide a valid Map\n");
   } else {
 #if defined(DEBUG) || defined(OBJECT_PRINT)
-    i::Map map = i::Map::unchecked_cast(o);
+    i::Tagged<i::Map> map = i::Map::unchecked_cast(o);
     i::TransitionsAccessor transitions(i::Isolate::Current(), map);
     transitions.PrintTransitionTree();
 #endif
diff --git a/src/diagnostics/perf-jit.cc b/src/diagnostics/perf-jit.cc
index 512dfe165fd..59a774b270c 100644
--- a/src/diagnostics/perf-jit.cc
+++ b/src/diagnostics/perf-jit.cc
@@ -220,8 +220,8 @@ uint64_t LinuxPerfJitLogger::GetTimestamp() {
 }
 
 void LinuxPerfJitLogger::LogRecordedBuffer(
-    AbstractCode abstract_code, MaybeHandle<SharedFunctionInfo> maybe_sfi,
-    const char* name, int length) {
+    Tagged<AbstractCode> abstract_code,
+    MaybeHandle<SharedFunctionInfo> maybe_sfi, const char* name, int length) {
   DisallowGarbageCollection no_gc;
   if (v8_flags.perf_basic_prof_only_functions) {
     CodeKind code_kind = abstract_code->kind(isolate_);
@@ -238,7 +238,7 @@ void LinuxPerfJitLogger::LogRecordedBuffer(
 
   // We only support non-interpreted functions.
   if (!IsCode(abstract_code, isolate_)) return;
-  Code code = Code::cast(abstract_code);
+  Tagged<Code> code = Code::cast(abstract_code);
 
   // Debug info has to be emitted first.
   Handle<SharedFunctionInfo> sfi;
@@ -307,13 +307,14 @@ constexpr size_t kUnknownScriptNameStringLen =
     arraysize(kUnknownScriptNameString) - 1;
 
 namespace {
-base::Vector<const char> GetScriptName(Object maybeScript,
+base::Vector<const char> GetScriptName(Tagged<Object> maybeScript,
                                        std::unique_ptr<char[]>* storage,
                                        const DisallowGarbageCollection& no_gc) {
   if (IsScript(maybeScript)) {
-    Object name_or_url = Script::cast(maybeScript)->GetNameOrSourceURL();
+    Tagged<Object> name_or_url =
+        Script::cast(maybeScript)->GetNameOrSourceURL();
     if (IsSeqOneByteString(name_or_url)) {
-      SeqOneByteString str = SeqOneByteString::cast(name_or_url);
+      Tagged<SeqOneByteString> str = SeqOneByteString::cast(name_or_url);
       return {reinterpret_cast<char*>(str->GetChars(no_gc)),
               static_cast<size_t>(str->length())};
     } else if (IsString(name_or_url)) {
@@ -329,7 +330,7 @@ base::Vector<const char> GetScriptName(Object maybeScript,
 
 }  // namespace
 
-SourcePositionInfo GetSourcePositionInfo(Isolate* isolate, Code code,
+SourcePositionInfo GetSourcePositionInfo(Isolate* isolate, Tagged<Code> code,
                                          Handle<SharedFunctionInfo> function,
                                          SourcePosition pos) {
   DisallowGarbageCollection disallow;
@@ -342,31 +343,31 @@ SourcePositionInfo GetSourcePositionInfo(Isolate* isolate, Code code,
 
 }  // namespace
 
-void LinuxPerfJitLogger::LogWriteDebugInfo(Code code,
+void LinuxPerfJitLogger::LogWriteDebugInfo(Tagged<Code> code,
                                            Handle<SharedFunctionInfo> shared) {
   // Line ends of all scripts have been initialized prior to this.
   DisallowGarbageCollection no_gc;
   // The WasmToJS wrapper stubs have source position entries.
-  SharedFunctionInfo raw_shared = *shared;
+  Tagged<SharedFunctionInfo> raw_shared = *shared;
   if (!raw_shared->HasSourceCode()) return;
 
   PerfJitCodeDebugInfo debug_info;
   uint32_t size = sizeof(debug_info);
 
-  ByteArray source_position_table =
+  Tagged<ByteArray> source_position_table =
       code->SourcePositionTable(isolate_, raw_shared);
   // Compute the entry count and get the names of all scripts.
   // Avoid additional work if the script name is repeated. Multiple script
   // names only occur for cross-script inlining.
   uint32_t entry_count = 0;
-  Object last_script = Smi::zero();
+  Tagged<Object> last_script = Smi::zero();
   size_t last_script_name_size = 0;
   std::vector<base::Vector<const char>> script_names;
   for (SourcePositionTableIterator iterator(source_position_table);
        !iterator.done(); iterator.Advance()) {
     SourcePositionInfo info(GetSourcePositionInfo(isolate_, code, shared,
                                                   iterator.source_position()));
-    Object current_script = *info.script;
+    Tagged<Object> current_script = *info.script;
     if (current_script != last_script) {
       std::unique_ptr<char[]> name_storage;
       auto name = GetScriptName(raw_shared->script(), &name_storage, no_gc);
@@ -411,7 +412,7 @@ void LinuxPerfJitLogger::LogWriteDebugInfo(Code code,
     entry.line_number_ = info.line + 1;
     entry.column_ = info.column + 1;
     LogWriteBytes(reinterpret_cast<const char*>(&entry), sizeof(entry));
-    Object current_script = *info.script;
+    Tagged<Object> current_script = *info.script;
     auto name_string = script_names[script_names_index];
     LogWriteBytes(name_string.begin(),
                   static_cast<uint32_t>(name_string.size()));
@@ -494,7 +495,7 @@ void LinuxPerfJitLogger::LogWriteDebugInfo(const wasm::WasmCode* code) {
 }
 #endif  // V8_ENABLE_WEBASSEMBLY
 
-void LinuxPerfJitLogger::LogWriteUnwindingInfo(Code code) {
+void LinuxPerfJitLogger::LogWriteUnwindingInfo(Tagged<Code> code) {
   PerfJitCodeUnwindingInfo unwinding_info_header;
   unwinding_info_header.event_ = PerfJitCodeLoad::kUnwindingInfo;
   unwinding_info_header.time_stamp_ = GetTimestamp();
diff --git a/src/diagnostics/perf-jit.h b/src/diagnostics/perf-jit.h
index 64161d07223..294c0cd32da 100644
--- a/src/diagnostics/perf-jit.h
+++ b/src/diagnostics/perf-jit.h
@@ -44,10 +44,12 @@ class LinuxPerfJitLogger : public CodeEventLogger {
   explicit LinuxPerfJitLogger(Isolate* isolate);
   ~LinuxPerfJitLogger() override;
 
-  void CodeMoveEvent(InstructionStream from, InstructionStream to) override {
+  void CodeMoveEvent(Tagged<InstructionStream> from,
+                     Tagged<InstructionStream> to) override {
     UNREACHABLE();  // Unsupported.
   }
-  void BytecodeMoveEvent(BytecodeArray from, BytecodeArray to) override {}
+  void BytecodeMoveEvent(Tagged<BytecodeArray> from,
+                         Tagged<BytecodeArray> to) override {}
   void CodeDisableOptEvent(Handle<AbstractCode> code,
                            Handle<SharedFunctionInfo> shared) override {}
 
@@ -58,7 +60,7 @@ class LinuxPerfJitLogger : public CodeEventLogger {
   void CloseMarkerFile(void* marker_address);
 
   uint64_t GetTimestamp();
-  void LogRecordedBuffer(AbstractCode code,
+  void LogRecordedBuffer(Tagged<AbstractCode> code,
                          MaybeHandle<SharedFunctionInfo> maybe_shared,
                          const char* name, int length) override;
 #if V8_ENABLE_WEBASSEMBLY
@@ -79,11 +81,11 @@ class LinuxPerfJitLogger : public CodeEventLogger {
 
   void LogWriteBytes(const char* bytes, int size);
   void LogWriteHeader();
-  void LogWriteDebugInfo(Code code, Handle<SharedFunctionInfo> shared);
+  void LogWriteDebugInfo(Tagged<Code> code, Handle<SharedFunctionInfo> shared);
 #if V8_ENABLE_WEBASSEMBLY
   void LogWriteDebugInfo(const wasm::WasmCode* code);
 #endif  // V8_ENABLE_WEBASSEMBLY
-  void LogWriteUnwindingInfo(Code code);
+  void LogWriteUnwindingInfo(Tagged<Code> code);
 
   static const uint32_t kElfMachIA32 = 3;
   static const uint32_t kElfMachX64 = 62;
diff --git a/src/execution/arguments-inl.h b/src/execution/arguments-inl.h
index b06da4caf9e..4caeb93bf65 100644
--- a/src/execution/arguments-inl.h
+++ b/src/execution/arguments-inl.h
@@ -17,7 +17,7 @@ namespace internal {
 template <ArgumentsType T>
 Arguments<T>::ChangeValueScope::ChangeValueScope(Isolate* isolate,
                                                  Arguments* args, int index,
-                                                 Object value)
+                                                 Tagged<Object> value)
     : location_(args->address_of_arg_at(index)) {
   old_value_ = handle(Object(*location_), isolate);
   *location_ = value.ptr();
@@ -25,7 +25,7 @@ Arguments<T>::ChangeValueScope::ChangeValueScope(Isolate* isolate,
 
 template <ArgumentsType T>
 int Arguments<T>::smi_value_at(int index) const {
-  Object obj = (*this)[index];
+  Tagged<Object> obj = (*this)[index];
   int value = Smi::ToInt(obj);
   DCHECK_IMPLIES(IsTaggedIndex(obj), value == tagged_index_value_at(index));
   return value;
diff --git a/src/execution/arguments.h b/src/execution/arguments.h
index 26dadeb3f3c..ec6d7444ca5 100644
--- a/src/execution/arguments.h
+++ b/src/execution/arguments.h
@@ -38,7 +38,7 @@ class Arguments {
   class ChangeValueScope {
    public:
     inline ChangeValueScope(Isolate* isolate, Arguments* args, int index,
-                            Object value);
+                            Tagged<Object> value);
     ~ChangeValueScope() { *location_ = old_value_->ptr(); }
 
    private:
@@ -51,7 +51,7 @@ class Arguments {
     DCHECK_GE(length_, 0);
   }
 
-  V8_INLINE Object operator[](int index) const {
+  V8_INLINE Tagged<Object> operator[](int index) const {
     return Object(*address_of_arg_at(index));
   }
 
diff --git a/src/execution/arm/simulator-arm.cc b/src/execution/arm/simulator-arm.cc
index 9785fbed8e5..4a3bdc8b9cc 100644
--- a/src/execution/arm/simulator-arm.cc
+++ b/src/execution/arm/simulator-arm.cc
@@ -309,7 +309,7 @@ bool ArmDebugger::ExecDebugCommand(ArrayUniquePtr<char> line_ptr) {
       int32_t value;
       StdoutStream os;
       if (GetValue(arg1, &value)) {
-        Object obj(value);
+        Tagged<Object> obj(value);
         os << arg1 << ": \n";
 #ifdef DEBUG
         Print(obj, os);
@@ -355,7 +355,7 @@ bool ArmDebugger::ExecDebugCommand(ArrayUniquePtr<char> line_ptr) {
     while (cur < end) {
       PrintF("  0x%08" V8PRIxPTR ":  0x%08x %10d",
              reinterpret_cast<intptr_t>(cur), *cur, *cur);
-      Object obj(*cur);
+      Tagged<Object> obj(*cur);
       Heap* current_heap = sim_->isolate_->heap();
       if (!skip_obj_print) {
         if (IsSmi(obj) ||
diff --git a/src/execution/arm64/simulator-arm64.cc b/src/execution/arm64/simulator-arm64.cc
index a88db6d71e3..beba51defc5 100644
--- a/src/execution/arm64/simulator-arm64.cc
+++ b/src/execution/arm64/simulator-arm64.cc
@@ -3945,7 +3945,7 @@ bool Simulator::ExecDebugCommand(ArrayUniquePtr<char> line_ptr) {
       int64_t value;
       StdoutStream os;
       if (GetValue(arg1, &value)) {
-        Object obj(value);
+        Tagged<Object> obj(value);
         os << arg1 << ": \n";
 #ifdef DEBUG
         Print(obj, os);
@@ -4003,7 +4003,7 @@ bool Simulator::ExecDebugCommand(ArrayUniquePtr<char> line_ptr) {
       PrintF("  0x%016" PRIx64 ":  0x%016" PRIx64 " %10" PRId64,
              reinterpret_cast<uint64_t>(cur), *cur, *cur);
       if (!skip_obj_print) {
-        Object obj(*cur);
+        Tagged<Object> obj(*cur);
         Heap* current_heap = isolate_->heap();
         if (IsSmi(obj) ||
             IsValidHeapObject(current_heap, HeapObject::cast(obj))) {
diff --git a/src/execution/execution.cc b/src/execution/execution.cc
index b4ee4093ce3..be918f4097c 100644
--- a/src/execution/execution.cc
+++ b/src/execution/execution.cc
@@ -195,7 +195,7 @@ MaybeHandle<Context> NewScriptContext(Isolate* isolate,
     return MaybeHandle<Context>();
   }
   SaveAndSwitchContext save(isolate, function->context());
-  SharedFunctionInfo sfi = function->shared();
+  Tagged<SharedFunctionInfo> sfi = function->shared();
   Handle<Script> script(Script::cast(sfi->script()), isolate);
   Handle<ScopeInfo> scope_info(sfi->scope_info(), isolate);
   Handle<NativeContext> native_context(NativeContext::cast(function->context()),
@@ -396,7 +396,7 @@ V8_WARN_UNUSED_RESULT MaybeHandle<Object> Invoke(Isolate* isolate,
   }
 
   // Placeholder for return value.
-  Object value;
+  Tagged<Object> value;
   Handle<Code> code =
       JSEntry(isolate, params.execution_target, params.is_construct);
   {
diff --git a/src/execution/frames-inl.h b/src/execution/frames-inl.h
index ca25c56df83..2f5582bbc39 100644
--- a/src/execution/frames-inl.h
+++ b/src/execution/frames-inl.h
@@ -127,7 +127,7 @@ inline ExitFrame::ExitFrame(StackFrameIteratorBase* iterator)
 inline BuiltinExitFrame::BuiltinExitFrame(StackFrameIteratorBase* iterator)
     : ExitFrame(iterator) {}
 
-inline Object BuiltinExitFrame::receiver_slot_object() const {
+inline Tagged<Object> BuiltinExitFrame::receiver_slot_object() const {
   // The receiver is the first argument on the frame.
   // fp[1]: return address.
   // ------- fixed extra builtin arguments -------
@@ -141,17 +141,17 @@ inline Object BuiltinExitFrame::receiver_slot_object() const {
   return Object(base::Memory<Address>(fp() + receiverOffset));
 }
 
-inline Object BuiltinExitFrame::argc_slot_object() const {
+inline Tagged<Object> BuiltinExitFrame::argc_slot_object() const {
   return Object(
       base::Memory<Address>(fp() + BuiltinExitFrameConstants::kArgcOffset));
 }
 
-inline Object BuiltinExitFrame::target_slot_object() const {
+inline Tagged<Object> BuiltinExitFrame::target_slot_object() const {
   return Object(
       base::Memory<Address>(fp() + BuiltinExitFrameConstants::kTargetOffset));
 }
 
-inline Object BuiltinExitFrame::new_target_slot_object() const {
+inline Tagged<Object> BuiltinExitFrame::new_target_slot_object() const {
   return Object(base::Memory<Address>(
       fp() + BuiltinExitFrameConstants::kNewTargetOffset));
 }
@@ -185,11 +185,11 @@ inline FullObjectSlot ApiCallbackExitFrame::new_target_slot() const {
 inline CommonFrame::CommonFrame(StackFrameIteratorBase* iterator)
     : StackFrame(iterator) {}
 
-inline Object CommonFrame::GetExpression(int index) const {
+inline Tagged<Object> CommonFrame::GetExpression(int index) const {
   return Object(base::Memory<Address>(GetExpressionAddress(index)));
 }
 
-inline void CommonFrame::SetExpression(int index, Object value) {
+inline void CommonFrame::SetExpression(int index, Tagged<Object> value) {
   base::Memory<Address>(GetExpressionAddress(index)) = value.ptr();
 }
 
@@ -224,11 +224,11 @@ inline int CommonFrameWithJSLinkage::GetActualArgumentCount() const {
   return 0;
 }
 
-inline void JavaScriptFrame::set_receiver(Object value) {
+inline void JavaScriptFrame::set_receiver(Tagged<Object> value) {
   base::Memory<Address>(GetParameterSlot(-1)) = value.ptr();
 }
 
-inline Object JavaScriptFrame::function_slot_object() const {
+inline Tagged<Object> JavaScriptFrame::function_slot_object() const {
   const int offset = StandardFrameConstants::kFunctionOffset;
   return Object(base::Memory<Address>(fp() + offset));
 }
diff --git a/src/execution/frames.cc b/src/execution/frames.cc
index fbd79bd129a..9c8b4d589f9 100644
--- a/src/execution/frames.cc
+++ b/src/execution/frames.cc
@@ -270,7 +270,8 @@ FrameSummary DebuggableStackFrameIterator::GetTopValidFrame() const {
 // static
 bool DebuggableStackFrameIterator::IsValidFrame(StackFrame* frame) {
   if (frame->is_java_script()) {
-    JSFunction function = static_cast<JavaScriptFrame*>(frame)->function();
+    Tagged<JSFunction> function =
+        static_cast<JavaScriptFrame*>(frame)->function();
     return function->shared()->IsSubjectToDebugging();
   }
 #if V8_ENABLE_WEBASSEMBLY
@@ -299,7 +300,7 @@ base::Optional<bool> IsInterpreterFramePc(Isolate* isolate, Address pc,
     MSAN_MEMORY_IS_INITIALIZED(
         state->fp + StandardFrameConstants::kFunctionOffset,
         kSystemPointerSize);
-    Object maybe_function = Object(
+    Tagged<Object> maybe_function = Object(
         Memory<Address>(state->fp + StandardFrameConstants::kFunctionOffset));
     // There's no need to run a full ContainsSlow if we know the frame can't be
     // an InterpretedFrame,  so we do these fast checks first
@@ -311,7 +312,7 @@ base::Optional<bool> IsInterpreterFramePc(Isolate* isolate, Address pc,
     if (!ThreadIsolation::CanLookupStartOfJitAllocationAt(pc)) {
       return {};
     }
-    Code interpreter_entry_trampoline =
+    Tagged<Code> interpreter_entry_trampoline =
         isolate->heap()->FindCodeForInnerPointer(pc);
     return interpreter_entry_trampoline->is_interpreter_trampoline_builtin();
   } else {
@@ -600,21 +601,21 @@ base::Optional<GcSafeCode> GetContainingCode(Isolate* isolate, Address pc) {
 
 }  // namespace
 
-GcSafeCode StackFrame::GcSafeLookupCode() const {
+Tagged<GcSafeCode> StackFrame::GcSafeLookupCode() const {
   base::Optional<GcSafeCode> result = GetContainingCode(isolate(), pc());
   DCHECK_GE(pc(), result->InstructionStart(isolate(), pc()));
   DCHECK_LT(pc(), result->InstructionEnd(isolate(), pc()));
   return result.value();
 }
 
-Code StackFrame::LookupCode() const {
+Tagged<Code> StackFrame::LookupCode() const {
   DCHECK_NE(isolate()->heap()->gc_state(), Heap::MARK_COMPACT);
   return GcSafeLookupCode()->UnsafeCastToCode();
 }
 
 void StackFrame::IteratePc(RootVisitor* v, Address* pc_address,
                            Address* constant_pool_address,
-                           GcSafeCode holder) const {
+                           Tagged<GcSafeCode> holder) const {
   const Address old_pc = ReadPC(pc_address);
   DCHECK_GE(old_pc, holder->InstructionStart(isolate(), old_pc));
   DCHECK_LT(old_pc, holder->InstructionEnd(isolate(), old_pc));
@@ -624,10 +625,11 @@ void StackFrame::IteratePc(RootVisitor* v, Address* pc_address,
   const uintptr_t pc_offset_from_start = old_pc - holder->instruction_start();
 
   // Visit.
-  GcSafeCode visited_holder = holder;
+  Tagged<GcSafeCode> visited_holder = holder;
   PtrComprCageBase code_cage_base{isolate()->code_cage_base()};
-  const Object old_istream = holder->raw_instruction_stream(code_cage_base);
-  Object visited_istream = old_istream;
+  const Tagged<Object> old_istream =
+      holder->raw_instruction_stream(code_cage_base);
+  Tagged<Object> visited_istream = old_istream;
   v->VisitRunningCode(FullObjectSlot{&visited_holder},
                       FullObjectSlot{&visited_istream});
   if (visited_istream == old_istream) {
@@ -639,7 +641,7 @@ void StackFrame::IteratePc(RootVisitor* v, Address* pc_address,
 
   DCHECK(visited_holder->has_instruction_stream());
 
-  InstructionStream istream =
+  Tagged<InstructionStream> istream =
       InstructionStream::unchecked_cast(visited_istream);
   const Address new_pc = istream->instruction_start() + pc_offset_from_start;
   // TODO(v8:10026): avoid replacing a signed pointer.
@@ -657,7 +659,7 @@ void StackFrame::SetReturnAddressLocationResolver(
 
 namespace {
 
-StackFrame::Type ComputeBuiltinFrameType(GcSafeCode code) {
+StackFrame::Type ComputeBuiltinFrameType(Tagged<GcSafeCode> code) {
   if (code->is_interpreter_trampoline_builtin() ||
       code->is_baseline_trampoline_builtin()) {
     // Frames for baseline entry trampolines on the stack are still interpreted
@@ -858,7 +860,7 @@ StackFrame::Type StackFrameIteratorForProfiler::ComputeStackFrameType(
 
   MSAN_MEMORY_IS_INITIALIZED(
       state->fp + StandardFrameConstants::kFunctionOffset, kSystemPointerSize);
-  Object maybe_function = Object(
+  Tagged<Object> maybe_function = Object(
       Memory<Address>(state->fp + StandardFrameConstants::kFunctionOffset));
   if (IsSmi(maybe_function)) {
     return StackFrame::NATIVE;
@@ -898,7 +900,7 @@ void NativeFrame::ComputeCallerState(State* state) const {
   state->constant_pool_address = nullptr;
 }
 
-HeapObject EntryFrame::unchecked_code() const {
+Tagged<HeapObject> EntryFrame::unchecked_code() const {
   return isolate()->builtins()->code(Builtin::kJSEntry);
 }
 
@@ -920,7 +922,7 @@ StackFrame::Type CWasmEntryFrame::GetCallerState(State* state) const {
 }
 #endif  // V8_ENABLE_WEBASSEMBLY
 
-HeapObject ConstructEntryFrame::unchecked_code() const {
+Tagged<HeapObject> ConstructEntryFrame::unchecked_code() const {
   return isolate()->builtins()->code(Builtin::kJSConstructEntry);
 }
 
@@ -961,7 +963,7 @@ StackFrame::Type ExitFrame::ComputeFrameType(Address fp) {
   // Distinguish between different exit frame types.
   // Default to EXIT in all hairy cases (e.g., when called from profiler).
   const int offset = ExitFrameConstants::kFrameTypeOffset;
-  Object marker(Memory<Address>(fp + offset));
+  Tagged<Object> marker(Memory<Address>(fp + offset));
 
   if (!IsSmi(marker)) {
     return EXIT;
@@ -1017,7 +1019,7 @@ void BuiltinExitFrame::Summarize(std::vector<FrameSummary>* frames) const {
   DCHECK(frames->empty());
   Handle<FixedArray> parameters = GetParameters();
   DisallowGarbageCollection no_gc;
-  Code code = LookupCode();
+  Tagged<Code> code = LookupCode();
   int code_offset = code->GetOffsetFromInstructionStart(isolate(), pc());
   FrameSummary::JavaScriptFrameSummary summary(
       isolate(), receiver(), function(), AbstractCode::cast(code), code_offset,
@@ -1025,13 +1027,15 @@ void BuiltinExitFrame::Summarize(std::vector<FrameSummary>* frames) const {
   frames->push_back(summary);
 }
 
-JSFunction BuiltinExitFrame::function() const {
+Tagged<JSFunction> BuiltinExitFrame::function() const {
   return JSFunction::cast(target_slot_object());
 }
 
-Object BuiltinExitFrame::receiver() const { return receiver_slot_object(); }
+Tagged<Object> BuiltinExitFrame::receiver() const {
+  return receiver_slot_object();
+}
 
-Object BuiltinExitFrame::GetParameter(int i) const {
+Tagged<Object> BuiltinExitFrame::GetParameter(int i) const {
   DCHECK(i >= 0 && i < ComputeParametersCount());
   int offset =
       BuiltinExitFrameConstants::kFirstArgumentOffset + i * kSystemPointerSize;
@@ -1039,7 +1043,7 @@ Object BuiltinExitFrame::GetParameter(int i) const {
 }
 
 int BuiltinExitFrame::ComputeParametersCount() const {
-  Object argc_slot = argc_slot_object();
+  Tagged<Object> argc_slot = argc_slot_object();
   DCHECK(IsSmi(argc_slot));
   // Argc also counts the receiver, target, new target, and argc itself as args,
   // therefore the real argument count is argc - 4.
@@ -1072,19 +1076,19 @@ static_assert(
 static_assert(ApiCallbackExitFrameConstants::kFunctionCallbackInfoArgsLength ==
               FunctionCallbackArguments::kArgsLength);
 
-HeapObject ApiCallbackExitFrame::target() const {
-  Object function = *target_slot();
+Tagged<HeapObject> ApiCallbackExitFrame::target() const {
+  Tagged<Object> function = *target_slot();
   DCHECK(IsJSFunction(function) || IsFunctionTemplateInfo(function));
   return HeapObject::cast(function);
 }
 
-void ApiCallbackExitFrame::set_target(HeapObject function) const {
+void ApiCallbackExitFrame::set_target(Tagged<HeapObject> function) const {
   DCHECK(IsJSFunction(function) || IsFunctionTemplateInfo(function));
   target_slot().store(function);
 }
 
 Handle<JSFunction> ApiCallbackExitFrame::GetFunction() const {
-  HeapObject maybe_function = target();
+  Tagged<HeapObject> maybe_function = target();
   if (IsJSFunction(maybe_function)) {
     return Handle<JSFunction>(target_slot().location());
   }
@@ -1106,9 +1110,11 @@ Handle<JSFunction> ApiCallbackExitFrame::GetFunction() const {
   return function;
 }
 
-Object ApiCallbackExitFrame::receiver() const { return *receiver_slot(); }
+Tagged<Object> ApiCallbackExitFrame::receiver() const {
+  return *receiver_slot();
+}
 
-Object ApiCallbackExitFrame::GetParameter(int i) const {
+Tagged<Object> ApiCallbackExitFrame::GetParameter(int i) const {
   DCHECK(i >= 0 && i < ComputeParametersCount());
   int offset = ApiCallbackExitFrameConstants::kFirstArgumentOffset +
                i * kSystemPointerSize;
@@ -1116,7 +1122,7 @@ Object ApiCallbackExitFrame::GetParameter(int i) const {
 }
 
 int ApiCallbackExitFrame::ComputeParametersCount() const {
-  Object argc_value = *argc_slot();
+  Tagged<Object> argc_value = *argc_slot();
   DCHECK(IsSmi(argc_value));
   int argc = Smi::ToInt(argc_value);
   DCHECK_GE(argc, 0);
@@ -1144,7 +1150,7 @@ void ApiCallbackExitFrame::Summarize(std::vector<FrameSummary>* frames) const {
   Handle<FixedArray> parameters = GetParameters();
   Handle<JSFunction> function = GetFunction();
   DisallowGarbageCollection no_gc;
-  Code code = LookupCode();
+  Tagged<Code> code = LookupCode();
   int code_offset = code->GetOffsetFromInstructionStart(isolate(), pc());
   FrameSummary::JavaScriptFrameSummary summary(
       isolate(), receiver(), *function, AbstractCode::cast(code), code_offset,
@@ -1182,8 +1188,8 @@ void StackFrame::Print(StringStream* accumulator, PrintMode mode,
 void BuiltinExitFrame::Print(StringStream* accumulator, PrintMode mode,
                              int index) const {
   DisallowGarbageCollection no_gc;
-  Object receiver = this->receiver();
-  JSFunction function = this->function();
+  Tagged<Object> receiver = this->receiver();
+  Tagged<JSFunction> function = this->function();
 
   accumulator->PrintSecurityTokenIfChanged(function);
   PrintIndex(accumulator, mode, index);
@@ -1206,7 +1212,7 @@ void ApiCallbackExitFrame::Print(StringStream* accumulator, PrintMode mode,
                                  int index) const {
   Handle<JSFunction> function = GetFunction();
   DisallowGarbageCollection no_gc;
-  Object receiver = this->receiver();
+  Tagged<Object> receiver = this->receiver();
 
   accumulator->PrintSecurityTokenIfChanged(*function);
   PrintIndex(accumulator, mode, index);
@@ -1235,12 +1241,12 @@ Address UnoptimizedFrame::GetExpressionAddress(int n) const {
   return fp() + offset - n * kSystemPointerSize;
 }
 
-Object CommonFrame::context() const {
+Tagged<Object> CommonFrame::context() const {
   return ReadOnlyRoots(isolate()).undefined_value();
 }
 
 int CommonFrame::position() const {
-  Code code = LookupCode();
+  Tagged<Code> code = LookupCode();
   int code_offset = code->GetOffsetFromInstructionStart(isolate(), pc());
   return AbstractCode::cast(code)->SourcePosition(isolate(), code_offset);
 }
@@ -1314,11 +1320,12 @@ void VisitSpillSlot(Isolate* isolate, RootVisitor* v,
           cage_base, static_cast<Tagged_t>(value));
       if (DEBUG_BOOL) {
         // Ensure that the spill slot contains correct heap object.
-        HeapObject raw = HeapObject::cast(Object(*spill_slot.location()));
+        Tagged<HeapObject> raw =
+            HeapObject::cast(Object(*spill_slot.location()));
         MapWord map_word = raw->map_word(cage_base, kRelaxedLoad);
-        HeapObject forwarded = map_word.IsForwardingAddress()
-                                   ? map_word.ToForwardingAddress(raw)
-                                   : raw;
+        Tagged<HeapObject> forwarded = map_word.IsForwardingAddress()
+                                           ? map_word.ToForwardingAddress(raw)
+                                           : raw;
         bool is_self_forwarded =
             forwarded->map_word(cage_base, kRelaxedLoad) ==
             MapWord::FromForwardingAddress(forwarded, forwarded);
@@ -1328,7 +1335,7 @@ void VisitSpillSlot(Isolate* isolate, RootVisitor* v,
           CHECK(BasicMemoryChunk::FromHeapObject(forwarded)
                     ->InNewLargeObjectSpace());
         } else {
-          HeapObject forwarded_map = forwarded->map(cage_base);
+          Tagged<HeapObject> forwarded_map = forwarded->map(cage_base);
           // The map might be forwarded as well.
           MapWord fwd_map_map_word =
               forwarded_map->map_word(cage_base, kRelaxedLoad);
@@ -1514,7 +1521,7 @@ void WasmFrame::Iterate(RootVisitor* v) const {
 }
 
 void TypedFrame::IterateParamsOfWasmToJSWrapper(RootVisitor* v) const {
-  Object maybe_signature = Object(
+  Tagged<Object> maybe_signature = Object(
       Memory<Address>(fp() + WasmToJSWrapperConstants::kSignatureOffset));
   // The signature slot contains a marker and not a signature, so there is
   // nothing we have to iterate here.
@@ -1527,11 +1534,12 @@ void TypedFrame::IterateParamsOfWasmToJSWrapper(RootVisitor* v) const {
 
   // Load the signature, considering forward pointers.
   PtrComprCageBase cage_base(isolate());
-  HeapObject raw = HeapObject::cast(maybe_signature);
+  Tagged<HeapObject> raw = HeapObject::cast(maybe_signature);
   MapWord map_word = raw->map_word(cage_base, kRelaxedLoad);
-  HeapObject forwarded =
+  Tagged<HeapObject> forwarded =
       map_word.IsForwardingAddress() ? map_word.ToForwardingAddress(raw) : raw;
-  PodArray<wasm::ValueType> sig = PodArray<wasm::ValueType>::cast(forwarded);
+  Tagged<PodArray<wasm::ValueType>> sig =
+      PodArray<wasm::ValueType>::cast(forwarded);
 
   size_t parameter_count = wasm::SerializedSignatureHelper::ParamCount(sig);
   wasm::LinkageLocationAllocator allocator(wasm::kGpParamRegisters,
@@ -1656,7 +1664,7 @@ void TypedFrame::Iterate(RootVisitor* v) const {
   InnerPointerToCodeCache::InnerPointerToCodeCacheEntry* entry =
       isolate()->inner_pointer_to_code_cache()->GetCacheEntry(inner_pointer);
   CHECK(entry->code.has_value());
-  GcSafeCode code = entry->code.value();
+  Tagged<GcSafeCode> code = entry->code.value();
 #if V8_ENABLE_WEBASSEMBLY
   if (code->is_builtin() &&
       code->builtin_id() == Builtin::kWasmToJsWrapperCSA) {
@@ -1751,7 +1759,7 @@ void MaglevFrame::Iterate(RootVisitor* v) const {
   InnerPointerToCodeCache::InnerPointerToCodeCacheEntry* entry =
       isolate()->inner_pointer_to_code_cache()->GetCacheEntry(inner_pointer);
   CHECK(entry->code.has_value());
-  GcSafeCode code = entry->code.value();
+  Tagged<GcSafeCode> code = entry->code.value();
   DCHECK(code->is_maglevved());
   MaglevSafepointEntry maglev_safepoint_entry =
       GetMaglevSafepointEntryFromCodeCache(isolate(), inner_pointer, entry);
@@ -1827,7 +1835,7 @@ Handle<JSFunction> MaglevFrame::GetInnermostFunction() const {
 
 BytecodeOffset MaglevFrame::GetBytecodeOffsetForOSR() const {
   int deopt_index = SafepointEntry::kNoDeoptIndex;
-  const DeoptimizationData data = GetDeoptimizationData(&deopt_index);
+  const Tagged<DeoptimizationData> data = GetDeoptimizationData(&deopt_index);
   if (deopt_index == SafepointEntry::kNoDeoptIndex) {
     CHECK(data.is_null());
     FATAL("Missing deoptimization information for OptimizedFrame::Summarize.");
@@ -1854,7 +1862,8 @@ BytecodeOffset MaglevFrame::GetBytecodeOffsetForOSR() const {
   return offset;
 }
 
-bool CommonFrame::HasTaggedOutgoingParams(GcSafeCode code_lookup) const {
+bool CommonFrame::HasTaggedOutgoingParams(
+    Tagged<GcSafeCode> code_lookup) const {
 #if V8_ENABLE_WEBASSEMBLY
   // With inlined JS-to-Wasm calls, we can be in an OptimizedFrame and
   // directly call a Wasm function from JavaScript. In this case the Wasm frame
@@ -1866,11 +1875,11 @@ bool CommonFrame::HasTaggedOutgoingParams(GcSafeCode code_lookup) const {
       wasm::GetWasmCodeManager()->LookupCode(callee_pc());
   return (wasm_callee == nullptr) && code_lookup->has_tagged_outgoing_params();
 #else
-  return code_lookup.has_tagged_outgoing_params();
+  return code_lookup->has_tagged_outgoing_params();
 #endif  // V8_ENABLE_WEBASSEMBLY
 }
 
-HeapObject TurbofanStubWithContextFrame::unchecked_code() const {
+Tagged<HeapObject> TurbofanStubWithContextFrame::unchecked_code() const {
   base::Optional<GcSafeCode> code_lookup =
       isolate()->heap()->GcSafeTryFindCodeForInnerPointer(pc());
   if (!code_lookup.has_value()) return {};
@@ -1908,7 +1917,7 @@ void CommonFrame::IterateTurbofanOptimizedFrame(RootVisitor* v) const {
   InnerPointerToCodeCache::InnerPointerToCodeCacheEntry* entry =
       isolate()->inner_pointer_to_code_cache()->GetCacheEntry(inner_pointer);
   CHECK(entry->code.has_value());
-  GcSafeCode code = entry->code.value();
+  Tagged<GcSafeCode> code = entry->code.value();
   DCHECK(code->is_turbofanned());
   SafepointEntry safepoint_entry =
       GetSafepointEntryFromCodeCache(isolate(), inner_pointer, entry);
@@ -1963,7 +1972,7 @@ void TurbofanFrame::Iterate(RootVisitor* v) const {
   return IterateTurbofanOptimizedFrame(v);
 }
 
-HeapObject StubFrame::unchecked_code() const {
+Tagged<HeapObject> StubFrame::unchecked_code() const {
   base::Optional<GcSafeCode> code_lookup =
       isolate()->heap()->GcSafeTryFindCodeForInnerPointer(pc());
   if (!code_lookup.has_value()) return {};
@@ -1971,7 +1980,7 @@ HeapObject StubFrame::unchecked_code() const {
 }
 
 int StubFrame::LookupExceptionHandlerInTable() {
-  Code code = LookupCode();
+  Tagged<Code> code = LookupCode();
   DCHECK(code->is_turbofanned());
   DCHECK(code->has_handler_table());
   HandlerTable table(code);
@@ -1981,7 +1990,7 @@ int StubFrame::LookupExceptionHandlerInTable() {
 
 void StubFrame::Summarize(std::vector<FrameSummary>* frames) const {
 #if V8_ENABLE_WEBASSEMBLY
-  Code code = LookupCode();
+  Tagged<Code> code = LookupCode();
   if (code->kind() != CodeKind::BUILTIN) return;
   // We skip most stub frames from stack traces, but a few builtins
   // specifically exist to pretend to be another builtin throwing an
@@ -2002,7 +2011,7 @@ void StubFrame::Summarize(std::vector<FrameSummary>* frames) const {
 #endif  // V8_ENABLE_WEBASSEMBLY
 }
 
-void JavaScriptFrame::SetParameterValue(int index, Object value) const {
+void JavaScriptFrame::SetParameterValue(int index, Tagged<Object> value) const {
   Memory<Address>(GetParameterSlot(index)) = value.ptr();
 }
 
@@ -2010,7 +2019,7 @@ bool JavaScriptFrame::IsConstructor() const {
   return IsConstructFrame(caller_fp());
 }
 
-HeapObject CommonFrameWithJSLinkage::unchecked_code() const {
+Tagged<HeapObject> CommonFrameWithJSLinkage::unchecked_code() const {
   return function()->code();
 }
 
@@ -2052,7 +2061,7 @@ bool CommonFrameWithJSLinkage::IsConstructor() const {
 void CommonFrameWithJSLinkage::Summarize(
     std::vector<FrameSummary>* functions) const {
   DCHECK(functions->empty());
-  GcSafeCode code = GcSafeLookupCode();
+  Tagged<GcSafeCode> code = GcSafeLookupCode();
   int offset = code->GetOffsetFromInstructionStart(isolate(), pc());
   Handle<AbstractCode> abstract_code(
       AbstractCode::cast(code->UnsafeCastToCode()), isolate());
@@ -2063,11 +2072,11 @@ void CommonFrameWithJSLinkage::Summarize(
   functions->push_back(summary);
 }
 
-JSFunction JavaScriptFrame::function() const {
+Tagged<JSFunction> JavaScriptFrame::function() const {
   return JSFunction::cast(function_slot_object());
 }
 
-Object JavaScriptFrame::unchecked_function() const {
+Tagged<Object> JavaScriptFrame::unchecked_function() const {
   // During deoptimization of an optimized function, we may have yet to
   // materialize some closures on the stack. The arguments marker object
   // marks this case.
@@ -2076,26 +2085,26 @@ Object JavaScriptFrame::unchecked_function() const {
   return function_slot_object();
 }
 
-Object CommonFrameWithJSLinkage::receiver() const {
+Tagged<Object> CommonFrameWithJSLinkage::receiver() const {
   // TODO(cbruni): document this better
   return GetParameter(-1);
 }
 
-Object JavaScriptFrame::context() const {
+Tagged<Object> JavaScriptFrame::context() const {
   const int offset = StandardFrameConstants::kContextOffset;
-  Object maybe_result(Memory<Address>(fp() + offset));
+  Tagged<Object> maybe_result(Memory<Address>(fp() + offset));
   DCHECK(!IsSmi(maybe_result));
   return maybe_result;
 }
 
-Script JavaScriptFrame::script() const {
+Tagged<Script> JavaScriptFrame::script() const {
   return Script::cast(function()->shared()->script());
 }
 
 int CommonFrameWithJSLinkage::LookupExceptionHandlerInTable(
     int* stack_depth, HandlerTable::CatchPrediction* prediction) {
   if (DEBUG_BOOL) {
-    Code code_lookup_result = LookupCode();
+    Tagged<Code> code_lookup_result = LookupCode();
     CHECK(!code_lookup_result->has_handler_table());
     CHECK(!code_lookup_result->is_optimized_code() ||
           code_lookup_result->kind() == CodeKind::BASELINE);
@@ -2103,24 +2112,24 @@ int CommonFrameWithJSLinkage::LookupExceptionHandlerInTable(
   return -1;
 }
 
-void JavaScriptFrame::PrintFunctionAndOffset(JSFunction function,
-                                             AbstractCode code, int code_offset,
-                                             FILE* file,
+void JavaScriptFrame::PrintFunctionAndOffset(Tagged<JSFunction> function,
+                                             Tagged<AbstractCode> code,
+                                             int code_offset, FILE* file,
                                              bool print_line_number) {
   PtrComprCageBase cage_base = GetPtrComprCageBase(function);
   PrintF(file, "%s", CodeKindToMarker(code->kind(cage_base)));
   function->PrintName(file);
   PrintF(file, "+%d", code_offset);
   if (print_line_number) {
-    SharedFunctionInfo shared = function->shared();
+    Tagged<SharedFunctionInfo> shared = function->shared();
     int source_pos = code->SourcePosition(cage_base, code_offset);
-    Object maybe_script = shared->script();
+    Tagged<Object> maybe_script = shared->script();
     if (IsScript(maybe_script)) {
-      Script script = Script::cast(maybe_script);
+      Tagged<Script> script = Script::cast(maybe_script);
       int line = script->GetLineNumber(source_pos) + 1;
-      Object script_name_raw = script->name();
+      Tagged<Object> script_name_raw = script->name();
       if (IsString(script_name_raw)) {
-        String script_name = String::cast(script->name());
+        Tagged<String> script_name = String::cast(script->name());
         std::unique_ptr<char[]> c_script_name =
             script_name->ToCString(DISALLOW_NULLS, ROBUST_STRING_TRAVERSAL);
         PrintF(file, " at %s:%d", c_script_name.get(), line);
@@ -2142,9 +2151,9 @@ void JavaScriptFrame::PrintTop(Isolate* isolate, FILE* file, bool print_args,
     if (it.frame()->is_java_script()) {
       JavaScriptFrame* frame = it.frame();
       if (frame->IsConstructor()) PrintF(file, "new ");
-      JSFunction function = frame->function();
+      Tagged<JSFunction> function = frame->function();
       int code_offset = 0;
-      AbstractCode abstract_code = function->abstract_code(isolate);
+      Tagged<AbstractCode> abstract_code = function->abstract_code(isolate);
       if (frame->is_interpreted()) {
         InterpretedFrame* iframe = reinterpret_cast<InterpretedFrame*>(frame);
         code_offset = iframe->GetBytecodeOffset();
@@ -2179,21 +2188,20 @@ void JavaScriptFrame::PrintTop(Isolate* isolate, FILE* file, bool print_args,
 }
 
 // static
-void JavaScriptFrame::CollectFunctionAndOffsetForICStats(JSFunction function,
-                                                         AbstractCode code,
-                                                         int code_offset) {
+void JavaScriptFrame::CollectFunctionAndOffsetForICStats(
+    Tagged<JSFunction> function, Tagged<AbstractCode> code, int code_offset) {
   auto ic_stats = ICStats::instance();
   ICInfo& ic_info = ic_stats->Current();
   PtrComprCageBase cage_base = GetPtrComprCageBase(function);
-  SharedFunctionInfo shared = function->shared(cage_base);
+  Tagged<SharedFunctionInfo> shared = function->shared(cage_base);
 
   ic_info.function_name = ic_stats->GetOrCacheFunctionName(function);
   ic_info.script_offset = code_offset;
 
   int source_pos = code->SourcePosition(cage_base, code_offset);
-  Object maybe_script = shared->script(cage_base, kAcquireLoad);
+  Tagged<Object> maybe_script = shared->script(cage_base, kAcquireLoad);
   if (IsScript(maybe_script, cage_base)) {
-    Script script = Script::cast(maybe_script);
+    Tagged<Script> script = Script::cast(maybe_script);
     Script::PositionInfo info;
     script->GetPositionInfo(source_pos, &info);
     ic_info.line_num = info.line + 1;
@@ -2202,7 +2210,7 @@ void JavaScriptFrame::CollectFunctionAndOffsetForICStats(JSFunction function,
   }
 }
 
-Object CommonFrameWithJSLinkage::GetParameter(int index) const {
+Tagged<Object> CommonFrameWithJSLinkage::GetParameter(int index) const {
   return Object(Memory<Address>(GetParameterSlot(index)));
 }
 
@@ -2234,7 +2242,7 @@ Handle<FixedArray> CommonFrameWithJSLinkage::GetParameters() const {
   return parameters;
 }
 
-JSFunction JavaScriptBuiltinContinuationFrame::function() const {
+Tagged<JSFunction> JavaScriptBuiltinContinuationFrame::function() const {
   const int offset = BuiltinContinuationFrameConstants::kFunctionOffset;
   return JSFunction::cast(Object(base::Memory<Address>(fp() + offset)));
 }
@@ -2244,7 +2252,7 @@ int JavaScriptBuiltinContinuationFrame::ComputeParametersCount() const {
   // register.
   DCHECK_EQ(RegisterConfiguration::Default()->GetAllocatableGeneralCode(0),
             kJavaScriptCallArgCountRegister.code());
-  Object argc_object(
+  Tagged<Object> argc_object(
       Memory<Address>(fp() + BuiltinContinuationFrameConstants::kArgCOffset));
   return Smi::ToInt(argc_object) - kJSArgcReceiverSlots;
 }
@@ -2256,13 +2264,13 @@ intptr_t JavaScriptBuiltinContinuationFrame::GetSPToFPDelta() const {
   return height;
 }
 
-Object JavaScriptBuiltinContinuationFrame::context() const {
+Tagged<Object> JavaScriptBuiltinContinuationFrame::context() const {
   return Object(Memory<Address>(
       fp() + BuiltinContinuationFrameConstants::kBuiltinContextOffset));
 }
 
 void JavaScriptBuiltinContinuationWithCatchFrame::SetException(
-    Object exception) {
+    Tagged<Object> exception) {
   int argc = ComputeParametersCount();
   Address exception_argument_slot =
       fp() + BuiltinContinuationFrameConstants::kFixedFrameSizeAboveFp +
@@ -2275,9 +2283,9 @@ void JavaScriptBuiltinContinuationWithCatchFrame::SetException(
 }
 
 FrameSummary::JavaScriptFrameSummary::JavaScriptFrameSummary(
-    Isolate* isolate, Object receiver, JSFunction function,
-    AbstractCode abstract_code, int code_offset, bool is_constructor,
-    FixedArray parameters)
+    Isolate* isolate, Tagged<Object> receiver, Tagged<JSFunction> function,
+    Tagged<AbstractCode> abstract_code, int code_offset, bool is_constructor,
+    Tagged<FixedArray> parameters)
     : FrameSummaryBase(isolate, FrameSummary::JAVA_SCRIPT),
       receiver_(receiver, isolate),
       function_(function, isolate),
@@ -2553,7 +2561,7 @@ void OptimizedFrame::Summarize(std::vector<FrameSummary>* frames) const {
   }
 
   int deopt_index = SafepointEntry::kNoDeoptIndex;
-  DeoptimizationData const data = GetDeoptimizationData(&deopt_index);
+  Tagged<DeoptimizationData> const data = GetDeoptimizationData(&deopt_index);
   if (deopt_index == SafepointEntry::kNoDeoptIndex) {
     // Hack: For maglevved function entry, we don't emit lazy deopt information,
     // so create an extra special summary here.
@@ -2644,7 +2652,7 @@ void OptimizedFrame::Summarize(std::vector<FrameSummary>* frames) const {
       Handle<SharedFunctionInfo> shared_info = it->shared_info();
       DCHECK_NE(isolate()->heap()->gc_state(), Heap::MARK_COMPACT);
 
-      WasmExportedFunctionData function_data =
+      Tagged<WasmExportedFunctionData> function_data =
           shared_info->wasm_exported_function_data();
       Handle<WasmInstanceObject> instance =
           handle(function_data->instance(), isolate());
@@ -2663,7 +2671,7 @@ int OptimizedFrame::LookupExceptionHandlerInTable(
   // to use FrameSummary to find the corresponding code offset in unoptimized
   // code to perform prediction there.
   DCHECK_NULL(prediction);
-  Code code = LookupCode();
+  Tagged<Code> code = LookupCode();
 
   HandlerTable table(code);
   if (table.NumberOfReturnEntries() == 0) return -1;
@@ -2681,14 +2689,15 @@ int OptimizedFrame::LookupExceptionHandlerInTable(
   return table.LookupReturn(pc_offset);
 }
 
-int MaglevFrame::FindReturnPCForTrampoline(Code code, int trampoline_pc) const {
+int MaglevFrame::FindReturnPCForTrampoline(Tagged<Code> code,
+                                           int trampoline_pc) const {
   DCHECK_EQ(code->kind(), CodeKind::MAGLEV);
   DCHECK(code->marked_for_deoptimization());
   MaglevSafepointTable safepoints(isolate(), pc(), code);
   return safepoints.find_return_pc(trampoline_pc);
 }
 
-int TurbofanFrame::FindReturnPCForTrampoline(Code code,
+int TurbofanFrame::FindReturnPCForTrampoline(Tagged<Code> code,
                                              int trampoline_pc) const {
   DCHECK_EQ(code->kind(), CodeKind::TURBOFAN);
   DCHECK(code->marked_for_deoptimization());
@@ -2696,12 +2705,12 @@ int TurbofanFrame::FindReturnPCForTrampoline(Code code,
   return safepoints.find_return_pc(trampoline_pc);
 }
 
-DeoptimizationData OptimizedFrame::GetDeoptimizationData(
+Tagged<DeoptimizationData> OptimizedFrame::GetDeoptimizationData(
     int* deopt_index) const {
   DCHECK(is_optimized());
 
-  JSFunction opt_function = function();
-  Code code = opt_function->code();
+  Tagged<JSFunction> opt_function = function();
+  Tagged<Code> code = opt_function->code();
 
   // The code object may have been replaced by lazy deoptimization. Fall back
   // to a slow search in this case to find the original optimized code object.
@@ -2739,17 +2748,17 @@ void OptimizedFrame::GetFunctions(
 
   // Delegate to JS frame in absence of turbofan deoptimization.
   // TODO(turbofan): Revisit once we support deoptimization across the board.
-  Code code = LookupCode();
+  Tagged<Code> code = LookupCode();
   if (code->kind() == CodeKind::BUILTIN) {
     return JavaScriptFrame::GetFunctions(functions);
   }
 
   DisallowGarbageCollection no_gc;
   int deopt_index = SafepointEntry::kNoDeoptIndex;
-  DeoptimizationData const data = GetDeoptimizationData(&deopt_index);
+  Tagged<DeoptimizationData> const data = GetDeoptimizationData(&deopt_index);
   DCHECK(!data.is_null());
   DCHECK_NE(SafepointEntry::kNoDeoptIndex, deopt_index);
-  DeoptimizationLiteralArray const literal_array = data->LiteralArray();
+  Tagged<DeoptimizationLiteralArray> const literal_array = data->LiteralArray();
 
   DeoptimizationFrameTranslation::Iterator it(
       data->FrameTranslation(), data->TranslationIndex(deopt_index).value());
@@ -2763,7 +2772,7 @@ void OptimizedFrame::GetFunctions(
     jsframe_count--;
 
     // The second operand of the frame points to the function.
-    Object shared = literal_array->get(it.NextOperand());
+    Tagged<Object> shared = literal_array->get(it.NextOperand());
     functions->push_back(SharedFunctionInfo::cast(shared));
 
     // Skip over remaining operands to advance to the next opcode.
@@ -2777,7 +2786,7 @@ int OptimizedFrame::StackSlotOffsetRelativeToFp(int slot_index) {
 }
 
 int UnoptimizedFrame::position() const {
-  AbstractCode code = AbstractCode::cast(GetBytecodeArray());
+  Tagged<AbstractCode> code = AbstractCode::cast(GetBytecodeArray());
   int code_offset = GetBytecodeOffset();
   return code->SourcePosition(isolate(), code_offset);
 }
@@ -2788,7 +2797,7 @@ int UnoptimizedFrame::LookupExceptionHandlerInTable(
   return table.LookupRange(GetBytecodeOffset(), context_register, prediction);
 }
 
-BytecodeArray UnoptimizedFrame::GetBytecodeArray() const {
+Tagged<BytecodeArray> UnoptimizedFrame::GetBytecodeArray() const {
   const int index = UnoptimizedFrameConstants::kBytecodeArrayExpressionIndex;
   DCHECK_EQ(UnoptimizedFrameConstants::kBytecodeArrayFromFp,
             UnoptimizedFrameConstants::kExpressionsOffset -
@@ -2796,7 +2805,8 @@ BytecodeArray UnoptimizedFrame::GetBytecodeArray() const {
   return BytecodeArray::cast(GetExpression(index));
 }
 
-Object UnoptimizedFrame::ReadInterpreterRegister(int register_index) const {
+Tagged<Object> UnoptimizedFrame::ReadInterpreterRegister(
+    int register_index) const {
   const int index = UnoptimizedFrameConstants::kRegisterFileExpressionIndex;
   DCHECK_EQ(UnoptimizedFrameConstants::kRegisterFileFromFp,
             UnoptimizedFrameConstants::kExpressionsOffset -
@@ -2845,7 +2855,8 @@ void InterpretedFrame::PatchBytecodeOffset(int new_offset) {
   SetExpression(index, Smi::FromInt(raw_offset));
 }
 
-void InterpretedFrame::PatchBytecodeArray(BytecodeArray bytecode_array) {
+void InterpretedFrame::PatchBytecodeArray(
+    Tagged<BytecodeArray> bytecode_array) {
   const int index = InterpreterFrameConstants::kBytecodeArrayExpressionIndex;
   DCHECK_EQ(InterpreterFrameConstants::kBytecodeArrayFromFp,
             InterpreterFrameConstants::kExpressionsOffset -
@@ -2854,22 +2865,22 @@ void InterpretedFrame::PatchBytecodeArray(BytecodeArray bytecode_array) {
 }
 
 int BaselineFrame::GetBytecodeOffset() const {
-  Code code = LookupCode();
+  Tagged<Code> code = LookupCode();
   return code->GetBytecodeOffsetForBaselinePC(this->pc(), GetBytecodeArray());
 }
 
 intptr_t BaselineFrame::GetPCForBytecodeOffset(int bytecode_offset) const {
-  Code code = LookupCode();
+  Tagged<Code> code = LookupCode();
   return code->GetBaselineStartPCForBytecodeOffset(bytecode_offset,
                                                    GetBytecodeArray());
 }
 
-void BaselineFrame::PatchContext(Context value) {
+void BaselineFrame::PatchContext(Tagged<Context> value) {
   base::Memory<Address>(fp() + BaselineFrameConstants::kContextOffset) =
       value.ptr();
 }
 
-JSFunction BuiltinFrame::function() const {
+Tagged<JSFunction> BuiltinFrame::function() const {
   const int offset = BuiltinFrameConstants::kFunctionOffset;
   return JSFunction::cast(Object(base::Memory<Address>(fp() + offset)));
 }
@@ -2915,9 +2926,9 @@ wasm::WasmCode* WasmFrame::wasm_code() const {
   return wasm::GetWasmCodeManager()->LookupCode(pc());
 }
 
-WasmInstanceObject WasmFrame::wasm_instance() const {
+Tagged<WasmInstanceObject> WasmFrame::wasm_instance() const {
   const int offset = WasmFrameConstants::kWasmInstanceOffset;
-  Object instance(Memory<Address>(fp() + offset));
+  Tagged<Object> instance(Memory<Address>(fp() + offset));
   return WasmInstanceObject::cast(instance);
 }
 
@@ -2925,7 +2936,7 @@ wasm::NativeModule* WasmFrame::native_module() const {
   return module_object()->native_module();
 }
 
-WasmModuleObject WasmFrame::module_object() const {
+Tagged<WasmModuleObject> WasmFrame::module_object() const {
   return wasm_instance()->module_object();
 }
 
@@ -2934,7 +2945,7 @@ int WasmFrame::function_index() const {
   return wasm_code()->index();
 }
 
-Script WasmFrame::script() const { return module_object()->script(); }
+Tagged<Script> WasmFrame::script() const { return module_object()->script(); }
 
 int WasmFrame::position() const {
   wasm::WasmCodeRefScope code_ref_scope;
@@ -2954,7 +2965,9 @@ bool WasmFrame::is_inspectable() const {
   return wasm_code()->is_inspectable();
 }
 
-Object WasmFrame::context() const { return wasm_instance()->native_context(); }
+Tagged<Object> WasmFrame::context() const {
+  return wasm_instance()->native_context();
+}
 
 void WasmFrame::Summarize(std::vector<FrameSummary>* functions) const {
   DCHECK(functions->empty());
@@ -3014,8 +3027,9 @@ bool WasmFrame::at_to_number_conversion() const {
   InnerPointerToCodeCache::InnerPointerToCodeCacheEntry* entry =
       isolate()->inner_pointer_to_code_cache()->GetCacheEntry(callee_pc());
   CHECK(entry->code.has_value());
-  GcSafeCode code = entry->code.value();
-  if (!code.is_builtin() || code.builtin_id() != Builtin::kWasmToJsWrapperCSA) {
+  Tagged<GcSafeCode> code = entry->code.value();
+  if (!code->is_builtin() ||
+      code->builtin_id() != Builtin::kWasmToJsWrapperCSA) {
     return false;
   }
   // The generic wasm-to-js wrapper initially pushes the signature on the stack,
@@ -3063,12 +3077,12 @@ void WasmDebugBreakFrame::Print(StringStream* accumulator, PrintMode mode,
   if (mode != OVERVIEW) accumulator->Add("\n");
 }
 
-WasmInstanceObject WasmToJsFrame::wasm_instance() const {
+Tagged<WasmInstanceObject> WasmToJsFrame::wasm_instance() const {
   // WasmToJsFrames hold the {WasmApiFunctionRef} object in the instance slot.
   // Load the instance from there.
   const int offset = WasmFrameConstants::kWasmInstanceOffset;
-  Object func_ref_obj(Memory<Address>(fp() + offset));
-  WasmApiFunctionRef func_ref = WasmApiFunctionRef::cast(func_ref_obj);
+  Tagged<Object> func_ref_obj(Memory<Address>(fp() + offset));
+  Tagged<WasmApiFunctionRef> func_ref = WasmApiFunctionRef::cast(func_ref_obj);
   return WasmInstanceObject::cast(func_ref->instance());
 }
 
@@ -3111,7 +3125,7 @@ void StackSwitchFrame::GetStateForJumpBuffer(wasm::JumpBuffer* jmpbuf,
 }
 
 int WasmLiftoffSetupFrame::GetDeclaredFunctionIndex() const {
-  Object func_index(Memory<Address>(
+  Tagged<Object> func_index(Memory<Address>(
       sp() + WasmLiftoffSetupFrameConstants::kDeclaredFunctionIndexOffset));
   return Smi::ToInt(func_index);
 }
@@ -3200,7 +3214,8 @@ void WasmLiftoffSetupFrame::Iterate(RootVisitor* v) const {
 
 namespace {
 
-void PrintFunctionSource(StringStream* accumulator, SharedFunctionInfo shared) {
+void PrintFunctionSource(StringStream* accumulator,
+                         Tagged<SharedFunctionInfo> shared) {
   if (v8_flags.max_stack_trace_source_length != 0) {
     std::ostringstream os;
     os << "--------- s o u r c e   c o d e ---------\n"
@@ -3218,8 +3233,8 @@ void JavaScriptFrame::Print(StringStream* accumulator, PrintMode mode,
   SharedFunctionInfo::EnsureSourcePositionsAvailable(isolate(), shared);
 
   DisallowGarbageCollection no_gc;
-  Object receiver = this->receiver();
-  JSFunction function = this->function();
+  Tagged<Object> receiver = this->receiver();
+  Tagged<JSFunction> function = this->function();
 
   accumulator->PrintSecurityTokenIfChanged(function);
   PrintIndex(accumulator, mode, index);
@@ -3232,16 +3247,16 @@ void JavaScriptFrame::Print(StringStream* accumulator, PrintMode mode,
   // doesn't contain scope info, scope_info will return 0 for the number of
   // parameters, stack local variables, context local variables, stack slots,
   // or context slots.
-  ScopeInfo scope_info = shared->scope_info();
-  Object script_obj = shared->script();
+  Tagged<ScopeInfo> scope_info = shared->scope_info();
+  Tagged<Object> script_obj = shared->script();
   if (IsScript(script_obj)) {
-    Script script = Script::cast(script_obj);
+    Tagged<Script> script = Script::cast(script_obj);
     accumulator->Add(" [");
     accumulator->PrintName(script->name());
 
     if (is_interpreted()) {
       const InterpretedFrame* iframe = InterpretedFrame::cast(this);
-      BytecodeArray bytecodes = iframe->GetBytecodeArray();
+      Tagged<BytecodeArray> bytecodes = iframe->GetBytecodeArray();
       int offset = iframe->GetBytecodeOffset();
       int source_pos =
           AbstractCode::cast(bytecodes)->SourcePosition(isolate(), offset);
@@ -3282,7 +3297,7 @@ void JavaScriptFrame::Print(StringStream* accumulator, PrintMode mode,
   int expressions_count = ComputeExpressionsCount();
 
   // Try to get hold of the context of this frame.
-  Context context;
+  Tagged<Context> context;
   if (IsContext(this->context())) {
     context = Context::cast(this->context());
     while (context->IsWithContext()) {
@@ -3295,7 +3310,7 @@ void JavaScriptFrame::Print(StringStream* accumulator, PrintMode mode,
   if (heap_locals_count > 0) {
     accumulator->Add("  // heap-allocated locals\n");
   }
-  for (auto it : ScopeInfo::IterateLocalNames(&scope_info, no_gc)) {
+  for (auto it : ScopeInfo::IterateLocalNames(scope_info, no_gc)) {
     accumulator->Add("  var ");
     accumulator->PrintName(it->name());
     accumulator->Add(" = ");
@@ -3354,7 +3369,7 @@ void JavaScriptFrame::Iterate(RootVisitor* v) const {
 }
 
 void InternalFrame::Iterate(RootVisitor* v) const {
-  GcSafeCode code = GcSafeLookupCode();
+  Tagged<GcSafeCode> code = GcSafeLookupCode();
   IteratePc(v, pc_address(), constant_pool_address(), code);
   // Internal frames typically do not receive any arguments, hence their stack
   // only contains tagged pointers.
diff --git a/src/execution/frames.h b/src/execution/frames.h
index 8baf5266fed..378b7570f01 100644
--- a/src/execution/frames.h
+++ b/src/execution/frames.h
@@ -315,15 +315,16 @@ class StackFrame {
   // This method is used by Isolate::PushStackTraceAndDie() for collecting a
   // stack trace on fatal error and thus it might be called in the middle of GC
   // and should be as safe as possible.
-  virtual HeapObject unchecked_code() const = 0;
+  virtual Tagged<HeapObject> unchecked_code() const = 0;
 
   // Search for the code associated with this frame.
-  V8_EXPORT_PRIVATE Code LookupCode() const;
-  V8_EXPORT_PRIVATE GcSafeCode GcSafeLookupCode() const;
+  V8_EXPORT_PRIVATE Tagged<Code> LookupCode() const;
+  V8_EXPORT_PRIVATE Tagged<GcSafeCode> GcSafeLookupCode() const;
 
   virtual void Iterate(RootVisitor* v) const = 0;
   void IteratePc(RootVisitor* v, Address* pc_address,
-                 Address* constant_pool_address, GcSafeCode holder) const;
+                 Address* constant_pool_address,
+                 Tagged<GcSafeCode> holder) const;
 
   // Sets a callback function for return-address rewriting profilers
   // to resolve the location of a return address to the location of the
@@ -404,10 +405,10 @@ class V8_EXPORT_PRIVATE FrameSummary {
 
   class JavaScriptFrameSummary : public FrameSummaryBase {
    public:
-    JavaScriptFrameSummary(Isolate* isolate, Object receiver,
-                           JSFunction function, AbstractCode abstract_code,
-                           int code_offset, bool is_constructor,
-                           FixedArray parameters);
+    JavaScriptFrameSummary(Isolate* isolate, Tagged<Object> receiver,
+                           Tagged<JSFunction> function,
+                           Tagged<AbstractCode> abstract_code, int code_offset,
+                           bool is_constructor, Tagged<FixedArray> parameters);
 
     void EnsureSourcePositionsAvailable();
     bool AreSourcePositionsAvailable() const;
@@ -558,13 +559,13 @@ class V8_EXPORT_PRIVATE FrameSummary {
 class CommonFrame : public StackFrame {
  public:
   // Accessors.
-  virtual Object context()
+  virtual Tagged<Object> context()
       const;  // TODO(victorgomes): CommonFrames don't have context.
   virtual int position() const;
 
   // Access the expressions in the stack frame including locals.
-  inline Object GetExpression(int index) const;
-  inline void SetExpression(int index, Object value);
+  inline Tagged<Object> GetExpression(int index) const;
+  inline void SetExpression(int index, Tagged<Object> value);
   int ComputeExpressionsCount() const;
 
   Address GetCallerStackPointer() const override;
@@ -582,7 +583,7 @@ class CommonFrame : public StackFrame {
  protected:
   inline explicit CommonFrame(StackFrameIteratorBase* iterator);
 
-  bool HasTaggedOutgoingParams(GcSafeCode code_lookup) const;
+  bool HasTaggedOutgoingParams(Tagged<GcSafeCode> code_lookup) const;
 
   void ComputeCallerState(State* state) const override;
 
@@ -606,7 +607,7 @@ class TurbofanStubWithContextFrame : public CommonFrame {
  public:
   Type type() const override { return TURBOFAN_STUB_WITH_CONTEXT; }
 
-  HeapObject unchecked_code() const override;
+  Tagged<HeapObject> unchecked_code() const override;
   void Iterate(RootVisitor* v) const override;
 
  protected:
@@ -619,7 +620,7 @@ class TurbofanStubWithContextFrame : public CommonFrame {
 
 class TypedFrame : public CommonFrame {
  public:
-  HeapObject unchecked_code() const override { return {}; }
+  Tagged<HeapObject> unchecked_code() const override { return {}; }
   void Iterate(RootVisitor* v) const override;
 
   void IterateParamsOfWasmToJSWrapper(RootVisitor* v) const;
@@ -631,16 +632,16 @@ class TypedFrame : public CommonFrame {
 class CommonFrameWithJSLinkage : public CommonFrame {
  public:
   // Accessors.
-  virtual JSFunction function() const = 0;
+  virtual Tagged<JSFunction> function() const = 0;
 
   // Access the parameters.
-  virtual Object receiver() const;
-  virtual Object GetParameter(int index) const;
+  virtual Tagged<Object> receiver() const;
+  virtual Tagged<Object> GetParameter(int index) const;
   virtual int ComputeParametersCount() const;
   Handle<FixedArray> GetParameters() const;
   virtual int GetActualArgumentCount() const;
 
-  HeapObject unchecked_code() const override;
+  Tagged<HeapObject> unchecked_code() const override;
 
   // Lookup exception handler for current {pc}, returns -1 if none found. Also
   // returns data associated with the handler site specific to the frame type:
@@ -677,16 +678,16 @@ class JavaScriptFrame : public CommonFrameWithJSLinkage {
   Type type() const override = 0;
 
   // Accessors.
-  JSFunction function() const override;
-  Object unchecked_function() const;
-  Script script() const;
-  Object context() const override;
+  Tagged<JSFunction> function() const override;
+  Tagged<Object> unchecked_function() const;
+  Tagged<Script> script() const;
+  Tagged<Object> context() const override;
   int GetActualArgumentCount() const override;
 
-  inline void set_receiver(Object value);
+  inline void set_receiver(Tagged<Object> value);
 
   // Debugger access.
-  void SetParameterValue(int index, Object value) const;
+  void SetParameterValue(int index, Tagged<Object> value) const;
 
   // Check if this frame is a constructor frame invoked through 'new'.
   bool IsConstructor() const override;
@@ -713,15 +714,15 @@ class JavaScriptFrame : public CommonFrameWithJSLinkage {
     return static_cast<JavaScriptFrame*>(frame);
   }
 
-  static void PrintFunctionAndOffset(JSFunction function, AbstractCode code,
-                                     int code_offset, FILE* file,
-                                     bool print_line_number);
+  static void PrintFunctionAndOffset(Tagged<JSFunction> function,
+                                     Tagged<AbstractCode> code, int code_offset,
+                                     FILE* file, bool print_line_number);
 
   static void PrintTop(Isolate* isolate, FILE* file, bool print_args,
                        bool print_line_number);
 
-  static void CollectFunctionAndOffsetForICStats(JSFunction function,
-                                                 AbstractCode code,
+  static void CollectFunctionAndOffsetForICStats(Tagged<JSFunction> function,
+                                                 Tagged<AbstractCode> code,
                                                  int code_offset);
 
  protected:
@@ -732,7 +733,7 @@ class JavaScriptFrame : public CommonFrameWithJSLinkage {
   virtual void PrintFrameKind(StringStream* accumulator) const {}
 
  private:
-  inline Object function_slot_object() const;
+  inline Tagged<Object> function_slot_object() const;
 
   friend class StackFrameIteratorBase;
 };
@@ -758,7 +759,7 @@ class EntryFrame : public TypedFrame {
  public:
   Type type() const override { return ENTRY; }
 
-  HeapObject unchecked_code() const override;
+  Tagged<HeapObject> unchecked_code() const override;
 
   // Garbage collection support.
   void Iterate(RootVisitor* v) const override;
@@ -787,7 +788,7 @@ class ConstructEntryFrame : public EntryFrame {
  public:
   Type type() const override { return CONSTRUCT_ENTRY; }
 
-  HeapObject unchecked_code() const override;
+  Tagged<HeapObject> unchecked_code() const override;
 
   static ConstructEntryFrame* cast(StackFrame* frame) {
     DCHECK(frame->is_construct_entry());
@@ -839,10 +840,10 @@ class BuiltinExitFrame : public ExitFrame {
  public:
   Type type() const override { return BUILTIN_EXIT; }
 
-  JSFunction function() const;
+  Tagged<JSFunction> function() const;
 
-  Object receiver() const;
-  Object GetParameter(int i) const;
+  Tagged<Object> receiver() const;
+  Tagged<Object> GetParameter(int i) const;
   int ComputeParametersCount() const;
   Handle<FixedArray> GetParameters() const;
 
@@ -859,10 +860,10 @@ class BuiltinExitFrame : public ExitFrame {
   inline explicit BuiltinExitFrame(StackFrameIteratorBase* iterator);
 
  private:
-  inline Object receiver_slot_object() const;
-  inline Object argc_slot_object() const;
-  inline Object target_slot_object() const;
-  inline Object new_target_slot_object() const;
+  inline Tagged<Object> receiver_slot_object() const;
+  inline Tagged<Object> argc_slot_object() const;
+  inline Tagged<Object> target_slot_object() const;
+  inline Tagged<Object> new_target_slot_object() const;
 
   friend class StackFrameIteratorBase;
 };
@@ -877,14 +878,14 @@ class ApiCallbackExitFrame : public ExitFrame {
 
   // ApiCallbackExitFrame might contain either FunctionTemplateInfo or
   // JSFunction in the function slot.
-  HeapObject target() const;
+  Tagged<HeapObject> target() const;
 
   // In case function slot contains FunctionTemplateInfo, instantiate the
   // function, stores it in the function slot and returns JSFunction handle.
   Handle<JSFunction> GetFunction() const;
 
-  Object receiver() const;
-  Object GetParameter(int i) const;
+  Tagged<Object> receiver() const;
+  Tagged<Object> GetParameter(int i) const;
   int ComputeParametersCount() const;
   Handle<FixedArray> GetParameters() const;
 
@@ -906,7 +907,7 @@ class ApiCallbackExitFrame : public ExitFrame {
   inline explicit ApiCallbackExitFrame(StackFrameIteratorBase* iterator);
 
  private:
-  inline void set_target(HeapObject function) const;
+  inline void set_target(Tagged<HeapObject> function) const;
 
   inline FullObjectSlot receiver_slot() const;
   inline FullObjectSlot argc_slot() const;
@@ -921,7 +922,7 @@ class StubFrame : public TypedFrame {
  public:
   Type type() const override { return STUB; }
 
-  HeapObject unchecked_code() const override;
+  Tagged<HeapObject> unchecked_code() const override;
 
   // Lookup exception handler for current {pc}, returns -1 if none found. Only
   // TurboFan stub frames are supported.
@@ -945,7 +946,7 @@ class OptimizedFrame : public JavaScriptFrame {
 
   void Summarize(std::vector<FrameSummary>* frames) const override;
 
-  DeoptimizationData GetDeoptimizationData(int* deopt_index) const;
+  Tagged<DeoptimizationData> GetDeoptimizationData(int* deopt_index) const;
 
   static int StackSlotOffsetRelativeToFp(int slot_index);
 
@@ -953,7 +954,8 @@ class OptimizedFrame : public JavaScriptFrame {
   int LookupExceptionHandlerInTable(
       int* data, HandlerTable::CatchPrediction* prediction) override;
 
-  virtual int FindReturnPCForTrampoline(Code code, int trampoline_pc) const = 0;
+  virtual int FindReturnPCForTrampoline(Tagged<Code> code,
+                                        int trampoline_pc) const = 0;
 
  protected:
   inline explicit OptimizedFrame(StackFrameIteratorBase* iterator);
@@ -975,10 +977,10 @@ class UnoptimizedFrame : public JavaScriptFrame {
   virtual int GetBytecodeOffset() const = 0;
 
   // Returns the frame's current bytecode array.
-  BytecodeArray GetBytecodeArray() const;
+  Tagged<BytecodeArray> GetBytecodeArray() const;
 
   // Access to the interpreter register file for this frame.
-  Object ReadInterpreterRegister(int register_index) const;
+  Tagged<Object> ReadInterpreterRegister(int register_index) const;
 
   // Build a list with summaries for this frame including all inlined frames.
   void Summarize(std::vector<FrameSummary>* frames) const override;
@@ -1010,7 +1012,7 @@ class InterpretedFrame : public UnoptimizedFrame {
 
   // Updates the frame's BytecodeArray with |bytecode_array|. Used by the
   // debugger to swap execution onto a BytecodeArray patched with breakpoints.
-  void PatchBytecodeArray(BytecodeArray bytecode_array);
+  void PatchBytecodeArray(Tagged<BytecodeArray> bytecode_array);
 
   static InterpretedFrame* cast(StackFrame* frame) {
     DCHECK(frame->is_interpreted());
@@ -1039,7 +1041,7 @@ class BaselineFrame : public UnoptimizedFrame {
 
   intptr_t GetPCForBytecodeOffset(int lookup_offset) const;
 
-  void PatchContext(Context value);
+  void PatchContext(Tagged<Context> value);
 
   static BaselineFrame* cast(StackFrame* frame) {
     DCHECK(frame->is_baseline());
@@ -1064,7 +1066,8 @@ class MaglevFrame : public OptimizedFrame {
 
   void Iterate(RootVisitor* v) const override;
 
-  int FindReturnPCForTrampoline(Code code, int trampoline_pc) const override;
+  int FindReturnPCForTrampoline(Tagged<Code> code,
+                                int trampoline_pc) const override;
 
   Handle<JSFunction> GetInnermostFunction() const;
   BytecodeOffset GetBytecodeOffsetForOSR() const;
@@ -1086,7 +1089,8 @@ class TurbofanFrame : public OptimizedFrame {
 
   void Iterate(RootVisitor* v) const override;
 
-  int FindReturnPCForTrampoline(Code code, int trampoline_pc) const override;
+  int FindReturnPCForTrampoline(Tagged<Code> code,
+                                int trampoline_pc) const override;
 
  protected:
   inline explicit TurbofanFrame(StackFrameIteratorBase* iterator);
@@ -1094,7 +1098,7 @@ class TurbofanFrame : public OptimizedFrame {
  private:
   friend class StackFrameIteratorBase;
 
-  Object StackSlotAt(int index) const;
+  Tagged<Object> StackSlotAt(int index) const;
 };
 
 // Builtin frames are built for builtins with JavaScript linkage, such as
@@ -1108,7 +1112,7 @@ class BuiltinFrame final : public TypedFrameWithJSLinkage {
     return static_cast<BuiltinFrame*>(frame);
   }
 
-  JSFunction function() const override;
+  Tagged<JSFunction> function() const override;
   int ComputeParametersCount() const override;
 
  protected:
@@ -1133,14 +1137,14 @@ class WasmFrame : public TypedFrame {
   void Iterate(RootVisitor* v) const override;
 
   // Accessors.
-  virtual V8_EXPORT_PRIVATE WasmInstanceObject wasm_instance() const;
+  virtual V8_EXPORT_PRIVATE Tagged<WasmInstanceObject> wasm_instance() const;
   V8_EXPORT_PRIVATE wasm::NativeModule* native_module() const;
   wasm::WasmCode* wasm_code() const;
   int function_index() const;
-  Script script() const;
+  Tagged<Script> script() const;
   // Byte position in the module, or asm.js source position.
   int position() const override;
-  Object context() const override;
+  Tagged<Object> context() const override;
   bool at_to_number_conversion() const;
   // Generated code byte offset in the function.
   int generated_code_offset() const;
@@ -1158,7 +1162,7 @@ class WasmFrame : public TypedFrame {
 
  private:
   friend class StackFrameIteratorBase;
-  WasmModuleObject module_object() const;
+  Tagged<WasmModuleObject> module_object() const;
 };
 
 class WasmExitFrame : public WasmFrame {
@@ -1200,7 +1204,7 @@ class WasmToJsFrame : public WasmFrame {
   Type type() const override { return WASM_TO_JS; }
 
   int position() const override { return 0; }
-  WasmInstanceObject wasm_instance() const override;
+  Tagged<WasmInstanceObject> wasm_instance() const override;
 
  protected:
   inline explicit WasmToJsFrame(StackFrameIteratorBase* iterator);
@@ -1364,11 +1368,11 @@ class JavaScriptBuiltinContinuationFrame : public TypedFrameWithJSLinkage {
     return static_cast<JavaScriptBuiltinContinuationFrame*>(frame);
   }
 
-  JSFunction function() const override;
+  Tagged<JSFunction> function() const override;
   int ComputeParametersCount() const override;
   intptr_t GetSPToFPDelta() const;
 
-  Object context() const override;
+  Tagged<Object> context() const override;
 
  protected:
   inline explicit JavaScriptBuiltinContinuationFrame(
@@ -1392,7 +1396,7 @@ class JavaScriptBuiltinContinuationWithCatchFrame
 
   // Patch in the exception object at the appropriate location into the stack
   // frame.
-  void SetException(Object exception);
+  void SetException(Tagged<Object> exception);
 
  protected:
   inline explicit JavaScriptBuiltinContinuationWithCatchFrame(
diff --git a/src/execution/futex-emulation.cc b/src/execution/futex-emulation.cc
index 938beb53f0c..4bd05339659 100644
--- a/src/execution/futex-emulation.cc
+++ b/src/execution/futex-emulation.cc
@@ -257,7 +257,7 @@ enum WaitReturnValue : int { kOk = 0, kNotEqualValue = 1, kTimedOut = 2 };
 
 namespace {
 
-Object WaitJsTranslateReturn(Isolate* isolate, Object res) {
+Tagged<Object> WaitJsTranslateReturn(Isolate* isolate, Tagged<Object> res) {
   if (IsSmi(res)) {
     int val = Smi::ToInt(res);
     switch (val) {
@@ -276,42 +276,45 @@ Object WaitJsTranslateReturn(Isolate* isolate, Object res) {
 
 }  // namespace
 
-Object FutexEmulation::WaitJs32(Isolate* isolate, WaitMode mode,
-                                Handle<JSArrayBuffer> array_buffer, size_t addr,
-                                int32_t value, double rel_timeout_ms) {
-  Object res =
+Tagged<Object> FutexEmulation::WaitJs32(Isolate* isolate, WaitMode mode,
+                                        Handle<JSArrayBuffer> array_buffer,
+                                        size_t addr, int32_t value,
+                                        double rel_timeout_ms) {
+  Tagged<Object> res =
       Wait<int32_t>(isolate, mode, array_buffer, addr, value, rel_timeout_ms);
   return WaitJsTranslateReturn(isolate, res);
 }
 
-Object FutexEmulation::WaitJs64(Isolate* isolate, WaitMode mode,
-                                Handle<JSArrayBuffer> array_buffer, size_t addr,
-                                int64_t value, double rel_timeout_ms) {
-  Object res =
+Tagged<Object> FutexEmulation::WaitJs64(Isolate* isolate, WaitMode mode,
+                                        Handle<JSArrayBuffer> array_buffer,
+                                        size_t addr, int64_t value,
+                                        double rel_timeout_ms) {
+  Tagged<Object> res =
       Wait<int64_t>(isolate, mode, array_buffer, addr, value, rel_timeout_ms);
   return WaitJsTranslateReturn(isolate, res);
 }
 
-Object FutexEmulation::WaitWasm32(Isolate* isolate,
-                                  Handle<JSArrayBuffer> array_buffer,
-                                  size_t addr, int32_t value,
-                                  int64_t rel_timeout_ns) {
+Tagged<Object> FutexEmulation::WaitWasm32(Isolate* isolate,
+                                          Handle<JSArrayBuffer> array_buffer,
+                                          size_t addr, int32_t value,
+                                          int64_t rel_timeout_ns) {
   return Wait<int32_t>(isolate, WaitMode::kSync, array_buffer, addr, value,
                        rel_timeout_ns >= 0, rel_timeout_ns, CallType::kIsWasm);
 }
 
-Object FutexEmulation::WaitWasm64(Isolate* isolate,
-                                  Handle<JSArrayBuffer> array_buffer,
-                                  size_t addr, int64_t value,
-                                  int64_t rel_timeout_ns) {
+Tagged<Object> FutexEmulation::WaitWasm64(Isolate* isolate,
+                                          Handle<JSArrayBuffer> array_buffer,
+                                          size_t addr, int64_t value,
+                                          int64_t rel_timeout_ns) {
   return Wait<int64_t>(isolate, WaitMode::kSync, array_buffer, addr, value,
                        rel_timeout_ns >= 0, rel_timeout_ns, CallType::kIsWasm);
 }
 
 template <typename T>
-Object FutexEmulation::Wait(Isolate* isolate, WaitMode mode,
-                            Handle<JSArrayBuffer> array_buffer, size_t addr,
-                            T value, double rel_timeout_ms) {
+Tagged<Object> FutexEmulation::Wait(Isolate* isolate, WaitMode mode,
+                                    Handle<JSArrayBuffer> array_buffer,
+                                    size_t addr, T value,
+                                    double rel_timeout_ms) {
   DCHECK_LT(addr, array_buffer->GetByteLength());
 
   bool use_timeout = rel_timeout_ms != V8_INFINITY;
@@ -344,10 +347,11 @@ double WaitTimeoutInMs(double timeout_ns) {
 }  // namespace
 
 template <typename T>
-Object FutexEmulation::Wait(Isolate* isolate, WaitMode mode,
-                            Handle<JSArrayBuffer> array_buffer, size_t addr,
-                            T value, bool use_timeout, int64_t rel_timeout_ns,
-                            CallType call_type) {
+Tagged<Object> FutexEmulation::Wait(Isolate* isolate, WaitMode mode,
+                                    Handle<JSArrayBuffer> array_buffer,
+                                    size_t addr, T value, bool use_timeout,
+                                    int64_t rel_timeout_ns,
+                                    CallType call_type) {
   if (mode == WaitMode::kSync) {
     return WaitSync(isolate, array_buffer, addr, value, use_timeout,
                     rel_timeout_ns, call_type);
@@ -358,10 +362,11 @@ Object FutexEmulation::Wait(Isolate* isolate, WaitMode mode,
 }
 
 template <typename T>
-Object FutexEmulation::WaitSync(Isolate* isolate,
-                                Handle<JSArrayBuffer> array_buffer, size_t addr,
-                                T value, bool use_timeout,
-                                int64_t rel_timeout_ns, CallType call_type) {
+Tagged<Object> FutexEmulation::WaitSync(Isolate* isolate,
+                                        Handle<JSArrayBuffer> array_buffer,
+                                        size_t addr, T value, bool use_timeout,
+                                        int64_t rel_timeout_ns,
+                                        CallType call_type) {
   VMState<ATOMICS_WAIT> state(isolate);
   base::TimeDelta rel_timeout =
       base::TimeDelta::FromNanoseconds(rel_timeout_ns);
@@ -444,7 +449,8 @@ Object FutexEmulation::WaitSync(Isolate* isolate,
       // notification will wake up the condition variable. node->waiting() will
       // be false, so we'll loop and then check interrupts.
       if (interrupted) {
-        Object interrupt_object = isolate->stack_guard()->HandleInterrupts();
+        Tagged<Object> interrupt_object =
+            isolate->stack_guard()->HandleInterrupts();
         if (IsException(interrupt_object, isolate)) {
           result = handle(interrupt_object, isolate);
           callback_result = AtomicsWaitEvent::kTerminatedExecution;
@@ -528,10 +534,11 @@ FutexWaitListNode::FutexWaitListNode(
 }
 
 template <typename T>
-Object FutexEmulation::WaitAsync(Isolate* isolate,
-                                 Handle<JSArrayBuffer> array_buffer,
-                                 size_t addr, T value, bool use_timeout,
-                                 int64_t rel_timeout_ns, CallType call_type) {
+Tagged<Object> FutexEmulation::WaitAsync(Isolate* isolate,
+                                         Handle<JSArrayBuffer> array_buffer,
+                                         size_t addr, T value, bool use_timeout,
+                                         int64_t rel_timeout_ns,
+                                         CallType call_type) {
   base::TimeDelta rel_timeout =
       base::TimeDelta::FromNanoseconds(rel_timeout_ns);
 
@@ -650,8 +657,8 @@ Object FutexEmulation::WaitAsync(Isolate* isolate,
   return *result;
 }
 
-Object FutexEmulation::Wake(Handle<JSArrayBuffer> array_buffer, size_t addr,
-                            uint32_t num_waiters_to_wake) {
+Tagged<Object> FutexEmulation::Wake(Handle<JSArrayBuffer> array_buffer,
+                                    size_t addr, uint32_t num_waiters_to_wake) {
   DCHECK_LT(addr, array_buffer->GetByteLength());
 
   int waiters_woken = 0;
@@ -913,8 +920,8 @@ void FutexEmulation::IsolateDeinit(Isolate* isolate) {
   g_wait_list.Pointer()->Verify();
 }
 
-Object FutexEmulation::NumWaitersForTesting(Handle<JSArrayBuffer> array_buffer,
-                                            size_t addr) {
+Tagged<Object> FutexEmulation::NumWaitersForTesting(
+    Handle<JSArrayBuffer> array_buffer, size_t addr) {
   DCHECK_LT(addr, array_buffer->GetByteLength());
   std::shared_ptr<BackingStore> backing_store = array_buffer->GetBackingStore();
 
@@ -941,7 +948,7 @@ Object FutexEmulation::NumWaitersForTesting(Handle<JSArrayBuffer> array_buffer,
   return Smi::FromInt(waiters);
 }
 
-Object FutexEmulation::NumAsyncWaitersForTesting(Isolate* isolate) {
+Tagged<Object> FutexEmulation::NumAsyncWaitersForTesting(Isolate* isolate) {
   NoGarbageCollectionMutexGuard lock_guard(g_mutex.Pointer());
 
   int waiters = 0;
@@ -958,7 +965,7 @@ Object FutexEmulation::NumAsyncWaitersForTesting(Isolate* isolate) {
   return Smi::FromInt(waiters);
 }
 
-Object FutexEmulation::NumUnresolvedAsyncPromisesForTesting(
+Tagged<Object> FutexEmulation::NumUnresolvedAsyncPromisesForTesting(
     Handle<JSArrayBuffer> array_buffer, size_t addr) {
   DCHECK_LT(addr, array_buffer->GetByteLength());
   std::shared_ptr<BackingStore> backing_store = array_buffer->GetBackingStore();
diff --git a/src/execution/futex-emulation.h b/src/execution/futex-emulation.h
index 844dc366f92..156b9c00be1 100644
--- a/src/execution/futex-emulation.h
+++ b/src/execution/futex-emulation.h
@@ -148,53 +148,53 @@ class FutexEmulation : public AllStatic {
   // |rel_timeout_ms| can be Infinity.
   // If woken, return "ok", otherwise return "timed-out". The initial check and
   // the decision to wait happen atomically.
-  static Object WaitJs32(Isolate* isolate, WaitMode mode,
-                         Handle<JSArrayBuffer> array_buffer, size_t addr,
-                         int32_t value, double rel_timeout_ms);
+  static Tagged<Object> WaitJs32(Isolate* isolate, WaitMode mode,
+                                 Handle<JSArrayBuffer> array_buffer,
+                                 size_t addr, int32_t value,
+                                 double rel_timeout_ms);
 
   // An version of WaitJs32 for int64_t values.
-  static Object WaitJs64(Isolate* isolate, WaitMode mode,
-                         Handle<JSArrayBuffer> array_buffer, size_t addr,
-                         int64_t value, double rel_timeout_ms);
+  static Tagged<Object> WaitJs64(Isolate* isolate, WaitMode mode,
+                                 Handle<JSArrayBuffer> array_buffer,
+                                 size_t addr, int64_t value,
+                                 double rel_timeout_ms);
 
   // Same as WaitJs above except it returns 0 (ok), 1 (not equal) and 2 (timed
   // out) as expected by Wasm.
-  V8_EXPORT_PRIVATE static Object WaitWasm32(Isolate* isolate,
-                                             Handle<JSArrayBuffer> array_buffer,
-                                             size_t addr, int32_t value,
-                                             int64_t rel_timeout_ns);
+  V8_EXPORT_PRIVATE static Tagged<Object> WaitWasm32(
+      Isolate* isolate, Handle<JSArrayBuffer> array_buffer, size_t addr,
+      int32_t value, int64_t rel_timeout_ns);
 
   // Same as Wait32 above except it checks for an int64_t value in the
   // array_buffer.
-  V8_EXPORT_PRIVATE static Object WaitWasm64(Isolate* isolate,
-                                             Handle<JSArrayBuffer> array_buffer,
-                                             size_t addr, int64_t value,
-                                             int64_t rel_timeout_ns);
+  V8_EXPORT_PRIVATE static Tagged<Object> WaitWasm64(
+      Isolate* isolate, Handle<JSArrayBuffer> array_buffer, size_t addr,
+      int64_t value, int64_t rel_timeout_ns);
 
   // Wake |num_waiters_to_wake| threads that are waiting on the given |addr|.
   // |num_waiters_to_wake| can be kWakeAll, in which case all waiters are
   // woken. The rest of the waiters will continue to wait. The return value is
   // the number of woken waiters.
-  V8_EXPORT_PRIVATE static Object Wake(Handle<JSArrayBuffer> array_buffer,
-                                       size_t addr,
-                                       uint32_t num_waiters_to_wake);
+  V8_EXPORT_PRIVATE static Tagged<Object> Wake(
+      Handle<JSArrayBuffer> array_buffer, size_t addr,
+      uint32_t num_waiters_to_wake);
 
   // Called before |isolate| dies. Removes async waiters owned by |isolate|.
   static void IsolateDeinit(Isolate* isolate);
 
   // Return the number of threads or async waiters waiting on |addr|. Should
   // only be used for testing.
-  static Object NumWaitersForTesting(Handle<JSArrayBuffer> array_buffer,
-                                     size_t addr);
+  static Tagged<Object> NumWaitersForTesting(Handle<JSArrayBuffer> array_buffer,
+                                             size_t addr);
 
   // Return the number of async waiters (which belong to |isolate|) waiting.
   // Should only be used for testing.
-  static Object NumAsyncWaitersForTesting(Isolate* isolate);
+  static Tagged<Object> NumAsyncWaitersForTesting(Isolate* isolate);
 
   // Return the number of async waiters which were waiting for |addr| and are
   // now waiting for the Promises to be resolved. Should only be used for
   // testing.
-  static Object NumUnresolvedAsyncPromisesForTesting(
+  static Tagged<Object> NumUnresolvedAsyncPromisesForTesting(
       Handle<JSArrayBuffer> array_buffer, size_t addr);
 
  private:
@@ -204,25 +204,27 @@ class FutexEmulation : public AllStatic {
   friend class AsyncWaiterTimeoutTask;
 
   template <typename T>
-  static Object Wait(Isolate* isolate, WaitMode mode,
-                     Handle<JSArrayBuffer> array_buffer, size_t addr, T value,
-                     double rel_timeout_ms);
+  static Tagged<Object> Wait(Isolate* isolate, WaitMode mode,
+                             Handle<JSArrayBuffer> array_buffer, size_t addr,
+                             T value, double rel_timeout_ms);
 
   template <typename T>
-  static Object Wait(Isolate* isolate, WaitMode mode,
-                     Handle<JSArrayBuffer> array_buffer, size_t addr, T value,
-                     bool use_timeout, int64_t rel_timeout_ns,
-                     CallType call_type = CallType::kIsNotWasm);
+  static Tagged<Object> Wait(Isolate* isolate, WaitMode mode,
+                             Handle<JSArrayBuffer> array_buffer, size_t addr,
+                             T value, bool use_timeout, int64_t rel_timeout_ns,
+                             CallType call_type = CallType::kIsNotWasm);
 
   template <typename T>
-  static Object WaitSync(Isolate* isolate, Handle<JSArrayBuffer> array_buffer,
-                         size_t addr, T value, bool use_timeout,
-                         int64_t rel_timeout_ns, CallType call_type);
+  static Tagged<Object> WaitSync(Isolate* isolate,
+                                 Handle<JSArrayBuffer> array_buffer,
+                                 size_t addr, T value, bool use_timeout,
+                                 int64_t rel_timeout_ns, CallType call_type);
 
   template <typename T>
-  static Object WaitAsync(Isolate* isolate, Handle<JSArrayBuffer> array_buffer,
-                          size_t addr, T value, bool use_timeout,
-                          int64_t rel_timeout_ns, CallType call_type);
+  static Tagged<Object> WaitAsync(Isolate* isolate,
+                                  Handle<JSArrayBuffer> array_buffer,
+                                  size_t addr, T value, bool use_timeout,
+                                  int64_t rel_timeout_ns, CallType call_type);
 
   // Resolve the Promises of the async waiters which belong to |isolate|.
   static void ResolveAsyncWaiterPromises(Isolate* isolate);
diff --git a/src/execution/isolate-inl.h b/src/execution/isolate-inl.h
index 07721dda57a..d956e5f088c 100644
--- a/src/execution/isolate-inl.h
+++ b/src/execution/isolate-inl.h
@@ -41,7 +41,7 @@ V8_INLINE Isolate* Isolate::Current() {
 
 bool Isolate::IsCurrent() const { return this == TryGetCurrent(); }
 
-void Isolate::set_context(Context context) {
+void Isolate::set_context(Tagged<Context> context) {
   DCHECK(context.is_null() || IsContext(context));
   thread_local_top()->context_ = context;
 }
@@ -51,17 +51,17 @@ Handle<NativeContext> Isolate::native_context() {
   return handle(context()->native_context(), this);
 }
 
-NativeContext Isolate::raw_native_context() {
+Tagged<NativeContext> Isolate::raw_native_context() {
   DCHECK(!context().is_null());
   return context()->native_context();
 }
 
-void Isolate::set_pending_message(Object message_obj) {
+void Isolate::set_pending_message(Tagged<Object> message_obj) {
   DCHECK(IsTheHole(message_obj, this) || IsJSMessageObject(message_obj));
   thread_local_top()->pending_message_ = message_obj;
 }
 
-Object Isolate::pending_message() {
+Tagged<Object> Isolate::pending_message() {
   return thread_local_top()->pending_message_;
 }
 
@@ -73,13 +73,13 @@ bool Isolate::has_pending_message() {
   return !IsTheHole(pending_message(), this);
 }
 
-Object Isolate::pending_exception() {
+Tagged<Object> Isolate::pending_exception() {
   CHECK(has_pending_exception());
   DCHECK(!IsException(thread_local_top()->pending_exception_, this));
   return thread_local_top()->pending_exception_;
 }
 
-void Isolate::set_pending_exception(Object exception_obj) {
+void Isolate::set_pending_exception(Tagged<Object> exception_obj) {
   DCHECK(!IsException(exception_obj, this));
   thread_local_top()->pending_exception_ = exception_obj;
 }
@@ -94,7 +94,7 @@ bool Isolate::has_pending_exception() {
   return !IsTheHole(thread_local_top()->pending_exception_, this);
 }
 
-Object Isolate::scheduled_exception() {
+Tagged<Object> Isolate::scheduled_exception() {
   DCHECK(has_scheduled_exception());
   DCHECK(!IsException(thread_local_top()->scheduled_exception_, this));
   return thread_local_top()->scheduled_exception_;
@@ -111,7 +111,7 @@ void Isolate::clear_scheduled_exception() {
   set_scheduled_exception(ReadOnlyRoots(this).the_hole_value());
 }
 
-void Isolate::set_scheduled_exception(Object exception) {
+void Isolate::set_scheduled_exception(Tagged<Object> exception) {
   thread_local_top()->scheduled_exception_ = exception;
 }
 
@@ -126,7 +126,7 @@ bool Isolate::is_execution_terminating() {
 }
 
 #ifdef DEBUG
-Object Isolate::VerifyBuiltinsResult(Object result) {
+Tagged<Object> Isolate::VerifyBuiltinsResult(Tagged<Object> result) {
   DCHECK_EQ(has_pending_exception(), result == ReadOnlyRoots(this).exception());
 #ifdef V8_COMPRESS_POINTERS
   // Check that the returned pointer is actually part of the current isolate,
@@ -162,11 +162,11 @@ ObjectPair Isolate::VerifyBuiltinsResult(ObjectPair pair) {
 }
 #endif  // DEBUG
 
-bool Isolate::is_catchable_by_javascript(Object exception) {
+bool Isolate::is_catchable_by_javascript(Tagged<Object> exception) {
   return exception != ReadOnlyRoots(heap()).termination_exception();
 }
 
-bool Isolate::is_catchable_by_wasm(Object exception) {
+bool Isolate::is_catchable_by_wasm(Tagged<Object> exception) {
   if (!is_catchable_by_javascript(exception)) return false;
   if (!IsJSObject(exception)) return true;
   // We don't allocate, but the LookupIterator interface expects a handle.
@@ -200,7 +200,7 @@ Isolate::ExceptionScope::~ExceptionScope() {
   isolate_->set_pending_exception(*pending_exception_);
 }
 
-bool Isolate::IsAnyInitialArrayPrototype(JSArray array) {
+bool Isolate::IsAnyInitialArrayPrototype(Tagged<JSArray> array) {
   DisallowGarbageCollection no_gc;
   return IsInAnyContext(array, Context::INITIAL_ARRAY_PROTOTYPE_INDEX);
 }
diff --git a/src/execution/isolate-utils-inl.h b/src/execution/isolate-utils-inl.h
index f378dfe3815..c3eafef7ca8 100644
--- a/src/execution/isolate-utils-inl.h
+++ b/src/execution/isolate-utils-inl.h
@@ -13,7 +13,7 @@
 namespace v8 {
 namespace internal {
 
-V8_INLINE Heap* GetHeapFromWritableObject(HeapObject object) {
+V8_INLINE Heap* GetHeapFromWritableObject(Tagged<HeapObject> object) {
   // Avoid using the below GetIsolateFromWritableObject because we want to be
   // able to get the heap, but not the isolate, for off-thread objects.
 
@@ -26,7 +26,7 @@ V8_INLINE Heap* GetHeapFromWritableObject(HeapObject object) {
 #endif  // V8_ENABLE_THIRD_PARTY_HEAP
 }
 
-V8_INLINE Isolate* GetIsolateFromWritableObject(HeapObject object) {
+V8_INLINE Isolate* GetIsolateFromWritableObject(Tagged<HeapObject> object) {
 #ifdef V8_ENABLE_THIRD_PARTY_HEAP
   return Heap::GetIsolateFromWritableObject(object);
 #else
@@ -34,7 +34,8 @@ V8_INLINE Isolate* GetIsolateFromWritableObject(HeapObject object) {
 #endif  // V8_ENABLE_THIRD_PARTY_HEAP
 }
 
-V8_INLINE bool GetIsolateFromHeapObject(HeapObject object, Isolate** isolate) {
+V8_INLINE bool GetIsolateFromHeapObject(Tagged<HeapObject> object,
+                                        Isolate** isolate) {
 #ifdef V8_ENABLE_THIRD_PARTY_HEAP
   *isolate = Heap::GetIsolateFromWritableObject(object);
   return true;
@@ -52,7 +53,7 @@ V8_INLINE bool GetIsolateFromHeapObject(HeapObject object, Isolate** isolate) {
 
 // Use this function instead of Internals::GetIsolateForSandbox for internal
 // code, as this function is fully inlinable.
-V8_INLINE static Isolate* GetIsolateForSandbox(HeapObject object) {
+V8_INLINE static Isolate* GetIsolateForSandbox(Tagged<HeapObject> object) {
 #ifdef V8_ENABLE_SANDBOX
   return GetIsolateFromWritableObject(object);
 #else
diff --git a/src/execution/isolate-utils.h b/src/execution/isolate-utils.h
index c41788d9452..c7176d233d4 100644
--- a/src/execution/isolate-utils.h
+++ b/src/execution/isolate-utils.h
@@ -15,16 +15,17 @@ namespace internal {
 // computation of cage base inside trivial accessors for optimizing value
 // decompression. When pointer compression is disabled this function always
 // returns nullptr.
-V8_INLINE PtrComprCageBase GetPtrComprCageBase(HeapObject object);
+V8_INLINE PtrComprCageBase GetPtrComprCageBase(Tagged<HeapObject> object);
 
-V8_INLINE Heap* GetHeapFromWritableObject(HeapObject object);
+V8_INLINE Heap* GetHeapFromWritableObject(Tagged<HeapObject> object);
 
-V8_INLINE Isolate* GetIsolateFromWritableObject(HeapObject object);
+V8_INLINE Isolate* GetIsolateFromWritableObject(Tagged<HeapObject> object);
 
 // Returns true if it succeeded to obtain isolate from given object.
 // If it fails then the object is definitely a read-only object but it may also
 // succeed for read only objects if pointer compression is enabled.
-V8_INLINE bool GetIsolateFromHeapObject(HeapObject object, Isolate** isolate);
+V8_INLINE bool GetIsolateFromHeapObject(Tagged<HeapObject> object,
+                                        Isolate** isolate);
 
 }  // namespace internal
 }  // namespace v8
diff --git a/src/execution/isolate.cc b/src/execution/isolate.cc
index 6a4ffa2fdb3..faab70f63a4 100644
--- a/src/execution/isolate.cc
+++ b/src/execution/isolate.cc
@@ -957,7 +957,8 @@ bool GetStackTraceLimit(Isolate* isolate, int* result) {
   return true;
 }
 
-bool IsBuiltinFunction(Isolate* isolate, HeapObject object, Builtin builtin) {
+bool IsBuiltinFunction(Isolate* isolate, Tagged<HeapObject> object,
+                       Builtin builtin) {
   if (!IsJSFunction(object)) return false;
   JSFunction const function = JSFunction::cast(object);
   return function->code() == isolate->builtins()->code(builtin);
@@ -1340,7 +1341,7 @@ Address Isolate::GetAbstractPC(int* line, int* column) {
   SharedFunctionInfo::EnsureSourcePositionsAvailable(this, shared);
   int position = frame->position();
 
-  Object maybe_script = frame->function()->shared()->script();
+  Tagged<Object> maybe_script = frame->function()->shared()->script();
   if (IsScript(maybe_script)) {
     Handle<Script> script(Script::cast(maybe_script), this);
     Script::PositionInfo info;
@@ -1544,7 +1545,7 @@ bool Isolate::MayAccess(Handle<NativeContext> accessing_context,
     DisallowGarbageCollection no_gc;
 
     if (IsJSGlobalProxy(*receiver)) {
-      Object receiver_context =
+      Tagged<Object> receiver_context =
           JSGlobalProxy::cast(*receiver)->native_context();
       if (!IsContext(receiver_context)) return false;
 
@@ -1563,7 +1564,7 @@ bool Isolate::MayAccess(Handle<NativeContext> accessing_context,
     DisallowGarbageCollection no_gc;
     AccessCheckInfo access_check_info = AccessCheckInfo::Get(this, receiver);
     if (access_check_info.is_null()) return false;
-    Object fun_obj = access_check_info->callback();
+    Tagged<Object> fun_obj = access_check_info->callback();
     callback = v8::ToCData<v8::AccessCheckCallback>(fun_obj);
     data = handle(access_check_info->data(), this);
   }
@@ -1576,7 +1577,7 @@ bool Isolate::MayAccess(Handle<NativeContext> accessing_context,
   }
 }
 
-Object Isolate::StackOverflow() {
+Tagged<Object> Isolate::StackOverflow() {
   // Whoever calls this method should not have overflown the stack limit by too
   // much. Otherwise we risk actually running out of stack space.
   // We allow for up to 8kB overflow, because we typically allow up to 4KB
@@ -1649,7 +1650,7 @@ Tagged<Object> Isolate::ThrowAt(Handle<JSObject> exception,
   return ThrowInternal(*exception, location);
 }
 
-Object Isolate::TerminateExecution() {
+Tagged<Object> Isolate::TerminateExecution() {
   return Throw(ReadOnlyRoots(this).termination_exception());
 }
 
@@ -1797,7 +1798,8 @@ Handle<JSMessageObject> Isolate::CreateMessageOrAbort(
   return message_obj;
 }
 
-Object Isolate::ThrowInternal(Object raw_exception, MessageLocation* location) {
+Tagged<Object> Isolate::ThrowInternal(Tagged<Object> raw_exception,
+                                      MessageLocation* location) {
   DCHECK(!has_pending_exception());
   IF_WASM(DCHECK_IMPLIES, trap_handler::IsTrapHandlerEnabled(),
           !trap_handler::IsThreadInWasm());
@@ -1858,7 +1860,8 @@ Object Isolate::ThrowInternal(Object raw_exception, MessageLocation* location) {
 
   // Notify debugger of exception.
   if (is_catchable_by_javascript(raw_exception)) {
-    base::Optional<Object> maybe_exception = debug()->OnThrow(exception);
+    base::Optional<Tagged<Object>> maybe_exception =
+        debug()->OnThrow(exception);
     if (maybe_exception.has_value()) {
       return *maybe_exception;
     }
@@ -1887,7 +1890,7 @@ Object Isolate::ThrowInternal(Object raw_exception, MessageLocation* location) {
   return ReadOnlyRoots(heap()).exception();
 }
 
-Object Isolate::ReThrow(Object exception) {
+Tagged<Object> Isolate::ReThrow(Tagged<Object> exception) {
   DCHECK(!has_pending_exception());
 
   // Set the exception being re-thrown.
@@ -1895,7 +1898,8 @@ Object Isolate::ReThrow(Object exception) {
   return ReadOnlyRoots(heap()).exception();
 }
 
-Object Isolate::ReThrow(Object exception, Object message) {
+Tagged<Object> Isolate::ReThrow(Tagged<Object> exception,
+                                Tagged<Object> message) {
   DCHECK(!has_pending_exception());
   DCHECK(!has_pending_message());
 
@@ -1926,7 +1930,7 @@ class SetThreadInWasmFlagScope {
 #endif  // V8_ENABLE_WEBASSEMBLY
 }  // namespace
 
-Object Isolate::UnwindAndFindHandler() {
+Tagged<Object> Isolate::UnwindAndFindHandler() {
   // TODO(v8:12676): Fix gcmole failures in this function.
   DisableGCMole no_gcmole;
   DisallowGarbageCollection no_gc;
@@ -1939,7 +1943,7 @@ Object Isolate::UnwindAndFindHandler() {
   // handler handles such non-wasm exceptions.
   SetThreadInWasmFlagScope set_thread_in_wasm_flag_scope;
 #endif  // V8_ENABLE_WEBASSEMBLY
-  Object exception = pending_exception();
+  Tagged<Object> exception = pending_exception();
 
   auto FoundHandler = [&](Context context, Address instruction_start,
                           intptr_t handler_offset,
@@ -1981,7 +1985,7 @@ Object Isolate::UnwindAndFindHandler() {
 
 #if V8_ENABLE_WEBASSEMBLY
   // Iterate the chain of stack segments for wasm stack switching.
-  WasmContinuationObject current_stack;
+  Tagged<WasmContinuationObject> current_stack;
   if (v8_flags.experimental_wasm_stack_switching) {
     current_stack =
         WasmContinuationObject::cast(root(RootIndex::kActiveContinuation));
@@ -2125,9 +2129,9 @@ Object Isolate::UnwindAndFindHandler() {
       case StackFrame::WASM_TO_JS:
         if (v8_flags.experimental_wasm_stack_switching) {
           // Decrement the Wasm-to-JS counter.
-          Object suspender_obj = root(RootIndex::kActiveSuspender);
+          Tagged<Object> suspender_obj = root(RootIndex::kActiveSuspender);
           if (!IsUndefined(suspender_obj)) {
-            WasmSuspenderObject suspender =
+            Tagged<WasmSuspenderObject> suspender =
                 WasmSuspenderObject::cast(suspender_obj);
             int wasm_to_js_counter = suspender->wasm_to_js_counter();
             DCHECK_LT(0, wasm_to_js_counter);
@@ -2434,7 +2438,7 @@ Tagged<Object> Isolate::ThrowIllegalOperation() {
   return Throw(ReadOnlyRoots(heap()).illegal_access_string());
 }
 
-void Isolate::ScheduleThrow(Object exception) {
+void Isolate::ScheduleThrow(Tagged<Object> exception) {
   // When scheduling a throw we first throw the exception to get the
   // error reporting if it is uncaught before rescheduling it.
   Throw(exception);
@@ -2452,7 +2456,7 @@ void Isolate::RestorePendingMessageFromTryCatch(v8::TryCatch* handler) {
   DCHECK(handler->HasCaught());
   DCHECK(handler->rethrow_);
   DCHECK(handler->capture_message_);
-  Object message(reinterpret_cast<Address>(handler->message_obj_));
+  Tagged<Object> message(reinterpret_cast<Address>(handler->message_obj_));
   DCHECK(IsJSMessageObject(message) || IsTheHole(message, this));
   set_pending_message(message);
 }
@@ -2479,8 +2483,8 @@ void Isolate::CancelScheduledExceptionFromTryCatch(v8::TryCatch* handler) {
   }
 }
 
-Object Isolate::PromoteScheduledException() {
-  Object thrown = scheduled_exception();
+Tagged<Object> Isolate::PromoteScheduledException() {
+  Tagged<Object> thrown = scheduled_exception();
   clear_scheduled_exception();
   // Re-throw the exception to avoid getting repeated error reporting.
   return ReThrow(thrown);
@@ -2644,7 +2648,7 @@ Handle<JSMessageObject> Isolate::CreateMessageFromException(
 }
 
 Isolate::ExceptionHandlerType Isolate::TopExceptionHandlerType(
-    Object exception) {
+    Tagged<Object> exception) {
   DCHECK_NE(ReadOnlyRoots(heap()).the_hole_value(), exception);
 
   Address js_handler = Isolate::handler(thread_local_top());
@@ -2691,7 +2695,7 @@ void Isolate::ReportPendingMessages() {
   // The embedder might run script in response to an exception.
   AllowJavascriptExecutionDebugOnly allow_script(this);
 
-  Object exception_obj = pending_exception();
+  Tagged<Object> exception_obj = pending_exception();
   ExceptionHandlerType top_handler = TopExceptionHandlerType(exception_obj);
 
   // Try to propagate the exception to an external v8::TryCatch handler. If
@@ -2702,7 +2706,7 @@ void Isolate::ReportPendingMessages() {
   if (!has_been_propagated) return;
 
   // Clear the pending message object early to avoid endless recursion.
-  Object message_obj = pending_message();
+  Tagged<Object> message_obj = pending_message();
   clear_pending_message();
 
   // For uncatchable exceptions we do nothing. If needed, the exception and the
@@ -3206,7 +3210,7 @@ void Isolate::SyncStackLimit() {
 }
 
 void Isolate::RecordStackSwitchForScanning() {
-  Object current = root(RootIndex::kActiveContinuation);
+  Tagged<Object> current = root(RootIndex::kActiveContinuation);
   DCHECK(!IsUndefined(current));
   stack().ClearStackSegments();
   wasm::StackMemory* wasm_stack =
@@ -3941,7 +3945,7 @@ void Isolate::SetTerminationOnExternalTryCatch() {
 
 bool Isolate::PropagatePendingExceptionToExternalTryCatch(
     ExceptionHandlerType top_handler) {
-  Object exception = pending_exception();
+  Tagged<Object> exception = pending_exception();
 
   if (top_handler == ExceptionHandlerType::kJavaScriptHandler) {
     thread_local_top()->external_caught_exception_ = false;
@@ -4299,7 +4303,7 @@ void Isolate::VerifyStaticRoots() {
   // such that the static map range is restored (consult static-roots.h for a
   // sorted list of addresses) or remove the offending entry from the list.
   for (auto idx = RootIndex::kFirstRoot; idx <= RootIndex::kLastRoot; ++idx) {
-    Object obj = roots_table().slot(idx).load(this);
+    Tagged<Object> obj = roots_table().slot(idx).load(this);
     if (obj.ptr() == kNullAddress || !IsMap(obj)) continue;
     Map map = Map::cast(obj);
 
@@ -4996,7 +5000,7 @@ bool Isolate::NeedsSourcePositions() const {
       is_profiling() || debug_->is_active() || v8_file_logger_->is_logging();
 }
 
-void Isolate::SetFeedbackVectorsForProfilingTools(Object value) {
+void Isolate::SetFeedbackVectorsForProfilingTools(Tagged<Object> value) {
   DCHECK(IsUndefined(value, this) || IsArrayList(value));
   heap()->set_feedback_vectors_for_profiling_tools(value);
 }
@@ -5013,8 +5017,8 @@ void Isolate::MaybeInitializeVectorListFromHeap() {
 
   {
     HeapObjectIterator heap_iterator(heap());
-    for (HeapObject current_obj = heap_iterator.Next(); !current_obj.is_null();
-         current_obj = heap_iterator.Next()) {
+    for (Tagged<HeapObject> current_obj = heap_iterator.Next();
+         !current_obj.is_null(); current_obj = heap_iterator.Next()) {
       if (!IsFeedbackVector(current_obj)) continue;
 
       FeedbackVector vector = FeedbackVector::cast(current_obj);
@@ -5042,8 +5046,8 @@ void Isolate::set_date_cache(DateCache* date_cache) {
 }
 
 Isolate::KnownPrototype Isolate::IsArrayOrObjectOrStringPrototype(
-    Object object) {
-  Object context = heap()->native_contexts_list();
+    Tagged<Object> object) {
+  Tagged<Object> context = heap()->native_contexts_list();
   while (!IsUndefined(context, this)) {
     Context current_context = Context::cast(context);
     if (current_context->initial_object_prototype() == object) {
@@ -5058,9 +5062,9 @@ Isolate::KnownPrototype Isolate::IsArrayOrObjectOrStringPrototype(
   return KnownPrototype::kNone;
 }
 
-bool Isolate::IsInAnyContext(Object object, uint32_t index) {
+bool Isolate::IsInAnyContext(Tagged<Object> object, uint32_t index) {
   DisallowGarbageCollection no_gc;
-  Object context = heap()->native_contexts_list();
+  Tagged<Object> context = heap()->native_contexts_list();
   while (!IsUndefined(context, this)) {
     Context current_context = Context::cast(context);
     if (current_context->get(index) == object) {
@@ -5984,13 +5988,13 @@ void Isolate::CollectSourcePositionsForAllBytecodeArrays() {
   std::vector<Handle<SharedFunctionInfo>> sfis;
   {
     HeapObjectIterator iterator(heap());
-    for (HeapObject obj = iterator.Next(); !obj.is_null();
+    for (Tagged<HeapObject> obj = iterator.Next(); !obj.is_null();
          obj = iterator.Next()) {
       if (!IsSharedFunctionInfo(obj)) continue;
       SharedFunctionInfo sfi = SharedFunctionInfo::cast(obj);
       // If the script is a Smi, then the SharedFunctionInfo is in
       // the process of being deserialized.
-      Object script = sfi->raw_script(kAcquireLoad);
+      Tagged<Object> script = sfi->raw_script(kAcquireLoad);
       if (IsSmi(script)) {
         DCHECK_EQ(script, Smi::uninitialized_deserialization_value());
         continue;
@@ -6128,7 +6132,7 @@ SaveContext::~SaveContext() {
 }
 
 SaveAndSwitchContext::SaveAndSwitchContext(Isolate* isolate,
-                                           Context new_context)
+                                           Tagged<Context> new_context)
     : SaveContext(isolate) {
   isolate->set_context(new_context);
 }
@@ -6377,14 +6381,14 @@ void Isolate::LocalsBlockListCacheSet(Handle<ScopeInfo> scope_info,
   heap()->set_locals_block_list_cache(*cache);
 }
 
-Object Isolate::LocalsBlockListCacheGet(Handle<ScopeInfo> scope_info) {
+Tagged<Object> Isolate::LocalsBlockListCacheGet(Handle<ScopeInfo> scope_info) {
   DisallowGarbageCollection no_gc;
 
   if (!IsEphemeronHashTable(heap()->locals_block_list_cache())) {
     return ReadOnlyRoots(this).the_hole_value();
   }
 
-  Object maybe_value =
+  Tagged<Object> maybe_value =
       EphemeronHashTable::cast(heap()->locals_block_list_cache())
           ->Lookup(scope_info);
   if (IsTuple2(maybe_value)) return Tuple2::cast(maybe_value)->value2();
diff --git a/src/execution/isolate.h b/src/execution/isolate.h
index 595f70a3086..0ead18153df 100644
--- a/src/execution/isolate.h
+++ b/src/execution/isolate.h
@@ -761,8 +761,8 @@ class V8_EXPORT_PRIVATE Isolate final : private HiddenFactory {
   Address get_address_from_id(IsolateAddressId id);
 
   // Access to top context (where the current function object was created).
-  Context context() const { return thread_local_top()->context_; }
-  inline void set_context(Context context);
+  Tagged<Context> context() const { return thread_local_top()->context_; }
+  inline void set_context(Tagged<Context> context);
   Context* context_address() { return &thread_local_top()->context_; }
 
   // Access to current thread id.
@@ -802,25 +802,25 @@ class V8_EXPORT_PRIVATE Isolate final : private HiddenFactory {
 
   // Interface to pending exception.
   THREAD_LOCAL_TOP_ADDRESS(Object, pending_exception)
-  inline Object pending_exception();
-  inline void set_pending_exception(Object exception_obj);
+  inline Tagged<Object> pending_exception();
+  inline void set_pending_exception(Tagged<Object> exception_obj);
   inline void clear_pending_exception();
   inline bool has_pending_exception();
 
   THREAD_LOCAL_TOP_ADDRESS(Object, pending_message)
   inline void clear_pending_message();
-  inline Object pending_message();
+  inline Tagged<Object> pending_message();
   inline bool has_pending_message();
-  inline void set_pending_message(Object message_obj);
+  inline void set_pending_message(Tagged<Object> message_obj);
 
   THREAD_LOCAL_TOP_ADDRESS(Object, scheduled_exception)
-  inline Object scheduled_exception();
+  inline Tagged<Object> scheduled_exception();
   inline bool has_scheduled_exception();
   inline void clear_scheduled_exception();
-  inline void set_scheduled_exception(Object exception);
+  inline void set_scheduled_exception(Tagged<Object> exception);
 
 #ifdef DEBUG
-  inline Object VerifyBuiltinsResult(Object result);
+  inline Tagged<Object> VerifyBuiltinsResult(Tagged<Object> result);
   inline ObjectPair VerifyBuiltinsResult(ObjectPair pair);
 #endif
 
@@ -830,10 +830,10 @@ class V8_EXPORT_PRIVATE Isolate final : private HiddenFactory {
     kNone
   };
 
-  ExceptionHandlerType TopExceptionHandlerType(Object exception);
+  ExceptionHandlerType TopExceptionHandlerType(Tagged<Object> exception);
 
-  inline bool is_catchable_by_javascript(Object exception);
-  inline bool is_catchable_by_wasm(Object exception);
+  inline bool is_catchable_by_javascript(Tagged<Object> exception);
+  inline bool is_catchable_by_wasm(Tagged<Object> exception);
   inline bool is_execution_terminating();
   inline bool is_execution_termination_pending();
 
@@ -977,7 +977,7 @@ class V8_EXPORT_PRIVATE Isolate final : private HiddenFactory {
 
   // Exception throwing support. The caller should use the result
   // of Throw() as its return value.
-  Tagged<Object> Throw(Object exception) {
+  Tagged<Object> Throw(Tagged<Object> exception) {
     return ThrowInternal(exception, nullptr);
   }
   Tagged<Object> ThrowAt(Handle<JSObject> exception, MessageLocation* location);
@@ -1022,12 +1022,12 @@ class V8_EXPORT_PRIVATE Isolate final : private HiddenFactory {
   // reporting was handled when the exception was thrown originally.
   // The first overload doesn't set the corresponding pending message, which
   // has to be set separately or be guaranteed to not have changed.
-  Object ReThrow(Object exception);
-  Object ReThrow(Object exception, Object message);
+  Tagged<Object> ReThrow(Tagged<Object> exception);
+  Tagged<Object> ReThrow(Tagged<Object> exception, Tagged<Object> message);
 
   // Find the correct handler for the current pending exception. This also
   // clears and returns the current pending exception.
-  Object UnwindAndFindHandler();
+  Tagged<Object> UnwindAndFindHandler();
 
   // Tries to predict whether an exception will be caught. Note that this can
   // only produce an estimate, because it is undecidable whether a finally
@@ -1041,7 +1041,7 @@ class V8_EXPORT_PRIVATE Isolate final : private HiddenFactory {
   };
   CatchType PredictExceptionCatcher();
 
-  void ScheduleThrow(Object exception);
+  void ScheduleThrow(Tagged<Object> exception);
   // Re-set pending message, script and positions reported to the TryCatch
   // back to the TLS for re-use when rethrowing.
   void RestorePendingMessageFromTryCatch(v8::TryCatch* handler);
@@ -1050,7 +1050,7 @@ class V8_EXPORT_PRIVATE Isolate final : private HiddenFactory {
   void ReportPendingMessages();
 
   // Promote a scheduled exception to pending. Asserts has_scheduled_exception.
-  Object PromoteScheduledException();
+  Tagged<Object> PromoteScheduledException();
 
   // Attempts to compute the current source location, storing the
   // result in the target out parameter. The source location is attached to a
@@ -1074,8 +1074,8 @@ class V8_EXPORT_PRIVATE Isolate final : private HiddenFactory {
   Handle<JSMessageObject> CreateMessageFromException(Handle<Object> exception);
 
   // Out of resource exception helpers.
-  Object StackOverflow();
-  Object TerminateExecution();
+  Tagged<Object> StackOverflow();
+  Tagged<Object> TerminateExecution();
   void CancelTerminateExecution();
 
   void RequestInterrupt(InterruptCallback callback, void* data);
@@ -1091,7 +1091,7 @@ class V8_EXPORT_PRIVATE Isolate final : private HiddenFactory {
 
   // Returns the current native context.
   inline Handle<NativeContext> native_context();
-  inline NativeContext raw_native_context();
+  inline Tagged<NativeContext> raw_native_context();
 
   Handle<NativeContext> GetIncumbentContext();
 
@@ -1230,7 +1230,9 @@ class V8_EXPORT_PRIVATE Isolate final : private HiddenFactory {
                                sizeof(IsolateData));
   }
 
-  Object root(RootIndex index) const { return Object(roots_table()[index]); }
+  Tagged<Object> root(RootIndex index) const {
+    return Object(roots_table()[index]);
+  }
 
   Handle<Object> root_handle(RootIndex index) {
     return Handle<Object>(&roots_table()[index]);
@@ -1493,7 +1495,7 @@ class V8_EXPORT_PRIVATE Isolate final : private HiddenFactory {
   // needed anymore. This keeps many feedback vectors alive, but code
   // coverage or type profile are used for debugging only and increase in
   // memory usage is expected.
-  void SetFeedbackVectorsForProfilingTools(Object value);
+  void SetFeedbackVectorsForProfilingTools(Tagged<Object> value);
 
   void MaybeInitializeVectorListFromHeap();
 
@@ -1533,7 +1535,7 @@ class V8_EXPORT_PRIVATE Isolate final : private HiddenFactory {
 
   enum class KnownPrototype { kNone, kObject, kArray, kString };
 
-  KnownPrototype IsArrayOrObjectOrStringPrototype(Object object);
+  KnownPrototype IsArrayOrObjectOrStringPrototype(Tagged<Object> object);
 
   // On intent to set an element in object, make sure that appropriate
   // notifications occur if the set is on the elements of the array or
@@ -1555,7 +1557,7 @@ class V8_EXPORT_PRIVATE Isolate final : private HiddenFactory {
   }
 
   // Returns true if array is the initial array prototype in any native context.
-  inline bool IsAnyInitialArrayPrototype(JSArray array);
+  inline bool IsAnyInitialArrayPrototype(Tagged<JSArray> array);
 
   std::unique_ptr<PersistentHandles> NewPersistentHandles();
 
@@ -1841,7 +1843,7 @@ class V8_EXPORT_PRIVATE Isolate final : private HiddenFactory {
     return lazy_compile_dispatcher_.get();
   }
 
-  bool IsInAnyContext(Object object, uint32_t index);
+  bool IsInAnyContext(Tagged<Object> object, uint32_t index);
 
   void ClearKeptObjects();
 
@@ -2081,7 +2083,7 @@ class V8_EXPORT_PRIVATE Isolate final : private HiddenFactory {
                                Handle<ScopeInfo> outer_scope_info,
                                Handle<StringSet> locals_blocklist);
   // Returns either `TheHole` or `StringSet`.
-  Object LocalsBlockListCacheGet(Handle<ScopeInfo> scope_info);
+  Tagged<Object> LocalsBlockListCacheGet(Handle<ScopeInfo> scope_info);
 
   void VerifyStaticRoots();
 
@@ -2211,7 +2213,8 @@ class V8_EXPORT_PRIVATE Isolate final : private HiddenFactory {
   void AddCrashKeysForIsolateAndHeapPointers();
 
   // Returns the Exception sentinel.
-  Object ThrowInternal(Object exception, MessageLocation* location);
+  Tagged<Object> ThrowInternal(Tagged<Object> exception,
+                               MessageLocation* location);
 
   // This class contains a collection of data accessible from both C++ runtime
   // and compiled code (including assembly stubs, builtins, interpreter bytecode
@@ -2620,7 +2623,7 @@ class V8_EXPORT_PRIVATE SaveContext {
 // constructor.
 class V8_EXPORT_PRIVATE SaveAndSwitchContext : public SaveContext {
  public:
-  SaveAndSwitchContext(Isolate* isolate, Context new_context);
+  SaveAndSwitchContext(Isolate* isolate, Tagged<Context> new_context);
 };
 
 // A scope which sets the given isolate's context to null for its lifetime to
diff --git a/src/execution/local-isolate-inl.h b/src/execution/local-isolate-inl.h
index f06f8e10580..c23aa483d0d 100644
--- a/src/execution/local-isolate-inl.h
+++ b/src/execution/local-isolate-inl.h
@@ -22,7 +22,7 @@ ReadOnlyHeap* LocalIsolate::read_only_heap() const {
   return isolate_->read_only_heap();
 }
 
-Object LocalIsolate::root(RootIndex index) const {
+Tagged<Object> LocalIsolate::root(RootIndex index) const {
   DCHECK(RootsTable::IsImmortalImmovable(index));
   return isolate_->root(index);
 }
diff --git a/src/execution/local-isolate.h b/src/execution/local-isolate.h
index d703c5443d1..b9cdec9e703 100644
--- a/src/execution/local-isolate.h
+++ b/src/execution/local-isolate.h
@@ -61,7 +61,7 @@ class V8_EXPORT_PRIVATE LocalIsolate final : private HiddenLocalFactory {
   inline Address cage_base() const;
   inline Address code_cage_base() const;
   inline ReadOnlyHeap* read_only_heap() const;
-  inline Object root(RootIndex index) const;
+  inline Tagged<Object> root(RootIndex index) const;
   inline Handle<Object> root_handle(RootIndex index) const;
 
   base::RandomNumberGenerator* fuzzer_rng() const {
diff --git a/src/execution/messages.cc b/src/execution/messages.cc
index 00ac78f7524..c599e83b00f 100644
--- a/src/execution/messages.cc
+++ b/src/execution/messages.cc
@@ -117,7 +117,7 @@ void MessageHandler::ReportMessage(Isolate* isolate, const MessageLocation* loc,
   // and ignore scheduled exceptions callbacks can throw.
 
   // We pass the exception object into the message handler callback though.
-  Object exception_object = ReadOnlyRoots(isolate).undefined_value();
+  Tagged<Object> exception_object = ReadOnlyRoots(isolate).undefined_value();
   if (isolate->has_pending_exception()) {
     exception_object = isolate->pending_exception();
   }
@@ -175,8 +175,8 @@ void MessageHandler::ReportMessageNoExceptions(
     for (int i = 0; i < global_length; i++) {
       HandleScope scope(isolate);
       if (IsUndefined(global_listeners->Get(i), isolate)) continue;
-      FixedArray listener = FixedArray::cast(global_listeners->Get(i));
-      Foreign callback_obj = Foreign::cast(listener->get(0));
+      Tagged<FixedArray> listener = FixedArray::cast(global_listeners->Get(i));
+      Tagged<Foreign> callback_obj = Foreign::cast(listener->get(0));
       int32_t message_levels =
           static_cast<int32_t>(Smi::ToInt(listener->get(2)));
       if (!(message_levels & error_level)) {
@@ -835,8 +835,9 @@ Handle<JSObject> ErrorUtils::NewIteratorError(Isolate* isolate,
   return isolate->factory()->NewTypeError(id, callsite);
 }
 
-Object ErrorUtils::ThrowSpreadArgError(Isolate* isolate, MessageTemplate id,
-                                       Handle<Object> object) {
+Tagged<Object> ErrorUtils::ThrowSpreadArgError(Isolate* isolate,
+                                               MessageTemplate id,
+                                               Handle<Object> object) {
   MessageLocation location;
   Handle<String> callsite;
   if (ComputeLocation(isolate, &location)) {
@@ -890,9 +891,8 @@ Handle<JSObject> ErrorUtils::NewConstructedNonConstructable(
   return isolate->factory()->NewTypeError(id, callsite);
 }
 
-Object ErrorUtils::ThrowLoadFromNullOrUndefined(Isolate* isolate,
-                                                Handle<Object> object,
-                                                MaybeHandle<Object> key) {
+Tagged<Object> ErrorUtils::ThrowLoadFromNullOrUndefined(
+    Isolate* isolate, Handle<Object> object, MaybeHandle<Object> key) {
   DCHECK(IsNullOrUndefined(*object));
 
   MaybeHandle<String> maybe_property_name;
diff --git a/src/execution/messages.h b/src/execution/messages.h
index 34031971764..1a9921f777b 100644
--- a/src/execution/messages.h
+++ b/src/execution/messages.h
@@ -106,12 +106,13 @@ class ErrorUtils : public AllStatic {
   static Handle<JSObject> NewConstructedNonConstructable(Isolate* isolate,
                                                          Handle<Object> source);
   // Returns the Exception sentinel.
-  static Object ThrowSpreadArgError(Isolate* isolate, MessageTemplate id,
-                                    Handle<Object> object);
+  static Tagged<Object> ThrowSpreadArgError(Isolate* isolate,
+                                            MessageTemplate id,
+                                            Handle<Object> object);
   // Returns the Exception sentinel.
-  static Object ThrowLoadFromNullOrUndefined(Isolate* isolate,
-                                             Handle<Object> object,
-                                             MaybeHandle<Object> key);
+  static Tagged<Object> ThrowLoadFromNullOrUndefined(Isolate* isolate,
+                                                     Handle<Object> object,
+                                                     MaybeHandle<Object> key);
 
   // Returns true if given object has own |error_stack_symbol| property.
   static bool HasErrorStackSymbolOwnProperty(Isolate* isolate,
diff --git a/src/execution/microtask-queue.cc b/src/execution/microtask-queue.cc
index 0adb8f1d5eb..290780fa181 100644
--- a/src/execution/microtask-queue.cc
+++ b/src/execution/microtask-queue.cc
@@ -71,7 +71,7 @@ MicrotaskQueue::~MicrotaskQueue() {
 Address MicrotaskQueue::CallEnqueueMicrotask(Isolate* isolate,
                                              intptr_t microtask_queue_pointer,
                                              Address raw_microtask) {
-  Microtask microtask = Microtask::cast(Object(raw_microtask));
+  Tagged<Microtask> microtask = Microtask::cast(Object(raw_microtask));
   reinterpret_cast<MicrotaskQueue*>(microtask_queue_pointer)
       ->EnqueueMicrotask(microtask);
   return Smi::zero().ptr();
@@ -97,7 +97,7 @@ void MicrotaskQueue::EnqueueMicrotask(v8::Isolate* v8_isolate,
   EnqueueMicrotask(*microtask);
 }
 
-void MicrotaskQueue::EnqueueMicrotask(Microtask microtask) {
+void MicrotaskQueue::EnqueueMicrotask(Tagged<Microtask> microtask) {
   if (size_ == capacity_) {
     // Keep the capacity of |ring_buffer_| power of 2, so that the JIT
     // implementation can calculate the modulo easily.
@@ -252,9 +252,9 @@ void MicrotaskQueue::OnCompleted(Isolate* isolate) const {
   }
 }
 
-Microtask MicrotaskQueue::get(intptr_t index) const {
+Tagged<Microtask> MicrotaskQueue::get(intptr_t index) const {
   DCHECK_LT(index, size_);
-  Object microtask(ring_buffer_[(index + start_) % capacity_]);
+  Tagged<Object> microtask(ring_buffer_[(index + start_) % capacity_]);
   return Microtask::cast(microtask);
 }
 
diff --git a/src/execution/microtask-queue.h b/src/execution/microtask-queue.h
index 6091fa3575c..8ef62824d47 100644
--- a/src/execution/microtask-queue.h
+++ b/src/execution/microtask-queue.h
@@ -21,6 +21,8 @@ class Isolate;
 class Microtask;
 class Object;
 class RootVisitor;
+template <typename T>
+class Tagged;
 
 class V8_EXPORT_PRIVATE MicrotaskQueue final : public v8::MicrotaskQueue {
  public:
@@ -51,7 +53,7 @@ class V8_EXPORT_PRIVATE MicrotaskQueue final : public v8::MicrotaskQueue {
            !HasMicrotasksSuppressions();
   }
 
-  void EnqueueMicrotask(Microtask microtask);
+  void EnqueueMicrotask(Tagged<Microtask> microtask);
   void AddMicrotasksCompletedCallback(
       MicrotasksCompletedCallbackWithData callback, void* data) override;
   void RemoveMicrotasksCompletedCallback(
@@ -100,7 +102,7 @@ class V8_EXPORT_PRIVATE MicrotaskQueue final : public v8::MicrotaskQueue {
   intptr_t size() const { return size_; }
   intptr_t start() const { return start_; }
 
-  Microtask get(intptr_t index) const;
+  Tagged<Microtask> get(intptr_t index) const;
 
   MicrotaskQueue* next() const { return next_; }
   MicrotaskQueue* prev() const { return prev_; }
diff --git a/src/execution/simulator.h b/src/execution/simulator.h
index 403e34b7d31..07495a148ec 100644
--- a/src/execution/simulator.h
+++ b/src/execution/simulator.h
@@ -134,7 +134,7 @@ class GeneratedCode {
     return GeneratedCode(isolate, reinterpret_cast<Signature*>(buffer));
   }
 
-  static GeneratedCode FromCode(Isolate* isolate, Code code) {
+  static GeneratedCode FromCode(Isolate* isolate, Tagged<Code> code) {
     return FromAddress(isolate, code->instruction_start());
   }
 
diff --git a/src/execution/stack-guard.cc b/src/execution/stack-guard.cc
index 92c6ee224c1..563916a4e96 100644
--- a/src/execution/stack-guard.cc
+++ b/src/execution/stack-guard.cc
@@ -278,7 +278,7 @@ class V8_NODISCARD ShouldBeZeroOnReturnScope final {
 
 }  // namespace
 
-Object StackGuard::HandleInterrupts(InterruptLevel level) {
+Tagged<Object> StackGuard::HandleInterrupts(InterruptLevel level) {
   TRACE_EVENT0("v8.execute", "V8.HandleInterrupts");
 
 #if DEBUG
diff --git a/src/execution/stack-guard.h b/src/execution/stack-guard.h
index 59ba84ee5fd..1f1fcc190ee 100644
--- a/src/execution/stack-guard.h
+++ b/src/execution/stack-guard.h
@@ -126,7 +126,8 @@ class V8_EXPORT_PRIVATE V8_NODISCARD StackGuard final {
   // stack overflow, then handle the interruption accordingly.
   // Only interrupts that match the given `InterruptLevel` will be handled,
   // leaving other interrupts pending as if this method had not been called.
-  Object HandleInterrupts(InterruptLevel level = InterruptLevel::kAnyEffect);
+  Tagged<Object> HandleInterrupts(
+      InterruptLevel level = InterruptLevel::kAnyEffect);
 
   // Special case of {HandleInterrupts}: checks for termination requests only.
   // This is guaranteed to never cause GC, so can be used to interrupt
diff --git a/src/execution/tiering-manager.cc b/src/execution/tiering-manager.cc
index 043dd51aa55..387cc1f0960 100644
--- a/src/execution/tiering-manager.cc
+++ b/src/execution/tiering-manager.cc
@@ -92,7 +92,8 @@ static_assert(sizeof(OptimizationDecision) <= kInt32Size);
 
 namespace {
 
-void TraceInOptimizationQueue(JSFunction function, CodeKind current_code_kind) {
+void TraceInOptimizationQueue(Tagged<JSFunction> function,
+                              CodeKind current_code_kind) {
   if (v8_flags.trace_opt_verbose) {
     PrintF("[not marking function %s (%s) for optimization: already queued]\n",
            function->DebugNameCStr().get(),
@@ -100,7 +101,7 @@ void TraceInOptimizationQueue(JSFunction function, CodeKind current_code_kind) {
   }
 }
 
-void TraceHeuristicOptimizationDisallowed(JSFunction function) {
+void TraceHeuristicOptimizationDisallowed(Tagged<JSFunction> function) {
   if (v8_flags.trace_opt_verbose) {
     PrintF(
         "[not marking function %s for optimization: marked with "
@@ -109,7 +110,7 @@ void TraceHeuristicOptimizationDisallowed(JSFunction function) {
   }
 }
 
-void TraceRecompile(Isolate* isolate, JSFunction function,
+void TraceRecompile(Isolate* isolate, Tagged<JSFunction> function,
                     OptimizationDecision d) {
   if (v8_flags.trace_opt) {
     CodeTracer::Scope scope(isolate->GetCodeTracer());
@@ -124,7 +125,7 @@ void TraceRecompile(Isolate* isolate, JSFunction function,
 
 }  // namespace
 
-void TraceManualRecompile(JSFunction function, CodeKind code_kind,
+void TraceManualRecompile(Tagged<JSFunction> function, CodeKind code_kind,
                           ConcurrencyMode concurrency_mode) {
   if (v8_flags.trace_opt) {
     PrintF("[manually marking ");
@@ -134,13 +135,14 @@ void TraceManualRecompile(JSFunction function, CodeKind code_kind,
   }
 }
 
-void TieringManager::Optimize(JSFunction function, OptimizationDecision d) {
+void TieringManager::Optimize(Tagged<JSFunction> function,
+                              OptimizationDecision d) {
   DCHECK(d.should_optimize());
   TraceRecompile(isolate_, function, d);
   function->MarkForOptimization(isolate_, d.code_kind, d.concurrency_mode);
 }
 
-void TieringManager::MarkForTurboFanOptimization(JSFunction function) {
+void TieringManager::MarkForTurboFanOptimization(Tagged<JSFunction> function) {
   Optimize(function, OptimizationDecision::TurbofanHotAndStable());
 }
 
@@ -148,7 +150,7 @@ namespace {
 
 // Returns true when |function| should be enqueued for sparkplug compilation for
 // the first time.
-bool FirstTimeTierUpToSparkplug(Isolate* isolate, JSFunction function) {
+bool FirstTimeTierUpToSparkplug(Isolate* isolate, Tagged<JSFunction> function) {
   return !function->has_feedback_vector() ||
          // We request sparkplug even in the presence of a fbv, if we are
          // running ignition and haven't enqueued the function for sparkplug
@@ -191,7 +193,7 @@ int InterruptBudgetFor(base::Optional<CodeKind> code_kind,
 
 // static
 int TieringManager::InterruptBudgetFor(
-    Isolate* isolate, JSFunction function,
+    Isolate* isolate, Tagged<JSFunction> function,
     base::Optional<CodeKind> override_active_tier) {
   DCHECK(function->shared()->is_compiled());
   const int bytecode_length =
@@ -215,14 +217,15 @@ int TieringManager::InterruptBudgetFor(
 
 namespace {
 
-void TrySetOsrUrgency(Isolate* isolate, JSFunction function, int osr_urgency) {
-  SharedFunctionInfo shared = function->shared();
+void TrySetOsrUrgency(Isolate* isolate, Tagged<JSFunction> function,
+                      int osr_urgency) {
+  Tagged<SharedFunctionInfo> shared = function->shared();
   if (V8_UNLIKELY(!v8_flags.use_osr)) return;
   if (V8_UNLIKELY(shared->optimization_disabled())) return;
 
   // We've passed all checks - bump the OSR urgency.
 
-  FeedbackVector fv = function->feedback_vector();
+  Tagged<FeedbackVector> fv = function->feedback_vector();
   if (V8_UNLIKELY(v8_flags.trace_osr)) {
     CodeTracer::Scope scope(isolate->GetCodeTracer());
     PrintF(scope.file(),
@@ -235,24 +238,25 @@ void TrySetOsrUrgency(Isolate* isolate, JSFunction function, int osr_urgency) {
   fv->set_osr_urgency(osr_urgency);
 }
 
-void TryIncrementOsrUrgency(Isolate* isolate, JSFunction function) {
+void TryIncrementOsrUrgency(Isolate* isolate, Tagged<JSFunction> function) {
   int old_urgency = function->feedback_vector()->osr_urgency();
   int new_urgency = std::min(old_urgency + 1, FeedbackVector::kMaxOsrUrgency);
   TrySetOsrUrgency(isolate, function, new_urgency);
 }
 
-void TryRequestOsrAtNextOpportunity(Isolate* isolate, JSFunction function) {
+void TryRequestOsrAtNextOpportunity(Isolate* isolate,
+                                    Tagged<JSFunction> function) {
   TrySetOsrUrgency(isolate, function, FeedbackVector::kMaxOsrUrgency);
 }
 
 }  // namespace
 
-void TieringManager::RequestOsrAtNextOpportunity(JSFunction function) {
+void TieringManager::RequestOsrAtNextOpportunity(Tagged<JSFunction> function) {
   DisallowGarbageCollection no_gc;
   TryRequestOsrAtNextOpportunity(isolate_, function);
 }
 
-void TieringManager::MaybeOptimizeFrame(JSFunction function,
+void TieringManager::MaybeOptimizeFrame(Tagged<JSFunction> function,
                                         CodeKind current_code_kind) {
   const TieringState tiering_state =
       function->feedback_vector()->tiering_state();
@@ -325,8 +329,8 @@ void TieringManager::MaybeOptimizeFrame(JSFunction function,
 }
 
 OptimizationDecision TieringManager::ShouldOptimize(
-    FeedbackVector feedback_vector, CodeKind current_code_kind) {
-  SharedFunctionInfo shared = feedback_vector->shared_function_info();
+    Tagged<FeedbackVector> feedback_vector, CodeKind current_code_kind) {
+  Tagged<SharedFunctionInfo> shared = feedback_vector->shared_function_info();
   if (TiersUpToMaglev(current_code_kind) &&
       shared->PassesFilter(v8_flags.maglev_filter) &&
       !shared->maglev_compilation_failed()) {
@@ -340,7 +344,7 @@ OptimizationDecision TieringManager::ShouldOptimize(
     return OptimizationDecision::DoNotOptimize();
   }
 
-  BytecodeArray bytecode = shared->GetBytecodeArray(isolate_);
+  Tagged<BytecodeArray> bytecode = shared->GetBytecodeArray(isolate_);
   if (bytecode->length() > v8_flags.max_optimized_bytecode_size) {
     return OptimizationDecision::DoNotOptimize();
   }
@@ -348,7 +352,7 @@ OptimizationDecision TieringManager::ShouldOptimize(
   return OptimizationDecision::TurbofanHotAndStable();
 }
 
-void TieringManager::NotifyICChanged(FeedbackVector vector) {
+void TieringManager::NotifyICChanged(Tagged<FeedbackVector> vector) {
   CodeKind code_kind = vector->has_optimized_code()
                            ? vector->optimized_code()->kind()
                        : vector->shared_function_info()->HasBaselineCode()
@@ -356,9 +360,9 @@ void TieringManager::NotifyICChanged(FeedbackVector vector) {
                            : CodeKind::INTERPRETED_FUNCTION;
   OptimizationDecision decision = ShouldOptimize(vector, code_kind);
   if (decision.should_optimize()) {
-    SharedFunctionInfo shared = vector->shared_function_info();
+    Tagged<SharedFunctionInfo> shared = vector->shared_function_info();
     int bytecode_length = shared->GetBytecodeArray(isolate_)->length();
-    FeedbackCell cell = vector->parent_feedback_cell();
+    Tagged<FeedbackCell> cell = vector->parent_feedback_cell();
     int invocations = v8_flags.minimum_invocations_after_ic_update;
     int bytecodes = std::min(bytecode_length, (kMaxInt >> 1) / invocations);
     int new_budget = invocations * bytecodes;
@@ -456,7 +460,7 @@ void TieringManager::OnInterruptTick(Handle<JSFunction> function,
 
   DisallowGarbageCollection no_gc;
   OnInterruptTickScope scope;
-  JSFunction function_obj = *function;
+  Tagged<JSFunction> function_obj = *function;
 
   MaybeOptimizeFrame(function_obj, code_kind);
 
diff --git a/src/execution/tiering-manager.h b/src/execution/tiering-manager.h
index 52ff7aed12b..59a8c746649 100644
--- a/src/execution/tiering-manager.h
+++ b/src/execution/tiering-manager.h
@@ -21,7 +21,7 @@ class OptimizationDecision;
 enum class CodeKind : uint8_t;
 enum class OptimizationReason : uint8_t;
 
-void TraceManualRecompile(JSFunction function, CodeKind code_kind,
+void TraceManualRecompile(Tagged<JSFunction> function, CodeKind code_kind,
                           ConcurrencyMode concurrency_mode);
 
 class TieringManager {
@@ -30,31 +30,31 @@ class TieringManager {
 
   void OnInterruptTick(Handle<JSFunction> function, CodeKind code_kind);
 
-  void NotifyICChanged(FeedbackVector vector);
+  void NotifyICChanged(Tagged<FeedbackVector> vector);
 
   // After this request, the next JumpLoop will perform OSR.
-  void RequestOsrAtNextOpportunity(JSFunction function);
+  void RequestOsrAtNextOpportunity(Tagged<JSFunction> function);
 
   // For use when a JSFunction is available.
   static int InterruptBudgetFor(
-      Isolate* isolate, JSFunction function,
+      Isolate* isolate, Tagged<JSFunction> function,
       base::Optional<CodeKind> override_active_tier = {});
 
-  void MarkForTurboFanOptimization(JSFunction function);
+  void MarkForTurboFanOptimization(Tagged<JSFunction> function);
 
  private:
   // Make the decision whether to optimize the given function, and mark it for
   // optimization if the decision was 'yes'.
   // This function is also responsible for bumping the OSR urgency.
-  void MaybeOptimizeFrame(JSFunction function, CodeKind code_kind);
+  void MaybeOptimizeFrame(Tagged<JSFunction> function, CodeKind code_kind);
 
   // After next tick indicates whether we've precremented the ticks before
   // calling this function, or whether we're pretending that we already got the
   // tick.
-  OptimizationDecision ShouldOptimize(FeedbackVector feedback_vector,
+  OptimizationDecision ShouldOptimize(Tagged<FeedbackVector> feedback_vector,
                                       CodeKind code_kind);
-  void Optimize(JSFunction function, OptimizationDecision decision);
-  void Baseline(JSFunction function, OptimizationReason reason);
+  void Optimize(Tagged<JSFunction> function, OptimizationDecision decision);
+  void Baseline(Tagged<JSFunction> function, OptimizationReason reason);
 
   class V8_NODISCARD OnInterruptTickScope final {
    public:
diff --git a/src/extensions/statistics-extension.cc b/src/extensions/statistics-extension.cc
index 249b70cdaca..a5e2c24e4ca 100644
--- a/src/extensions/statistics-extension.cc
+++ b/src/extensions/statistics-extension.cc
@@ -9,6 +9,7 @@
 #include "src/execution/isolate.h"
 #include "src/heap/heap-inl.h"  // crbug.com/v8/8499
 #include "src/logging/counters.h"
+#include "src/objects/tagged.h"
 #include "src/roots/roots.h"
 
 namespace v8 {
@@ -141,11 +142,11 @@ void StatisticsExtension::GetCounters(
     HeapObjectIterator iterator(
         reinterpret_cast<Isolate*>(info.GetIsolate())->heap());
     DCHECK(!AllowGarbageCollection::IsAllowed());
-    for (HeapObject obj = iterator.Next(); !obj.is_null();
+    for (Tagged<HeapObject> obj = iterator.Next(); !obj.is_null();
          obj = iterator.Next()) {
-      Object maybe_source_positions;
+      Tagged<Object> maybe_source_positions;
       if (IsCode(obj)) {
-        Code code = Code::cast(obj);
+        Tagged<Code> code = Code::cast(obj);
         reloc_info_total += code->relocation_size();
         // Baseline code doesn't have source positions since it uses
         // interpreter code positions.
@@ -158,7 +159,8 @@ void StatisticsExtension::GetCounters(
         continue;
       }
       if (!IsByteArray(maybe_source_positions)) continue;
-      ByteArray source_positions = ByteArray::cast(maybe_source_positions);
+      Tagged<ByteArray> source_positions =
+          ByteArray::cast(maybe_source_positions);
       if (source_positions->length() == 0) continue;
       source_position_table_total += source_positions->Size();
     }
diff --git a/src/handles/global-handles.cc b/src/handles/global-handles.cc
index eeb32cfcd5c..7bf9188e72b 100644
--- a/src/handles/global-handles.cc
+++ b/src/handles/global-handles.cc
@@ -319,7 +319,7 @@ class NodeBase {
   }
 
   // Publishes all internal state to be consumed by other threads.
-  Handle<Object> Publish(Object object) {
+  Handle<Object> Publish(Tagged<Object> object) {
     DCHECK(!AsChild()->IsInUse());
     data_.parameter = nullptr;
     AsChild()->MarkAsUsed();
@@ -335,7 +335,7 @@ class NodeBase {
     DCHECK(!AsChild()->IsInUse());
   }
 
-  Object object() const { return Object(object_); }
+  Tagged<Object> object() const { return Object(object_); }
   FullObjectSlot location() { return FullObjectSlot(&object_); }
   Handle<Object> handle() { return Handle<Object>(&object_); }
   Address raw_object() const { return object_; }
@@ -407,7 +407,8 @@ class NodeBase {
 
 namespace {
 
-void ExtractInternalFields(JSObject jsobject, void** embedder_fields, int len) {
+void ExtractInternalFields(Tagged<JSObject> jsobject, void** embedder_fields,
+                           int len) {
   int field_count = jsobject->GetEmbedderFieldCount();
   Isolate* isolate = GetIsolateForSandbox(jsobject);
   for (int i = 0; i < len; ++i) {
@@ -614,7 +615,7 @@ GlobalHandles::~GlobalHandles() = default;
 namespace {
 
 template <typename NodeType>
-bool NeedsTrackingInYoungNodes(Object value, NodeType* node) {
+bool NeedsTrackingInYoungNodes(Tagged<Object> value, NodeType* node) {
   return ObjectInYoungGeneration(value) && !node->is_in_young_list();
 }
 
@@ -1038,10 +1039,11 @@ void EternalHandles::PostGarbageCollectionProcessing() {
   young_node_indices_.resize(last);
 }
 
-void EternalHandles::Create(Isolate* isolate, Object object, int* index) {
+void EternalHandles::Create(Isolate* isolate, Tagged<Object> object,
+                            int* index) {
   DCHECK_EQ(kInvalidIndex, *index);
   if (object == Object()) return;
-  Object the_hole = ReadOnlyRoots(isolate).the_hole_value();
+  Tagged<Object> the_hole = ReadOnlyRoots(isolate).the_hole_value();
   DCHECK_NE(the_hole, object);
   int block = size_ >> kShift;
   int offset = size_ & kMask;
diff --git a/src/handles/global-handles.h b/src/handles/global-handles.h
index 3781d440e7d..31f91ec5553 100644
--- a/src/handles/global-handles.h
+++ b/src/handles/global-handles.h
@@ -199,7 +199,8 @@ class EternalHandles final {
   EternalHandles& operator=(const EternalHandles&) = delete;
 
   // Create an EternalHandle, overwriting the index.
-  V8_EXPORT_PRIVATE void Create(Isolate* isolate, Object object, int* index);
+  V8_EXPORT_PRIVATE void Create(Isolate* isolate, Tagged<Object> object,
+                                int* index);
 
   // Grab the handle for an existing EternalHandle.
   inline Handle<Object> Get(int index) {
diff --git a/src/handles/maybe-handles-inl.h b/src/handles/maybe-handles-inl.h
index 033f9fbc0f0..85d5a10a6d1 100644
--- a/src/handles/maybe-handles-inl.h
+++ b/src/handles/maybe-handles-inl.h
@@ -22,7 +22,7 @@ MaybeHandle<T>::MaybeHandle(T object, LocalHeap* local_heap)
     : MaybeHandle(handle(object, local_heap)) {}
 
 MaybeObjectHandle::MaybeObjectHandle(MaybeObject object, Isolate* isolate) {
-  HeapObject heap_object;
+  Tagged<HeapObject> heap_object;
   DCHECK(!object->IsCleared());
   if (object->GetHeapObjectIfWeak(&heap_object)) {
     handle_ = handle(heap_object, isolate);
@@ -35,7 +35,7 @@ MaybeObjectHandle::MaybeObjectHandle(MaybeObject object, Isolate* isolate) {
 
 MaybeObjectHandle::MaybeObjectHandle(MaybeObject object,
                                      LocalHeap* local_heap) {
-  HeapObject heap_object;
+  Tagged<HeapObject> heap_object;
   DCHECK(!object->IsCleared());
   if (object->GetHeapObjectIfWeak(&heap_object)) {
     handle_ = handle(heap_object, local_heap);
@@ -49,15 +49,16 @@ MaybeObjectHandle::MaybeObjectHandle(MaybeObject object,
 MaybeObjectHandle::MaybeObjectHandle(Handle<Object> object)
     : reference_type_(HeapObjectReferenceType::STRONG), handle_(object) {}
 
-MaybeObjectHandle::MaybeObjectHandle(Object object, Isolate* isolate)
+MaybeObjectHandle::MaybeObjectHandle(Tagged<Object> object, Isolate* isolate)
     : reference_type_(HeapObjectReferenceType::STRONG),
       handle_(object, isolate) {}
 
-MaybeObjectHandle::MaybeObjectHandle(Object object, LocalHeap* local_heap)
+MaybeObjectHandle::MaybeObjectHandle(Tagged<Object> object,
+                                     LocalHeap* local_heap)
     : reference_type_(HeapObjectReferenceType::STRONG),
       handle_(object, local_heap) {}
 
-MaybeObjectHandle::MaybeObjectHandle(Object object,
+MaybeObjectHandle::MaybeObjectHandle(Tagged<Object> object,
                                      HeapObjectReferenceType reference_type,
                                      Isolate* isolate)
     : reference_type_(reference_type), handle_(handle(object, isolate)) {}
@@ -70,7 +71,8 @@ MaybeObjectHandle MaybeObjectHandle::Weak(Handle<Object> object) {
   return MaybeObjectHandle(object, HeapObjectReferenceType::WEAK);
 }
 
-MaybeObjectHandle MaybeObjectHandle::Weak(Object object, Isolate* isolate) {
+MaybeObjectHandle MaybeObjectHandle::Weak(Tagged<Object> object,
+                                          Isolate* isolate) {
   return MaybeObjectHandle(object, HeapObjectReferenceType::WEAK, isolate);
 }
 
@@ -135,7 +137,7 @@ inline std::ostream& operator<<(std::ostream& os, MaybeDirectHandle<T> handle) {
 
 MaybeObjectDirectHandle::MaybeObjectDirectHandle(MaybeObject object,
                                                  Isolate* isolate) {
-  HeapObject heap_object;
+  Tagged<HeapObject> heap_object;
   DCHECK(!object->IsCleared());
   if (object->GetHeapObjectIfWeak(&heap_object)) {
     handle_ = direct_handle(heap_object, isolate);
@@ -148,7 +150,7 @@ MaybeObjectDirectHandle::MaybeObjectDirectHandle(MaybeObject object,
 
 MaybeObjectDirectHandle::MaybeObjectDirectHandle(MaybeObject object,
                                                  LocalHeap* local_heap) {
-  HeapObject heap_object;
+  Tagged<HeapObject> heap_object;
   DCHECK(!object->IsCleared());
   if (object->GetHeapObjectIfWeak(&heap_object)) {
     handle_ = direct_handle(heap_object, local_heap);
diff --git a/src/handles/maybe-handles.h b/src/handles/maybe-handles.h
index f061173c1e6..b3134b6d757 100644
--- a/src/handles/maybe-handles.h
+++ b/src/handles/maybe-handles.h
@@ -104,12 +104,12 @@ class MaybeObjectHandle {
   inline MaybeObjectHandle()
       : reference_type_(HeapObjectReferenceType::STRONG) {}
   inline MaybeObjectHandle(MaybeObject object, Isolate* isolate);
-  inline MaybeObjectHandle(Object object, Isolate* isolate);
+  inline MaybeObjectHandle(Tagged<Object> object, Isolate* isolate);
   inline MaybeObjectHandle(MaybeObject object, LocalHeap* local_heap);
-  inline MaybeObjectHandle(Object object, LocalHeap* local_heap);
+  inline MaybeObjectHandle(Tagged<Object> object, LocalHeap* local_heap);
   inline explicit MaybeObjectHandle(Handle<Object> object);
 
-  static inline MaybeObjectHandle Weak(Object object, Isolate* isolate);
+  static inline MaybeObjectHandle Weak(Tagged<Object> object, Isolate* isolate);
   static inline MaybeObjectHandle Weak(Handle<Object> object);
 
   inline MaybeObject operator*() const;
@@ -120,7 +120,7 @@ class MaybeObjectHandle {
   bool is_null() const { return handle_.is_null(); }
 
  private:
-  inline MaybeObjectHandle(Object object,
+  inline MaybeObjectHandle(Tagged<Object> object,
                            HeapObjectReferenceType reference_type,
                            Isolate* isolate);
   inline MaybeObjectHandle(Handle<Object> object,
diff --git a/src/handles/shared-object-conveyor-handles.cc b/src/handles/shared-object-conveyor-handles.cc
index f978d6ba4ef..1fa3ce82546 100644
--- a/src/handles/shared-object-conveyor-handles.cc
+++ b/src/handles/shared-object-conveyor-handles.cc
@@ -15,7 +15,8 @@ SharedObjectConveyorHandles::SharedObjectConveyorHandles(Isolate* isolate)
     : persistent_handles_(
           isolate->shared_space_isolate()->NewPersistentHandles()) {}
 
-uint32_t SharedObjectConveyorHandles::Persist(HeapObject shared_object) {
+uint32_t SharedObjectConveyorHandles::Persist(
+    Tagged<HeapObject> shared_object) {
   DCHECK(IsShared(shared_object));
   uint32_t id = static_cast<uint32_t>(shared_objects_.size());
   shared_objects_.push_back(persistent_handles_->NewHandle(shared_object));
diff --git a/src/handles/shared-object-conveyor-handles.h b/src/handles/shared-object-conveyor-handles.h
index f6b50de6c36..073ae1d335a 100644
--- a/src/handles/shared-object-conveyor-handles.h
+++ b/src/handles/shared-object-conveyor-handles.h
@@ -34,13 +34,13 @@ class SharedObjectConveyorHandles {
   SharedObjectConveyorHandles& operator=(const SharedObjectConveyorHandles&) =
       delete;
 
-  uint32_t Persist(HeapObject shared_object);
+  uint32_t Persist(Tagged<HeapObject> shared_object);
 
   bool HasPersisted(uint32_t object_id) const {
     return object_id < shared_objects_.size();
   }
 
-  HeapObject GetPersisted(uint32_t object_id) const {
+  Tagged<HeapObject> GetPersisted(uint32_t object_id) const {
     DCHECK(HasPersisted(object_id));
     return *shared_objects_[object_id];
   }
diff --git a/src/handles/traced-handles.cc b/src/handles/traced-handles.cc
index 98393a8c199..4c99ae927e3 100644
--- a/src/handles/traced-handles.cc
+++ b/src/handles/traced-handles.cc
@@ -110,11 +110,11 @@ class TracedNode final {
     }
   }
   Address raw_object() const { return object_; }
-  Object object() const { return Object(object_); }
+  Tagged<Object> object() const { return Tagged<Object>(object_); }
   Handle<Object> handle() { return Handle<Object>(&object_); }
   FullObjectSlot location() { return FullObjectSlot(&object_); }
 
-  Handle<Object> Publish(Object object, bool needs_young_bit_update,
+  Handle<Object> Publish(Tagged<Object> object, bool needs_young_bit_update,
                          bool needs_black_allocation, bool has_old_host);
   void Release();
 
@@ -152,7 +152,8 @@ TracedNode::TracedNode(IndexType index, IndexType next_free_index)
 }
 
 // Publishes all internal state to be consumed by other threads.
-Handle<Object> TracedNode::Publish(Object object, bool needs_young_bit_update,
+Handle<Object> TracedNode::Publish(Tagged<Object> object,
+                                   bool needs_young_bit_update,
                                    bool needs_black_allocation,
                                    bool has_old_host) {
   DCHECK(!is_in_use());
@@ -545,7 +546,8 @@ class TracedHandlesImpl final {
   TracedNode* AllocateNode();
   void FreeNode(TracedNode*);
 
-  bool NeedsToBeRemembered(Object value, TracedNode* node, Address* slot,
+  bool NeedsToBeRemembered(Tagged<Object> value, TracedNode* node,
+                           Address* slot,
                            GlobalHandleStoreMode store_mode) const;
 
   TracedNodeBlock::OverallList blocks_;
@@ -632,13 +634,13 @@ TracedHandlesImpl::~TracedHandlesImpl() {
 }
 
 namespace {
-bool NeedsTrackingInYoungNodes(Object object, TracedNode* node) {
+bool NeedsTrackingInYoungNodes(Tagged<Object> object, TracedNode* node) {
   return ObjectInYoungGeneration(object) && !node->is_in_young_list();
 }
 }  // namespace
 
 bool TracedHandlesImpl::NeedsToBeRemembered(
-    Object object, TracedNode* node, Address* slot,
+    Tagged<Object> object, TracedNode* node, Address* slot,
     GlobalHandleStoreMode store_mode) const {
   DCHECK(!node->has_old_host());
   if (store_mode == GlobalHandleStoreMode::kInitializingStore) {
@@ -658,7 +660,7 @@ bool TracedHandlesImpl::NeedsToBeRemembered(
 
 Handle<Object> TracedHandlesImpl::Create(Address value, Address* slot,
                                          GlobalHandleStoreMode store_mode) {
-  Object object(value);
+  Tagged<Object> object(value);
   auto* node = AllocateNode();
   bool needs_young_bit_update = false;
   if (NeedsTrackingInYoungNodes(object, node)) {
@@ -717,7 +719,7 @@ void TracedHandlesImpl::Copy(const TracedNode& from_node, Address** to) {
   SetSlotThreadSafe(to, o.location());
 #ifdef VERIFY_HEAP
   if (v8_flags.verify_heap) {
-    Object::ObjectVerify(Object(**to), isolate_);
+    Object::ObjectVerify(Tagged<Object>(**to), isolate_);
   }
 #endif  // VERIFY_HEAP
 }
@@ -1124,8 +1126,8 @@ void TracedHandles::Move(Address** from, Address** to) {
 }
 
 namespace {
-Object MarkObject(Object obj, TracedNode& node,
-                  TracedHandles::MarkMode mark_mode) {
+Tagged<Object> MarkObject(Tagged<Object> obj, TracedNode& node,
+                          TracedHandles::MarkMode mark_mode) {
   if (mark_mode == TracedHandles::MarkMode::kOnlyYoung &&
       !node.is_in_young_list())
     return Smi::zero();
@@ -1140,11 +1142,11 @@ Object MarkObject(Object obj, TracedNode& node,
 }  // namespace
 
 // static
-Object TracedHandles::Mark(Address* location, MarkMode mark_mode) {
+Tagged<Object> TracedHandles::Mark(Address* location, MarkMode mark_mode) {
   // The load synchronizes internal bitfields that are also read atomically
   // from the concurrent marker. The counterpart is `TracedNode::Publish()`.
-  Object object =
-      Object(reinterpret_cast<std::atomic<Address>*>(location)->load(
+  Tagged<Object> object =
+      Tagged<Object>(reinterpret_cast<std::atomic<Address>*>(location)->load(
           std::memory_order_acquire));
   auto* node = TracedNode::FromLocation(location);
   DCHECK(node->is_in_use<AccessMode::ATOMIC>());
@@ -1152,9 +1154,9 @@ Object TracedHandles::Mark(Address* location, MarkMode mark_mode) {
 }
 
 // static
-Object TracedHandles::MarkConservatively(Address* inner_location,
-                                         Address* traced_node_block_base,
-                                         MarkMode mark_mode) {
+Tagged<Object> TracedHandles::MarkConservatively(
+    Address* inner_location, Address* traced_node_block_base,
+    MarkMode mark_mode) {
   // Compute the `TracedNode` address based on its inner pointer.
   const ptrdiff_t delta = reinterpret_cast<uintptr_t>(inner_location) -
                           reinterpret_cast<uintptr_t>(traced_node_block_base);
diff --git a/src/handles/traced-handles.h b/src/handles/traced-handles.h
index b1817e148c1..c5de08b4101 100644
--- a/src/handles/traced-handles.h
+++ b/src/handles/traced-handles.h
@@ -28,10 +28,10 @@ class V8_EXPORT_PRIVATE TracedHandles final {
   static void Copy(const Address* const* from, Address** to);
   static void Move(Address** from, Address** to);
 
-  static Object Mark(Address* location, MarkMode mark_mode);
-  static Object MarkConservatively(Address* inner_location,
-                                   Address* traced_node_block_base,
-                                   MarkMode mark_mode);
+  static Tagged<Object> Mark(Address* location, MarkMode mark_mode);
+  static Tagged<Object> MarkConservatively(Address* inner_location,
+                                           Address* traced_node_block_base,
+                                           MarkMode mark_mode);
 
   explicit TracedHandles(Isolate*);
   ~TracedHandles();
diff --git a/src/heap/allocation-result.h b/src/heap/allocation-result.h
index df723a77d7d..e8fcd9dce24 100644
--- a/src/heap/allocation-result.h
+++ b/src/heap/allocation-result.h
@@ -28,7 +28,7 @@ class AllocationResult final {
  public:
   static AllocationResult Failure() { return AllocationResult(); }
 
-  static AllocationResult FromObject(HeapObject heap_object) {
+  static AllocationResult FromObject(Tagged<HeapObject> heap_object) {
     return AllocationResult(heap_object);
   }
 
@@ -45,12 +45,12 @@ class AllocationResult final {
     return true;
   }
 
-  HeapObject ToObjectChecked() const {
+  Tagged<HeapObject> ToObjectChecked() const {
     CHECK(!IsFailure());
     return HeapObject::cast(object_);
   }
 
-  HeapObject ToObject() const {
+  Tagged<HeapObject> ToObject() const {
     DCHECK(!IsFailure());
     return HeapObject::cast(object_);
   }
@@ -61,7 +61,8 @@ class AllocationResult final {
   }
 
  private:
-  explicit AllocationResult(HeapObject heap_object) : object_(heap_object) {}
+  explicit AllocationResult(Tagged<HeapObject> heap_object)
+      : object_(heap_object) {}
 
   HeapObject object_;
 };
diff --git a/src/heap/array-buffer-sweeper.cc b/src/heap/array-buffer-sweeper.cc
index e26ccf80fd2..0c2839dbd15 100644
--- a/src/heap/array-buffer-sweeper.cc
+++ b/src/heap/array-buffer-sweeper.cc
@@ -246,7 +246,7 @@ void ArrayBufferSweeper::ReleaseAll(ArrayBufferList* list) {
   *list = ArrayBufferList();
 }
 
-void ArrayBufferSweeper::Append(JSArrayBuffer object,
+void ArrayBufferSweeper::Append(Tagged<JSArrayBuffer> object,
                                 ArrayBufferExtension* extension) {
   size_t bytes = extension->accounting_length();
 
@@ -261,7 +261,7 @@ void ArrayBufferSweeper::Append(JSArrayBuffer object,
   IncrementExternalMemoryCounters(bytes);
 }
 
-void ArrayBufferSweeper::Detach(JSArrayBuffer object,
+void ArrayBufferSweeper::Detach(Tagged<JSArrayBuffer> object,
                                 ArrayBufferExtension* extension) {
   // Finish sweeping here first such that the code below is guaranteed to
   // observe the same sweeping state.
diff --git a/src/heap/array-buffer-sweeper.h b/src/heap/array-buffer-sweeper.h
index 59c195c1a06..2acbb2236ba 100644
--- a/src/heap/array-buffer-sweeper.h
+++ b/src/heap/array-buffer-sweeper.h
@@ -57,10 +57,10 @@ class ArrayBufferSweeper final {
   void EnsureFinished();
 
   // Track the given ArrayBufferExtension for the given JSArrayBuffer.
-  void Append(JSArrayBuffer object, ArrayBufferExtension* extension);
+  void Append(Tagged<JSArrayBuffer> object, ArrayBufferExtension* extension);
 
   // Detaches an ArrayBufferExtension from a JSArrayBuffer.
-  void Detach(JSArrayBuffer object, ArrayBufferExtension* extension);
+  void Detach(Tagged<JSArrayBuffer> object, ArrayBufferExtension* extension);
 
   const ArrayBufferList& young() const { return young_; }
   const ArrayBufferList& old() const { return old_; }
diff --git a/src/heap/code-stats.cc b/src/heap/code-stats.cc
index 64a17e889d8..f016c695f0e 100644
--- a/src/heap/code-stats.cc
+++ b/src/heap/code-stats.cc
@@ -15,22 +15,23 @@ namespace v8 {
 namespace internal {
 
 // Record code statistics.
-void CodeStatistics::RecordCodeAndMetadataStatistics(HeapObject object,
+void CodeStatistics::RecordCodeAndMetadataStatistics(Tagged<HeapObject> object,
                                                      Isolate* isolate) {
   PtrComprCageBase cage_base(isolate);
   if (IsScript(object, cage_base)) {
-    Script script = Script::cast(object);
+    Tagged<Script> script = Script::cast(object);
     // Log the size of external source code.
-    Object source = script->source(cage_base);
+    Tagged<Object> source = script->source(cage_base);
     if (IsExternalString(source, cage_base)) {
-      ExternalString external_source_string = ExternalString::cast(source);
+      Tagged<ExternalString> external_source_string =
+          ExternalString::cast(source);
       int size = isolate->external_script_source_size();
       size += external_source_string->ExternalPayloadSize();
       isolate->set_external_script_source_size(size);
     }
   } else if (IsAbstractCode(object, cage_base)) {
     // Record code+metadata statistics.
-    AbstractCode abstract_code = AbstractCode::cast(object);
+    Tagged<AbstractCode> abstract_code = AbstractCode::cast(object);
     int size = abstract_code->SizeIncludingMetadata(cage_base);
     if (IsCode(abstract_code, cage_base)) {
       size += isolate->code_and_metadata_size();
@@ -197,7 +198,7 @@ void CodeStatistics::CollectCommentStatistics(Isolate* isolate,
 }
 
 // Collects code comment statistics.
-void CodeStatistics::CollectCodeCommentStatistics(AbstractCode obj,
+void CodeStatistics::CollectCodeCommentStatistics(Tagged<AbstractCode> obj,
                                                   Isolate* isolate) {
   // Bytecode objects do not contain RelocInfo.
   PtrComprCageBase cage_base{isolate};
diff --git a/src/heap/code-stats.h b/src/heap/code-stats.h
index 90c0fff4b8c..66958b50e9b 100644
--- a/src/heap/code-stats.h
+++ b/src/heap/code-stats.h
@@ -14,6 +14,8 @@ class HeapObject;
 class Isolate;
 class OldLargeObjectSpace;
 class PagedSpace;
+template <typename T>
+class Tagged;
 
 class CodeStatistics {
  public:
@@ -33,13 +35,14 @@ class CodeStatistics {
 #endif
 
  private:
-  static void RecordCodeAndMetadataStatistics(HeapObject object,
+  static void RecordCodeAndMetadataStatistics(Tagged<HeapObject> object,
                                               Isolate* isolate);
 
 #ifdef DEBUG
   static void CollectCommentStatistics(Isolate* isolate,
                                        CodeCommentsIterator* it);
-  static void CollectCodeCommentStatistics(AbstractCode obj, Isolate* isolate);
+  static void CollectCodeCommentStatistics(Tagged<AbstractCode> obj,
+                                           Isolate* isolate);
   static void EnterComment(Isolate* isolate, const char* comment, int delta);
   static void ResetCodeStatistics(Isolate* isolate);
 #endif
diff --git a/src/heap/combined-heap.h b/src/heap/combined-heap.h
index 2df13248250..655f90979a5 100644
--- a/src/heap/combined-heap.h
+++ b/src/heap/combined-heap.h
@@ -30,7 +30,7 @@ class V8_EXPORT_PRIVATE CombinedHeapObjectIterator final {
 };
 
 V8_WARN_UNUSED_RESULT inline bool IsValidHeapObject(Heap* heap,
-                                                    HeapObject object) {
+                                                    Tagged<HeapObject> object) {
   if (V8_ENABLE_THIRD_PARTY_HEAP_BOOL) {
     return third_party_heap::Heap::IsValidHeapObject(object);
   }
@@ -39,7 +39,7 @@ V8_WARN_UNUSED_RESULT inline bool IsValidHeapObject(Heap* heap,
 }
 
 V8_WARN_UNUSED_RESULT inline bool IsValidCodeObject(Heap* heap,
-                                                    HeapObject object) {
+                                                    Tagged<HeapObject> object) {
   if (V8_ENABLE_THIRD_PARTY_HEAP_BOOL) {
     return third_party_heap::Heap::IsValidCodeObject(object);
   }
diff --git a/src/heap/concurrent-allocator-inl.h b/src/heap/concurrent-allocator-inl.h
index 2ca61615394..5328899ce39 100644
--- a/src/heap/concurrent-allocator-inl.h
+++ b/src/heap/concurrent-allocator-inl.h
@@ -52,7 +52,8 @@ AllocationResult ConcurrentAllocator::AllocateInLabFastUnaligned(
     return AllocationResult::Failure();
   }
 
-  HeapObject object = HeapObject::FromAddress(lab_.IncrementTop(size_in_bytes));
+  Tagged<HeapObject> object =
+      HeapObject::FromAddress(lab_.IncrementTop(size_in_bytes));
   return AllocationResult::FromObject(object);
 }
 
@@ -67,7 +68,8 @@ AllocationResult ConcurrentAllocator::AllocateInLabFastAligned(
     return AllocationResult::Failure();
   }
 
-  HeapObject object = HeapObject::FromAddress(lab_.IncrementTop(aligned_size));
+  Tagged<HeapObject> object =
+      HeapObject::FromAddress(lab_.IncrementTop(aligned_size));
 
   if (filler_size > 0) {
     object = owning_heap()->PrecedeWithFillerBackground(object, filler_size);
diff --git a/src/heap/concurrent-allocator.cc b/src/heap/concurrent-allocator.cc
index 9c08123d9c3..47fdc61e5c8 100644
--- a/src/heap/concurrent-allocator.cc
+++ b/src/heap/concurrent-allocator.cc
@@ -279,7 +279,7 @@ AllocationResult ConcurrentAllocator::AllocateOutsideLab(
 
   DCHECK_GE(result->second, aligned_size_in_bytes);
 
-  HeapObject object = HeapObject::FromAddress(result->first);
+  Tagged<HeapObject> object = HeapObject::FromAddress(result->first);
   if (requested_filler_size > 0) {
     object = owning_heap()->AlignWithFillerBackground(
         object, size_in_bytes, static_cast<int>(result->second), alignment);
diff --git a/src/heap/concurrent-marking.cc b/src/heap/concurrent-marking.cc
index 4a7df47fee1..4da587bf2d7 100644
--- a/src/heap/concurrent-marking.cc
+++ b/src/heap/concurrent-marking.cc
@@ -74,7 +74,7 @@ class ConcurrentMarkingVisitor final
 
   // Implements ephemeron semantics: Marks value if key is already reachable.
   // Returns true if value was actually marked.
-  bool ProcessEphemeron(HeapObject key, HeapObject value) {
+  bool ProcessEphemeron(Tagged<HeapObject> key, Tagged<HeapObject> value) {
     if (IsMarked(key)) {
       if (TryMark(value)) {
         local_marking_worklists_->Push(value);
@@ -88,7 +88,8 @@ class ConcurrentMarkingVisitor final
   }
 
   template <typename TSlot>
-  void RecordSlot(HeapObject object, TSlot slot, HeapObject target) {
+  void RecordSlot(Tagged<HeapObject> object, TSlot slot,
+                  Tagged<HeapObject> target) {
     MarkCompactCollector::RecordSlot(object, slot, target);
   }
 
@@ -99,8 +100,8 @@ class ConcurrentMarkingVisitor final
   }
 
  private:
-  void RecordRelocSlot(InstructionStream host, RelocInfo* rinfo,
-                       HeapObject target) {
+  void RecordRelocSlot(Tagged<InstructionStream> host, RelocInfo* rinfo,
+                       Tagged<HeapObject> target) {
     if (!MarkCompactCollector::ShouldRecordRelocSlot(host, rinfo, target)) {
       return;
     }
@@ -293,7 +294,7 @@ void ConcurrentMarking::RunMajor(JobDelegate* delegate,
       int objects_processed = 0;
       while (current_marked_bytes < kBytesUntilInterruptCheck &&
              objects_processed < kObjectsUntilInterruptCheck) {
-        HeapObject object;
+        Tagged<HeapObject> object;
         if (!local_marking_worklists.Pop(&object)) {
           done = true;
           break;
@@ -322,7 +323,7 @@ void ConcurrentMarking::RunMajor(JobDelegate* delegate,
             addr == new_large_object) {
           local_marking_worklists.PushOnHold(object);
         } else {
-          Map map = object->map(isolate, kAcquireLoad);
+          Tagged<Map> map = object->map(isolate, kAcquireLoad);
           // The marking worklist should never contain filler objects.
           CHECK(!IsFreeSpaceOrFillerMap(map));
           if (is_per_context_mode) {
@@ -381,7 +382,7 @@ void ConcurrentMarking::RunMajor(JobDelegate* delegate,
 
 namespace {
 
-V8_INLINE bool IsYoungObjectInLab(Heap* heap, HeapObject heap_object) {
+V8_INLINE bool IsYoungObjectInLab(Heap* heap, Tagged<HeapObject> heap_object) {
   // The order of the two loads is important.
   Address new_space_top = heap->new_space()->original_top_acquire();
   Address new_space_limit = heap->new_space()->original_limit_relaxed();
@@ -412,12 +413,12 @@ V8_INLINE size_t MinorMarkingLoopImpl(
     if (delegate->IsJoiningThread()) {
       marking_worklists_local.MergeOnHold();
     }
-    HeapObject heap_object;
+    Tagged<HeapObject> heap_object;
     while (marking_worklists_local.Pop(&heap_object)) {
       if (IsYoungObjectInLab(heap, heap_object)) {
         visitor.marking_worklists_local().PushOnHold(heap_object);
       } else {
-        Map map = heap_object->map(isolate);
+        Tagged<Map> map = heap_object->map(isolate);
         const auto visited_size = visitor.Visit(map, heap_object);
         if (visited_size) {
           current_marked_bytes += visited_size;
diff --git a/src/heap/cppgc-js/cpp-heap.cc b/src/heap/cppgc-js/cpp-heap.cc
index 6a2091409bf..0e60f91a52a 100644
--- a/src/heap/cppgc-js/cpp-heap.cc
+++ b/src/heap/cppgc-js/cpp-heap.cc
@@ -830,7 +830,7 @@ bool CppHeap::FinishConcurrentMarkingIfNeeded() {
   return marker_->JoinConcurrentMarkingIfNeeded();
 }
 
-void CppHeap::WriteBarrier(JSObject js_object) {
+void CppHeap::WriteBarrier(Tagged<JSObject> js_object) {
   DCHECK(js_object->MayHaveEmbedderFields());
   DCHECK_NOT_NULL(isolate()->heap()->mark_compact_collector());
 
diff --git a/src/heap/cppgc-js/cpp-heap.h b/src/heap/cppgc-js/cpp-heap.h
index a293029ec44..5c6079866fb 100644
--- a/src/heap/cppgc-js/cpp-heap.h
+++ b/src/heap/cppgc-js/cpp-heap.h
@@ -146,7 +146,7 @@ class V8_EXPORT_PRIVATE CppHeap final
   void TraceEpilogue();
   void EnterFinalPause(cppgc::EmbedderStackState stack_state);
   bool FinishConcurrentMarkingIfNeeded();
-  void WriteBarrier(JSObject);
+  void WriteBarrier(Tagged<JSObject>);
 
   bool ShouldFinalizeIncrementalMarking() const;
 
@@ -180,7 +180,7 @@ class V8_EXPORT_PRIVATE CppHeap final
   size_t epoch() const override;
 
   V8_INLINE void RememberCrossHeapReferenceIfNeeded(
-      v8::internal::JSObject host_obj, void* value);
+      v8::internal::Tagged<v8::internal::JSObject> host_obj, void* value);
   template <typename F>
   inline void VisitCrossHeapRememberedSetIfNeeded(F f);
   void ResetCrossHeapRememberedSet();
@@ -252,7 +252,7 @@ class V8_EXPORT_PRIVATE CppHeap final
 };
 
 void CppHeap::RememberCrossHeapReferenceIfNeeded(
-    v8::internal::JSObject host_obj, void* value) {
+    v8::internal::Tagged<v8::internal::JSObject> host_obj, void* value) {
   if (!generational_gc_supported()) return;
   DCHECK(isolate_);
   cross_heap_remembered_set_.RememberReferenceIfNeeded(*isolate_, host_obj,
diff --git a/src/heap/cppgc-js/cpp-marking-state-inl.h b/src/heap/cppgc-js/cpp-marking-state-inl.h
index 7652bea05dd..16d95a66948 100644
--- a/src/heap/cppgc-js/cpp-marking-state-inl.h
+++ b/src/heap/cppgc-js/cpp-marking-state-inl.h
@@ -15,7 +15,7 @@ namespace v8 {
 namespace internal {
 
 bool CppMarkingState::ExtractEmbedderDataSnapshot(
-    Map map, JSObject object, EmbedderDataSnapshot& snapshot) {
+    Tagged<Map> map, Tagged<JSObject> object, EmbedderDataSnapshot& snapshot) {
   if (JSObject::GetEmbedderFieldCount(map) < 2) return false;
 
   EmbedderDataSlot::PopulateEmbedderDataSnapshot(
diff --git a/src/heap/cppgc-js/cpp-marking-state.h b/src/heap/cppgc-js/cpp-marking-state.h
index 7aae48a26bb..15260b2f017 100644
--- a/src/heap/cppgc-js/cpp-marking-state.h
+++ b/src/heap/cppgc-js/cpp-marking-state.h
@@ -43,7 +43,8 @@ class CppMarkingState final {
 
   void Publish() { marking_state_.Publish(); }
 
-  inline bool ExtractEmbedderDataSnapshot(Map, JSObject, EmbedderDataSnapshot&);
+  inline bool ExtractEmbedderDataSnapshot(Tagged<Map>, Tagged<JSObject>,
+                                          EmbedderDataSnapshot&);
 
   inline void MarkAndPush(const EmbedderDataSnapshot&);
   inline void MarkAndPush(const EmbedderDataSlot type_slot,
diff --git a/src/heap/cppgc-js/cpp-snapshot.cc b/src/heap/cppgc-js/cpp-snapshot.cc
index 194f8ccce8c..b2316db611e 100644
--- a/src/heap/cppgc-js/cpp-snapshot.cc
+++ b/src/heap/cppgc-js/cpp-snapshot.cc
@@ -369,7 +369,7 @@ void* ExtractEmbedderDataBackref(Isolate* isolate, CppHeap& cpp_heap,
       !JSObject::cast(*v8_object)->MayHaveEmbedderFields())
     return nullptr;
 
-  JSObject js_object = JSObject::cast(*v8_object);
+  Tagged<JSObject> js_object = JSObject::cast(*v8_object);
 
   const auto maybe_info =
       WrappableInfo::From(isolate, js_object, cpp_heap.wrapper_descriptor());
diff --git a/src/heap/cppgc-js/cross-heap-remembered-set.cc b/src/heap/cppgc-js/cross-heap-remembered-set.cc
index 5c70fdeb22a..926e27b4121 100644
--- a/src/heap/cppgc-js/cross-heap-remembered-set.cc
+++ b/src/heap/cppgc-js/cross-heap-remembered-set.cc
@@ -10,9 +10,8 @@
 
 namespace v8::internal {
 
-void CrossHeapRememberedSet::RememberReferenceIfNeeded(Isolate& isolate,
-                                                       JSObject host_obj,
-                                                       void* cppgc_object) {
+void CrossHeapRememberedSet::RememberReferenceIfNeeded(
+    Isolate& isolate, Tagged<JSObject> host_obj, void* cppgc_object) {
   DCHECK_NOT_NULL(cppgc_object);
   // Any in-cage pointer must point to a vaild, not freed cppgc object.
   auto* page =
diff --git a/src/heap/cppgc-js/cross-heap-remembered-set.h b/src/heap/cppgc-js/cross-heap-remembered-set.h
index bd755ef945a..cff3320ff01 100644
--- a/src/heap/cppgc-js/cross-heap-remembered-set.h
+++ b/src/heap/cppgc-js/cross-heap-remembered-set.h
@@ -28,7 +28,7 @@ class V8_EXPORT_PRIVATE CrossHeapRememberedSet final {
   CrossHeapRememberedSet(const CrossHeapRememberedSet&) = delete;
   CrossHeapRememberedSet(CrossHeapRememberedSet&&) = delete;
 
-  void RememberReferenceIfNeeded(Isolate& isolate, JSObject host_obj,
+  void RememberReferenceIfNeeded(Isolate& isolate, Tagged<JSObject> host_obj,
                                  void* cppgc_object);
   void Reset(Isolate& isolate);
 
diff --git a/src/heap/cppgc-js/unified-heap-marking-state-inl.h b/src/heap/cppgc-js/unified-heap-marking-state-inl.h
index ba6d16a7b1d..7d3ba074497 100644
--- a/src/heap/cppgc-js/unified-heap-marking-state-inl.h
+++ b/src/heap/cppgc-js/unified-heap-marking-state-inl.h
@@ -40,13 +40,14 @@ void UnifiedHeapMarkingState::MarkAndPush(
   if (!traced_handle_location) {
     return;
   }
-  Object object = TracedHandles::Mark(traced_handle_location, mark_mode_);
+  Tagged<Object> object =
+      TracedHandles::Mark(traced_handle_location, mark_mode_);
   if (!IsHeapObject(object)) {
     // The embedder is not aware of whether numbers are materialized as heap
     // objects are just passed around as Smis.
     return;
   }
-  HeapObject heap_object = HeapObject::cast(object);
+  Tagged<HeapObject> heap_object = HeapObject::cast(object);
   if (heap_object.InReadOnlySpace()) return;
   if (!ShouldMarkObject(heap_object)) return;
   if (marking_state_->TryMark(heap_object)) {
@@ -57,7 +58,8 @@ void UnifiedHeapMarkingState::MarkAndPush(
   }
 }
 
-bool UnifiedHeapMarkingState::ShouldMarkObject(HeapObject object) const {
+bool UnifiedHeapMarkingState::ShouldMarkObject(
+    Tagged<HeapObject> object) const {
   // Keep up-to-date with MarkCompactCollector::ShouldMarkObject.
   if (V8_LIKELY(!has_shared_space_)) return true;
   if (is_shared_space_isolate_) return true;
diff --git a/src/heap/cppgc-js/unified-heap-marking-state.h b/src/heap/cppgc-js/unified-heap-marking-state.h
index c0f36ef3041..f550bd3dd25 100644
--- a/src/heap/cppgc-js/unified-heap-marking-state.h
+++ b/src/heap/cppgc-js/unified-heap-marking-state.h
@@ -27,7 +27,7 @@ class UnifiedHeapMarkingState final {
   void Update(MarkingWorklists::Local*);
 
   V8_INLINE void MarkAndPush(const TracedReferenceBase&);
-  V8_INLINE bool ShouldMarkObject(HeapObject object) const;
+  V8_INLINE bool ShouldMarkObject(Tagged<HeapObject> object) const;
 
  private:
   Heap* const heap_;
diff --git a/src/heap/cppgc-js/wrappable-info-inl.h b/src/heap/cppgc-js/wrappable-info-inl.h
index e5631c43acc..4f9e0ebf3d3 100644
--- a/src/heap/cppgc-js/wrappable-info-inl.h
+++ b/src/heap/cppgc-js/wrappable-info-inl.h
@@ -14,7 +14,7 @@ namespace v8::internal {
 
 // static
 base::Optional<WrappableInfo> WrappableInfo::From(
-    Isolate* isolate, JSObject wrapper,
+    Isolate* isolate, Tagged<JSObject> wrapper,
     const WrapperDescriptor& wrapper_descriptor) {
   DCHECK(wrapper->MayHaveEmbedderFields());
   return wrapper->GetEmbedderFieldCount() < 2
diff --git a/src/heap/cppgc-js/wrappable-info.h b/src/heap/cppgc-js/wrappable-info.h
index 7a11daa10aa..11292783f5c 100644
--- a/src/heap/cppgc-js/wrappable-info.h
+++ b/src/heap/cppgc-js/wrappable-info.h
@@ -16,7 +16,8 @@ class Isolate;
 
 struct WrappableInfo final {
  public:
-  static V8_INLINE base::Optional<WrappableInfo> From(Isolate*, JSObject,
+  static V8_INLINE base::Optional<WrappableInfo> From(Isolate*,
+                                                      Tagged<JSObject>,
                                                       const WrapperDescriptor&);
   static V8_INLINE base::Optional<WrappableInfo> From(
       Isolate*, const EmbedderDataSlot& type_slot,
diff --git a/src/heap/ephemeron-remembered-set.cc b/src/heap/ephemeron-remembered-set.cc
index 51250edae8e..b019d1a1798 100644
--- a/src/heap/ephemeron-remembered-set.cc
+++ b/src/heap/ephemeron-remembered-set.cc
@@ -9,8 +9,8 @@
 
 namespace v8::internal {
 
-void EphemeronRememberedSet::RecordEphemeronKeyWrite(EphemeronHashTable table,
-                                                     Address slot) {
+void EphemeronRememberedSet::RecordEphemeronKeyWrite(
+    Tagged<EphemeronHashTable> table, Address slot) {
   DCHECK(ObjectInYoungGeneration(HeapObjectSlot(slot).ToHeapObject()));
   int slot_index = EphemeronHashTable::SlotToIndex(table.address(), slot);
   InternalIndex entry = EphemeronHashTable::IndexToEntry(slot_index);
@@ -19,8 +19,8 @@ void EphemeronRememberedSet::RecordEphemeronKeyWrite(EphemeronHashTable table,
   it.first->second.insert(entry.as_int());
 }
 
-void EphemeronRememberedSet::RecordEphemeronKeyWrites(EphemeronHashTable table,
-                                                      IndicesSet indices) {
+void EphemeronRememberedSet::RecordEphemeronKeyWrites(
+    Tagged<EphemeronHashTable> table, IndicesSet indices) {
   base::MutexGuard guard(&insertion_mutex_);
   auto it = tables_.find(table);
   if (it != tables_.end()) {
diff --git a/src/heap/ephemeron-remembered-set.h b/src/heap/ephemeron-remembered-set.h
index b03ee71352d..277337a225c 100644
--- a/src/heap/ephemeron-remembered-set.h
+++ b/src/heap/ephemeron-remembered-set.h
@@ -22,15 +22,17 @@ namespace v8::internal {
 class EphemeronRememberedSet final {
  public:
   static constexpr int kEphemeronTableListSegmentSize = 128;
-  using TableList = ::heap::base::Worklist<EphemeronHashTable,
+  using TableList = ::heap::base::Worklist<Tagged<EphemeronHashTable>,
                                            kEphemeronTableListSegmentSize>;
 
   using IndicesSet = std::unordered_set<int>;
-  using TableMap =
-      std::unordered_map<EphemeronHashTable, IndicesSet, Object::Hasher>;
+  using TableMap = std::unordered_map<Tagged<EphemeronHashTable>, IndicesSet,
+                                      Object::Hasher>;
 
-  void RecordEphemeronKeyWrite(EphemeronHashTable table, Address key_slot);
-  void RecordEphemeronKeyWrites(EphemeronHashTable table, IndicesSet indices);
+  void RecordEphemeronKeyWrite(Tagged<EphemeronHashTable> table,
+                               Address key_slot);
+  void RecordEphemeronKeyWrites(Tagged<EphemeronHashTable> table,
+                                IndicesSet indices);
 
   TableMap* tables() { return &tables_; }
 
diff --git a/src/heap/evacuation-allocator-inl.h b/src/heap/evacuation-allocator-inl.h
index a53017090e3..4e61e577863 100644
--- a/src/heap/evacuation-allocator-inl.h
+++ b/src/heap/evacuation-allocator-inl.h
@@ -34,8 +34,8 @@ AllocationResult EvacuationAllocator::Allocate(AllocationSpace space,
   }
 }
 
-void EvacuationAllocator::FreeLast(AllocationSpace space, HeapObject object,
-                                   int object_size) {
+void EvacuationAllocator::FreeLast(AllocationSpace space,
+                                   Tagged<HeapObject> object, int object_size) {
   object_size = ALIGN_TO_ALLOCATION_ALIGNMENT(object_size);
   switch (space) {
     case NEW_SPACE:
@@ -53,7 +53,7 @@ void EvacuationAllocator::FreeLast(AllocationSpace space, HeapObject object,
   }
 }
 
-void EvacuationAllocator::FreeLastInNewSpace(HeapObject object,
+void EvacuationAllocator::FreeLastInNewSpace(Tagged<HeapObject> object,
                                              int object_size) {
   if (!new_space_lab_.TryFreeLast(object, object_size)) {
     // We couldn't free the last object so we have to write a proper filler.
@@ -62,7 +62,7 @@ void EvacuationAllocator::FreeLastInNewSpace(HeapObject object,
 }
 
 void EvacuationAllocator::FreeLastInCompactionSpace(AllocationSpace space,
-                                                    HeapObject object,
+                                                    Tagged<HeapObject> object,
                                                     int object_size) {
   if (!compaction_spaces_.Get(space)->TryFreeLast(object.address(),
                                                   object_size)) {
diff --git a/src/heap/evacuation-allocator.h b/src/heap/evacuation-allocator.h
index 8503e74889f..e1e1686076d 100644
--- a/src/heap/evacuation-allocator.h
+++ b/src/heap/evacuation-allocator.h
@@ -49,7 +49,7 @@ class EvacuationAllocator {
   inline AllocationResult Allocate(AllocationSpace space, int object_size,
                                    AllocationOrigin origin,
                                    AllocationAlignment alignment);
-  inline void FreeLast(AllocationSpace space, HeapObject object,
+  inline void FreeLast(AllocationSpace space, Tagged<HeapObject> object,
                        int object_size);
 
  private:
@@ -59,9 +59,10 @@ class EvacuationAllocator {
   inline bool NewLocalAllocationBuffer();
   inline AllocationResult AllocateInLAB(int object_size,
                                         AllocationAlignment alignment);
-  inline void FreeLastInNewSpace(HeapObject object, int object_size);
+  inline void FreeLastInNewSpace(Tagged<HeapObject> object, int object_size);
   inline void FreeLastInCompactionSpace(AllocationSpace space,
-                                        HeapObject object, int object_size);
+                                        Tagged<HeapObject> object,
+                                        int object_size);
 
   Heap* const heap_;
   NewSpace* const new_space_;
diff --git a/src/heap/evacuation-verifier-inl.h b/src/heap/evacuation-verifier-inl.h
index a0f0f1a24c5..832b4437967 100644
--- a/src/heap/evacuation-verifier-inl.h
+++ b/src/heap/evacuation-verifier-inl.h
@@ -14,14 +14,14 @@ namespace internal {
 
 #ifdef VERIFY_HEAP
 
-void EvacuationVerifier::VerifyHeapObjectImpl(HeapObject heap_object) {
+void EvacuationVerifier::VerifyHeapObjectImpl(Tagged<HeapObject> heap_object) {
   if (!ShouldVerifyObject(heap_object)) return;
   CHECK_IMPLIES(Heap::InYoungGeneration(heap_object),
                 Heap::InToPage(heap_object));
   CHECK(!MarkCompactCollector::IsOnEvacuationCandidate(heap_object));
 }
 
-bool EvacuationVerifier::ShouldVerifyObject(HeapObject heap_object) {
+bool EvacuationVerifier::ShouldVerifyObject(Tagged<HeapObject> heap_object) {
   const bool in_shared_heap = heap_object.InWritableSharedSpace();
   return heap_->isolate()->is_shared_space_isolate() ? in_shared_heap
                                                      : !in_shared_heap;
@@ -31,7 +31,7 @@ template <typename TSlot>
 void EvacuationVerifier::VerifyPointersImpl(TSlot start, TSlot end) {
   for (TSlot current = start; current < end; ++current) {
     typename TSlot::TObject object = current.load(cage_base());
-    HeapObject heap_object;
+    Tagged<HeapObject> heap_object;
     if (object.GetHeapObjectIfStrong(&heap_object)) {
       VerifyHeapObjectImpl(heap_object);
     }
diff --git a/src/heap/evacuation-verifier.cc b/src/heap/evacuation-verifier.cc
index 7b35a1e7751..332b5fa2d74 100644
--- a/src/heap/evacuation-verifier.cc
+++ b/src/heap/evacuation-verifier.cc
@@ -24,20 +24,21 @@ void EvacuationVerifier::Run() {
   if (heap_->shared_space()) VerifyEvacuation(heap_->shared_space());
 }
 
-void EvacuationVerifier::VisitPointers(HeapObject host, ObjectSlot start,
-                                       ObjectSlot end) {
+void EvacuationVerifier::VisitPointers(Tagged<HeapObject> host,
+                                       ObjectSlot start, ObjectSlot end) {
   VerifyPointersImpl(start, end);
 }
 
-void EvacuationVerifier::VisitPointers(HeapObject host, MaybeObjectSlot start,
+void EvacuationVerifier::VisitPointers(Tagged<HeapObject> host,
+                                       MaybeObjectSlot start,
                                        MaybeObjectSlot end) {
   VerifyPointersImpl(start, end);
 }
 
 void EvacuationVerifier::VisitInstructionStreamPointer(
-    Code host, InstructionStreamSlot slot) {
-  Object maybe_code = slot.load(code_cage_base());
-  HeapObject code;
+    Tagged<Code> host, InstructionStreamSlot slot) {
+  Tagged<Object> maybe_code = slot.load(code_cage_base());
+  Tagged<HeapObject> code;
   // The slot might contain smi during Code creation, so skip it.
   if (maybe_code.GetHeapObject(&code)) {
     VerifyHeapObjectImpl(code);
@@ -50,18 +51,18 @@ void EvacuationVerifier::VisitRootPointers(Root root, const char* description,
   VerifyPointersImpl(start, end);
 }
 
-void EvacuationVerifier::VisitMapPointer(HeapObject object) {
+void EvacuationVerifier::VisitMapPointer(Tagged<HeapObject> object) {
   VerifyHeapObjectImpl(object->map(cage_base()));
 }
 
-void EvacuationVerifier::VisitCodeTarget(InstructionStream host,
+void EvacuationVerifier::VisitCodeTarget(Tagged<InstructionStream> host,
                                          RelocInfo* rinfo) {
-  InstructionStream target =
+  Tagged<InstructionStream> target =
       InstructionStream::FromTargetAddress(rinfo->target_address());
   VerifyHeapObjectImpl(target);
 }
 
-void EvacuationVerifier::VisitEmbeddedPointer(InstructionStream host,
+void EvacuationVerifier::VisitEmbeddedPointer(Tagged<InstructionStream> host,
                                               RelocInfo* rinfo) {
   VerifyHeapObjectImpl(rinfo->target_object(cage_base()));
 }
@@ -75,7 +76,7 @@ void EvacuationVerifier::VerifyRoots() {
 void EvacuationVerifier::VerifyEvacuationOnPage(Address start, Address end) {
   Address current = start;
   while (current < end) {
-    HeapObject object = HeapObject::FromAddress(current);
+    Tagged<HeapObject> object = HeapObject::FromAddress(current);
     if (!IsFreeSpaceOrFiller(object, cage_base())) {
       object->Iterate(cage_base(), this);
     }
diff --git a/src/heap/evacuation-verifier.h b/src/heap/evacuation-verifier.h
index 83eef166689..dea0601e64d 100644
--- a/src/heap/evacuation-verifier.h
+++ b/src/heap/evacuation-verifier.h
@@ -22,20 +22,22 @@ class EvacuationVerifier final : public ObjectVisitorWithCageBases,
 
   void Run();
 
-  void VisitPointers(HeapObject host, ObjectSlot start, ObjectSlot end) final;
-  void VisitPointers(HeapObject host, MaybeObjectSlot start,
+  void VisitPointers(Tagged<HeapObject> host, ObjectSlot start,
+                     ObjectSlot end) final;
+  void VisitPointers(Tagged<HeapObject> host, MaybeObjectSlot start,
                      MaybeObjectSlot end) final;
-  void VisitInstructionStreamPointer(Code host,
+  void VisitInstructionStreamPointer(Tagged<Code> host,
                                      InstructionStreamSlot slot) final;
   void VisitRootPointers(Root root, const char* description,
                          FullObjectSlot start, FullObjectSlot end) final;
-  void VisitMapPointer(HeapObject object) final;
-  void VisitCodeTarget(InstructionStream host, RelocInfo* rinfo) final;
-  void VisitEmbeddedPointer(InstructionStream host, RelocInfo* rinfo) final;
+  void VisitMapPointer(Tagged<HeapObject> object) final;
+  void VisitCodeTarget(Tagged<InstructionStream> host, RelocInfo* rinfo) final;
+  void VisitEmbeddedPointer(Tagged<InstructionStream> host,
+                            RelocInfo* rinfo) final;
 
  private:
-  V8_INLINE void VerifyHeapObjectImpl(HeapObject heap_object);
-  V8_INLINE bool ShouldVerifyObject(HeapObject heap_object);
+  V8_INLINE void VerifyHeapObjectImpl(Tagged<HeapObject> heap_object);
+  V8_INLINE bool ShouldVerifyObject(Tagged<HeapObject> heap_object);
 
   template <typename TSlot>
   void VerifyPointersImpl(TSlot start, TSlot end);
diff --git a/src/heap/factory-base.cc b/src/heap/factory-base.cc
index a676365ccde..e8146d99405 100644
--- a/src/heap/factory-base.cc
+++ b/src/heap/factory-base.cc
@@ -168,7 +168,7 @@ Handle<FixedArray> FactoryBase<Impl>::NewFixedArrayWithFiller(
   DCHECK(ReadOnlyHeap::Contains(*map));
   DCHECK(ReadOnlyHeap::Contains(*filler));
   result->set_map_after_allocation(*map, SKIP_WRITE_BARRIER);
-  FixedArray array = Tagged<FixedArray>::cast(result);
+  Tagged<FixedArray> array = Tagged<FixedArray>::cast(result);
   array->set_length(length);
   MemsetTagged(array->data_start(), *filler, length);
   return handle(array, isolate());
@@ -186,7 +186,7 @@ Handle<FixedArray> FactoryBase<Impl>::NewFixedArrayWithZeroes(
   DisallowGarbageCollection no_gc;
   result->set_map_after_allocation(read_only_roots().fixed_array_map(),
                                    SKIP_WRITE_BARRIER);
-  FixedArray array = Tagged<FixedArray>::cast(result);
+  Tagged<FixedArray> array = Tagged<FixedArray>::cast(result);
   array->set_length(length);
   MemsetTagged(array->data_start(), Smi::zero(), length);
   return handle(array, isolate());
@@ -206,7 +206,7 @@ Handle<FixedArrayBase> FactoryBase<Impl>::NewFixedDoubleArray(
   Tagged<HeapObject> result =
       AllocateRawWithImmortalMap(size, allocation, map, kDoubleAligned);
   DisallowGarbageCollection no_gc;
-  FixedDoubleArray array = Tagged<FixedDoubleArray>::cast(result);
+  Tagged<FixedDoubleArray> array = Tagged<FixedDoubleArray>::cast(result);
   array->set_length(length);
   return handle(array, isolate());
 }
@@ -222,7 +222,7 @@ Handle<WeakFixedArray> FactoryBase<Impl>::NewWeakFixedArrayWithMap(
       AllocateRawArray(WeakFixedArray::SizeFor(length), allocation);
   result->set_map_after_allocation(map, SKIP_WRITE_BARRIER);
   DisallowGarbageCollection no_gc;
-  WeakFixedArray array = Tagged<WeakFixedArray>::cast(result);
+  Tagged<WeakFixedArray> array = Tagged<WeakFixedArray>::cast(result);
   array->set_length(length);
   MemsetTagged(ObjectSlot(array->data_start()),
                read_only_roots().undefined_value(), length);
@@ -251,7 +251,7 @@ Handle<ByteArray> FactoryBase<Impl>::NewByteArray(int length,
   Tagged<HeapObject> result = AllocateRawWithImmortalMap(
       size, allocation, read_only_roots().byte_array_map());
   DisallowGarbageCollection no_gc;
-  ByteArray array = Tagged<ByteArray>::cast(result);
+  Tagged<ByteArray> array = Tagged<ByteArray>::cast(result);
   array->set_length(length);
   array->clear_padding();
   return handle(array, isolate());
@@ -267,10 +267,10 @@ Handle<ExternalPointerArray> FactoryBase<Impl>::NewExternalPointerArray(
   if (length == 0) return impl()->empty_external_pointer_array();
   int size =
       ALIGN_TO_ALLOCATION_ALIGNMENT(ExternalPointerArray::SizeFor(length));
-  HeapObject result = AllocateRawWithImmortalMap(
+  Tagged<HeapObject> result = AllocateRawWithImmortalMap(
       size, allocation, read_only_roots().external_pointer_array_map());
   DisallowGarbageCollection no_gc;
-  ExternalPointerArray array = ExternalPointerArray::cast(result);
+  Tagged<ExternalPointerArray> array = ExternalPointerArray::cast(result);
   // ExternalPointerArrays must be initialized to zero so that when the sandbox
   // is enabled, they contain all kNullExternalPointerHandle values.
   static_assert(kNullExternalPointerHandle == 0);
@@ -311,17 +311,17 @@ Handle<BytecodeArray> FactoryBase<Impl>::NewBytecodeArray(
   Tagged<HeapObject> result = AllocateRawWithImmortalMap(
       size, AllocationType::kOld, read_only_roots().bytecode_array_map());
   DisallowGarbageCollection no_gc;
-  BytecodeArray instance = Tagged<BytecodeArray>::cast(result);
+  Tagged<BytecodeArray> instance = Tagged<BytecodeArray>::cast(result);
   instance->set_length(length);
   instance->set_frame_size(frame_size);
   instance->set_parameter_count(parameter_count);
   instance->set_incoming_new_target_or_generator_register(
       interpreter::Register::invalid_value());
   instance->set_constant_pool(*constant_pool);
-  instance.set_handler_table(read_only_roots().empty_byte_array(),
-                             SKIP_WRITE_BARRIER);
-  instance.set_source_position_table(read_only_roots().undefined_value(),
-                                     kReleaseStore, SKIP_WRITE_BARRIER);
+  instance->set_handler_table(read_only_roots().empty_byte_array(),
+                              SKIP_WRITE_BARRIER);
+  instance->set_source_position_table(read_only_roots().undefined_value(),
+                                      kReleaseStore, SKIP_WRITE_BARRIER);
   CopyBytes(reinterpret_cast<uint8_t*>(instance->GetFirstBytecodeAddress()),
             raw_bytecodes, length);
   instance->clear_padding();
@@ -346,7 +346,7 @@ Handle<Script> FactoryBase<Impl>::NewScriptWithId(
       NewStructInternal<Script>(SCRIPT_TYPE, AllocationType::kOld), isolate());
   {
     DisallowGarbageCollection no_gc;
-    Script raw = *script;
+    Tagged<Script> raw = *script;
     raw->set_source(*source);
     raw->set_name(roots.undefined_value(), SKIP_WRITE_BARRIER);
     raw->set_id(script_id);
@@ -367,7 +367,7 @@ Handle<Script> FactoryBase<Impl>::NewScriptWithId(
     raw->set_compiled_lazy_function_positions(roots.undefined_value(),
                                               SKIP_WRITE_BARRIER);
 #ifdef V8_SCRIPTORMODULE_LEGACY_LIFETIME
-    raw.set_script_or_modules(roots.empty_array_list());
+    raw->set_script_or_modules(roots.empty_array_list());
 #endif
   }
   impl()->ProcessNewScript(script, script_event_type);
@@ -382,8 +382,8 @@ Handle<ArrayList> FactoryBase<Impl>::NewArrayList(int size,
       NewFixedArray(size + ArrayList::kFirstIndex, allocation);
   {
     DisallowGarbageCollection no_gc;
-    FixedArray raw = *fixed_array;
-    raw.set_map_no_write_barrier(read_only_roots().array_list_map());
+    Tagged<FixedArray> raw = *fixed_array;
+    raw->set_map_no_write_barrier(read_only_roots().array_list_map());
     ArrayList::cast(raw)->SetLength(0);
   }
   return Handle<ArrayList>::cast(fixed_array);
@@ -408,7 +408,7 @@ Handle<SharedFunctionInfo> FactoryBase<Impl>::CloneSharedFunctionInfo(
     Handle<SharedFunctionInfo> other) {
   Tagged<Map> map = read_only_roots().shared_function_info_map();
 
-  SharedFunctionInfo shared =
+  Tagged<SharedFunctionInfo> shared =
       SharedFunctionInfo::cast(NewWithImmortalMap(map, AllocationType::kOld));
   DisallowGarbageCollection no_gc;
 
@@ -483,7 +483,7 @@ Handle<SharedFunctionInfo> FactoryBase<Impl>::NewSharedFunctionInfo(
   Handle<SharedFunctionInfo> shared =
       NewSharedFunctionInfo(AllocationType::kOld);
   DisallowGarbageCollection no_gc;
-  SharedFunctionInfo raw = *shared;
+  Tagged<SharedFunctionInfo> raw = *shared;
   // Function names are assumed to be flat elsewhere.
   Handle<String> shared_name;
   bool has_shared_name = maybe_name.ToHandle(&shared_name);
@@ -513,7 +513,7 @@ Handle<SharedFunctionInfo> FactoryBase<Impl>::NewSharedFunctionInfo(
   raw->set_kind(kind);
 
 #ifdef VERIFY_HEAP
-  if (v8_flags.verify_heap) raw.SharedFunctionInfoVerify(isolate());
+  if (v8_flags.verify_heap) raw->SharedFunctionInfoVerify(isolate());
 #endif  // VERIFY_HEAP
   return shared;
 }
@@ -625,7 +625,7 @@ Handle<CoverageInfo> FactoryBase<Impl>::NewCoverageInfo(
 
   int size = CoverageInfo::SizeFor(slot_count);
   Tagged<Map> map = read_only_roots().coverage_info_map();
-  CoverageInfo info = CoverageInfo::cast(
+  Tagged<CoverageInfo> info = CoverageInfo::cast(
       AllocateRawWithImmortalMap(size, AllocationType::kOld, map));
   info->set_slot_count(slot_count);
   for (int i = 0; i < slot_count; i++) {
@@ -894,7 +894,7 @@ Handle<String> FactoryBase<Impl>::LookupSingleCharacterStringFromCode(
     uint16_t code) {
   if (code <= unibrow::Latin1::kMaxChar) {
     DisallowGarbageCollection no_gc;
-    Object value = single_character_string_table()->get(code);
+    Tagged<Object> value = single_character_string_table()->get(code);
     DCHECK_NE(value, *undefined_value());
     return handle(String::cast(value), isolate());
   }
@@ -1016,7 +1016,7 @@ inline Handle<String> FactoryBase<Impl>::SmiToString(Tagged<Smi> number,
   static_assert(Smi::kMaxValue <= std::numeric_limits<uint32_t>::max());
   {
     DisallowGarbageCollection no_gc;
-    String raw = *result;
+    Tagged<String> raw = *result;
     if (raw->raw_hash_field() == String::kEmptyHashField &&
         number.value() >= 0) {
       uint32_t raw_hash_field = StringHasher::MakeArrayIndexHash(
@@ -1037,7 +1037,8 @@ Handle<FreshlyAllocatedBigInt> FactoryBase<Impl>::NewBigInt(
   Tagged<HeapObject> result = AllocateRawWithImmortalMap(
       BigInt::SizeFor(length), allocation, read_only_roots().bigint_map());
   DisallowGarbageCollection no_gc;
-  FreshlyAllocatedBigInt bigint = Tagged<FreshlyAllocatedBigInt>::cast(result);
+  Tagged<FreshlyAllocatedBigInt> bigint =
+      Tagged<FreshlyAllocatedBigInt>::cast(result);
   bigint->clear_padding();
   return handle(bigint, isolate());
 }
@@ -1049,7 +1050,7 @@ Handle<ScopeInfo> FactoryBase<Impl>::NewScopeInfo(int length,
   int size = ScopeInfo::SizeFor(length);
   Tagged<HeapObject> obj = AllocateRawWithImmortalMap(
       size, type, read_only_roots().scope_info_map());
-  ScopeInfo scope_info = ScopeInfo::cast(obj);
+  Tagged<ScopeInfo> scope_info = ScopeInfo::cast(obj);
   MemsetTagged(scope_info->data_start(), read_only_roots().undefined_value(),
                length);
   return handle(scope_info, isolate());
@@ -1066,13 +1067,13 @@ template <typename Impl>
 Handle<SharedFunctionInfo> FactoryBase<Impl>::NewSharedFunctionInfo(
     AllocationType allocation) {
   Tagged<Map> map = read_only_roots().shared_function_info_map();
-  SharedFunctionInfo shared =
+  Tagged<SharedFunctionInfo> shared =
       SharedFunctionInfo::cast(NewWithImmortalMap(map, allocation));
 
   DisallowGarbageCollection no_gc;
-  shared.Init(read_only_roots(), isolate()->GetAndIncNextUniqueSfiId());
+  shared->Init(read_only_roots(), isolate()->GetAndIncNextUniqueSfiId());
 #ifdef VERIFY_HEAP
-  if (v8_flags.verify_heap) shared.SharedFunctionInfoVerify(isolate());
+  if (v8_flags.verify_heap) shared->SharedFunctionInfoVerify(isolate());
 #endif  // VERIFY_HEAP
 
   return handle(shared, isolate());
@@ -1087,7 +1088,7 @@ Handle<DescriptorArray> FactoryBase<Impl>::NewDescriptorArray(
   int size = DescriptorArray::SizeFor(number_of_all_descriptors);
   Tagged<HeapObject> obj = AllocateRawWithImmortalMap(
       size, allocation, read_only_roots().descriptor_array_map());
-  DescriptorArray array = DescriptorArray::cast(obj);
+  Tagged<DescriptorArray> array = DescriptorArray::cast(obj);
 
   auto raw_gc_state = DescriptorArrayMarkingState::kInitialGCState;
   if (allocation != AllocationType::kYoung &&
@@ -1101,9 +1102,9 @@ Handle<DescriptorArray> FactoryBase<Impl>::NewDescriptorArray(
           heap->mark_compact_collector()->epoch(), number_of_descriptors);
     }
   }
-  array.Initialize(read_only_roots().empty_enum_cache(),
-                   read_only_roots().undefined_value(), number_of_descriptors,
-                   slack, raw_gc_state);
+  array->Initialize(read_only_roots().empty_enum_cache(),
+                    read_only_roots().undefined_value(), number_of_descriptors,
+                    slack, raw_gc_state);
   return handle(array, isolate());
 }
 
@@ -1133,7 +1134,7 @@ FactoryBase<Impl>::AllocateRawOneByteInternalizedString(
                                                : AllocationType::kOld,
           map);
   Tagged<HeapObject> result = AllocateRawWithImmortalMap(size, allocation, map);
-  SeqOneByteString answer = Tagged<SeqOneByteString>::cast(result);
+  Tagged<SeqOneByteString> answer = Tagged<SeqOneByteString>::cast(result);
   DisallowGarbageCollection no_gc;
   answer->clear_padding_destructively(length);
   answer->set_length(length);
@@ -1151,11 +1152,12 @@ FactoryBase<Impl>::AllocateRawTwoByteInternalizedString(
 
   Tagged<Map> map = read_only_roots().internalized_two_byte_string_map();
   int size = SeqTwoByteString::SizeFor(length);
-  SeqTwoByteString answer = SeqTwoByteString::cast(AllocateRawWithImmortalMap(
-      size,
-      RefineAllocationTypeForInPlaceInternalizableString(AllocationType::kOld,
-                                                         map),
-      map));
+  Tagged<SeqTwoByteString> answer =
+      SeqTwoByteString::cast(AllocateRawWithImmortalMap(
+          size,
+          RefineAllocationTypeForInPlaceInternalizableString(
+              AllocationType::kOld, map),
+          map));
   DisallowGarbageCollection no_gc;
   answer->clear_padding_destructively(length);
   answer->set_length(length);
@@ -1248,10 +1250,10 @@ FactoryBase<Impl>::NewSwissNameDictionaryWithCapacity(
 
   Tagged<Map> map = read_only_roots().swiss_name_dictionary_map();
   int size = SwissNameDictionary::SizeFor(capacity);
-  SwissNameDictionary table = SwissNameDictionary::cast(
+  Tagged<SwissNameDictionary> table = SwissNameDictionary::cast(
       AllocateRawWithImmortalMap(size, allocation, map));
   DisallowGarbageCollection no_gc;
-  table.Initialize(isolate(), *meta_table, capacity);
+  table->Initialize(isolate(), *meta_table, capacity);
   return handle(table, isolate());
 }
 
diff --git a/src/heap/factory-inl.h b/src/heap/factory-inl.h
index afb11a56083..26b36e664e2 100644
--- a/src/heap/factory-inl.h
+++ b/src/heap/factory-inl.h
@@ -120,7 +120,7 @@ void Factory::NumberToStringCacheSet(Handle<Object> number, int hash,
     }
   }
   DisallowGarbageCollection no_gc;
-  FixedArray cache = *number_string_cache();
+  Tagged<FixedArray> cache = *number_string_cache();
   cache->set(hash * 2, *number);
   cache->set(hash * 2 + 1, *js_string);
 }
diff --git a/src/heap/factory.cc b/src/heap/factory.cc
index 597ac1187fb..462a3944e14 100644
--- a/src/heap/factory.cc
+++ b/src/heap/factory.cc
@@ -151,7 +151,7 @@ MaybeHandle<Code> Factory::CodeBuilder::BuildInternal(
     // The InstructionStream object has not been fully initialized yet. We
     // rely on the fact that no allocation will happen from this point on.
     DisallowGarbageCollection no_gc;
-    InstructionStream raw_istream = InstructionStream::Initialize(
+    Tagged<InstructionStream> raw_istream = InstructionStream::Initialize(
         istream_allocation,
         ReadOnlyRoots(local_isolate_).instruction_stream_map(),
         code_desc_.body_size(), *reloc_info);
@@ -2098,9 +2098,10 @@ Handle<Map> Factory::NewMap(InstanceType type, int instance_size,
                 isolate());
 }
 
-Map Factory::InitializeMap(Tagged<Map> map, InstanceType type,
-                           int instance_size, ElementsKind elements_kind,
-                           int inobject_properties, Heap* roots) {
+Tagged<Map> Factory::InitializeMap(Tagged<Map> map, InstanceType type,
+                                   int instance_size,
+                                   ElementsKind elements_kind,
+                                   int inobject_properties, Heap* roots) {
   DisallowGarbageCollection no_gc;
   map->set_bit_field(0);
   map->set_bit_field2(Map::Bits2::NewTargetIsBaseBit::encode(true));
@@ -3630,7 +3631,7 @@ Handle<Map> Factory::ObjectLiteralMapFromCache(Handle<NativeContext> context,
 
   // Check to see whether there is a matching element in the cache.
   MaybeObject result = cache->Get(number_of_properties);
-  HeapObject heap_object;
+  Tagged<HeapObject> heap_object;
   if (result.GetHeapObjectIfWeak(&heap_object)) {
     Tagged<Map> map = Tagged<Map>::cast(heap_object);
     DCHECK(!map->is_dictionary_map());
diff --git a/src/heap/factory.h b/src/heap/factory.h
index f2872d9e8d4..5c71e925d3c 100644
--- a/src/heap/factory.h
+++ b/src/heap/factory.h
@@ -481,9 +481,9 @@ class V8_EXPORT_PRIVATE Factory : public FactoryBase<Factory> {
   // Initializes the fields of a newly created Map using roots from the
   // passed-in Heap. Exposed for tests and heap setup; other code should just
   // call NewMap which takes care of it.
-  Map InitializeMap(Tagged<Map> map, InstanceType type, int instance_size,
-                    ElementsKind elements_kind, int inobject_properties,
-                    Heap* roots);
+  Tagged<Map> InitializeMap(Tagged<Map> map, InstanceType type,
+                            int instance_size, ElementsKind elements_kind,
+                            int inobject_properties, Heap* roots);
 
   // Allocate a block of memory of the given AllocationType (filled with a
   // filler). Used as a fall-back for generated code when the space is full.
diff --git a/src/heap/finalization-registry-cleanup-task.cc b/src/heap/finalization-registry-cleanup-task.cc
index 7bf25a0bd0b..dc8ceb49c18 100644
--- a/src/heap/finalization-registry-cleanup-task.cc
+++ b/src/heap/finalization-registry-cleanup-task.cc
@@ -89,7 +89,7 @@ void FinalizationRegistryCleanupTask::RunInternal() {
                                             finalization_registry, callback);
   if (finalization_registry->NeedsCleanup() &&
       !finalization_registry->scheduled_for_cleanup()) {
-    auto nop = [](HeapObject, ObjectSlot, Object) {};
+    auto nop = [](Tagged<HeapObject>, ObjectSlot, Tagged<Object>) {};
     heap_->EnqueueDirtyJSFinalizationRegistry(*finalization_registry, nop);
   }
 
diff --git a/src/heap/free-list.cc b/src/heap/free-list.cc
index 5e531c77f16..7fa99151631 100644
--- a/src/heap/free-list.cc
+++ b/src/heap/free-list.cc
@@ -28,9 +28,9 @@ void FreeListCategory::Reset(FreeList* owner) {
   available_ = 0;
 }
 
-FreeSpace FreeListCategory::PickNodeFromList(size_t minimum_size,
-                                             size_t* node_size) {
-  FreeSpace node = top();
+Tagged<FreeSpace> FreeListCategory::PickNodeFromList(size_t minimum_size,
+                                                     size_t* node_size) {
+  Tagged<FreeSpace> node = top();
   DCHECK(!node.is_null());
   DCHECK(Page::FromHeapObject(node)->CanAllocate());
   if (static_cast<size_t>(node->Size()) < minimum_size) {
@@ -43,10 +43,10 @@ FreeSpace FreeListCategory::PickNodeFromList(size_t minimum_size,
   return node;
 }
 
-FreeSpace FreeListCategory::SearchForNodeInList(size_t minimum_size,
-                                                size_t* node_size) {
-  FreeSpace prev_non_evac_node;
-  for (FreeSpace cur_node = top(); !cur_node.is_null();
+Tagged<FreeSpace> FreeListCategory::SearchForNodeInList(size_t minimum_size,
+                                                        size_t* node_size) {
+  Tagged<FreeSpace> prev_non_evac_node;
+  for (Tagged<FreeSpace> cur_node = top(); !cur_node.is_null();
        cur_node = cur_node->next()) {
     DCHECK(Page::FromHeapObject(cur_node)->CanAllocate());
     size_t size = cur_node->size(kRelaxedLoad);
@@ -72,7 +72,8 @@ FreeSpace FreeListCategory::SearchForNodeInList(size_t minimum_size,
 
 void FreeListCategory::Free(Address start, size_t size_in_bytes, FreeMode mode,
                             FreeList* owner) {
-  FreeSpace free_space = FreeSpace::cast(HeapObject::FromAddress(start));
+  Tagged<FreeSpace> free_space =
+      FreeSpace::cast(HeapObject::FromAddress(start));
   DCHECK_EQ(free_space->Size(), size_in_bytes);
   {
     CodePageMemoryModificationScope memory_modification_scope(
@@ -91,8 +92,8 @@ void FreeListCategory::Free(Address start, size_t size_in_bytes, FreeMode mode,
 }
 
 void FreeListCategory::RepairFreeList(Heap* heap) {
-  Map free_space_map = ReadOnlyRoots(heap).free_space_map();
-  FreeSpace n = top();
+  Tagged<Map> free_space_map = ReadOnlyRoots(heap).free_space_map();
+  Tagged<FreeSpace> n = top();
   while (!n.is_null()) {
     ObjectSlot map_slot = n->map_slot();
     if (map_slot.contains_map_value(kNullAddress)) {
@@ -120,11 +121,12 @@ std::unique_ptr<FreeList> FreeList::CreateFreeListForNewSpace() {
   return std::make_unique<FreeListManyCachedFastPathForNewSpace>();
 }
 
-FreeSpace FreeList::TryFindNodeIn(FreeListCategoryType type,
-                                  size_t minimum_size, size_t* node_size) {
+Tagged<FreeSpace> FreeList::TryFindNodeIn(FreeListCategoryType type,
+                                          size_t minimum_size,
+                                          size_t* node_size) {
   FreeListCategory* category = categories_[type];
   if (category == nullptr) return FreeSpace();
-  FreeSpace node = category->PickNodeFromList(minimum_size, node_size);
+  Tagged<FreeSpace> node = category->PickNodeFromList(minimum_size, node_size);
   if (!node.is_null()) {
     DecreaseAvailableBytes(*node_size);
     VerifyAvailable();
@@ -135,11 +137,11 @@ FreeSpace FreeList::TryFindNodeIn(FreeListCategoryType type,
   return node;
 }
 
-FreeSpace FreeList::SearchForNodeInList(FreeListCategoryType type,
-                                        size_t minimum_size,
-                                        size_t* node_size) {
+Tagged<FreeSpace> FreeList::SearchForNodeInList(FreeListCategoryType type,
+                                                size_t minimum_size,
+                                                size_t* node_size) {
   FreeListCategoryIterator it(this, type);
-  FreeSpace node;
+  Tagged<FreeSpace> node;
   while (it.HasNext()) {
     FreeListCategory* current = it.Next();
     node = current->SearchForNodeInList(minimum_size, node_size);
@@ -217,10 +219,11 @@ Page* FreeListMany::GetPageForSize(size_t size_in_bytes) {
   return page;
 }
 
-FreeSpace FreeListMany::Allocate(size_t size_in_bytes, size_t* node_size,
-                                 AllocationOrigin origin) {
+Tagged<FreeSpace> FreeListMany::Allocate(size_t size_in_bytes,
+                                         size_t* node_size,
+                                         AllocationOrigin origin) {
   DCHECK_GE(kMaxBlockSize, size_in_bytes);
-  FreeSpace node;
+  Tagged<FreeSpace> node;
   FreeListCategoryType type = SelectFreeListCategoryType(size_in_bytes);
   for (int i = type; i < last_category_ && node.is_null(); i++) {
     node = TryFindNodeIn(static_cast<FreeListCategoryType>(i), size_in_bytes,
@@ -309,12 +312,13 @@ size_t FreeListManyCached::Free(Address start, size_t size_in_bytes,
   return 0;
 }
 
-FreeSpace FreeListManyCached::Allocate(size_t size_in_bytes, size_t* node_size,
-                                       AllocationOrigin origin) {
+Tagged<FreeSpace> FreeListManyCached::Allocate(size_t size_in_bytes,
+                                               size_t* node_size,
+                                               AllocationOrigin origin) {
   USE(origin);
   DCHECK_GE(kMaxBlockSize, size_in_bytes);
 
-  FreeSpace node;
+  Tagged<FreeSpace> node;
   FreeListCategoryType type = SelectFreeListCategoryType(size_in_bytes);
   type = next_nonempty_category[type];
   for (; type < last_category_; type = next_nonempty_category[type + 1]) {
@@ -348,12 +352,11 @@ FreeSpace FreeListManyCached::Allocate(size_t size_in_bytes, size_t* node_size,
 // ------------------------------------------------
 // FreeListManyCachedFastPathBase implementation
 
-FreeSpace FreeListManyCachedFastPathBase::Allocate(size_t size_in_bytes,
-                                                   size_t* node_size,
-                                                   AllocationOrigin origin) {
+Tagged<FreeSpace> FreeListManyCachedFastPathBase::Allocate(
+    size_t size_in_bytes, size_t* node_size, AllocationOrigin origin) {
   USE(origin);
   DCHECK_GE(kMaxBlockSize, size_in_bytes);
-  FreeSpace node;
+  Tagged<FreeSpace> node;
 
   // Fast path part 1: searching the last categories
   FreeListCategoryType first_category =
@@ -414,9 +417,9 @@ FreeSpace FreeListManyCachedFastPathBase::Allocate(size_t size_in_bytes,
 // ------------------------------------------------
 // FreeListManyCachedOrigin implementation
 
-FreeSpace FreeListManyCachedOrigin::Allocate(size_t size_in_bytes,
-                                             size_t* node_size,
-                                             AllocationOrigin origin) {
+Tagged<FreeSpace> FreeListManyCachedOrigin::Allocate(size_t size_in_bytes,
+                                                     size_t* node_size,
+                                                     AllocationOrigin origin) {
   if (origin == AllocationOrigin::kGC) {
     return FreeListManyCached::Allocate(size_in_bytes, node_size, origin);
   } else {
@@ -508,7 +511,7 @@ void FreeList::PrintCategories(FreeListCategoryType type) {
 
 size_t FreeListCategory::SumFreeList() {
   size_t sum = 0;
-  FreeSpace cur = top();
+  Tagged<FreeSpace> cur = top();
   while (!cur.is_null()) {
     // We can't use "cur->map()" here because both cur's map and the
     // root can be null during bootstrapping.
@@ -525,7 +528,7 @@ size_t FreeListCategory::SumFreeList() {
 }
 int FreeListCategory::FreeListLength() {
   int length = 0;
-  FreeSpace cur = top();
+  Tagged<FreeSpace> cur = top();
   while (!cur.is_null()) {
     length++;
     cur = cur->next();
diff --git a/src/heap/free-list.h b/src/heap/free-list.h
index 51bb7fa329d..374ee3a2ba1 100644
--- a/src/heap/free-list.h
+++ b/src/heap/free-list.h
@@ -65,12 +65,12 @@ class FreeListCategory {
   // Performs a single try to pick a node of at least |minimum_size| from the
   // category. Stores the actual size in |node_size|. Returns nullptr if no
   // node is found.
-  V8_EXPORT_PRIVATE FreeSpace PickNodeFromList(size_t minimum_size,
-                                               size_t* node_size);
+  V8_EXPORT_PRIVATE Tagged<FreeSpace> PickNodeFromList(size_t minimum_size,
+                                                       size_t* node_size);
 
   // Picks a node of at least |minimum_size| from the category. Stores the
   // actual size in |node_size|. Returns nullptr if no node is found.
-  FreeSpace SearchForNodeInList(size_t minimum_size, size_t* node_size);
+  Tagged<FreeSpace> SearchForNodeInList(size_t minimum_size, size_t* node_size);
 
   inline bool is_linked(FreeList* owner) const;
   bool is_empty() { return top().is_null(); }
@@ -81,7 +81,7 @@ class FreeListCategory {
 
   template <typename Callback>
   void IterateNodesForTesting(Callback callback) {
-    for (FreeSpace cur_node = top(); !cur_node.is_null();
+    for (Tagged<FreeSpace> cur_node = top(); !cur_node.is_null();
          cur_node = cur_node->next()) {
       callback(cur_node);
     }
@@ -96,8 +96,8 @@ class FreeListCategory {
   // allocation of size |allocation_size|.
   inline void UpdateCountersAfterAllocation(size_t allocation_size);
 
-  FreeSpace top() { return top_; }
-  void set_top(FreeSpace top) { top_ = top; }
+  Tagged<FreeSpace> top() { return top_; }
+  void set_top(Tagged<FreeSpace> top) { top_ = top; }
   FreeListCategory* prev() { return prev_; }
   void set_prev(FreeListCategory* prev) { prev_ = prev; }
   FreeListCategory* next() { return next_; }
@@ -155,9 +155,8 @@ class FreeList {
   // bytes. Returns the actual node size in node_size which can be bigger than
   // size_in_bytes. This method returns null if the allocation request cannot be
   // handled by the free list.
-  virtual V8_WARN_UNUSED_RESULT FreeSpace Allocate(size_t size_in_bytes,
-                                                   size_t* node_size,
-                                                   AllocationOrigin origin) = 0;
+  virtual V8_WARN_UNUSED_RESULT Tagged<FreeSpace> Allocate(
+      size_t size_in_bytes, size_t* node_size, AllocationOrigin origin) = 0;
 
   // Returns a page containing an entry for a given type, or nullptr otherwise.
   V8_EXPORT_PRIVATE virtual Page* GetPageForSize(size_t size_in_bytes) = 0;
@@ -242,12 +241,12 @@ class FreeList {
   // Tries to retrieve a node from the first category in a given |type|.
   // Returns nullptr if the category is empty or the top entry is smaller
   // than minimum_size.
-  FreeSpace TryFindNodeIn(FreeListCategoryType type, size_t minimum_size,
-                          size_t* node_size);
+  Tagged<FreeSpace> TryFindNodeIn(FreeListCategoryType type,
+                                  size_t minimum_size, size_t* node_size);
 
   // Searches a given |type| for a node of at least |minimum_size|.
-  FreeSpace SearchForNodeInList(FreeListCategoryType type, size_t minimum_size,
-                                size_t* node_size);
+  Tagged<FreeSpace> SearchForNodeInList(FreeListCategoryType type,
+                                        size_t minimum_size, size_t* node_size);
 
   // Returns the smallest category in which an object of |size_in_bytes| could
   // fit.
@@ -294,9 +293,9 @@ class V8_EXPORT_PRIVATE FreeListMany : public FreeList {
   FreeListMany();
   ~FreeListMany() override;
 
-  V8_WARN_UNUSED_RESULT FreeSpace Allocate(size_t size_in_bytes,
-                                           size_t* node_size,
-                                           AllocationOrigin origin) override;
+  V8_WARN_UNUSED_RESULT Tagged<FreeSpace> Allocate(
+      size_t size_in_bytes, size_t* node_size,
+      AllocationOrigin origin) override;
 
  protected:
   static const size_t kMinBlockSize = 3 * kTaggedSize;
@@ -350,9 +349,9 @@ class V8_EXPORT_PRIVATE FreeListManyCached : public FreeListMany {
  public:
   FreeListManyCached();
 
-  V8_WARN_UNUSED_RESULT FreeSpace Allocate(size_t size_in_bytes,
-                                           size_t* node_size,
-                                           AllocationOrigin origin) override;
+  V8_WARN_UNUSED_RESULT Tagged<FreeSpace> Allocate(
+      size_t size_in_bytes, size_t* node_size,
+      AllocationOrigin origin) override;
 
   size_t Free(Address start, size_t size_in_bytes, FreeMode mode) override;
 
@@ -437,9 +436,9 @@ class V8_EXPORT_PRIVATE FreeListManyCachedFastPathBase
     }
   }
 
-  V8_WARN_UNUSED_RESULT FreeSpace Allocate(size_t size_in_bytes,
-                                           size_t* node_size,
-                                           AllocationOrigin origin) override;
+  V8_WARN_UNUSED_RESULT Tagged<FreeSpace> Allocate(
+      size_t size_in_bytes, size_t* node_size,
+      AllocationOrigin origin) override;
 
  protected:
   // Objects in the 18th category are at least 2048 bytes
@@ -501,9 +500,9 @@ class FreeListManyCachedFastPathForNewSpace
 class V8_EXPORT_PRIVATE FreeListManyCachedOrigin
     : public FreeListManyCachedFastPath {
  public:
-  V8_WARN_UNUSED_RESULT FreeSpace Allocate(size_t size_in_bytes,
-                                           size_t* node_size,
-                                           AllocationOrigin origin) override;
+  V8_WARN_UNUSED_RESULT Tagged<FreeSpace> Allocate(
+      size_t size_in_bytes, size_t* node_size,
+      AllocationOrigin origin) override;
 };
 
 }  // namespace internal
diff --git a/src/heap/heap-allocator-inl.h b/src/heap/heap-allocator-inl.h
index 95002310cc4..139844a832a 100644
--- a/src/heap/heap-allocator-inl.h
+++ b/src/heap/heap-allocator-inl.h
@@ -86,7 +86,7 @@ V8_WARN_UNUSED_RESULT V8_INLINE AllocationResult HeapAllocator::AllocateRaw(
   const bool large_object =
       static_cast<size_t>(size_in_bytes) > large_object_threshold;
 
-  HeapObject object;
+  Tagged<HeapObject> object;
   AllocationResult allocation;
 
   if (V8_ENABLE_THIRD_PARTY_HEAP_BOOL) {
@@ -195,11 +195,12 @@ AllocationResult HeapAllocator::AllocateRawData(int size_in_bytes,
 }
 
 template <HeapAllocator::AllocationRetryMode mode>
-V8_WARN_UNUSED_RESULT V8_INLINE HeapObject HeapAllocator::AllocateRawWith(
-    int size, AllocationType allocation, AllocationOrigin origin,
-    AllocationAlignment alignment) {
+V8_WARN_UNUSED_RESULT V8_INLINE Tagged<HeapObject>
+HeapAllocator::AllocateRawWith(int size, AllocationType allocation,
+                               AllocationOrigin origin,
+                               AllocationAlignment alignment) {
   AllocationResult result;
-  HeapObject object;
+  Tagged<HeapObject> object;
   size = ALIGN_TO_ALLOCATION_ALIGNMENT(size);
   if (allocation == AllocationType::kYoung) {
     result = AllocateRaw<AllocationType::kYoung>(size, origin, alignment);
diff --git a/src/heap/heap-allocator.h b/src/heap/heap-allocator.h
index ec85af38ada..568ce0f0abe 100644
--- a/src/heap/heap-allocator.h
+++ b/src/heap/heap-allocator.h
@@ -60,10 +60,10 @@ class V8_EXPORT_PRIVATE HeapAllocator final {
 
   // Supports all `AllocationType` types and allows specifying retry handling.
   template <AllocationRetryMode mode>
-  V8_WARN_UNUSED_RESULT V8_INLINE HeapObject
-  AllocateRawWith(int size, AllocationType allocation,
-                  AllocationOrigin origin = AllocationOrigin::kRuntime,
-                  AllocationAlignment alignment = kTaggedAligned);
+  V8_WARN_UNUSED_RESULT V8_INLINE Tagged<HeapObject> AllocateRawWith(
+      int size, AllocationType allocation,
+      AllocationOrigin origin = AllocationOrigin::kRuntime,
+      AllocationAlignment alignment = kTaggedAligned);
 
   V8_INLINE bool CanAllocateInReadOnlySpace() const;
 
diff --git a/src/heap/heap-inl.h b/src/heap/heap-inl.h
index be5b68f34be..87b4a2006cd 100644
--- a/src/heap/heap-inl.h
+++ b/src/heap/heap-inl.h
@@ -57,14 +57,14 @@ namespace v8 {
 namespace internal {
 
 template <typename T>
-T ForwardingAddress(T heap_obj) {
-  MapWord map_word = heap_obj.map_word(kRelaxedLoad);
+Tagged<T> ForwardingAddress(Tagged<T> heap_obj) {
+  MapWord map_word = heap_obj->map_word(kRelaxedLoad);
 
   if (map_word.IsForwardingAddress()) {
-    return T::cast(map_word.ToForwardingAddress(heap_obj));
+    return Tagged<T>::cast(map_word.ToForwardingAddress(heap_obj));
   } else if (Heap::InFromPage(heap_obj)) {
     DCHECK(!v8_flags.minor_ms);
-    return T();
+    return Tagged<T>();
   } else {
     return heap_obj;
   }
@@ -273,19 +273,19 @@ Address Heap::NewSpaceTop() {
   return new_space_ ? new_space_->top() : kNullAddress;
 }
 
-bool Heap::InYoungGeneration(Object object) {
+bool Heap::InYoungGeneration(Tagged<Object> object) {
   DCHECK(!HasWeakHeapObjectTag(object));
   return IsHeapObject(object) && InYoungGeneration(HeapObject::cast(object));
 }
 
 // static
 bool Heap::InYoungGeneration(MaybeObject object) {
-  HeapObject heap_object;
+  Tagged<HeapObject> heap_object;
   return object->GetHeapObject(&heap_object) && InYoungGeneration(heap_object);
 }
 
 // static
-bool Heap::InYoungGeneration(HeapObject heap_object) {
+bool Heap::InYoungGeneration(Tagged<HeapObject> heap_object) {
   if (V8_ENABLE_THIRD_PARTY_HEAP_BOOL) return false;
   bool result =
       BasicMemoryChunk::FromHeapObject(heap_object)->InYoungGeneration();
@@ -302,68 +302,47 @@ bool Heap::InYoungGeneration(HeapObject heap_object) {
   return result;
 }
 
-// static
-template <typename T>
-inline bool Heap::InYoungGeneration(Tagged<T> object) {
-  static_assert(kTaggedCanConvertToRawObjects);
-  return InYoungGeneration(*object);
-}
-
 // static
 bool Heap::InWritableSharedSpace(MaybeObject object) {
-  HeapObject heap_object;
+  Tagged<HeapObject> heap_object;
   return object->GetHeapObject(&heap_object) &&
          heap_object.InWritableSharedSpace();
 }
 
 // static
-bool Heap::InFromPage(Object object) {
+bool Heap::InFromPage(Tagged<Object> object) {
   DCHECK(!HasWeakHeapObjectTag(object));
   return IsHeapObject(object) && InFromPage(HeapObject::cast(object));
 }
 
 // static
 bool Heap::InFromPage(MaybeObject object) {
-  HeapObject heap_object;
+  Tagged<HeapObject> heap_object;
   return object->GetHeapObject(&heap_object) && InFromPage(heap_object);
 }
 
 // static
-bool Heap::InFromPage(HeapObject heap_object) {
+bool Heap::InFromPage(Tagged<HeapObject> heap_object) {
   return BasicMemoryChunk::FromHeapObject(heap_object)->IsFromPage();
 }
 
 // static
-template <typename T>
-inline bool Heap::InFromPage(Tagged<T> object) {
-  static_assert(kTaggedCanConvertToRawObjects);
-  return InFromPage(*object);
-}
-
-// static
-bool Heap::InToPage(Object object) {
+bool Heap::InToPage(Tagged<Object> object) {
   DCHECK(!HasWeakHeapObjectTag(object));
   return IsHeapObject(object) && InToPage(HeapObject::cast(object));
 }
 
 // static
 bool Heap::InToPage(MaybeObject object) {
-  HeapObject heap_object;
+  Tagged<HeapObject> heap_object;
   return object->GetHeapObject(&heap_object) && InToPage(heap_object);
 }
 
 // static
-bool Heap::InToPage(HeapObject heap_object) {
+bool Heap::InToPage(Tagged<HeapObject> heap_object) {
   return BasicMemoryChunk::FromHeapObject(heap_object)->IsToPage();
 }
 
-// static
-template <typename T>
-inline bool Heap::InToPage(Tagged<T> object) {
-  static_assert(kTaggedCanConvertToRawObjects);
-  return InToPage(*object);
-}
-
 bool Heap::InOldSpace(Tagged<Object> object) {
   if (V8_ENABLE_THIRD_PARTY_HEAP_BOOL) {
     return object.IsHeapObject() &&
@@ -449,7 +428,7 @@ bool Heap::IsPendingAllocationInternal(Tagged<HeapObject> object) {
   UNREACHABLE();
 }
 
-bool Heap::IsPendingAllocation(HeapObject object) {
+bool Heap::IsPendingAllocation(Tagged<HeapObject> object) {
   bool result = IsPendingAllocationInternal(object);
   if (v8_flags.trace_pending_allocations && result) {
     StdoutStream{} << "Pending allocation: " << std::hex << "0x" << object.ptr()
@@ -458,7 +437,7 @@ bool Heap::IsPendingAllocation(HeapObject object) {
   return result;
 }
 
-bool Heap::IsPendingAllocation(Object object) {
+bool Heap::IsPendingAllocation(Tagged<Object> object) {
   return IsHeapObject(object) && IsPendingAllocation(HeapObject::cast(object));
 }
 
@@ -489,8 +468,8 @@ Tagged<Boolean> Heap::ToBoolean(bool condition) {
 
 int Heap::NextScriptId() {
   FullObjectSlot last_script_id_slot(&roots_table()[RootIndex::kLastScriptId]);
-  Smi last_id = Smi::cast(last_script_id_slot.Relaxed_Load());
-  Smi new_id, last_id_before_cas;
+  Tagged<Smi> last_id = Smi::cast(last_script_id_slot.Relaxed_Load());
+  Tagged<Smi> new_id, last_id_before_cas;
   do {
     if (last_id.value() == Smi::kMaxValue) {
       static_assert(v8::UnboundScript::kNoScriptId == 0);
@@ -585,7 +564,7 @@ CodePageMemoryModificationScope::CodePageMemoryModificationScope(
 }
 #else
 CodePageMemoryModificationScope::CodePageMemoryModificationScope(
-    InstructionStream code)
+    Tagged<InstructionStream> code)
     : CodePageMemoryModificationScope(BasicMemoryChunk::FromHeapObject(code)) {}
 #endif
 
diff --git a/src/heap/heap-verifier.cc b/src/heap/heap-verifier.cc
index 3b39e8cc1b5..993d22562b4 100644
--- a/src/heap/heap-verifier.cc
+++ b/src/heap/heap-verifier.cc
@@ -35,7 +35,8 @@ namespace v8 {
 namespace internal {
 
 namespace {
-thread_local HeapObject pending_layout_change_object = HeapObject();
+thread_local Tagged<HeapObject> pending_layout_change_object =
+    Tagged<HeapObject>();
 }  // namespace
 
 // Verify that all objects are Smis.
@@ -60,50 +61,52 @@ class VerifyPointersVisitor : public ObjectVisitorWithCageBases,
   V8_INLINE explicit VerifyPointersVisitor(Heap* heap)
       : ObjectVisitorWithCageBases(heap), heap_(heap) {}
 
-  void VisitPointers(HeapObject host, ObjectSlot start,
+  void VisitPointers(Tagged<HeapObject> host, ObjectSlot start,
                      ObjectSlot end) override;
-  void VisitPointers(HeapObject host, MaybeObjectSlot start,
+  void VisitPointers(Tagged<HeapObject> host, MaybeObjectSlot start,
                      MaybeObjectSlot end) override;
-  void VisitInstructionStreamPointer(Code host,
+  void VisitInstructionStreamPointer(Tagged<Code> host,
                                      InstructionStreamSlot slot) override;
-  void VisitCodeTarget(InstructionStream host, RelocInfo* rinfo) override;
-  void VisitEmbeddedPointer(InstructionStream host, RelocInfo* rinfo) override;
+  void VisitCodeTarget(Tagged<InstructionStream> host,
+                       RelocInfo* rinfo) override;
+  void VisitEmbeddedPointer(Tagged<InstructionStream> host,
+                            RelocInfo* rinfo) override;
 
   void VisitRootPointers(Root root, const char* description,
                          FullObjectSlot start, FullObjectSlot end) override;
   void VisitRootPointers(Root root, const char* description,
                          OffHeapObjectSlot start,
                          OffHeapObjectSlot end) override;
-  void VisitMapPointer(HeapObject host) override;
+  void VisitMapPointer(Tagged<HeapObject> host) override;
 
  protected:
-  V8_INLINE void VerifyHeapObjectImpl(HeapObject heap_object);
-  V8_INLINE void VerifyCodeObjectImpl(HeapObject heap_object);
+  V8_INLINE void VerifyHeapObjectImpl(Tagged<HeapObject> heap_object);
+  V8_INLINE void VerifyCodeObjectImpl(Tagged<HeapObject> heap_object);
 
   template <typename TSlot>
   V8_INLINE void VerifyPointersImpl(TSlot start, TSlot end);
 
-  virtual void VerifyPointers(HeapObject host, MaybeObjectSlot start,
+  virtual void VerifyPointers(Tagged<HeapObject> host, MaybeObjectSlot start,
                               MaybeObjectSlot end);
 
   Heap* heap_;
 };
 
-void VerifyPointersVisitor::VisitPointers(HeapObject host, ObjectSlot start,
-                                          ObjectSlot end) {
+void VerifyPointersVisitor::VisitPointers(Tagged<HeapObject> host,
+                                          ObjectSlot start, ObjectSlot end) {
   VerifyPointers(host, MaybeObjectSlot(start), MaybeObjectSlot(end));
 }
 
-void VerifyPointersVisitor::VisitPointers(HeapObject host,
+void VerifyPointersVisitor::VisitPointers(Tagged<HeapObject> host,
                                           MaybeObjectSlot start,
                                           MaybeObjectSlot end) {
   VerifyPointers(host, start, end);
 }
 
 void VerifyPointersVisitor::VisitInstructionStreamPointer(
-    Code host, InstructionStreamSlot slot) {
+    Tagged<Code> host, InstructionStreamSlot slot) {
   Object maybe_code = slot.load(code_cage_base());
-  HeapObject code;
+  Tagged<HeapObject> code;
   // The slot might contain smi during Code creation.
   if (maybe_code.GetHeapObject(&code)) {
     VerifyCodeObjectImpl(code);
@@ -126,16 +129,18 @@ void VerifyPointersVisitor::VisitRootPointers(Root root,
   VerifyPointersImpl(start, end);
 }
 
-void VerifyPointersVisitor::VisitMapPointer(HeapObject host) {
+void VerifyPointersVisitor::VisitMapPointer(Tagged<HeapObject> host) {
   VerifyHeapObjectImpl(host->map(cage_base()));
 }
 
-void VerifyPointersVisitor::VerifyHeapObjectImpl(HeapObject heap_object) {
+void VerifyPointersVisitor::VerifyHeapObjectImpl(
+    Tagged<HeapObject> heap_object) {
   CHECK(IsValidHeapObject(heap_, heap_object));
   CHECK(IsMap(heap_object->map(cage_base())));
 }
 
-void VerifyPointersVisitor::VerifyCodeObjectImpl(HeapObject heap_object) {
+void VerifyPointersVisitor::VerifyCodeObjectImpl(
+    Tagged<HeapObject> heap_object) {
   CHECK(IsValidCodeObject(heap_, heap_object));
   CHECK(IsMap(heap_object->map(cage_base())));
   CHECK(heap_object->map(cage_base())->instance_type() ==
@@ -146,7 +151,7 @@ template <typename TSlot>
 void VerifyPointersVisitor::VerifyPointersImpl(TSlot start, TSlot end) {
   for (TSlot slot = start; slot < end; ++slot) {
     typename TSlot::TObject object = slot.load(cage_base());
-    HeapObject heap_object;
+    Tagged<HeapObject> heap_object;
     if (object.GetHeapObject(&heap_object)) {
       VerifyHeapObjectImpl(heap_object);
     } else {
@@ -156,7 +161,7 @@ void VerifyPointersVisitor::VerifyPointersImpl(TSlot start, TSlot end) {
   }
 }
 
-void VerifyPointersVisitor::VerifyPointers(HeapObject host,
+void VerifyPointersVisitor::VerifyPointers(Tagged<HeapObject> host,
                                            MaybeObjectSlot start,
                                            MaybeObjectSlot end) {
   // If this DCHECK fires then you probably added a pointer field
@@ -167,14 +172,14 @@ void VerifyPointersVisitor::VerifyPointers(HeapObject host,
   VerifyPointersImpl(start, end);
 }
 
-void VerifyPointersVisitor::VisitCodeTarget(InstructionStream host,
+void VerifyPointersVisitor::VisitCodeTarget(Tagged<InstructionStream> host,
                                             RelocInfo* rinfo) {
-  InstructionStream target =
+  Tagged<InstructionStream> target =
       InstructionStream::FromTargetAddress(rinfo->target_address());
   VerifyHeapObjectImpl(target);
 }
 
-void VerifyPointersVisitor::VisitEmbeddedPointer(InstructionStream host,
+void VerifyPointersVisitor::VisitEmbeddedPointer(Tagged<InstructionStream> host,
                                                  RelocInfo* rinfo) {
   VerifyHeapObjectImpl(rinfo->target_object(cage_base()));
 }
@@ -185,7 +190,7 @@ class VerifyReadOnlyPointersVisitor : public VerifyPointersVisitor {
       : VerifyPointersVisitor(heap) {}
 
  protected:
-  void VerifyPointers(HeapObject host, MaybeObjectSlot start,
+  void VerifyPointers(Tagged<HeapObject> host, MaybeObjectSlot start,
                       MaybeObjectSlot end) override {
     if (!host.is_null()) {
       CHECK(ReadOnlyHeap::Contains(host->map()));
@@ -193,7 +198,7 @@ class VerifyReadOnlyPointersVisitor : public VerifyPointersVisitor {
     VerifyPointersVisitor::VerifyPointers(host, start, end);
 
     for (MaybeObjectSlot current = start; current < end; ++current) {
-      HeapObject heap_object;
+      Tagged<HeapObject> heap_object;
       if ((*current)->GetHeapObject(&heap_object)) {
         CHECK(ReadOnlyHeap::Contains(heap_object));
       }
@@ -212,7 +217,7 @@ class VerifySharedHeapObjectVisitor : public VerifyPointersVisitor {
   }
 
  protected:
-  void VerifyPointers(HeapObject host, MaybeObjectSlot start,
+  void VerifyPointers(Tagged<HeapObject> host, MaybeObjectSlot start,
                       MaybeObjectSlot end) override {
     if (!host.is_null()) {
       Map map = host->map();
@@ -221,7 +226,7 @@ class VerifySharedHeapObjectVisitor : public VerifyPointersVisitor {
     VerifyPointersVisitor::VerifyPointers(host, start, end);
 
     for (MaybeObjectSlot current = start; current < end; ++current) {
-      HeapObject heap_object;
+      Tagged<HeapObject> heap_object;
       if ((*current)->GetHeapObject(&heap_object)) {
         CHECK(ReadOnlyHeap::Contains(heap_object) ||
               shared_space_->Contains(heap_object) ||
@@ -250,12 +255,12 @@ class HeapVerification final : public SpaceVerificationVisitor {
   void VerifyPage(const BasicMemoryChunk* chunk) final;
   void VerifyPageDone(const BasicMemoryChunk* chunk) final;
 
-  void VerifyObject(HeapObject object) final;
-  void VerifyObjectMap(HeapObject object);
-  void VerifyOutgoingPointers(HeapObject object);
+  void VerifyObject(Tagged<HeapObject> object) final;
+  void VerifyObjectMap(Tagged<HeapObject> object);
+  void VerifyOutgoingPointers(Tagged<HeapObject> object);
   // Verifies OLD_TO_NEW, OLD_TO_NEW_BACKGROUND and OLD_TO_SHARED remembered
   // sets for this object.
-  void VerifyRememberedSetFor(HeapObject object);
+  void VerifyRememberedSetFor(Tagged<HeapObject> object);
 
   ReadOnlySpace* read_only_space() const { return heap_->read_only_space(); }
   NewSpace* new_space() const { return heap_->new_space(); }
@@ -363,7 +368,7 @@ void HeapVerification::VerifyPageDone(const BasicMemoryChunk* chunk) {
   current_chunk_.reset();
 }
 
-void HeapVerification::VerifyObject(HeapObject object) {
+void HeapVerification::VerifyObject(Tagged<HeapObject> object) {
   CHECK_EQ(BasicMemoryChunk::FromHeapObject(object), *current_chunk_);
 
   // Verify object map.
@@ -382,7 +387,7 @@ void HeapVerification::VerifyObject(HeapObject object) {
   }
 }
 
-void HeapVerification::VerifyOutgoingPointers(HeapObject object) {
+void HeapVerification::VerifyOutgoingPointers(Tagged<HeapObject> object) {
   switch (current_space_identity()) {
     case RO_SPACE: {
       VerifyReadOnlyPointersVisitor visitor(heap());
@@ -405,7 +410,7 @@ void HeapVerification::VerifyOutgoingPointers(HeapObject object) {
   }
 }
 
-void HeapVerification::VerifyObjectMap(HeapObject object) {
+void HeapVerification::VerifyObjectMap(Tagged<HeapObject> object) {
   // The first word should be a map, and we expect all map pointers to be
   // in map space or read-only space.
   Map map = object->map(cage_base_);
@@ -434,9 +439,10 @@ class SlotVerifyingVisitor : public ObjectVisitorWithCageBases {
                        std::set<std::pair<SlotType, Address>>* typed)
       : ObjectVisitorWithCageBases(isolate), untyped_(untyped), typed_(typed) {}
 
-  virtual bool ShouldHaveBeenRecorded(HeapObject host, MaybeObject target) = 0;
+  virtual bool ShouldHaveBeenRecorded(Tagged<HeapObject> host,
+                                      MaybeObject target) = 0;
 
-  void VisitPointers(HeapObject host, ObjectSlot start,
+  void VisitPointers(Tagged<HeapObject> host, ObjectSlot start,
                      ObjectSlot end) override {
 #ifdef DEBUG
     for (ObjectSlot slot = start; slot < end; ++slot) {
@@ -447,7 +453,7 @@ class SlotVerifyingVisitor : public ObjectVisitorWithCageBases {
     VisitPointers(host, MaybeObjectSlot(start), MaybeObjectSlot(end));
   }
 
-  void VisitPointers(HeapObject host, MaybeObjectSlot start,
+  void VisitPointers(Tagged<HeapObject> host, MaybeObjectSlot start,
                      MaybeObjectSlot end) final {
     for (MaybeObjectSlot slot = start; slot < end; ++slot) {
       if (ShouldHaveBeenRecorded(host, slot.load(cage_base()))) {
@@ -456,7 +462,7 @@ class SlotVerifyingVisitor : public ObjectVisitorWithCageBases {
     }
   }
 
-  void VisitInstructionStreamPointer(Code host,
+  void VisitInstructionStreamPointer(Tagged<Code> host,
                                      InstructionStreamSlot slot) override {
     if (ShouldHaveBeenRecorded(
             host, MaybeObject::FromObject(slot.load(code_cage_base())))) {
@@ -464,7 +470,8 @@ class SlotVerifyingVisitor : public ObjectVisitorWithCageBases {
     }
   }
 
-  void VisitCodeTarget(InstructionStream host, RelocInfo* rinfo) override {
+  void VisitCodeTarget(Tagged<InstructionStream> host,
+                       RelocInfo* rinfo) override {
     Object target =
         InstructionStream::FromTargetAddress(rinfo->target_address());
     if (ShouldHaveBeenRecorded(host, MaybeObject::FromObject(target))) {
@@ -475,7 +482,8 @@ class SlotVerifyingVisitor : public ObjectVisitorWithCageBases {
     }
   }
 
-  void VisitEmbeddedPointer(InstructionStream host, RelocInfo* rinfo) override {
+  void VisitEmbeddedPointer(Tagged<InstructionStream> host,
+                            RelocInfo* rinfo) override {
     Object target = rinfo->target_object(cage_base());
     if (ShouldHaveBeenRecorded(host, MaybeObject::FromObject(target))) {
       CHECK(InTypedSet(SlotType::kEmbeddedObjectFull, rinfo->pc()) ||
@@ -511,14 +519,15 @@ class OldToNewSlotVerifyingVisitor : public SlotVerifyingVisitor {
       : SlotVerifyingVisitor(isolate, untyped, typed),
         ephemeron_remembered_set_(ephemeron_remembered_set) {}
 
-  bool ShouldHaveBeenRecorded(HeapObject host, MaybeObject target) override {
+  bool ShouldHaveBeenRecorded(Tagged<HeapObject> host,
+                              MaybeObject target) override {
     DCHECK_IMPLIES(target->IsStrongOrWeak() && Heap::InYoungGeneration(target),
                    Heap::InToPage(target));
     return target->IsStrongOrWeak() && Heap::InYoungGeneration(target) &&
            !Heap::InYoungGeneration(host);
   }
 
-  void VisitEphemeron(HeapObject host, int index, ObjectSlot key,
+  void VisitEphemeron(Tagged<HeapObject> host, int index, ObjectSlot key,
                       ObjectSlot target) override {
     VisitPointer(host, target);
     if (v8_flags.minor_ms) return;
@@ -546,7 +555,8 @@ class OldToSharedSlotVerifyingVisitor : public SlotVerifyingVisitor {
                                   std::set<std::pair<SlotType, Address>>* typed)
       : SlotVerifyingVisitor(isolate, untyped, typed) {}
 
-  bool ShouldHaveBeenRecorded(HeapObject host, MaybeObject target) override {
+  bool ShouldHaveBeenRecorded(Tagged<HeapObject> host,
+                              MaybeObject target) override {
     return target->IsStrongOrWeak() && Heap::InWritableSharedSpace(target) &&
            !Heap::InYoungGeneration(host) && !host.InWritableSharedSpace();
   }
@@ -577,18 +587,18 @@ void CollectSlots(MemoryChunk* chunk, Address start, Address end,
 // Helper class for collecting slot addresses.
 class SlotCollectingVisitor final : public ObjectVisitor {
  public:
-  void VisitPointers(HeapObject host, ObjectSlot start,
+  void VisitPointers(Tagged<HeapObject> host, ObjectSlot start,
                      ObjectSlot end) override {
     VisitPointers(host, MaybeObjectSlot(start), MaybeObjectSlot(end));
   }
-  void VisitPointers(HeapObject host, MaybeObjectSlot start,
+  void VisitPointers(Tagged<HeapObject> host, MaybeObjectSlot start,
                      MaybeObjectSlot end) final {
     for (MaybeObjectSlot p = start; p < end; ++p) {
       slots_.push_back(p);
     }
   }
 
-  void VisitInstructionStreamPointer(Code host,
+  void VisitInstructionStreamPointer(Tagged<Code> host,
                                      InstructionStreamSlot slot) override {
     CHECK(V8_EXTERNAL_CODE_SPACE_BOOL);
 #ifdef V8_EXTERNAL_CODE_SPACE
@@ -596,15 +606,17 @@ class SlotCollectingVisitor final : public ObjectVisitor {
 #endif
   }
 
-  void VisitCodeTarget(InstructionStream host, RelocInfo* rinfo) final {
+  void VisitCodeTarget(Tagged<InstructionStream> host, RelocInfo* rinfo) final {
     UNREACHABLE();
   }
 
-  void VisitEmbeddedPointer(InstructionStream host, RelocInfo* rinfo) override {
+  void VisitEmbeddedPointer(Tagged<InstructionStream> host,
+                            RelocInfo* rinfo) override {
     UNREACHABLE();
   }
 
-  void VisitMapPointer(HeapObject object) override {}  // do nothing by default
+  void VisitMapPointer(Tagged<HeapObject> object) override {
+  }  // do nothing by default
 
   int number_of_slots() { return static_cast<int>(slots_.size()); }
 
@@ -621,7 +633,7 @@ class SlotCollectingVisitor final : public ObjectVisitor {
 #endif
 };
 
-void HeapVerification::VerifyRememberedSetFor(HeapObject object) {
+void HeapVerification::VerifyRememberedSetFor(Tagged<HeapObject> object) {
   if (current_space_identity() == RO_SPACE ||
       v8_flags.verify_heap_skip_remembered_set) {
     return;
@@ -694,8 +706,8 @@ void HeapVerifier::VerifyReadOnlyHeap(Heap* heap) {
 }
 
 // static
-void HeapVerifier::VerifyObjectLayoutChangeIsAllowed(Heap* heap,
-                                                     HeapObject object) {
+void HeapVerifier::VerifyObjectLayoutChangeIsAllowed(
+    Heap* heap, Tagged<HeapObject> object) {
   if (object.InWritableSharedSpace()) {
     // Out of objects in the shared heap, only strings can change layout.
     DCHECK(IsString(object));
@@ -713,15 +725,17 @@ void HeapVerifier::VerifyObjectLayoutChangeIsAllowed(Heap* heap,
 }
 
 // static
-void HeapVerifier::SetPendingLayoutChangeObject(Heap* heap, HeapObject object) {
+void HeapVerifier::SetPendingLayoutChangeObject(Heap* heap,
+                                                Tagged<HeapObject> object) {
   VerifyObjectLayoutChangeIsAllowed(heap, object);
   DCHECK(pending_layout_change_object.is_null());
   pending_layout_change_object = object;
 }
 
 // static
-void HeapVerifier::VerifyObjectLayoutChange(Heap* heap, HeapObject object,
-                                            Map new_map) {
+void HeapVerifier::VerifyObjectLayoutChange(Heap* heap,
+                                            Tagged<HeapObject> object,
+                                            Tagged<Map> new_map) {
   // Object layout changes are currently not supported on background threads.
   DCHECK_NULL(LocalHeap::Current());
 
@@ -744,8 +758,9 @@ void HeapVerifier::VerifyObjectLayoutChange(Heap* heap, HeapObject object,
 }
 
 // static
-void HeapVerifier::VerifySafeMapTransition(Heap* heap, HeapObject object,
-                                           Map new_map) {
+void HeapVerifier::VerifySafeMapTransition(Heap* heap,
+                                           Tagged<HeapObject> object,
+                                           Tagged<Map> new_map) {
   PtrComprCageBase cage_base(heap->isolate());
 
   if (IsJSObject(object, cage_base)) {
diff --git a/src/heap/heap-verifier.h b/src/heap/heap-verifier.h
index 0bf9074bce7..08ebfc9ddf3 100644
--- a/src/heap/heap-verifier.h
+++ b/src/heap/heap-verifier.h
@@ -23,7 +23,7 @@ class SpaceVerificationVisitor {
   virtual ~SpaceVerificationVisitor() = default;
 
   // This method will be invoked for every object in the space.
-  virtual void VerifyObject(HeapObject object) = 0;
+  virtual void VerifyObject(Tagged<HeapObject> object) = 0;
 
   // This method will be invoked for each page in the space before verifying an
   // object on it.
@@ -44,23 +44,23 @@ class HeapVerifier final {
   V8_EXPORT_PRIVATE static void VerifyReadOnlyHeap(Heap* heap);
 
   // Checks that this is a safe map transition.
-  V8_EXPORT_PRIVATE static void VerifySafeMapTransition(Heap* heap,
-                                                        HeapObject object,
-                                                        Map new_map);
+  V8_EXPORT_PRIVATE static void VerifySafeMapTransition(
+      Heap* heap, Tagged<HeapObject> object, Tagged<Map> new_map);
 
   // This function checks that either
   // - the map transition is safe,
   // - or it was communicated to GC using NotifyObjectLayoutChange.
-  V8_EXPORT_PRIVATE static void VerifyObjectLayoutChange(Heap* heap,
-                                                         HeapObject object,
-                                                         Map new_map);
+  V8_EXPORT_PRIVATE static void VerifyObjectLayoutChange(
+      Heap* heap, Tagged<HeapObject> object, Tagged<Map> new_map);
 
   // Verifies that that the object is allowed to change layout. Checks that if
   // the object is in shared space, it must be a string as no other objects in
   // shared space change layouts.
-  static void VerifyObjectLayoutChangeIsAllowed(Heap* heap, HeapObject object);
+  static void VerifyObjectLayoutChangeIsAllowed(Heap* heap,
+                                                Tagged<HeapObject> object);
 
-  static void SetPendingLayoutChangeObject(Heap* heap, HeapObject object);
+  static void SetPendingLayoutChangeObject(Heap* heap,
+                                           Tagged<HeapObject> object);
 
 #else
   static void VerifyHeap(Heap* heap) {}
diff --git a/src/heap/heap-write-barrier-inl.h b/src/heap/heap-write-barrier-inl.h
index 7a13c1372e4..f4c7abeb733 100644
--- a/src/heap/heap-write-barrier-inl.h
+++ b/src/heap/heap-write-barrier-inl.h
@@ -38,7 +38,7 @@ V8_EXPORT_PRIVATE void Heap_GenerationalBarrierForCodeSlow(
 V8_EXPORT_PRIVATE void Heap_GenerationalEphemeronKeyBarrierSlow(
     Heap* heap, Tagged<HeapObject> table, Address slot);
 
-inline bool IsCodeSpaceObject(HeapObject object);
+inline bool IsCodeSpaceObject(Tagged<HeapObject> object);
 
 // Do not use these internal details anywhere outside of this file. These
 // internals are only intended to shortcut write barrier checks.
@@ -55,7 +55,7 @@ struct MemoryChunk {
   static constexpr uintptr_t kIsExecutableBit = uintptr_t{1} << 19;
 
   V8_INLINE static heap_internals::MemoryChunk* FromHeapObject(
-      HeapObject object) {
+      Tagged<HeapObject> object) {
     DCHECK(!V8_ENABLE_THIRD_PARTY_HEAP_BOOL);
     return reinterpret_cast<MemoryChunk*>(object.ptr() & ~kPageAlignmentMask);
   }
@@ -99,8 +99,9 @@ struct MemoryChunk {
   V8_INLINE bool InCodeSpace() const { return GetFlags() & kIsExecutableBit; }
 };
 
-inline void CombinedWriteBarrierInternal(HeapObject host, HeapObjectSlot slot,
-                                         HeapObject value,
+inline void CombinedWriteBarrierInternal(Tagged<HeapObject> host,
+                                         HeapObjectSlot slot,
+                                         Tagged<HeapObject> value,
                                          WriteBarrierMode mode) {
   DCHECK_EQ(mode, UPDATE_WRITE_BARRIER);
 
@@ -133,15 +134,17 @@ inline void CombinedWriteBarrierInternal(HeapObject host, HeapObjectSlot slot,
 
 }  // namespace heap_internals
 
-inline void WriteBarrierForCode(InstructionStream host, RelocInfo* rinfo,
-                                Object value, WriteBarrierMode mode) {
+inline void WriteBarrierForCode(Tagged<InstructionStream> host,
+                                RelocInfo* rinfo, Tagged<Object> value,
+                                WriteBarrierMode mode) {
   DCHECK(!HasWeakHeapObjectTag(value));
-  if (!IsHeapObject(value)) return;
+  if (!value.IsHeapObject()) return;
   WriteBarrierForCode(host, rinfo, HeapObject::cast(value), mode);
 }
 
-inline void WriteBarrierForCode(InstructionStream host, RelocInfo* rinfo,
-                                HeapObject value, WriteBarrierMode mode) {
+inline void WriteBarrierForCode(Tagged<InstructionStream> host,
+                                RelocInfo* rinfo, Tagged<HeapObject> value,
+                                WriteBarrierMode mode) {
   if (mode == SKIP_WRITE_BARRIER) {
     SLOW_DCHECK(!WriteBarrier::IsRequired(host, value));
     return;
@@ -160,33 +163,33 @@ inline void WriteBarrierForCode(InstructionStream host, RelocInfo* rinfo,
   WriteBarrier::Marking(host, rinfo, value);
 }
 
-inline void CombinedWriteBarrier(HeapObject host, ObjectSlot slot, Object value,
-                                 WriteBarrierMode mode) {
+inline void CombinedWriteBarrier(Tagged<HeapObject> host, ObjectSlot slot,
+                                 Tagged<Object> value, WriteBarrierMode mode) {
   if (mode == SKIP_WRITE_BARRIER) {
     SLOW_DCHECK(!WriteBarrier::IsRequired(host, value));
     return;
   }
 
-  if (!IsHeapObject(value)) return;
+  if (!value.IsHeapObject()) return;
   heap_internals::CombinedWriteBarrierInternal(host, HeapObjectSlot(slot),
                                                HeapObject::cast(value), mode);
 }
 
-inline void CombinedWriteBarrier(HeapObject host, MaybeObjectSlot slot,
+inline void CombinedWriteBarrier(Tagged<HeapObject> host, MaybeObjectSlot slot,
                                  MaybeObject value, WriteBarrierMode mode) {
   if (mode == SKIP_WRITE_BARRIER) {
     SLOW_DCHECK(!WriteBarrier::IsRequired(host, value));
     return;
   }
 
-  HeapObject value_object;
+  Tagged<HeapObject> value_object;
   if (!value->GetHeapObject(&value_object)) return;
   heap_internals::CombinedWriteBarrierInternal(host, HeapObjectSlot(slot),
                                                value_object, mode);
 }
 
-inline void CombinedEphemeronWriteBarrier(EphemeronHashTable host,
-                                          ObjectSlot slot, Object value,
+inline void CombinedEphemeronWriteBarrier(Tagged<EphemeronHashTable> host,
+                                          ObjectSlot slot, Tagged<Object> value,
                                           WriteBarrierMode mode) {
   if (mode == SKIP_WRITE_BARRIER) {
     SLOW_DCHECK(!WriteBarrier::IsRequired(host, value));
@@ -194,12 +197,12 @@ inline void CombinedEphemeronWriteBarrier(EphemeronHashTable host,
   }
 
   DCHECK_EQ(mode, UPDATE_WRITE_BARRIER);
-  if (!IsHeapObject(value)) return;
+  if (!value.IsHeapObject()) return;
 
   heap_internals::MemoryChunk* host_chunk =
       heap_internals::MemoryChunk::FromHeapObject(host);
 
-  HeapObject heap_object_value = HeapObject::cast(value);
+  Tagged<HeapObject> heap_object_value = HeapObject::cast(value);
   heap_internals::MemoryChunk* value_chunk =
       heap_internals::MemoryChunk::FromHeapObject(heap_object_value);
 
@@ -223,9 +226,9 @@ inline void CombinedEphemeronWriteBarrier(EphemeronHashTable host,
   }
 }
 
-inline void IndirectPointerWriteBarrier(HeapObject host,
+inline void IndirectPointerWriteBarrier(Tagged<HeapObject> host,
                                         IndirectPointerSlot slot,
-                                        HeapObject value,
+                                        Tagged<HeapObject> value,
                                         WriteBarrierMode mode) {
   // Indirect pointers are only used when the sandbox is enabled.
   DCHECK(V8_CODE_POINTER_SANDBOXING_BOOL);
@@ -244,8 +247,9 @@ inline void IndirectPointerWriteBarrier(HeapObject host,
   WriteBarrier::Marking(host, slot);
 }
 
-inline void GenerationalBarrierForCode(InstructionStream host, RelocInfo* rinfo,
-                                       HeapObject object) {
+inline void GenerationalBarrierForCode(Tagged<InstructionStream> host,
+                                       RelocInfo* rinfo,
+                                       Tagged<HeapObject> object) {
   if (V8_ENABLE_THIRD_PARTY_HEAP_BOOL) return;
   heap_internals::MemoryChunk* object_chunk =
       heap_internals::MemoryChunk::FromHeapObject(object);
@@ -254,7 +258,7 @@ inline void GenerationalBarrierForCode(InstructionStream host, RelocInfo* rinfo,
 }
 
 inline WriteBarrierMode GetWriteBarrierModeForObject(
-    HeapObject object, const DisallowGarbageCollection* promise) {
+    Tagged<HeapObject> object, const DisallowGarbageCollection* promise) {
   if (v8_flags.disable_write_barriers) return SKIP_WRITE_BARRIER;
   DCHECK(Heap_PageFlagsAreConsistent(object));
   heap_internals::MemoryChunk* chunk =
@@ -264,11 +268,11 @@ inline WriteBarrierMode GetWriteBarrierModeForObject(
   return UPDATE_WRITE_BARRIER;
 }
 
-inline bool ObjectInYoungGeneration(Object object) {
+inline bool ObjectInYoungGeneration(Tagged<Object> object) {
   // TODO(rong): Fix caller of this function when we deploy
   // v8_use_third_party_heap.
   if (v8_flags.single_generation) return false;
-  if (IsSmi(object)) return false;
+  if (object.IsSmi()) return false;
   return heap_internals::MemoryChunk::FromHeapObject(HeapObject::cast(object))
       ->InYoungGeneration();
 }
@@ -280,23 +284,24 @@ inline bool IsReadOnlyHeapObject(Tagged<HeapObject> object) {
   return chunk->InReadOnlySpace();
 }
 
-inline bool IsCodeSpaceObject(HeapObject object) {
+inline bool IsCodeSpaceObject(Tagged<HeapObject> object) {
   heap_internals::MemoryChunk* chunk =
       heap_internals::MemoryChunk::FromHeapObject(object);
   return chunk->InCodeSpace();
 }
 
-bool WriteBarrier::IsMarking(HeapObject object) {
+bool WriteBarrier::IsMarking(Tagged<HeapObject> object) {
   if (V8_ENABLE_THIRD_PARTY_HEAP_BOOL) return false;
   heap_internals::MemoryChunk* chunk =
       heap_internals::MemoryChunk::FromHeapObject(object);
   return chunk->IsMarking();
 }
 
-void WriteBarrier::Marking(HeapObject host, ObjectSlot slot, Object value) {
+void WriteBarrier::Marking(Tagged<HeapObject> host, ObjectSlot slot,
+                           Tagged<Object> value) {
   DCHECK(!HasWeakHeapObjectTag(value));
-  if (!IsHeapObject(value)) return;
-  HeapObject value_heap_object = HeapObject::cast(value);
+  if (!value.IsHeapObject()) return;
+  Tagged<HeapObject> value_heap_object = HeapObject::cast(value);
   // Currently this marking barrier is never used for InstructionStream values.
   // If this ever changes then the CodePageHeaderModificationScope might be
   // required here.
@@ -304,9 +309,9 @@ void WriteBarrier::Marking(HeapObject host, ObjectSlot slot, Object value) {
   Marking(host, HeapObjectSlot(slot), value_heap_object);
 }
 
-void WriteBarrier::Marking(HeapObject host, MaybeObjectSlot slot,
+void WriteBarrier::Marking(Tagged<HeapObject> host, MaybeObjectSlot slot,
                            MaybeObject value) {
-  HeapObject value_heap_object;
+  Tagged<HeapObject> value_heap_object;
   if (!value->GetHeapObject(&value_heap_object)) return;
   // This barrier is called from generated code and from C++ code.
   // There must be no stores of InstructionStream values from generated code and
@@ -316,20 +321,20 @@ void WriteBarrier::Marking(HeapObject host, MaybeObjectSlot slot,
   Marking(host, HeapObjectSlot(slot), value_heap_object);
 }
 
-void WriteBarrier::Marking(HeapObject host, HeapObjectSlot slot,
-                           HeapObject value) {
+void WriteBarrier::Marking(Tagged<HeapObject> host, HeapObjectSlot slot,
+                           Tagged<HeapObject> value) {
   if (!IsMarking(host)) return;
   MarkingSlow(host, slot, value);
 }
 
-void WriteBarrier::Marking(InstructionStream host, RelocInfo* reloc_info,
-                           HeapObject value) {
+void WriteBarrier::Marking(Tagged<InstructionStream> host,
+                           RelocInfo* reloc_info, Tagged<HeapObject> value) {
   if (!IsMarking(host)) return;
   MarkingSlow(host, reloc_info, value);
 }
 
-void WriteBarrier::Shared(InstructionStream host, RelocInfo* reloc_info,
-                          HeapObject value) {
+void WriteBarrier::Shared(Tagged<InstructionStream> host, RelocInfo* reloc_info,
+                          Tagged<HeapObject> value) {
   if (V8_ENABLE_THIRD_PARTY_HEAP_BOOL) return;
 
   heap_internals::MemoryChunk* value_chunk =
@@ -339,38 +344,39 @@ void WriteBarrier::Shared(InstructionStream host, RelocInfo* reloc_info,
   SharedSlow(host, reloc_info, value);
 }
 
-void WriteBarrier::Marking(JSArrayBuffer host,
+void WriteBarrier::Marking(Tagged<JSArrayBuffer> host,
                            ArrayBufferExtension* extension) {
   if (!extension || !IsMarking(host)) return;
   MarkingSlow(host, extension);
 }
 
-void WriteBarrier::Marking(DescriptorArray descriptor_array,
+void WriteBarrier::Marking(Tagged<DescriptorArray> descriptor_array,
                            int number_of_own_descriptors) {
   if (!IsMarking(descriptor_array)) return;
   MarkingSlow(descriptor_array, number_of_own_descriptors);
 }
 
-void WriteBarrier::Marking(HeapObject host, IndirectPointerSlot slot) {
+void WriteBarrier::Marking(Tagged<HeapObject> host, IndirectPointerSlot slot) {
   if (!IsMarking(host)) return;
   MarkingSlow(host, slot);
 }
 
 // static
-void WriteBarrier::MarkingFromGlobalHandle(Object value) {
+void WriteBarrier::MarkingFromGlobalHandle(Tagged<Object> value) {
   if (V8_ENABLE_THIRD_PARTY_HEAP_BOOL) return;
-  if (!IsHeapObject(value)) return;
+  if (!value.IsHeapObject()) return;
   MarkingSlowFromGlobalHandle(HeapObject::cast(value));
 }
 
 // static
-void WriteBarrier::CombinedBarrierFromInternalFields(JSObject host,
+void WriteBarrier::CombinedBarrierFromInternalFields(Tagged<JSObject> host,
                                                      void* value) {
   CombinedBarrierFromInternalFields(host, 1, &value);
 }
 
 // static
-void WriteBarrier::CombinedBarrierFromInternalFields(JSObject host, size_t argc,
+void WriteBarrier::CombinedBarrierFromInternalFields(Tagged<JSObject> host,
+                                                     size_t argc,
                                                      void** values) {
   if (V8_ENABLE_THIRD_PARTY_HEAP_BOOL) return;
   if (V8_LIKELY(!IsMarking(host))) {
@@ -388,13 +394,13 @@ void WriteBarrier::CombinedBarrierFromInternalFields(JSObject host, size_t argc,
 }
 
 // static
-void WriteBarrier::GenerationalBarrierFromInternalFields(JSObject host,
+void WriteBarrier::GenerationalBarrierFromInternalFields(Tagged<JSObject> host,
                                                          void* value) {
   GenerationalBarrierFromInternalFields(host, 1, &value);
 }
 
 // static
-void WriteBarrier::GenerationalBarrierFromInternalFields(JSObject host,
+void WriteBarrier::GenerationalBarrierFromInternalFields(Tagged<JSObject> host,
                                                          size_t argc,
                                                          void** values) {
   auto* memory_chunk = MemoryChunk::FromHeapObject(host);
@@ -411,11 +417,11 @@ void WriteBarrier::GenerationalBarrierFromInternalFields(JSObject host,
 #ifdef ENABLE_SLOW_DCHECKS
 // static
 template <typename T>
-bool WriteBarrier::IsRequired(HeapObject host, T value) {
+bool WriteBarrier::IsRequired(Tagged<HeapObject> host, T value) {
   if (BasicMemoryChunk::FromHeapObject(host)->InYoungGeneration()) return false;
   if (IsSmi(value)) return false;
   if (value.IsCleared()) return false;
-  HeapObject target = value.GetHeapObject();
+  Tagged<HeapObject> target = value.GetHeapObject();
   if (ReadOnlyHeap::Contains(target)) return false;
   return !IsImmortalImmovableHeapObject(target);
 }
diff --git a/src/heap/heap-write-barrier.cc b/src/heap/heap-write-barrier.cc
index 16aa472b1f0..e7239e957f8 100644
--- a/src/heap/heap-write-barrier.cc
+++ b/src/heap/heap-write-barrier.cc
@@ -21,7 +21,7 @@ thread_local MarkingBarrier* current_marking_barrier = nullptr;
 }  // namespace
 
 MarkingBarrier* WriteBarrier::CurrentMarkingBarrier(
-    HeapObject verification_candidate) {
+    Tagged<HeapObject> verification_candidate) {
   MarkingBarrier* marking_barrier = current_marking_barrier;
   DCHECK_NOT_NULL(marking_barrier);
 #if DEBUG
@@ -43,33 +43,35 @@ MarkingBarrier* WriteBarrier::SetForThread(MarkingBarrier* marking_barrier) {
   return existing;
 }
 
-void WriteBarrier::MarkingSlow(HeapObject host, HeapObjectSlot slot,
-                               HeapObject value) {
+void WriteBarrier::MarkingSlow(Tagged<HeapObject> host, HeapObjectSlot slot,
+                               Tagged<HeapObject> value) {
   MarkingBarrier* marking_barrier = CurrentMarkingBarrier(host);
   marking_barrier->Write(host, slot, value);
 }
 
 // static
-void WriteBarrier::MarkingSlowFromGlobalHandle(HeapObject value) {
+void WriteBarrier::MarkingSlowFromGlobalHandle(Tagged<HeapObject> value) {
   MarkingBarrier* marking_barrier = CurrentMarkingBarrier(value);
   marking_barrier->WriteWithoutHost(value);
 }
 
 // static
-void WriteBarrier::MarkingSlowFromInternalFields(Heap* heap, JSObject host) {
+void WriteBarrier::MarkingSlowFromInternalFields(Heap* heap,
+                                                 Tagged<JSObject> host) {
   if (auto* cpp_heap = heap->cpp_heap()) {
     CppHeap::From(cpp_heap)->WriteBarrier(host);
   }
 }
 
-void WriteBarrier::MarkingSlow(InstructionStream host, RelocInfo* reloc_info,
-                               HeapObject value) {
+void WriteBarrier::MarkingSlow(Tagged<InstructionStream> host,
+                               RelocInfo* reloc_info,
+                               Tagged<HeapObject> value) {
   MarkingBarrier* marking_barrier = CurrentMarkingBarrier(host);
   marking_barrier->Write(host, reloc_info, value);
 }
 
-void WriteBarrier::SharedSlow(InstructionStream host, RelocInfo* reloc_info,
-                              HeapObject value) {
+void WriteBarrier::SharedSlow(Tagged<InstructionStream> host,
+                              RelocInfo* reloc_info, Tagged<HeapObject> value) {
   MarkCompactCollector::RecordRelocSlotInfo info =
       MarkCompactCollector::ProcessRelocInfo(host, reloc_info, value);
 
@@ -78,25 +80,26 @@ void WriteBarrier::SharedSlow(InstructionStream host, RelocInfo* reloc_info,
                                             info.offset);
 }
 
-void WriteBarrier::MarkingSlow(JSArrayBuffer host,
+void WriteBarrier::MarkingSlow(Tagged<JSArrayBuffer> host,
                                ArrayBufferExtension* extension) {
   MarkingBarrier* marking_barrier = CurrentMarkingBarrier(host);
   marking_barrier->Write(host, extension);
 }
 
-void WriteBarrier::MarkingSlow(DescriptorArray descriptor_array,
+void WriteBarrier::MarkingSlow(Tagged<DescriptorArray> descriptor_array,
                                int number_of_own_descriptors) {
   MarkingBarrier* marking_barrier = CurrentMarkingBarrier(descriptor_array);
   marking_barrier->Write(descriptor_array, number_of_own_descriptors);
 }
 
-void WriteBarrier::MarkingSlow(HeapObject host, IndirectPointerSlot slot) {
+void WriteBarrier::MarkingSlow(Tagged<HeapObject> host,
+                               IndirectPointerSlot slot) {
   MarkingBarrier* marking_barrier = CurrentMarkingBarrier(host);
   marking_barrier->Write(host, slot);
 }
 
 int WriteBarrier::MarkingFromCode(Address raw_host, Address raw_slot) {
-  HeapObject host = HeapObject::cast(Object(raw_host));
+  Tagged<HeapObject> host = HeapObject::cast(Object(raw_host));
   MaybeObjectSlot slot(raw_slot);
   Address value = (*slot).ptr();
 
@@ -129,7 +132,7 @@ int WriteBarrier::MarkingFromCode(Address raw_host, Address raw_slot) {
 
 int WriteBarrier::IndirectPointerMarkingFromCode(Address raw_host,
                                                  Address raw_slot) {
-  HeapObject host = HeapObject::cast(Object(raw_host));
+  Tagged<HeapObject> host = HeapObject::cast(Object(raw_host));
   IndirectPointerSlot slot(raw_slot);
 
 #if DEBUG
@@ -151,7 +154,7 @@ int WriteBarrier::IndirectPointerMarkingFromCode(Address raw_host,
 }
 
 int WriteBarrier::SharedMarkingFromCode(Address raw_host, Address raw_slot) {
-  HeapObject host = HeapObject::cast(Object(raw_host));
+  Tagged<HeapObject> host = HeapObject::cast(Object(raw_host));
   MaybeObjectSlot slot(raw_slot);
   Address raw_value = (*slot).ptr();
   MaybeObject value(raw_value);
@@ -178,7 +181,7 @@ int WriteBarrier::SharedMarkingFromCode(Address raw_host, Address raw_slot) {
 }
 
 int WriteBarrier::SharedFromCode(Address raw_host, Address raw_slot) {
-  HeapObject host = HeapObject::cast(Object(raw_host));
+  Tagged<HeapObject> host = HeapObject::cast(Object(raw_host));
 
   if (!host.InWritableSharedSpace()) {
     Heap::SharedHeapBarrierSlow(host, raw_slot);
@@ -189,7 +192,7 @@ int WriteBarrier::SharedFromCode(Address raw_host, Address raw_slot) {
 }
 
 #ifdef ENABLE_SLOW_DCHECKS
-bool WriteBarrier::IsImmortalImmovableHeapObject(HeapObject object) {
+bool WriteBarrier::IsImmortalImmovableHeapObject(Tagged<HeapObject> object) {
   BasicMemoryChunk* basic_chunk = BasicMemoryChunk::FromHeapObject(object);
   // All objects in readonly space are immortal and immovable.
   return basic_chunk->InReadOnlySpace();
diff --git a/src/heap/heap-write-barrier.h b/src/heap/heap-write-barrier.h
index c3ef39c6b89..b72af20dd82 100644
--- a/src/heap/heap-write-barrier.h
+++ b/src/heap/heap-write-barrier.h
@@ -29,42 +29,49 @@ class RelocInfo;
 // object-macros.h.
 
 // Combined write barriers.
-void WriteBarrierForCode(InstructionStream host, RelocInfo* rinfo, Object value,
+void WriteBarrierForCode(Tagged<InstructionStream> host, RelocInfo* rinfo,
+                         Tagged<Object> value,
                          WriteBarrierMode mode = UPDATE_WRITE_BARRIER);
-void WriteBarrierForCode(InstructionStream host, RelocInfo* rinfo,
-                         HeapObject value,
+void WriteBarrierForCode(Tagged<InstructionStream> host, RelocInfo* rinfo,
+                         Tagged<HeapObject> value,
                          WriteBarrierMode mode = UPDATE_WRITE_BARRIER);
 
-void CombinedWriteBarrier(HeapObject object, ObjectSlot slot, Object value,
-                          WriteBarrierMode mode);
-void CombinedWriteBarrier(HeapObject object, MaybeObjectSlot slot,
+void CombinedWriteBarrier(Tagged<HeapObject> object, ObjectSlot slot,
+                          Tagged<Object> value, WriteBarrierMode mode);
+void CombinedWriteBarrier(Tagged<HeapObject> object, MaybeObjectSlot slot,
                           MaybeObject value, WriteBarrierMode mode);
 
-void CombinedEphemeronWriteBarrier(EphemeronHashTable object, ObjectSlot slot,
-                                   Object value, WriteBarrierMode mode);
-void IndirectPointerWriteBarrier(HeapObject host, IndirectPointerSlot slot,
-                                 HeapObject value, WriteBarrierMode mode);
+void CombinedEphemeronWriteBarrier(Tagged<EphemeronHashTable> object,
+                                   ObjectSlot slot, Tagged<Object> value,
+                                   WriteBarrierMode mode);
+void IndirectPointerWriteBarrier(Tagged<HeapObject> host,
+                                 IndirectPointerSlot slot,
+                                 Tagged<HeapObject> value,
+                                 WriteBarrierMode mode);
 
 // Generational write barrier.
-void GenerationalBarrierForCode(InstructionStream host, RelocInfo* rinfo,
-                                HeapObject object);
+void GenerationalBarrierForCode(Tagged<InstructionStream> host,
+                                RelocInfo* rinfo, Tagged<HeapObject> object);
 
 inline bool IsReadOnlyHeapObject(Tagged<HeapObject> object);
 
 class V8_EXPORT_PRIVATE WriteBarrier {
  public:
-  static inline void Marking(HeapObject host, ObjectSlot, Object value);
-  static inline void Marking(HeapObject host, HeapObjectSlot, HeapObject value);
-  static inline void Marking(HeapObject host, MaybeObjectSlot,
+  static inline void Marking(Tagged<HeapObject> host, ObjectSlot,
+                             Tagged<Object> value);
+  static inline void Marking(Tagged<HeapObject> host, HeapObjectSlot,
+                             Tagged<HeapObject> value);
+  static inline void Marking(Tagged<HeapObject> host, MaybeObjectSlot,
                              MaybeObject value);
-  static inline void Marking(InstructionStream host, RelocInfo*,
-                             HeapObject value);
-  static inline void Marking(JSArrayBuffer host, ArrayBufferExtension*);
-  static inline void Marking(DescriptorArray, int number_of_own_descriptors);
-  static inline void Marking(HeapObject host, IndirectPointerSlot slot);
+  static inline void Marking(Tagged<InstructionStream> host, RelocInfo*,
+                             Tagged<HeapObject> value);
+  static inline void Marking(Tagged<JSArrayBuffer> host, ArrayBufferExtension*);
+  static inline void Marking(Tagged<DescriptorArray>,
+                             int number_of_own_descriptors);
+  static inline void Marking(Tagged<HeapObject> host, IndirectPointerSlot slot);
 
-  static inline void Shared(InstructionStream host, RelocInfo*,
-                            HeapObject value);
+  static inline void Shared(Tagged<InstructionStream> host, RelocInfo*,
+                            Tagged<HeapObject> value);
 
   // It is invoked from generated code and has to take raw addresses.
   static int MarkingFromCode(Address raw_host, Address raw_slot);
@@ -73,44 +80,47 @@ class V8_EXPORT_PRIVATE WriteBarrier {
   static int SharedFromCode(Address raw_host, Address raw_slot);
 
   // Invoked from global handles where no host object is available.
-  static inline void MarkingFromGlobalHandle(Object value);
+  static inline void MarkingFromGlobalHandle(Tagged<Object> value);
 
-  static inline void CombinedBarrierFromInternalFields(JSObject host,
+  static inline void CombinedBarrierFromInternalFields(Tagged<JSObject> host,
                                                        void* value);
-  static inline void CombinedBarrierFromInternalFields(JSObject host,
+  static inline void CombinedBarrierFromInternalFields(Tagged<JSObject> host,
                                                        size_t argc,
                                                        void** values);
 
   static MarkingBarrier* SetForThread(MarkingBarrier*);
 
   static MarkingBarrier* CurrentMarkingBarrier(
-      HeapObject verification_candidate);
+      Tagged<HeapObject> verification_candidate);
 
 #ifdef ENABLE_SLOW_DCHECKS
   template <typename T>
-  static inline bool IsRequired(HeapObject host, T value);
-  static bool IsImmortalImmovableHeapObject(HeapObject object);
+  static inline bool IsRequired(Tagged<HeapObject> host, T value);
+  static bool IsImmortalImmovableHeapObject(Tagged<HeapObject> object);
 #endif
 
-  static void MarkingSlow(HeapObject host, HeapObjectSlot, HeapObject value);
+  static void MarkingSlow(Tagged<HeapObject> host, HeapObjectSlot,
+                          Tagged<HeapObject> value);
 
  private:
-  static inline bool IsMarking(HeapObject object);
-
-  static void MarkingSlow(InstructionStream host, RelocInfo*, HeapObject value);
-  static void MarkingSlow(JSArrayBuffer host, ArrayBufferExtension*);
-  static void MarkingSlow(DescriptorArray, int number_of_own_descriptors);
-  static void MarkingSlow(HeapObject host, IndirectPointerSlot slot);
-  static void MarkingSlowFromGlobalHandle(HeapObject value);
-  static void MarkingSlowFromInternalFields(Heap* heap, JSObject host);
-
-  static inline void GenerationalBarrierFromInternalFields(JSObject host,
-                                                           void* value);
-  static inline void GenerationalBarrierFromInternalFields(JSObject host,
-                                                           size_t argc,
-                                                           void** values);
-
-  static void SharedSlow(InstructionStream host, RelocInfo*, HeapObject value);
+  static inline bool IsMarking(Tagged<HeapObject> object);
+
+  static void MarkingSlow(Tagged<InstructionStream> host, RelocInfo*,
+                          Tagged<HeapObject> value);
+  static void MarkingSlow(Tagged<JSArrayBuffer> host, ArrayBufferExtension*);
+  static void MarkingSlow(Tagged<DescriptorArray>,
+                          int number_of_own_descriptors);
+  static void MarkingSlow(Tagged<HeapObject> host, IndirectPointerSlot slot);
+  static void MarkingSlowFromGlobalHandle(Tagged<HeapObject> value);
+  static void MarkingSlowFromInternalFields(Heap* heap, Tagged<JSObject> host);
+
+  static inline void GenerationalBarrierFromInternalFields(
+      Tagged<JSObject> host, void* value);
+  static inline void GenerationalBarrierFromInternalFields(
+      Tagged<JSObject> host, size_t argc, void** values);
+
+  static void SharedSlow(Tagged<InstructionStream> host, RelocInfo*,
+                         Tagged<HeapObject> value);
 
   friend class Heap;
 };
diff --git a/src/heap/heap.cc b/src/heap/heap.cc
index 5df84652f9f..97a20266797 100644
--- a/src/heap/heap.cc
+++ b/src/heap/heap.cc
@@ -6258,34 +6258,36 @@ class UnreachableObjectsFilter : public HeapObjectsFilter {
     explicit MarkingVisitor(UnreachableObjectsFilter* filter)
         : ObjectVisitorWithCageBases(filter->heap_), filter_(filter) {}
 
-    void VisitMapPointer(HeapObject object) override {
+    void VisitMapPointer(Tagged<HeapObject> object) override {
       MarkHeapObject(Map::unchecked_cast(object->map(cage_base())));
     }
-    void VisitPointers(HeapObject host, ObjectSlot start,
+    void VisitPointers(Tagged<HeapObject> host, ObjectSlot start,
                        ObjectSlot end) override {
       MarkPointers(MaybeObjectSlot(start), MaybeObjectSlot(end));
     }
 
-    void VisitPointers(HeapObject host, MaybeObjectSlot start,
+    void VisitPointers(Tagged<HeapObject> host, MaybeObjectSlot start,
                        MaybeObjectSlot end) final {
       MarkPointers(start, end);
     }
 
-    void VisitInstructionStreamPointer(Code host,
+    void VisitInstructionStreamPointer(Tagged<Code> host,
                                        InstructionStreamSlot slot) override {
       Tagged<Object> maybe_code = slot.load(code_cage_base());
-      HeapObject heap_object;
+      Tagged<HeapObject> heap_object;
       if (maybe_code->GetHeapObject(&heap_object)) {
         MarkHeapObject(heap_object);
       }
     }
 
-    void VisitCodeTarget(InstructionStream host, RelocInfo* rinfo) final {
+    void VisitCodeTarget(Tagged<InstructionStream> host,
+                         RelocInfo* rinfo) final {
       Tagged<InstructionStream> target =
           InstructionStream::FromTargetAddress(rinfo->target_address());
       MarkHeapObject(target);
     }
-    void VisitEmbeddedPointer(InstructionStream host, RelocInfo* rinfo) final {
+    void VisitEmbeddedPointer(Tagged<InstructionStream> host,
+                              RelocInfo* rinfo) final {
       MarkHeapObject(rinfo->target_object(cage_base()));
     }
 
@@ -6317,7 +6319,7 @@ class UnreachableObjectsFilter : public HeapObjectsFilter {
       // Treat weak references as strong.
       for (TSlot p = start; p < end; ++p) {
         typename TSlot::TObject object = p.load(cage_base());
-        HeapObject heap_object;
+        Tagged<HeapObject> heap_object;
         if (object.GetHeapObject(&heap_object)) {
           MarkHeapObject(heap_object);
         }
@@ -6791,7 +6793,7 @@ void Heap::CreateObjectStats() {
   }
 }
 
-Map Heap::GcSafeMapOfHeapObject(Tagged<HeapObject> object) {
+Tagged<Map> Heap::GcSafeMapOfHeapObject(Tagged<HeapObject> object) {
   PtrComprCageBase cage_base(isolate());
   MapWord map_word = object->map_word(cage_base, kRelaxedLoad);
   if (map_word.IsForwardingAddress()) {
@@ -6800,9 +6802,9 @@ Map Heap::GcSafeMapOfHeapObject(Tagged<HeapObject> object) {
   return map_word.ToMap();
 }
 
-GcSafeCode Heap::GcSafeGetCodeFromInstructionStream(
+Tagged<GcSafeCode> Heap::GcSafeGetCodeFromInstructionStream(
     Tagged<HeapObject> instruction_stream, Address inner_pointer) {
-  InstructionStream istream =
+  Tagged<InstructionStream> istream =
       InstructionStream::unchecked_cast(instruction_stream);
   DCHECK(!istream.is_null());
   DCHECK(GcSafeInstructionStreamContains(istream, inner_pointer));
@@ -6858,11 +6860,11 @@ base::Optional<GcSafeCode> Heap::GcSafeTryFindCodeForInnerPointer(
   return GcSafeGetCodeFromInstructionStream(*maybe_istream, inner_pointer);
 }
 
-Code Heap::FindCodeForInnerPointer(Address inner_pointer) {
+Tagged<Code> Heap::FindCodeForInnerPointer(Address inner_pointer) {
   return GcSafeFindCodeForInnerPointer(inner_pointer)->UnsafeCastToCode();
 }
 
-GcSafeCode Heap::GcSafeFindCodeForInnerPointer(Address inner_pointer) {
+Tagged<GcSafeCode> Heap::GcSafeFindCodeForInnerPointer(Address inner_pointer) {
   base::Optional<GcSafeCode> maybe_code =
       GcSafeTryFindCodeForInnerPointer(inner_pointer);
   // Callers expect that the code object is found.
@@ -6972,7 +6974,7 @@ void Heap::WriteBarrierForRangeImpl(MemoryChunk* source_page,
 
   for (TSlot slot = start_slot; slot < end_slot; ++slot) {
     typename TSlot::TObject value = *slot;
-    HeapObject value_heap_object;
+    Tagged<HeapObject> value_heap_object;
     if (!value.GetHeapObject(&value_heap_object)) continue;
 
     if (kModeMask & kDoGenerationalOrShared) {
diff --git a/src/heap/heap.h b/src/heap/heap.h
index 2ecc62b0a9c..229a7801c43 100644
--- a/src/heap/heap.h
+++ b/src/heap/heap.h
@@ -1128,21 +1128,15 @@ class Heap final {
   // ===========================================================================
 
   // Returns whether the object resides in new space.
-  static inline bool InYoungGeneration(Object object);
+  static inline bool InYoungGeneration(Tagged<Object> object);
   static inline bool InYoungGeneration(MaybeObject object);
-  static inline bool InYoungGeneration(HeapObject heap_object);
-  template <typename T>
-  static inline bool InYoungGeneration(Tagged<T> object);
-  static inline bool InFromPage(Object object);
+  static inline bool InYoungGeneration(Tagged<HeapObject> heap_object);
+  static inline bool InFromPage(Tagged<Object> object);
   static inline bool InFromPage(MaybeObject object);
-  static inline bool InFromPage(HeapObject heap_object);
-  template <typename T>
-  static inline bool InFromPage(Tagged<T> object);
-  static inline bool InToPage(Object object);
+  static inline bool InFromPage(Tagged<HeapObject> heap_object);
+  static inline bool InToPage(Tagged<Object> object);
   static inline bool InToPage(MaybeObject object);
-  static inline bool InToPage(HeapObject heap_object);
-  template <typename T>
-  static inline bool InToPage(Tagged<T> object);
+  static inline bool InToPage(Tagged<HeapObject> heap_object);
 
   // Returns whether the object resides in old space.
   inline bool InOldSpace(Tagged<Object> object);
@@ -1452,12 +1446,8 @@ class Heap final {
   // Check if the given object was recently allocated and its fields may appear
   // as uninitialized to background threads.
   // This predicate may be invoked from a background thread.
-  inline bool IsPendingAllocation(HeapObject object);
-  inline bool IsPendingAllocation(Object object);
-  template <typename T>
-  inline bool IsPendingAllocation(Tagged<T> object) {
-    return IsPendingAllocation(*object);
-  }
+  inline bool IsPendingAllocation(Tagged<HeapObject> object);
+  inline bool IsPendingAllocation(Tagged<Object> object);
 
   // Notifies that all previously allocated objects are properly initialized
   // and ensures that IsPendingAllocation returns false for them. This function
@@ -1491,9 +1481,9 @@ class Heap final {
   // ===========================================================================
 
   // Searches for a Code object by the given interior pointer.
-  V8_EXPORT_PRIVATE Code FindCodeForInnerPointer(Address inner_pointer);
+  V8_EXPORT_PRIVATE Tagged<Code> FindCodeForInnerPointer(Address inner_pointer);
   // Use the GcSafe family of functions if called while GC is in progress.
-  GcSafeCode GcSafeFindCodeForInnerPointer(Address inner_pointer);
+  Tagged<GcSafeCode> GcSafeFindCodeForInnerPointer(Address inner_pointer);
   base::Optional<GcSafeCode> GcSafeTryFindCodeForInnerPointer(
       Address inner_pointer);
   base::Optional<InstructionStream>
@@ -1806,11 +1796,11 @@ class Heap final {
 
   // Casts a heap object to an InstructionStream, DCHECKs that the
   // inner_pointer is within the object, and returns the attached Code object.
-  GcSafeCode GcSafeGetCodeFromInstructionStream(
+  Tagged<GcSafeCode> GcSafeGetCodeFromInstructionStream(
       Tagged<HeapObject> instruction_stream, Address inner_pointer);
   // Returns the map of a HeapObject. Can be used during garbage collection,
   // i.e. it supports a forwarded map.
-  Map GcSafeMapOfHeapObject(Tagged<HeapObject> object);
+  Tagged<Map> GcSafeMapOfHeapObject(Tagged<HeapObject> object);
 
   // ===========================================================================
   // Actual GC. ================================================================
@@ -1975,10 +1965,10 @@ class Heap final {
   // otherwise it falls back to a slower path indicated by the mode.
   enum AllocationRetryMode { kLightRetry, kRetryOrFail };
   template <AllocationRetryMode mode>
-  V8_WARN_UNUSED_RESULT V8_INLINE HeapObject
-  AllocateRawWith(int size, AllocationType allocation,
-                  AllocationOrigin origin = AllocationOrigin::kRuntime,
-                  AllocationAlignment alignment = kTaggedAligned);
+  V8_WARN_UNUSED_RESULT V8_INLINE Tagged<HeapObject> AllocateRawWith(
+      int size, AllocationType allocation,
+      AllocationOrigin origin = AllocationOrigin::kRuntime,
+      AllocationAlignment alignment = kTaggedAligned);
 
   // Call AllocateRawWith with kRetryOrFail. Matches the method in LocalHeap.
   V8_WARN_UNUSED_RESULT inline Address AllocateRawOrFail(
@@ -2522,7 +2512,8 @@ using CodePageHeaderModificationScope = NopRwxMemoryWriteScope;
 class V8_NODISCARD CodePageMemoryModificationScope {
  public:
   explicit inline CodePageMemoryModificationScope(BasicMemoryChunk* chunk);
-  explicit inline CodePageMemoryModificationScope(InstructionStream object);
+  explicit inline CodePageMemoryModificationScope(
+      Tagged<InstructionStream> object);
   inline ~CodePageMemoryModificationScope();
 
  private:
diff --git a/src/heap/incremental-marking-inl.h b/src/heap/incremental-marking-inl.h
index fcba7d095c3..596d881307b 100644
--- a/src/heap/incremental-marking-inl.h
+++ b/src/heap/incremental-marking-inl.h
@@ -15,7 +15,8 @@
 namespace v8 {
 namespace internal {
 
-void IncrementalMarking::TransferColor(HeapObject from, HeapObject to) {
+void IncrementalMarking::TransferColor(Tagged<HeapObject> from,
+                                       Tagged<HeapObject> to) {
   if (marking_state()->IsMarked(to)) {
     DCHECK(black_allocation());
     return;
diff --git a/src/heap/incremental-marking.cc b/src/heap/incremental-marking.cc
index 54d9ead09bb..e640bff20c5 100644
--- a/src/heap/incremental-marking.cc
+++ b/src/heap/incremental-marking.cc
@@ -113,7 +113,8 @@ IncrementalMarking::IncrementalMarking(Heap* heap, WeakObjects* weak_objects)
                                kMajorGCOldGenerationAllocationObserverStep),
       minor_gc_observer_(this) {}
 
-void IncrementalMarking::MarkBlackBackground(HeapObject obj, int object_size) {
+void IncrementalMarking::MarkBlackBackground(Tagged<HeapObject> obj,
+                                             int object_size) {
   CHECK(marking_state()->TryMark(obj));
   base::MutexGuard guard(&background_live_bytes_mutex_);
   background_live_bytes_[MemoryChunk::FromHeapObject(obj)] +=
@@ -216,7 +217,7 @@ void IncrementalMarking::Start(GarbageCollector garbage_collector,
   }
 }
 
-bool IncrementalMarking::WhiteToGreyAndPush(HeapObject obj) {
+bool IncrementalMarking::WhiteToGreyAndPush(Tagged<HeapObject> obj) {
   if (marking_state()->TryMark(obj)) {
     local_marking_worklists()->Push(obj);
     return true;
@@ -246,10 +247,10 @@ class IncrementalMarking::IncrementalMarkingRootMarkingVisitor final
 
  private:
   void MarkObjectByPointer(Root root, FullObjectSlot p) {
-    Object object = *p;
+    Tagged<Object> object = *p;
     if (!IsHeapObject(object)) return;
     DCHECK(!MapWord::IsPacked(object.ptr()));
-    HeapObject heap_object = HeapObject::cast(object);
+    Tagged<HeapObject> heap_object = HeapObject::cast(object);
 
     if (heap_object.InAnySharedSpace() || heap_object.InReadOnlySpace()) return;
 
@@ -493,8 +494,9 @@ void IncrementalMarking::UpdateMarkingWorklistAfterScavenge() {
   PtrComprCageBase cage_base(isolate());
   major_collector_->marking_worklists()->Update([this, marking_state, cage_base,
                                                  filler_map](
-                                                    HeapObject obj,
-                                                    HeapObject* out) -> bool {
+                                                    Tagged<HeapObject> obj,
+                                                    Tagged<HeapObject>* out)
+                                                    -> bool {
     DCHECK(IsHeapObject(obj));
     USE(marking_state);
 
diff --git a/src/heap/incremental-marking.h b/src/heap/incremental-marking.h
index fc0822808c9..986f7307f20 100644
--- a/src/heap/incremental-marking.h
+++ b/src/heap/incremental-marking.h
@@ -66,7 +66,7 @@ class V8_EXPORT_PRIVATE IncrementalMarking final {
     bool paused_ = false;
   };
 
-  V8_INLINE void TransferColor(HeapObject from, HeapObject to);
+  V8_INLINE void TransferColor(Tagged<HeapObject> from, Tagged<HeapObject> to);
 
   IncrementalMarking(Heap* heap, WeakObjects* weak_objects);
 
@@ -132,7 +132,7 @@ class V8_EXPORT_PRIVATE IncrementalMarking final {
 
   bool IsBelowActivationThresholds() const;
 
-  void MarkBlackBackground(HeapObject obj, int object_size);
+  void MarkBlackBackground(Tagged<HeapObject> obj, int object_size);
 
   void MarkRootsForTesting();
 
@@ -175,7 +175,7 @@ class V8_EXPORT_PRIVATE IncrementalMarking final {
   void MarkRoots();
   // Returns true if the function succeeds in transitioning the object
   // from white to grey.
-  bool WhiteToGreyAndPush(HeapObject obj);
+  bool WhiteToGreyAndPush(Tagged<HeapObject> obj);
   void PublishWriteBarrierWorklists();
 
   // Fetches marked byte counters from the concurrent marker.
diff --git a/src/heap/large-page.h b/src/heap/large-page.h
index d6ed85fca16..ef21dba00c7 100644
--- a/src/heap/large-page.h
+++ b/src/heap/large-page.h
@@ -26,7 +26,7 @@ class LargePage : public MemoryChunk {
     return cast(MemoryChunk::cast(chunk));
   }
 
-  static LargePage* FromHeapObject(HeapObject o) {
+  static LargePage* FromHeapObject(Tagged<HeapObject> o) {
     DCHECK(!V8_ENABLE_THIRD_PARTY_HEAP_BOOL);
     return cast(MemoryChunk::FromHeapObject(o));
   }
diff --git a/src/heap/large-spaces.cc b/src/heap/large-spaces.cc
index 88a01945f24..4ebf5f6f746 100644
--- a/src/heap/large-spaces.cc
+++ b/src/heap/large-spaces.cc
@@ -107,7 +107,7 @@ AllocationResult OldLargeObjectSpace::AllocateRaw(int object_size,
   if (page == nullptr) return AllocationResult::Failure();
   page->SetOldGenerationPageFlags(
       heap()->incremental_marking()->marking_mode());
-  HeapObject object = page->GetObject();
+  Tagged<HeapObject> object = page->GetObject();
   UpdatePendingObject(object);
   heap()->StartIncrementalMarkingIfAllocationLimitIsReached(
       heap()->GCFlagsForIncrementalMarking(),
@@ -145,7 +145,7 @@ AllocationResult OldLargeObjectSpace::AllocateRawBackground(
   if (page == nullptr) return AllocationResult::Failure();
   page->SetOldGenerationPageFlags(
       heap()->incremental_marking()->marking_mode());
-  HeapObject object = page->GetObject();
+  Tagged<HeapObject> object = page->GetObject();
   heap()->StartIncrementalMarkingIfAllocationLimitIsReachedBackground();
   if (heap()->incremental_marking()->black_allocation()) {
     heap()->marking_state()->TryMarkAndAccountLiveBytes(object);
@@ -176,7 +176,7 @@ LargePage* LargeObjectSpace::AllocateLargePage(int object_size,
     AddPage(page, object_size);
   }
 
-  HeapObject object = page->GetObject();
+  Tagged<HeapObject> object = page->GetObject();
 
   heap()->CreateFillerObjectAt(object.address(), object_size);
   return page;
@@ -230,7 +230,7 @@ void LargeObjectSpace::RemovePage(LargePage* page) {
 }
 
 void LargeObjectSpace::ShrinkPageToObjectSize(LargePage* page,
-                                              HeapObject object,
+                                              Tagged<HeapObject> object,
                                               size_t object_size) {
 #ifdef DEBUG
   PtrComprCageBase cage_base(heap()->isolate());
@@ -266,7 +266,7 @@ void LargeObjectSpace::ShrinkPageToObjectSize(LargePage* page,
   DCHECK_EQ(object_size, page->area_size());
 }
 
-bool LargeObjectSpace::Contains(HeapObject object) const {
+bool LargeObjectSpace::Contains(Tagged<HeapObject> object) const {
   BasicMemoryChunk* chunk = BasicMemoryChunk::FromHeapObject(object);
 
   bool owned = (chunk->owner() == this);
@@ -304,7 +304,7 @@ void LargeObjectSpace::Verify(Isolate* isolate,
 
     // Each chunk contains an object that starts at the large object page's
     // object area start.
-    HeapObject object = chunk->GetObject();
+    Tagged<HeapObject> object = chunk->GetObject();
     Page* page = Page::FromHeapObject(object);
     CHECK(object.address() == page->area_start());
 
@@ -367,13 +367,13 @@ void LargeObjectSpace::Verify(Isolate* isolate,
 void LargeObjectSpace::Print() {
   StdoutStream os;
   LargeObjectSpaceObjectIterator it(this);
-  for (HeapObject obj = it.Next(); !obj.is_null(); obj = it.Next()) {
+  for (Tagged<HeapObject> obj = it.Next(); !obj.is_null(); obj = it.Next()) {
     i::Print(obj, os);
   }
 }
 #endif  // DEBUG
 
-void LargeObjectSpace::UpdatePendingObject(HeapObject object) {
+void LargeObjectSpace::UpdatePendingObject(Tagged<HeapObject> object) {
   base::SharedMutexGuard<base::kExclusive> guard(&pending_allocation_mutex_);
   pending_object_.store(object.address(), std::memory_order_release);
 }
@@ -407,7 +407,7 @@ AllocationResult NewLargeObjectSpace::AllocateRaw(int object_size) {
   // The size of the first object may exceed the capacity.
   capacity_ = std::max(capacity_, SizeOfObjects());
 
-  HeapObject result = page->GetObject();
+  Tagged<HeapObject> result = page->GetObject();
   page->SetYoungGenerationPageFlags(
       heap()->incremental_marking()->marking_mode());
   page->SetFlag(MemoryChunk::TO_PAGE);
@@ -436,7 +436,7 @@ void NewLargeObjectSpace::Flip() {
 }
 
 void NewLargeObjectSpace::FreeDeadObjects(
-    const std::function<bool(HeapObject)>& is_dead) {
+    const std::function<bool(Tagged<HeapObject>)>& is_dead) {
   bool is_marking = heap()->incremental_marking()->IsMarking();
   DCHECK_IMPLIES(v8_flags.minor_ms, !is_marking);
   DCHECK_IMPLIES(is_marking, heap()->incremental_marking()->IsMajorMarking());
@@ -445,7 +445,7 @@ void NewLargeObjectSpace::FreeDeadObjects(
   for (auto it = begin(); it != end();) {
     LargePage* page = *it;
     it++;
-    HeapObject object = page->GetObject();
+    Tagged<HeapObject> object = page->GetObject();
     if (is_dead(object)) {
       RemovePage(page);
       heap()->memory_allocator()->Free(MemoryAllocator::FreeMode::kConcurrently,
diff --git a/src/heap/large-spaces.h b/src/heap/large-spaces.h
index 9e4e887f608..de168f68663 100644
--- a/src/heap/large-spaces.h
+++ b/src/heap/large-spaces.h
@@ -51,11 +51,11 @@ class V8_EXPORT_PRIVATE LargeObjectSpace : public Space {
 
   int PageCount() const { return page_count_; }
 
-  void ShrinkPageToObjectSize(LargePage* page, HeapObject object,
+  void ShrinkPageToObjectSize(LargePage* page, Tagged<HeapObject> object,
                               size_t object_size);
 
   // Checks whether a heap object is in this space; O(1).
-  bool Contains(HeapObject obj) const;
+  bool Contains(Tagged<HeapObject> obj) const;
   // Checks whether an address is in the object area in this space. Iterates all
   // objects in the space. May be slow.
   bool ContainsSlow(Address addr) const;
@@ -112,7 +112,7 @@ class V8_EXPORT_PRIVATE LargeObjectSpace : public Space {
 
   LargePage* AllocateLargePage(int object_size, Executability executable);
 
-  void UpdatePendingObject(HeapObject object);
+  void UpdatePendingObject(Tagged<HeapObject> object);
 
   std::atomic<size_t> size_;  // allocated bytes
   int page_count_;       // number of chunks
@@ -176,7 +176,7 @@ class NewLargeObjectSpace : public LargeObjectSpace {
 
   void Flip();
 
-  void FreeDeadObjects(const std::function<bool(HeapObject)>& is_dead);
+  void FreeDeadObjects(const std::function<bool(Tagged<HeapObject>)>& is_dead);
 
   void SetCapacity(size_t capacity);
 
diff --git a/src/heap/local-heap-inl.h b/src/heap/local-heap-inl.h
index a6b021fdb4b..beb47e66d51 100644
--- a/src/heap/local-heap-inl.h
+++ b/src/heap/local-heap-inl.h
@@ -50,7 +50,7 @@ AllocationResult LocalHeap::AllocateRaw(int size_in_bytes, AllocationType type,
       alloc =
           code_space_allocator()->AllocateRaw(size_in_bytes, alignment, origin);
     }
-    HeapObject object;
+    Tagged<HeapObject> object;
     if (heap::ShouldZapGarbage() && alloc.To(&object) &&
         !V8_ENABLE_THIRD_PARTY_HEAP_BOOL) {
       heap::ZapCodeBlock(object.address(), size_in_bytes);
@@ -77,13 +77,14 @@ AllocationResult LocalHeap::AllocateRaw(int size_in_bytes, AllocationType type,
 }
 
 template <typename LocalHeap::AllocationRetryMode mode>
-HeapObject LocalHeap::AllocateRawWith(int object_size, AllocationType type,
-                                      AllocationOrigin origin,
-                                      AllocationAlignment alignment) {
+Tagged<HeapObject> LocalHeap::AllocateRawWith(int object_size,
+                                              AllocationType type,
+                                              AllocationOrigin origin,
+                                              AllocationAlignment alignment) {
   object_size = ALIGN_TO_ALLOCATION_ALIGNMENT(object_size);
   DCHECK(!v8_flags.enable_third_party_heap);
   AllocationResult result = AllocateRaw(object_size, type, origin, alignment);
-  HeapObject object;
+  Tagged<HeapObject> object;
   if (result.To(&object)) return object;
   result =
       PerformCollectionAndAllocateAgain(object_size, type, origin, alignment);
diff --git a/src/heap/local-heap.cc b/src/heap/local-heap.cc
index 1125a5af2e0..426466a48ca 100644
--- a/src/heap/local-heap.cc
+++ b/src/heap/local-heap.cc
@@ -489,7 +489,7 @@ void LocalHeap::InvokeGCEpilogueCallbacksInSafepoint(
 }
 
 void LocalHeap::NotifyObjectSizeChange(
-    HeapObject object, int old_size, int new_size,
+    Tagged<HeapObject> object, int old_size, int new_size,
     ClearRecordedSlots clear_recorded_slots) {
   heap()->NotifyObjectSizeChange(object, old_size, new_size,
                                  clear_recorded_slots);
diff --git a/src/heap/local-heap.h b/src/heap/local-heap.h
index 2b2da86c7fa..1062f87b928 100644
--- a/src/heap/local-heap.h
+++ b/src/heap/local-heap.h
@@ -156,7 +156,7 @@ class V8_EXPORT_PRIVATE LocalHeap {
   // Allocate an uninitialized object.
   enum AllocationRetryMode { kLightRetry, kRetryOrFail };
   template <AllocationRetryMode mode>
-  HeapObject AllocateRawWith(
+  Tagged<HeapObject> AllocateRawWith(
       int size_in_bytes, AllocationType allocation,
       AllocationOrigin origin = AllocationOrigin::kRuntime,
       AllocationAlignment alignment = kTaggedAligned);
@@ -168,7 +168,8 @@ class V8_EXPORT_PRIVATE LocalHeap {
       AllocationOrigin origin = AllocationOrigin::kRuntime,
       AllocationAlignment alignment = kTaggedAligned);
 
-  void NotifyObjectSizeChange(HeapObject object, int old_size, int new_size,
+  void NotifyObjectSizeChange(Tagged<HeapObject> object, int old_size,
+                              int new_size,
                               ClearRecordedSlots clear_recorded_slots);
 
   bool is_main_thread() const { return is_main_thread_; }
diff --git a/src/heap/mark-compact-inl.h b/src/heap/mark-compact-inl.h
index 54f1cba044b..42a57030f0c 100644
--- a/src/heap/mark-compact-inl.h
+++ b/src/heap/mark-compact-inl.h
@@ -20,7 +20,8 @@
 namespace v8 {
 namespace internal {
 
-void MarkCompactCollector::MarkObject(HeapObject host, HeapObject obj) {
+void MarkCompactCollector::MarkObject(Tagged<HeapObject> host,
+                                      Tagged<HeapObject> obj) {
   DCHECK(ReadOnlyHeap::Contains(obj) || heap_->Contains(obj));
   if (marking_state_->TryMark(obj)) {
     local_marking_worklists_->Push(obj);
@@ -31,14 +32,16 @@ void MarkCompactCollector::MarkObject(HeapObject host, HeapObject obj) {
 }
 
 // static
-void MarkCompactCollector::RecordSlot(HeapObject object, ObjectSlot slot,
-                                      HeapObject target) {
+void MarkCompactCollector::RecordSlot(Tagged<HeapObject> object,
+                                      ObjectSlot slot,
+                                      Tagged<HeapObject> target) {
   RecordSlot(object, HeapObjectSlot(slot), target);
 }
 
 // static
-void MarkCompactCollector::RecordSlot(HeapObject object, HeapObjectSlot slot,
-                                      HeapObject target) {
+void MarkCompactCollector::RecordSlot(Tagged<HeapObject> object,
+                                      HeapObjectSlot slot,
+                                      Tagged<HeapObject> target) {
   MemoryChunk* source_page = MemoryChunk::FromHeapObject(object);
   if (!source_page->ShouldSkipEvacuationSlotRecording()) {
     RecordSlot(source_page, slot, target);
@@ -47,7 +50,8 @@ void MarkCompactCollector::RecordSlot(HeapObject object, HeapObjectSlot slot,
 
 // static
 void MarkCompactCollector::RecordSlot(MemoryChunk* source_page,
-                                      HeapObjectSlot slot, HeapObject target) {
+                                      HeapObjectSlot slot,
+                                      Tagged<HeapObject> target) {
   BasicMemoryChunk* target_page = BasicMemoryChunk::FromHeapObject(target);
   if (target_page->IsEvacuationCandidate()) {
     if (target_page->IsFlagSet(MemoryChunk::IS_EXECUTABLE)) {
@@ -60,7 +64,7 @@ void MarkCompactCollector::RecordSlot(MemoryChunk* source_page,
   }
 }
 
-void MarkCompactCollector::AddTransitionArray(TransitionArray array) {
+void MarkCompactCollector::AddTransitionArray(Tagged<TransitionArray> array) {
   local_weak_objects()->transition_arrays_local.Push(array);
 }
 
diff --git a/src/heap/mark-compact.cc b/src/heap/mark-compact.cc
index 3e62e0eb201..5a6a01dd7ae 100644
--- a/src/heap/mark-compact.cc
+++ b/src/heap/mark-compact.cc
@@ -121,11 +121,11 @@ class FullMarkingVerifier : public MarkingVerifierBase {
     return chunk->marking_bitmap();
   }
 
-  bool IsMarked(HeapObject object) override {
+  bool IsMarked(Tagged<HeapObject> object) override {
     return marking_state_->IsMarked(object);
   }
 
-  void VerifyMap(Map map) override { VerifyHeapObjectImpl(map); }
+  void VerifyMap(Tagged<Map> map) override { VerifyHeapObjectImpl(map); }
 
   void VerifyPointers(ObjectSlot start, ObjectSlot end) override {
     VerifyPointersImpl(start, end);
@@ -136,8 +136,8 @@ class FullMarkingVerifier : public MarkingVerifierBase {
   }
 
   void VerifyCodePointer(InstructionStreamSlot slot) override {
-    Object maybe_code = slot.load(code_cage_base());
-    HeapObject code;
+    Tagged<Object> maybe_code = slot.load(code_cage_base());
+    Tagged<HeapObject> code;
     // The slot might contain smi during Code creation, so skip it.
     if (maybe_code.GetHeapObject(&code)) {
       VerifyHeapObjectImpl(code);
@@ -148,23 +148,25 @@ class FullMarkingVerifier : public MarkingVerifierBase {
     VerifyPointersImpl(start, end);
   }
 
-  void VisitCodeTarget(InstructionStream host, RelocInfo* rinfo) override {
-    InstructionStream target =
+  void VisitCodeTarget(Tagged<InstructionStream> host,
+                       RelocInfo* rinfo) override {
+    Tagged<InstructionStream> target =
         InstructionStream::FromTargetAddress(rinfo->target_address());
     VerifyHeapObjectImpl(target);
   }
 
-  void VisitEmbeddedPointer(InstructionStream host, RelocInfo* rinfo) override {
+  void VisitEmbeddedPointer(Tagged<InstructionStream> host,
+                            RelocInfo* rinfo) override {
     DCHECK(RelocInfo::IsEmbeddedObjectMode(rinfo->rmode()));
-    HeapObject target_object = rinfo->target_object(cage_base());
-    Code code = Code::unchecked_cast(host->raw_code(kAcquireLoad));
+    Tagged<HeapObject> target_object = rinfo->target_object(cage_base());
+    Tagged<Code> code = Code::unchecked_cast(host->raw_code(kAcquireLoad));
     if (!code->IsWeakObject(target_object)) {
       VerifyHeapObjectImpl(target_object);
     }
   }
 
  private:
-  V8_INLINE void VerifyHeapObjectImpl(HeapObject heap_object) {
+  V8_INLINE void VerifyHeapObjectImpl(Tagged<HeapObject> heap_object) {
     if (!ShouldVerifyObject(heap_object)) return;
 
     if (heap_->MustBeInSharedOldSpace(heap_object)) {
@@ -175,7 +177,7 @@ class FullMarkingVerifier : public MarkingVerifierBase {
           marking_state_->IsMarked(heap_object));
   }
 
-  V8_INLINE bool ShouldVerifyObject(HeapObject heap_object) {
+  V8_INLINE bool ShouldVerifyObject(Tagged<HeapObject> heap_object) {
     const bool in_shared_heap = heap_object.InWritableSharedSpace();
     return heap_->isolate()->is_shared_space_isolate() ? in_shared_heap
                                                        : !in_shared_heap;
@@ -185,7 +187,7 @@ class FullMarkingVerifier : public MarkingVerifierBase {
   V8_INLINE void VerifyPointersImpl(TSlot start, TSlot end) {
     for (TSlot slot = start; slot < end; ++slot) {
       typename TSlot::TObject object = slot.load(cage_base());
-      HeapObject heap_object;
+      Tagged<HeapObject> heap_object;
       if (object.GetHeapObjectIfStrong(&heap_object)) {
         VerifyHeapObjectImpl(heap_object);
       }
@@ -245,12 +247,13 @@ class MainMarkingVisitor final
   // Functions required by MarkingVisitorBase.
 
   template <typename TSlot>
-  void RecordSlot(HeapObject object, TSlot slot, HeapObject target) {
+  void RecordSlot(Tagged<HeapObject> object, TSlot slot,
+                  Tagged<HeapObject> target) {
     MarkCompactCollector::RecordSlot(object, slot, target);
   }
 
-  void RecordRelocSlot(InstructionStream host, RelocInfo* rinfo,
-                       HeapObject target) {
+  void RecordRelocSlot(Tagged<InstructionStream> host, RelocInfo* rinfo,
+                       Tagged<HeapObject> target) {
     MarkCompactCollector::RecordRelocSlot(host, rinfo, target);
   }
 
@@ -422,7 +425,7 @@ void MarkCompactCollector::VerifyMarkbitsAreClean(NewSpace* space) {
 void MarkCompactCollector::VerifyMarkbitsAreClean(LargeObjectSpace* space) {
   if (!space) return;
   LargeObjectSpaceObjectIterator it(space);
-  for (HeapObject obj = it.Next(); !obj.is_null(); obj = it.Next()) {
+  for (Tagged<HeapObject> obj = it.Next(); !obj.is_null(); obj = it.Next()) {
     CHECK(non_atomic_marking_state_->IsUnmarked(obj));
     CHECK_EQ(0, MemoryChunk::FromHeapObject(obj)->live_bytes());
   }
@@ -727,7 +730,7 @@ void ShrinkPagesToObjectSizes(Heap* heap, OldLargeObjectSpace* space) {
   PtrComprCageBase cage_base(heap->isolate());
   for (auto it = space->begin(); it != space->end();) {
     LargePage* current = *(it++);
-    HeapObject object = current->GetObject();
+    Tagged<HeapObject> object = current->GetObject();
     const size_t object_size = static_cast<size_t>(object->Size(cage_base));
     space->ShrinkPageToObjectSize(current, object, object_size);
     surviving_object_size += object_size;
@@ -840,7 +843,7 @@ void MarkCompactCollector::SweepArrayBufferExtensions() {
       ArrayBufferSweeper::TreatAllYoungAsPromoted::kYes);
 }
 
-void MarkCompactCollector::MarkRootObject(Root root, HeapObject obj) {
+void MarkCompactCollector::MarkRootObject(Root root, Tagged<HeapObject> obj) {
   DCHECK(ReadOnlyHeap::Contains(obj) || heap_->Contains(obj));
   if (marking_state_->TryMark(obj)) {
     local_marking_worklists_->Push(obj);
@@ -850,7 +853,7 @@ void MarkCompactCollector::MarkRootObject(Root root, HeapObject obj) {
   }
 }
 
-bool MarkCompactCollector::ShouldMarkObject(HeapObject object) const {
+bool MarkCompactCollector::ShouldMarkObject(Tagged<HeapObject> object) const {
   if (object.InReadOnlySpace()) return false;
   if (V8_LIKELY(!uses_shared_heap_)) return true;
   if (is_shared_space_isolate_) return true;
@@ -878,10 +881,10 @@ class MarkCompactCollector::RootMarkingVisitor final : public RootVisitor {
   // Keep this synced with RootsReferencesExtractor::VisitRunningCode.
   void VisitRunningCode(FullObjectSlot code_slot,
                         FullObjectSlot istream_or_smi_zero_slot) final {
-    Object istream_or_smi_zero = *istream_or_smi_zero_slot;
+    Tagged<Object> istream_or_smi_zero = *istream_or_smi_zero_slot;
     DCHECK(istream_or_smi_zero == Smi::zero() ||
            IsInstructionStream(istream_or_smi_zero));
-    Code code = Code::cast(*code_slot);
+    Tagged<Code> code = Code::cast(*code_slot);
     DCHECK_EQ(code->raw_instruction_stream(PtrComprCageBase{
                   collector_->heap_->isolate()->code_cage_base()}),
               istream_or_smi_zero);
@@ -899,9 +902,9 @@ class MarkCompactCollector::RootMarkingVisitor final : public RootVisitor {
 
  private:
   V8_INLINE void MarkObjectByPointer(Root root, FullObjectSlot p) {
-    Object object = *p;
+    Tagged<Object> object = *p;
     if (!IsHeapObject(object)) return;
-    HeapObject heap_object = HeapObject::cast(object);
+    Tagged<HeapObject> heap_object = HeapObject::cast(object);
     if (!collector_->ShouldMarkObject(heap_object)) return;
     collector_->MarkRootObject(root, heap_object);
   }
@@ -928,15 +931,16 @@ class MarkCompactCollector::CustomRootBodyMarkingVisitor final
       : ObjectVisitorWithCageBases(collector->heap_->isolate()),
         collector_(collector) {}
 
-  void VisitPointer(HeapObject host, ObjectSlot p) final {
+  void VisitPointer(Tagged<HeapObject> host, ObjectSlot p) final {
     MarkObject(host, p.load(cage_base()));
   }
 
-  void VisitMapPointer(HeapObject host) final {
+  void VisitMapPointer(Tagged<HeapObject> host) final {
     MarkObject(host, host->map(cage_base()));
   }
 
-  void VisitPointers(HeapObject host, ObjectSlot start, ObjectSlot end) final {
+  void VisitPointers(Tagged<HeapObject> host, ObjectSlot start,
+                     ObjectSlot end) final {
     for (ObjectSlot p = start; p < end; ++p) {
       // The map slot should be handled in VisitMapPointer.
       DCHECK_NE(host->map_slot(), p);
@@ -945,31 +949,33 @@ class MarkCompactCollector::CustomRootBodyMarkingVisitor final
     }
   }
 
-  void VisitInstructionStreamPointer(Code host,
+  void VisitInstructionStreamPointer(Tagged<Code> host,
                                      InstructionStreamSlot slot) override {
     MarkObject(host, slot.load(code_cage_base()));
   }
 
-  void VisitPointers(HeapObject host, MaybeObjectSlot start,
+  void VisitPointers(Tagged<HeapObject> host, MaybeObjectSlot start,
                      MaybeObjectSlot end) final {
     // At the moment, custom roots cannot contain weak pointers.
     UNREACHABLE();
   }
 
-  void VisitCodeTarget(InstructionStream host, RelocInfo* rinfo) override {
-    InstructionStream target =
+  void VisitCodeTarget(Tagged<InstructionStream> host,
+                       RelocInfo* rinfo) override {
+    Tagged<InstructionStream> target =
         InstructionStream::FromTargetAddress(rinfo->target_address());
     MarkObject(host, target);
   }
 
-  void VisitEmbeddedPointer(InstructionStream host, RelocInfo* rinfo) override {
+  void VisitEmbeddedPointer(Tagged<InstructionStream> host,
+                            RelocInfo* rinfo) override {
     MarkObject(host, rinfo->target_object(cage_base()));
   }
 
  private:
-  V8_INLINE void MarkObject(HeapObject host, Object object) {
+  V8_INLINE void MarkObject(Tagged<HeapObject> host, Tagged<Object> object) {
     if (!IsHeapObject(object)) return;
-    HeapObject heap_object = HeapObject::cast(object);
+    Tagged<HeapObject> heap_object = HeapObject::cast(object);
     if (!collector_->ShouldMarkObject(heap_object)) return;
     collector_->MarkObject(host, heap_object);
   }
@@ -984,22 +990,23 @@ class MarkCompactCollector::SharedHeapObjectVisitor final
       : ObjectVisitorWithCageBases(collector->heap_->isolate()),
         collector_(collector) {}
 
-  void VisitPointer(HeapObject host, ObjectSlot p) final {
+  void VisitPointer(Tagged<HeapObject> host, ObjectSlot p) final {
     CheckForSharedObject(host, p, p.load(cage_base()));
   }
 
-  void VisitPointer(HeapObject host, MaybeObjectSlot p) final {
+  void VisitPointer(Tagged<HeapObject> host, MaybeObjectSlot p) final {
     MaybeObject object = p.load(cage_base());
-    HeapObject heap_object;
+    Tagged<HeapObject> heap_object;
     if (object.GetHeapObject(&heap_object))
       CheckForSharedObject(host, ObjectSlot(p), heap_object);
   }
 
-  void VisitMapPointer(HeapObject host) final {
+  void VisitMapPointer(Tagged<HeapObject> host) final {
     CheckForSharedObject(host, host->map_slot(), host->map(cage_base()));
   }
 
-  void VisitPointers(HeapObject host, ObjectSlot start, ObjectSlot end) final {
+  void VisitPointers(Tagged<HeapObject> host, ObjectSlot start,
+                     ObjectSlot end) final {
     for (ObjectSlot p = start; p < end; ++p) {
       // The map slot should be handled in VisitMapPointer.
       DCHECK_NE(host->map_slot(), p);
@@ -1008,12 +1015,12 @@ class MarkCompactCollector::SharedHeapObjectVisitor final
     }
   }
 
-  void VisitInstructionStreamPointer(Code host,
+  void VisitInstructionStreamPointer(Tagged<Code> host,
                                      InstructionStreamSlot slot) override {
     UNREACHABLE();
   }
 
-  void VisitPointers(HeapObject host, MaybeObjectSlot start,
+  void VisitPointers(Tagged<HeapObject> host, MaybeObjectSlot start,
                      MaybeObjectSlot end) final {
     for (MaybeObjectSlot p = start; p < end; ++p) {
       // The map slot should be handled in VisitMapPointer.
@@ -1022,20 +1029,22 @@ class MarkCompactCollector::SharedHeapObjectVisitor final
     }
   }
 
-  void VisitCodeTarget(InstructionStream host, RelocInfo* rinfo) override {
+  void VisitCodeTarget(Tagged<InstructionStream> host,
+                       RelocInfo* rinfo) override {
     UNREACHABLE();
   }
 
-  void VisitEmbeddedPointer(InstructionStream host, RelocInfo* rinfo) override {
+  void VisitEmbeddedPointer(Tagged<InstructionStream> host,
+                            RelocInfo* rinfo) override {
     UNREACHABLE();
   }
 
  private:
-  V8_INLINE void CheckForSharedObject(HeapObject host, ObjectSlot slot,
-                                      Object object) {
+  V8_INLINE void CheckForSharedObject(Tagged<HeapObject> host, ObjectSlot slot,
+                                      Tagged<Object> object) {
     DCHECK(!host.InAnySharedSpace());
     if (!IsHeapObject(object)) return;
-    HeapObject heap_object = HeapObject::cast(object);
+    Tagged<HeapObject> heap_object = HeapObject::cast(object);
     if (!heap_object.InWritableSharedSpace()) return;
     DCHECK(heap_object.InWritableSharedSpace());
     MemoryChunk* host_chunk = MemoryChunk::FromHeapObject(host);
@@ -1065,9 +1074,9 @@ class InternalizedStringTableCleaner final : public RootVisitor {
     auto* marking_state = heap_->marking_state();
     Isolate* const isolate = heap_->isolate();
     for (OffHeapObjectSlot p = start; p < end; ++p) {
-      Object o = p.load(isolate);
+      Tagged<Object> o = p.load(isolate);
       if (IsHeapObject(o)) {
-        HeapObject heap_object = HeapObject::cast(o);
+        Tagged<HeapObject> heap_object = HeapObject::cast(o);
         DCHECK(!Heap::InYoungGeneration(heap_object));
         if (!heap_object.InReadOnlySpace() &&
             marking_state->IsUnmarked(heap_object)) {
@@ -1097,11 +1106,11 @@ class MarkExternalPointerFromExternalStringTable : public RootVisitor {
                          FullObjectSlot start, FullObjectSlot end) override {
     // Visit all HeapObject pointers in [start, end).
     for (FullObjectSlot p = start; p < end; ++p) {
-      Object o = *p;
+      Tagged<Object> o = *p;
       if (IsHeapObject(o)) {
-        HeapObject heap_object = HeapObject::cast(o);
+        Tagged<HeapObject> heap_object = HeapObject::cast(o);
         if (IsExternalString(heap_object)) {
-          ExternalString string = ExternalString::cast(heap_object);
+          Tagged<ExternalString> string = ExternalString::cast(heap_object);
           string->VisitExternalPointers(&visitor);
         } else {
           // The original external string may have been internalized.
@@ -1117,29 +1126,30 @@ class MarkExternalPointerFromExternalStringTable : public RootVisitor {
     explicit MarkExternalPointerTableVisitor(ExternalPointerTable* table,
                                              ExternalPointerTable::Space* space)
         : table_(table), space_(space) {}
-    void VisitExternalPointer(HeapObject host, ExternalPointerSlot slot,
+    void VisitExternalPointer(Tagged<HeapObject> host, ExternalPointerSlot slot,
                               ExternalPointerTag tag) override {
       DCHECK_NE(tag, kExternalPointerNullTag);
       DCHECK(IsSharedExternalPointerType(tag));
       ExternalPointerHandle handle = slot.Relaxed_LoadHandle();
       table_->Mark(space_, handle, slot.address());
     }
-    void VisitPointers(HeapObject host, ObjectSlot start,
+    void VisitPointers(Tagged<HeapObject> host, ObjectSlot start,
                        ObjectSlot end) override {
       UNREACHABLE();
     }
-    void VisitPointers(HeapObject host, MaybeObjectSlot start,
+    void VisitPointers(Tagged<HeapObject> host, MaybeObjectSlot start,
                        MaybeObjectSlot end) override {
       UNREACHABLE();
     }
-    void VisitInstructionStreamPointer(Code host,
+    void VisitInstructionStreamPointer(Tagged<Code> host,
                                        InstructionStreamSlot slot) override {
       UNREACHABLE();
     }
-    void VisitCodeTarget(InstructionStream host, RelocInfo* rinfo) override {
+    void VisitCodeTarget(Tagged<InstructionStream> host,
+                         RelocInfo* rinfo) override {
       UNREACHABLE();
     }
-    void VisitEmbeddedPointer(InstructionStream host,
+    void VisitEmbeddedPointer(Tagged<InstructionStream> host,
                               RelocInfo* rinfo) override {
       UNREACHABLE();
     }
@@ -1169,9 +1179,9 @@ class MarkCompactWeakObjectRetainer : public WeakObjectRetainer {
       // "dead" AllocationSites need to live long enough for a traversal of new
       // space. These sites get a one-time reprieve.
 
-      Object nested = object;
+      Tagged<Object> nested = object;
       while (IsAllocationSite(nested)) {
-        AllocationSite current_site = AllocationSite::cast(nested);
+        Tagged<AllocationSite> current_site = AllocationSite::cast(nested);
         // MarkZombie will override the nested_site, read it first before
         // marking
         nested = current_site->nested_site();
@@ -1197,22 +1207,22 @@ class RecordMigratedSlotVisitor : public ObjectVisitorWithCageBases {
         heap_(heap),
         ephemeron_remembered_set_(ephemeron_remembered_set) {}
 
-  inline void VisitPointer(HeapObject host, ObjectSlot p) final {
+  inline void VisitPointer(Tagged<HeapObject> host, ObjectSlot p) final {
     DCHECK(!HasWeakHeapObjectTag(p.load(cage_base())));
     RecordMigratedSlot(host, MaybeObject::FromObject(p.load(cage_base())),
                        p.address());
   }
 
-  inline void VisitMapPointer(HeapObject host) final {
+  inline void VisitMapPointer(Tagged<HeapObject> host) final {
     VisitPointer(host, host->map_slot());
   }
 
-  inline void VisitPointer(HeapObject host, MaybeObjectSlot p) final {
+  inline void VisitPointer(Tagged<HeapObject> host, MaybeObjectSlot p) final {
     DCHECK(!MapWord::IsPacked(p.Relaxed_Load(cage_base()).ptr()));
     RecordMigratedSlot(host, p.load(cage_base()), p.address());
   }
 
-  inline void VisitPointers(HeapObject host, ObjectSlot start,
+  inline void VisitPointers(Tagged<HeapObject> host, ObjectSlot start,
                             ObjectSlot end) final {
     while (start < end) {
       VisitPointer(host, start);
@@ -1220,7 +1230,7 @@ class RecordMigratedSlotVisitor : public ObjectVisitorWithCageBases {
     }
   }
 
-  inline void VisitPointers(HeapObject host, MaybeObjectSlot start,
+  inline void VisitPointers(Tagged<HeapObject> host, MaybeObjectSlot start,
                             MaybeObjectSlot end) final {
     while (start < end) {
       VisitPointer(host, start);
@@ -1228,16 +1238,16 @@ class RecordMigratedSlotVisitor : public ObjectVisitorWithCageBases {
     }
   }
 
-  inline void VisitInstructionStreamPointer(Code host,
+  inline void VisitInstructionStreamPointer(Tagged<Code> host,
                                             InstructionStreamSlot slot) final {
     // This code is similar to the implementation of VisitPointer() modulo
     // new kind of slot.
     DCHECK(!HasWeakHeapObjectTag(slot.load(code_cage_base())));
-    Object code = slot.load(code_cage_base());
+    Tagged<Object> code = slot.load(code_cage_base());
     RecordMigratedSlot(host, MaybeObject::FromObject(code), slot.address());
   }
 
-  inline void VisitEphemeron(HeapObject host, int index, ObjectSlot key,
+  inline void VisitEphemeron(Tagged<HeapObject> host, int index, ObjectSlot key,
                              ObjectSlot value) override {
     DCHECK(IsEphemeronHashTable(host));
     DCHECK(!Heap::InYoungGeneration(host));
@@ -1254,10 +1264,10 @@ class RecordMigratedSlotVisitor : public ObjectVisitorWithCageBases {
     }
   }
 
-  inline void VisitCodeTarget(InstructionStream host,
+  inline void VisitCodeTarget(Tagged<InstructionStream> host,
                               RelocInfo* rinfo) override {
     DCHECK(RelocInfo::IsCodeTargetMode(rinfo->rmode()));
-    InstructionStream target =
+    Tagged<InstructionStream> target =
         InstructionStream::FromTargetAddress(rinfo->target_address());
     // The target is always in old space, we don't have to record the slot in
     // the old-to-new remembered set.
@@ -1266,27 +1276,29 @@ class RecordMigratedSlotVisitor : public ObjectVisitorWithCageBases {
     heap_->mark_compact_collector()->RecordRelocSlot(host, rinfo, target);
   }
 
-  inline void VisitEmbeddedPointer(InstructionStream host,
+  inline void VisitEmbeddedPointer(Tagged<InstructionStream> host,
                                    RelocInfo* rinfo) override {
     DCHECK(RelocInfo::IsEmbeddedObjectMode(rinfo->rmode()));
-    HeapObject object = rinfo->target_object(cage_base());
+    Tagged<HeapObject> object = rinfo->target_object(cage_base());
     GenerationalBarrierForCode(host, rinfo, object);
     WriteBarrier::Shared(host, rinfo, object);
     heap_->mark_compact_collector()->RecordRelocSlot(host, rinfo, object);
   }
 
   // Entries that are skipped for recording.
-  inline void VisitExternalReference(InstructionStream host,
+  inline void VisitExternalReference(Tagged<InstructionStream> host,
                                      RelocInfo* rinfo) final {}
-  inline void VisitInternalReference(InstructionStream host,
+  inline void VisitInternalReference(Tagged<InstructionStream> host,
                                      RelocInfo* rinfo) final {}
-  inline void VisitExternalPointer(HeapObject host, ExternalPointerSlot slot,
+  inline void VisitExternalPointer(Tagged<HeapObject> host,
+                                   ExternalPointerSlot slot,
                                    ExternalPointerTag tag) final {}
 
-  inline void VisitIndirectPointer(HeapObject host, IndirectPointerSlot slot,
+  inline void VisitIndirectPointer(Tagged<HeapObject> host,
+                                   IndirectPointerSlot slot,
                                    IndirectPointerMode mode) final {}
 
-  inline void VisitIndirectPointerTableEntry(HeapObject host,
+  inline void VisitIndirectPointerTableEntry(Tagged<HeapObject> host,
                                              IndirectPointerSlot slot) final {
 #ifdef V8_CODE_POINTER_SANDBOXING
     // When an object owning an indirect pointer table entry is relocated, it
@@ -1303,7 +1315,7 @@ class RecordMigratedSlotVisitor : public ObjectVisitorWithCageBases {
   }
 
  protected:
-  inline void RecordMigratedSlot(HeapObject host, MaybeObject value,
+  inline void RecordMigratedSlot(Tagged<HeapObject> host, MaybeObject value,
                                  Address slot) {
     if (value->IsStrongOrWeak()) {
       BasicMemoryChunk* p = BasicMemoryChunk::FromAddress(value.ptr());
@@ -1337,8 +1349,8 @@ class MigrationObserver {
   explicit MigrationObserver(Heap* heap) : heap_(heap) {}
 
   virtual ~MigrationObserver() = default;
-  virtual void Move(AllocationSpace dest, HeapObject src, HeapObject dst,
-                    int size) = 0;
+  virtual void Move(AllocationSpace dest, Tagged<HeapObject> src,
+                    Tagged<HeapObject> dst, int size) = 0;
 
  protected:
   Heap* heap_;
@@ -1348,8 +1360,8 @@ class ProfilingMigrationObserver final : public MigrationObserver {
  public:
   explicit ProfilingMigrationObserver(Heap* heap) : MigrationObserver(heap) {}
 
-  inline void Move(AllocationSpace dest, HeapObject src, HeapObject dst,
-                   int size) final {
+  inline void Move(AllocationSpace dest, Tagged<HeapObject> src,
+                   Tagged<HeapObject> dst, int size) final {
     // Note this method is called in a concurrent setting. The current object
     // (src and dst) is somewhat safe to access without precautions, but other
     // objects may be subject to concurrent modification.
@@ -1367,7 +1379,7 @@ class ProfilingMigrationObserver final : public MigrationObserver {
 class HeapObjectVisitor {
  public:
   virtual ~HeapObjectVisitor() = default;
-  virtual bool Visit(HeapObject object, int size) = 0;
+  virtual bool Visit(Tagged<HeapObject> object, int size) = 0;
 };
 
 class EvacuateVisitorBase : public HeapObjectVisitor {
@@ -1412,13 +1424,15 @@ class EvacuateVisitorBase : public HeapObjectVisitor {
 #endif  // V8_COMPRESS_POINTERS
   }
 
-  using MigrateFunction = void (*)(EvacuateVisitorBase* base, HeapObject dst,
-                                   HeapObject src, int size,
+  using MigrateFunction = void (*)(EvacuateVisitorBase* base,
+                                   Tagged<HeapObject> dst,
+                                   Tagged<HeapObject> src, int size,
                                    AllocationSpace dest);
 
   template <MigrationMode mode>
-  static void RawMigrateObject(EvacuateVisitorBase* base, HeapObject dst,
-                               HeapObject src, int size, AllocationSpace dest) {
+  static void RawMigrateObject(EvacuateVisitorBase* base,
+                               Tagged<HeapObject> dst, Tagged<HeapObject> src,
+                               int size, AllocationSpace dest) {
     Address dst_addr = dst.address();
     Address src_addr = src.address();
     PtrComprCageBase cage_base = base->cage_base();
@@ -1450,7 +1464,7 @@ class EvacuateVisitorBase : public HeapObjectVisitor {
             ThreadIsolation::RegisterInstructionStreamAllocation(dst_addr,
                                                                  size);
         base->heap_->CopyBlock(dst_addr, src_addr, size);
-        InstructionStream istream = InstructionStream::cast(dst);
+        Tagged<InstructionStream> istream = InstructionStream::cast(dst);
         istream->Relocate(dst_addr - src_addr);
       }
       if (mode != MigrationMode::kFast) {
@@ -1489,8 +1503,9 @@ class EvacuateVisitorBase : public HeapObjectVisitor {
 #endif  // DEBUG
   }
 
-  inline bool TryEvacuateObject(AllocationSpace target_space, HeapObject object,
-                                int size, HeapObject* target_object) {
+  inline bool TryEvacuateObject(AllocationSpace target_space,
+                                Tagged<HeapObject> object, int size,
+                                Tagged<HeapObject>* target_object) {
 #if DEBUG
     DCHECK_LE(abort_evacuation_at_address_,
               MemoryChunk::FromHeapObject(object)->area_end());
@@ -1502,7 +1517,7 @@ class EvacuateVisitorBase : public HeapObjectVisitor {
     }
 #endif  // DEBUG
 
-    Map map = object->map(cage_base());
+    Tagged<Map> map = object->map(cage_base());
     AllocationAlignment alignment = HeapObject::RequiredAlignment(map);
     AllocationResult allocation;
     if (target_space == OLD_SPACE && ShouldPromoteIntoSharedHeap(map)) {
@@ -1525,7 +1540,7 @@ class EvacuateVisitorBase : public HeapObjectVisitor {
     return false;
   }
 
-  inline bool ShouldPromoteIntoSharedHeap(Map map) {
+  inline bool ShouldPromoteIntoSharedHeap(Tagged<Map> map) {
     if (shared_string_table_) {
       return String::IsInPlaceInternalizableExcludingExternal(
           map->instance_type());
@@ -1533,15 +1548,16 @@ class EvacuateVisitorBase : public HeapObjectVisitor {
     return false;
   }
 
-  inline void ExecuteMigrationObservers(AllocationSpace dest, HeapObject src,
-                                        HeapObject dst, int size) {
+  inline void ExecuteMigrationObservers(AllocationSpace dest,
+                                        Tagged<HeapObject> src,
+                                        Tagged<HeapObject> dst, int size) {
     for (MigrationObserver* obs : observers_) {
       obs->Move(dest, src, dst, size);
     }
   }
 
-  inline void MigrateObject(HeapObject dst, HeapObject src, int size,
-                            AllocationSpace dest) {
+  inline void MigrateObject(Tagged<HeapObject> dst, Tagged<HeapObject> src,
+                            int size, AllocationSpace dest) {
     migration_function_(this, dst, src, size, dest);
   }
 
@@ -1579,9 +1595,9 @@ class EvacuateNewSpaceVisitor final : public EvacuateVisitorBase {
                    heap->incremental_marking()->IsMajorMarking());
   }
 
-  inline bool Visit(HeapObject object, int size) override {
+  inline bool Visit(Tagged<HeapObject> object, int size) override {
     if (TryEvacuateWithoutCopy(object)) return true;
-    HeapObject target_object;
+    Tagged<HeapObject> target_object;
 
     pretenuring_handler_->UpdateAllocationSite(object->map(), object,
                                                local_pretenuring_feedback_);
@@ -1599,16 +1615,16 @@ class EvacuateNewSpaceVisitor final : public EvacuateVisitorBase {
   intptr_t semispace_copied_size() { return semispace_copied_size_; }
 
  private:
-  inline bool TryEvacuateWithoutCopy(HeapObject object) {
+  inline bool TryEvacuateWithoutCopy(Tagged<HeapObject> object) {
     DCHECK(!is_incremental_marking_);
 
     if (!shortcut_strings_) return false;
 
-    Map map = object->map();
+    Tagged<Map> map = object->map();
 
     // Some objects can be evacuated without creating a copy.
     if (map->visitor_id() == kVisitThinString) {
-      HeapObject actual = ThinString::cast(object)->unchecked_actual();
+      Tagged<HeapObject> actual = ThinString::cast(object)->unchecked_actual();
       if (MarkCompactCollector::IsOnEvacuationCandidate(actual)) return false;
       object->set_map_word_forwarded(actual, kRelaxedStore);
       return true;
@@ -1618,7 +1634,8 @@ class EvacuateNewSpaceVisitor final : public EvacuateVisitorBase {
     return false;
   }
 
-  inline AllocationSpace AllocateTargetObject(HeapObject old_object, int size,
+  inline AllocationSpace AllocateTargetObject(Tagged<HeapObject> old_object,
+                                              int size,
                                               HeapObject* target_object) {
     AllocationAlignment alignment =
         HeapObject::RequiredAlignment(old_object->map());
@@ -1670,7 +1687,7 @@ class EvacuateNewToOldSpacePageVisitor final : public HeapObjectVisitor {
     page->heap()->new_space()->PromotePageToOldSpace(page);
   }
 
-  inline bool Visit(HeapObject object, int size) override {
+  inline bool Visit(Tagged<HeapObject> object, int size) override {
     if (v8_flags.minor_ms) {
       pretenuring_handler_->UpdateAllocationSite(object->map(), object,
                                                  local_pretenuring_feedback_);
@@ -1700,8 +1717,8 @@ class EvacuateOldSpaceVisitor final : public EvacuateVisitorBase {
       : EvacuateVisitorBase(heap, local_allocator, shared_old_allocator,
                             record_visitor) {}
 
-  inline bool Visit(HeapObject object, int size) override {
-    HeapObject target_object;
+  inline bool Visit(Tagged<HeapObject> object, int size) override {
+    Tagged<HeapObject> target_object;
     if (TryEvacuateObject(Page::FromHeapObject(object)->owner_identity(),
                           object, size, &target_object)) {
       DCHECK(object->map_word(heap_->isolate(), kRelaxedLoad)
@@ -1717,10 +1734,10 @@ class EvacuateRecordOnlyVisitor final : public HeapObjectVisitor {
   explicit EvacuateRecordOnlyVisitor(Heap* heap)
       : heap_(heap), cage_base_(heap->isolate()) {}
 
-  bool Visit(HeapObject object, int size) override {
+  bool Visit(Tagged<HeapObject> object, int size) override {
     RecordMigratedSlotVisitor visitor(
         heap_, heap_->ephemeron_remembered_set()->tables());
-    Map map = object->map(cage_base_);
+    Tagged<Map> map = object->map(cage_base_);
     // Instead of calling object.IterateFast(cage_base(), &visitor) here
     // we can shortcut and use the precomputed size value passed to the visitor.
     DCHECK_EQ(object->SizeFromMap(map), size);
@@ -1739,9 +1756,9 @@ class EvacuateRecordOnlyVisitor final : public HeapObjectVisitor {
 
 // static
 bool MarkCompactCollector::IsUnmarkedHeapObject(Heap* heap, FullObjectSlot p) {
-  Object o = *p;
+  Tagged<Object> o = *p;
   if (!IsHeapObject(o)) return false;
-  HeapObject heap_object = HeapObject::cast(o);
+  Tagged<HeapObject> heap_object = HeapObject::cast(o);
   if (heap_object.InReadOnlySpace()) return false;
   MarkCompactCollector* collector = heap->mark_compact_collector();
   if (V8_UNLIKELY(collector->uses_shared_heap_) &&
@@ -1754,9 +1771,9 @@ bool MarkCompactCollector::IsUnmarkedHeapObject(Heap* heap, FullObjectSlot p) {
 // static
 bool MarkCompactCollector::IsUnmarkedSharedHeapObject(Heap* heap,
                                                       FullObjectSlot p) {
-  Object o = *p;
+  Tagged<Object> o = *p;
   if (!IsHeapObject(o)) return false;
-  HeapObject heap_object = HeapObject::cast(o);
+  Tagged<HeapObject> heap_object = HeapObject::cast(o);
   Isolate* shared_space_isolate = heap->isolate()->shared_space_isolate();
   MarkCompactCollector* collector =
       shared_space_isolate->heap()->mark_compact_collector();
@@ -1847,7 +1864,7 @@ void MarkCompactCollector::MarkObjectsFromClientHeap(Isolate* client) {
     new_space->MakeLinearAllocationAreaIterable();
 
     for (Page* page : *new_space) {
-      for (HeapObject obj : HeapObjectRange(page)) {
+      for (Tagged<HeapObject> obj : HeapObjectRange(page)) {
         obj->IterateFast(cage_base, &visitor);
       }
     }
@@ -1856,7 +1873,7 @@ void MarkCompactCollector::MarkObjectsFromClientHeap(Isolate* client) {
   if (heap->new_lo_space()) {
     std::unique_ptr<ObjectIterator> iterator =
         heap->new_lo_space()->GetObjectIterator(heap);
-    for (HeapObject obj = iterator->Next(); !obj.is_null();
+    for (Tagged<HeapObject> obj = iterator->Next(); !obj.is_null();
          obj = iterator->Next()) {
       obj->IterateFast(cage_base, &visitor);
     }
@@ -1875,7 +1892,7 @@ void MarkCompactCollector::MarkObjectsFromClientHeap(Isolate* client) {
         chunk,
         [collector = this, cage_base](MaybeObjectSlot slot) {
           MaybeObject obj = slot.Relaxed_Load(cage_base);
-          HeapObject heap_object;
+          Tagged<HeapObject> heap_object;
 
           if (obj.GetHeapObject(&heap_object) &&
               heap_object.InWritableSharedSpace()) {
@@ -1892,7 +1909,7 @@ void MarkCompactCollector::MarkObjectsFromClientHeap(Isolate* client) {
 
     const auto typed_slot_count = RememberedSet<OLD_TO_SHARED>::IterateTyped(
         chunk, [collector = this, heap](SlotType slot_type, Address slot) {
-          HeapObject heap_object =
+          Tagged<HeapObject> heap_object =
               UpdateTypedSlotHelper::GetTargetObject(heap, slot_type, slot);
           if (heap_object.InWritableSharedSpace()) {
             collector->MarkRootObject(Root::kClientHeap, heap_object);
@@ -2082,10 +2099,10 @@ void MarkCompactCollector::MarkTransitiveClosureLinear() {
       // This is the good case: newly_discovered stores all discovered
       // objects. Now use key_to_values to see if discovered objects keep more
       // objects alive due to ephemeron semantics.
-      for (HeapObject object : ephemeron_marking_.newly_discovered) {
+      for (Tagged<HeapObject> object : ephemeron_marking_.newly_discovered) {
         auto range = key_to_values.equal_range(object);
         for (auto it = range.first; it != range.second; ++it) {
-          HeapObject value = it->second;
+          Tagged<HeapObject> value = it->second;
           MarkObject(object, value);
         }
       }
@@ -2132,7 +2149,7 @@ constexpr size_t kDeadlineCheckInterval = 128u;
 std::pair<size_t, size_t> MarkCompactCollector::ProcessMarkingWorklist(
     v8::base::TimeDelta max_duration, size_t max_bytes_to_process,
     MarkingWorklistProcessingMode mode) {
-  HeapObject object;
+  Tagged<HeapObject> object;
   size_t bytes_processed = 0;
   size_t objects_processed = 0;
   bool is_per_context_mode = local_marking_worklists_->IsPerContextMode();
@@ -2160,7 +2177,7 @@ std::pair<size_t, size_t> MarkCompactCollector::ProcessMarkingWorklist(
                     kTrackNewlyDiscoveredObjects) {
       AddNewlyDiscovered(object);
     }
-    Map map = object->map(cage_base);
+    Tagged<Map> map = object->map(cage_base);
     if (is_per_context_mode) {
       Address context;
       if (native_context_inferrer_.Infer(isolate, map, object, &context)) {
@@ -2191,7 +2208,8 @@ std::pair<size_t, size_t> MarkCompactCollector::ProcessMarkingWorklist(
   return std::make_pair(bytes_processed, objects_processed);
 }
 
-bool MarkCompactCollector::ProcessEphemeron(HeapObject key, HeapObject value) {
+bool MarkCompactCollector::ProcessEphemeron(Tagged<HeapObject> key,
+                                            Tagged<HeapObject> value) {
   // Objects in the shared heap are prohibited from being used as keys in
   // WeakMaps and WeakSets and therefore cannot be ephemeron keys, because that
   // would enable thread local -> shared heap edges.
@@ -2247,10 +2265,10 @@ void MarkCompactCollector::ProcessTopOptimizedFrame(ObjectVisitor* visitor,
        it.Advance()) {
     if (it.frame()->is_unoptimized()) return;
     if (it.frame()->is_optimized()) {
-      GcSafeCode lookup_result = it.frame()->GcSafeLookupCode();
+      Tagged<GcSafeCode> lookup_result = it.frame()->GcSafeLookupCode();
       if (!lookup_result->has_instruction_stream()) return;
       if (!lookup_result->CanDeoptAt(isolate, it.frame()->pc())) {
-        InstructionStream istream = InstructionStream::unchecked_cast(
+        Tagged<InstructionStream> istream = InstructionStream::unchecked_cast(
             lookup_result->raw_instruction_stream());
         PtrComprCageBase cage_base(isolate);
         InstructionStream::BodyDescriptor::IterateBody(istream->map(cage_base),
@@ -2290,12 +2308,12 @@ void MarkCompactCollector::RecordObjectStats() {
 
 namespace {
 
-bool ShouldRetainMap(MarkingState* marking_state, Map map, int age) {
+bool ShouldRetainMap(MarkingState* marking_state, Tagged<Map> map, int age) {
   if (age == 0) {
     // The map has aged. Do not retain this map.
     return false;
   }
-  Object constructor = map->GetConstructor();
+  Tagged<Object> constructor = map->GetConstructor();
   if (!IsHeapObject(constructor) ||
       (!HeapObject::cast(constructor).InReadOnlySpace() &&
        marking_state->IsUnmarked(HeapObject::cast(constructor)))) {
@@ -2315,17 +2333,17 @@ void MarkCompactCollector::RetainMaps() {
   const bool should_retain_maps =
       !heap_->ShouldReduceMemory() && v8_flags.retain_maps_for_n_gc != 0;
 
-  for (WeakArrayList retained_maps : heap_->FindAllRetainedMaps()) {
+  for (Tagged<WeakArrayList> retained_maps : heap_->FindAllRetainedMaps()) {
     DCHECK_EQ(0, retained_maps->length() % 2);
     for (int i = 0; i < retained_maps->length(); i += 2) {
       MaybeObject value = retained_maps->Get(i);
-      HeapObject map_heap_object;
+      Tagged<HeapObject> map_heap_object;
       if (!value->GetHeapObjectIfWeak(&map_heap_object)) {
         continue;
       }
       int age = retained_maps->Get(i + 1).ToSmi().value();
       int new_age;
-      Map map = Map::cast(map_heap_object);
+      Tagged<Map> map = Map::cast(map_heap_object);
       if (should_retain_maps && marking_state_->IsUnmarked(map)) {
         if (ShouldRetainMap(marking_state_, map, age)) {
           if (marking_state_->TryMark(map)) {
@@ -2335,7 +2353,7 @@ void MarkCompactCollector::RetainMaps() {
             heap_->AddRetainingRoot(Root::kRetainMaps, map);
           }
         }
-        Object prototype = map->prototype();
+        Tagged<Object> prototype = map->prototype();
         if (age > 0 && IsHeapObject(prototype) &&
             (!HeapObject::cast(prototype).InReadOnlySpace() &&
              marking_state_->IsUnmarked(HeapObject::cast(prototype)))) {
@@ -2570,14 +2588,14 @@ class FullStringForwardingTableCleaner final
 
  private:
   void MarkForwardObject(StringForwardingTable::Record* record) {
-    Object original = record->OriginalStringObject(isolate_);
+    Tagged<Object> original = record->OriginalStringObject(isolate_);
     if (!IsHeapObject(original)) {
       DCHECK_EQ(original, StringForwardingTable::deleted_element());
       return;
     }
-    String original_string = String::cast(original);
+    Tagged<String> original_string = String::cast(original);
     if (marking_state_->IsMarked(original_string)) {
-      Object forward = record->ForwardStringObjectOrHash(isolate_);
+      Tagged<Object> forward = record->ForwardStringObjectOrHash(isolate_);
       if (!IsHeapObject(forward) ||
           HeapObject::cast(forward).InReadOnlySpace()) {
         return;
@@ -2590,13 +2608,13 @@ class FullStringForwardingTableCleaner final
   }
 
   void TransitionStrings(StringForwardingTable::Record* record) {
-    Object original = record->OriginalStringObject(isolate_);
+    Tagged<Object> original = record->OriginalStringObject(isolate_);
     if (!IsHeapObject(original)) {
       DCHECK_EQ(original, StringForwardingTable::deleted_element());
       return;
     }
     if (marking_state_->IsMarked(HeapObject::cast(original))) {
-      String original_string = String::cast(original);
+      Tagged<String> original_string = String::cast(original);
       if (IsThinString(original_string)) {
         original_string = ThinString::cast(original_string)->actual();
       }
@@ -2608,7 +2626,7 @@ class FullStringForwardingTableCleaner final
     }
   }
 
-  void TryExternalize(String original_string,
+  void TryExternalize(Tagged<String> original_string,
                       StringForwardingTable::Record* record) {
     // If the string is already external, dispose the resource.
     if (IsExternalString(original_string)) {
@@ -2633,14 +2651,14 @@ class FullStringForwardingTableCleaner final
     }
   }
 
-  void TryInternalize(String original_string,
+  void TryInternalize(Tagged<String> original_string,
                       StringForwardingTable::Record* record) {
     if (IsInternalizedString(original_string)) return;
-    Object forward = record->ForwardStringObjectOrHash(isolate_);
+    Tagged<Object> forward = record->ForwardStringObjectOrHash(isolate_);
     if (!IsHeapObject(forward)) {
       return;
     }
-    String forward_string = String::cast(forward);
+    Tagged<String> forward_string = String::cast(forward);
 
     // Mark the forwarded string to keep it alive.
     if (!forward_string.InReadOnlySpace()) {
@@ -2807,11 +2825,11 @@ void MarkCompactCollector::ClearNonLiveReferences() {
 }
 
 void MarkCompactCollector::MarkDependentCodeForDeoptimization() {
-  std::pair<HeapObject, Code> weak_object_in_code;
+  HeapObjectAndCode weak_object_in_code;
   while (local_weak_objects()->weak_objects_in_code_local.Pop(
       &weak_object_in_code)) {
-    HeapObject object = weak_object_in_code.first;
-    Code code = weak_object_in_code.second;
+    Tagged<HeapObject> object = weak_object_in_code.first;
+    Tagged<Code> code = weak_object_in_code.second;
     if (!non_atomic_marking_state_->IsMarked(object) &&
         !code->embedded_objects_cleared()) {
       if (!code->marked_for_deoptimization()) {
@@ -2824,11 +2842,12 @@ void MarkCompactCollector::MarkDependentCodeForDeoptimization() {
   }
 }
 
-void MarkCompactCollector::ClearPotentialSimpleMapTransition(Map dead_target) {
+void MarkCompactCollector::ClearPotentialSimpleMapTransition(
+    Tagged<Map> dead_target) {
   DCHECK(non_atomic_marking_state_->IsUnmarked(dead_target));
-  Object potential_parent = dead_target->constructor_or_back_pointer();
+  Tagged<Object> potential_parent = dead_target->constructor_or_back_pointer();
   if (IsMap(potential_parent)) {
-    Map parent = Map::cast(potential_parent);
+    Tagged<Map> parent = Map::cast(potential_parent);
     DisallowGarbageCollection no_gc_obviously;
     if (non_atomic_marking_state_->IsMarked(parent) &&
         TransitionsAccessor(heap_->isolate(), parent)
@@ -2838,14 +2857,15 @@ void MarkCompactCollector::ClearPotentialSimpleMapTransition(Map dead_target) {
   }
 }
 
-void MarkCompactCollector::ClearPotentialSimpleMapTransition(Map map,
-                                                             Map dead_target) {
+void MarkCompactCollector::ClearPotentialSimpleMapTransition(
+    Tagged<Map> map, Tagged<Map> dead_target) {
   DCHECK(!map->is_prototype_map());
   DCHECK(!dead_target->is_prototype_map());
   DCHECK_EQ(map->raw_transitions(), HeapObjectReference::Weak(dead_target));
   // Take ownership of the descriptor array.
   int number_of_own_descriptors = map->NumberOfOwnDescriptors();
-  DescriptorArray descriptors = map->instance_descriptors(heap_->isolate());
+  Tagged<DescriptorArray> descriptors =
+      map->instance_descriptors(heap_->isolate());
   if (descriptors == dead_target->instance_descriptors(heap_->isolate()) &&
       number_of_own_descriptors > 0) {
     TrimDescriptorArray(map, descriptors);
@@ -2854,19 +2874,18 @@ void MarkCompactCollector::ClearPotentialSimpleMapTransition(Map map,
 }
 
 void MarkCompactCollector::FlushBytecodeFromSFI(
-    SharedFunctionInfo shared_info) {
+    Tagged<SharedFunctionInfo> shared_info) {
   DCHECK(shared_info->HasBytecodeArray());
 
   // Retain objects required for uncompiled data.
-  String inferred_name = shared_info->inferred_name();
+  Tagged<String> inferred_name = shared_info->inferred_name();
   int start_position = shared_info->StartPosition();
   int end_position = shared_info->EndPosition();
 
   shared_info->DiscardCompiledMetadata(
       heap_->isolate(),
-      [](HeapObject object, ObjectSlot slot, HeapObject target) {
-        RecordSlot(object, slot, target);
-      });
+      [](Tagged<HeapObject> object, ObjectSlot slot,
+         Tagged<HeapObject> target) { RecordSlot(object, slot, target); });
 
   // The size of the bytecode array should always be larger than an
   // UncompiledData object.
@@ -2874,7 +2893,8 @@ void MarkCompactCollector::FlushBytecodeFromSFI(
                 UncompiledDataWithoutPreparseData::kSize);
 
   // Replace bytecode array with an uncompiled data array.
-  HeapObject compiled_data = shared_info->GetBytecodeArray(heap_->isolate());
+  Tagged<HeapObject> compiled_data =
+      shared_info->GetBytecodeArray(heap_->isolate());
   Address compiled_data_start = compiled_data.address();
   int compiled_data_size = ALIGN_TO_ALLOCATION_ALIGNMENT(compiled_data->Size());
   MemoryChunk* chunk = MemoryChunk::FromAddress(compiled_data_start);
@@ -2908,12 +2928,11 @@ void MarkCompactCollector::FlushBytecodeFromSFI(
   }
 
   // Initialize the uncompiled data.
-  UncompiledData uncompiled_data = UncompiledData::cast(compiled_data);
+  Tagged<UncompiledData> uncompiled_data = UncompiledData::cast(compiled_data);
   uncompiled_data->InitAfterBytecodeFlush(
       inferred_name, start_position, end_position,
-      [](HeapObject object, ObjectSlot slot, HeapObject target) {
-        RecordSlot(object, slot, target);
-      });
+      [](Tagged<HeapObject> object, ObjectSlot slot,
+         Tagged<HeapObject> target) { RecordSlot(object, slot, target); });
 
   // Mark the uncompiled data as black, and ensure all fields have already been
   // marked.
@@ -2930,7 +2949,7 @@ void MarkCompactCollector::FlushBytecodeFromSFI(
 void MarkCompactCollector::ProcessOldCodeCandidates() {
   DCHECK(v8_flags.flush_bytecode || v8_flags.flush_baseline_code ||
          weak_objects_.code_flushing_candidates.IsEmpty());
-  SharedFunctionInfo flushing_candidate;
+  Tagged<SharedFunctionInfo> flushing_candidate;
   int number_of_flushed_sfis = 0;
   while (local_weak_objects()->code_flushing_candidates_local.Pop(
       &flushing_candidate)) {
@@ -2957,7 +2976,7 @@ void MarkCompactCollector::ProcessOldCodeCandidates() {
 }
 
 bool MarkCompactCollector::ProcessOldBytecodeSFI(
-    SharedFunctionInfo flushing_candidate) {
+    Tagged<SharedFunctionInfo> flushing_candidate) {
   // During flushing a BytecodeArray is transformed into an UncompiledData
   // in place. Seeing an UncompiledData here implies that another
   // SharedFunctionInfo had a reference to the same BytecodeArray and
@@ -2978,13 +2997,14 @@ bool MarkCompactCollector::ProcessOldBytecodeSFI(
 }
 
 bool MarkCompactCollector::ProcessOldBaselineSFI(
-    SharedFunctionInfo flushing_candidate) {
-  Code baseline_code =
+    Tagged<SharedFunctionInfo> flushing_candidate) {
+  Tagged<Code> baseline_code =
       Code::cast(flushing_candidate->function_data(kAcquireLoad));
   // Safe to do a relaxed load here since the Code was acquire-loaded.
-  InstructionStream baseline_istream = baseline_code->instruction_stream(
-      baseline_code->code_cage_base(), kRelaxedLoad);
-  HeapObject baseline_bytecode_or_interpreter_data =
+  Tagged<InstructionStream> baseline_istream =
+      baseline_code->instruction_stream(baseline_code->code_cage_base(),
+                                        kRelaxedLoad);
+  Tagged<HeapObject> baseline_bytecode_or_interpreter_data =
       baseline_code->bytecode_or_interpreter_data();
 
   // During flushing a BytecodeArray is transformed into an UncompiledData
@@ -3028,7 +3048,7 @@ bool MarkCompactCollector::ProcessOldBaselineSFI(
   return is_bytecode_live;
 }
 
-void MarkCompactCollector::FlushSFI(SharedFunctionInfo sfi,
+void MarkCompactCollector::FlushSFI(Tagged<SharedFunctionInfo> sfi,
                                     bool bytecode_already_decompiled) {
   // If baseline code flushing is disabled we should only flush bytecode
   // from functions that don't have baseline data.
@@ -3037,9 +3057,8 @@ void MarkCompactCollector::FlushSFI(SharedFunctionInfo sfi,
   if (bytecode_already_decompiled) {
     sfi->DiscardCompiledMetadata(
         heap_->isolate(),
-        [](HeapObject object, ObjectSlot slot, HeapObject target) {
-          RecordSlot(object, slot, target);
-        });
+        [](Tagged<HeapObject> object, ObjectSlot slot,
+           Tagged<HeapObject> target) { RecordSlot(object, slot, target); });
   } else {
     // If the BytecodeArray is dead, flush it, which will replace the field
     // with an uncompiled data object.
@@ -3050,11 +3069,11 @@ void MarkCompactCollector::FlushSFI(SharedFunctionInfo sfi,
 void MarkCompactCollector::ClearFlushedJsFunctions() {
   DCHECK(v8_flags.flush_bytecode ||
          weak_objects_.flushed_js_functions.IsEmpty());
-  JSFunction flushed_js_function;
+  Tagged<JSFunction> flushed_js_function;
   while (local_weak_objects()->flushed_js_functions_local.Pop(
       &flushed_js_function)) {
-    auto gc_notify_updated_slot = [](HeapObject object, ObjectSlot slot,
-                                     Object target) {
+    auto gc_notify_updated_slot = [](Tagged<HeapObject> object, ObjectSlot slot,
+                                     Tagged<Object> target) {
       RecordSlot(object, slot, HeapObject::cast(target));
     };
     flushed_js_function->ResetIfCodeFlushed(gc_notify_updated_slot);
@@ -3064,11 +3083,11 @@ void MarkCompactCollector::ClearFlushedJsFunctions() {
 void MarkCompactCollector::ProcessFlushedBaselineCandidates() {
   DCHECK(v8_flags.flush_baseline_code ||
          weak_objects_.baseline_flushing_candidates.IsEmpty());
-  JSFunction flushed_js_function;
+  Tagged<JSFunction> flushed_js_function;
   while (local_weak_objects()->baseline_flushing_candidates_local.Pop(
       &flushed_js_function)) {
-    auto gc_notify_updated_slot = [](HeapObject object, ObjectSlot slot,
-                                     Object target) {
+    auto gc_notify_updated_slot = [](Tagged<HeapObject> object, ObjectSlot slot,
+                                     Tagged<Object> target) {
       RecordSlot(object, slot, HeapObject::cast(target));
     };
     flushed_js_function->ResetIfCodeFlushed(gc_notify_updated_slot);
@@ -3087,24 +3106,25 @@ void MarkCompactCollector::ProcessFlushedBaselineCandidates() {
 }
 
 void MarkCompactCollector::ClearFullMapTransitions() {
-  TransitionArray array;
+  Tagged<TransitionArray> array;
   Isolate* const isolate = heap_->isolate();
   while (local_weak_objects()->transition_arrays_local.Pop(&array)) {
     int num_transitions = array->number_of_entries();
     if (num_transitions > 0) {
-      Map map;
+      Tagged<Map> map;
       // The array might contain "undefined" elements because it's not yet
       // filled. Allow it.
       if (array->GetTargetIfExists(0, isolate, &map)) {
         DCHECK(!map.is_null());  // Weak pointers aren't cleared yet.
-        Object constructor_or_back_pointer = map->constructor_or_back_pointer();
+        Tagged<Object> constructor_or_back_pointer =
+            map->constructor_or_back_pointer();
         if (IsSmi(constructor_or_back_pointer)) {
           DCHECK(isolate->has_active_deserializer());
           DCHECK_EQ(constructor_or_back_pointer,
                     Smi::uninitialized_deserialization_value());
           continue;
         }
-        Map parent = Map::cast(map->constructor_or_back_pointer());
+        Tagged<Map> parent = Map::cast(map->constructor_or_back_pointer());
         bool parent_is_alive = non_atomic_marking_state_->IsMarked(parent);
         Tagged<DescriptorArray> descriptors =
             parent_is_alive ? parent->instance_descriptors(isolate)
@@ -3122,7 +3142,7 @@ void MarkCompactCollector::ClearFullMapTransitions() {
 // Returns false if no maps have died, or if the transition array is
 // still being deserialized.
 bool MarkCompactCollector::TransitionArrayNeedsCompaction(
-    TransitionArray transitions, int num_transitions) {
+    Tagged<TransitionArray> transitions, int num_transitions) {
   for (int i = 0; i < num_transitions; ++i) {
     MaybeObject raw_target = transitions->GetRawTarget(i);
     if (raw_target.IsSmi()) {
@@ -3152,9 +3172,9 @@ bool MarkCompactCollector::TransitionArrayNeedsCompaction(
   return false;
 }
 
-bool MarkCompactCollector::CompactTransitionArray(Map map,
-                                                  TransitionArray transitions,
-                                                  DescriptorArray descriptors) {
+bool MarkCompactCollector::CompactTransitionArray(
+    Tagged<Map> map, Tagged<TransitionArray> transitions,
+    Tagged<DescriptorArray> descriptors) {
   DCHECK(!map->is_prototype_map());
   int num_transitions = transitions->number_of_entries();
   if (!TransitionArrayNeedsCompaction(transitions, num_transitions)) {
@@ -3164,7 +3184,7 @@ bool MarkCompactCollector::CompactTransitionArray(Map map,
   int transition_index = 0;
   // Compact all live transitions to the left.
   for (int i = 0; i < num_transitions; ++i) {
-    Map target = transitions->GetTarget(i);
+    Tagged<Map> target = transitions->GetTarget(i);
     DCHECK_EQ(target->constructor_or_back_pointer(), map);
     if (non_atomic_marking_state_->IsUnmarked(target)) {
       if (!descriptors.is_null() &&
@@ -3174,7 +3194,7 @@ bool MarkCompactCollector::CompactTransitionArray(Map map,
       }
     } else {
       if (i != transition_index) {
-        Name key = transitions->GetKey(i);
+        Tagged<Name> key = transitions->GetKey(i);
         transitions->SetKey(transition_index, key);
         HeapObjectSlot key_slot = transitions->GetKeySlot(transition_index);
         RecordSlot(transitions, key_slot, key);
@@ -3205,8 +3225,8 @@ bool MarkCompactCollector::CompactTransitionArray(Map map,
   return descriptors_owner_died;
 }
 
-void MarkCompactCollector::RightTrimDescriptorArray(DescriptorArray array,
-                                                    int descriptors_to_trim) {
+void MarkCompactCollector::RightTrimDescriptorArray(
+    Tagged<DescriptorArray> array, int descriptors_to_trim) {
   int old_nof_all_descriptors = array->number_of_all_descriptors();
   int new_nof_all_descriptors = old_nof_all_descriptors - descriptors_to_trim;
   DCHECK_LT(0, descriptors_to_trim);
@@ -3248,11 +3268,11 @@ void MarkCompactCollector::RecordStrongDescriptorArraysForWeakening(
 }
 
 void MarkCompactCollector::WeakenStrongDescriptorArrays() {
-  Map descriptor_array_map =
+  Tagged<Map> descriptor_array_map =
       ReadOnlyRoots(heap_->isolate()).descriptor_array_map();
   for (auto vec : strong_descriptor_arrays_) {
     for (auto it = vec.begin(); it != vec.end(); ++it) {
-      DescriptorArray raw = it.raw();
+      Tagged<DescriptorArray> raw = it.raw();
       DCHECK(IsStrongDescriptorArray(raw));
       raw->set_map_safe_transition_no_write_barrier(descriptor_array_map);
       DCHECK_EQ(raw->raw_gc_state(kRelaxedLoad), 0);
@@ -3261,8 +3281,8 @@ void MarkCompactCollector::WeakenStrongDescriptorArrays() {
   strong_descriptor_arrays_.clear();
 }
 
-void MarkCompactCollector::TrimDescriptorArray(Map map,
-                                               DescriptorArray descriptors) {
+void MarkCompactCollector::TrimDescriptorArray(
+    Tagged<Map> map, Tagged<DescriptorArray> descriptors) {
   int number_of_own_descriptors = map->NumberOfOwnDescriptors();
   if (number_of_own_descriptors == 0) {
     DCHECK(descriptors == ReadOnlyRoots(heap_).empty_descriptor_array());
@@ -3281,20 +3301,21 @@ void MarkCompactCollector::TrimDescriptorArray(Map map,
   map->set_owns_descriptors(true);
 }
 
-void MarkCompactCollector::TrimEnumCache(Map map, DescriptorArray descriptors) {
+void MarkCompactCollector::TrimEnumCache(Tagged<Map> map,
+                                         Tagged<DescriptorArray> descriptors) {
   int live_enum = map->EnumLength();
   if (live_enum == kInvalidEnumCacheSentinel) {
     live_enum = map->NumberOfEnumerableProperties();
   }
   if (live_enum == 0) return descriptors->ClearEnumCache();
-  EnumCache enum_cache = descriptors->enum_cache();
+  Tagged<EnumCache> enum_cache = descriptors->enum_cache();
 
-  FixedArray keys = enum_cache->keys();
+  Tagged<FixedArray> keys = enum_cache->keys();
   int to_trim = keys->length() - live_enum;
   if (to_trim <= 0) return;
   heap_->RightTrimFixedArray(keys, to_trim);
 
-  FixedArray indices = enum_cache->indices();
+  Tagged<FixedArray> indices = enum_cache->indices();
   to_trim = indices->length() - live_enum;
   if (to_trim <= 0) return;
   heap_->RightTrimFixedArray(indices, to_trim);
@@ -3302,15 +3323,15 @@ void MarkCompactCollector::TrimEnumCache(Map map, DescriptorArray descriptors) {
 
 void MarkCompactCollector::ClearWeakCollections() {
   TRACE_GC(heap_->tracer(), GCTracer::Scope::MC_CLEAR_WEAK_COLLECTIONS);
-  EphemeronHashTable table;
+  Tagged<EphemeronHashTable> table;
   while (local_weak_objects()->ephemeron_hash_tables_local.Pop(&table)) {
     for (InternalIndex i : table->IterateEntries()) {
-      HeapObject key = HeapObject::cast(table->KeyAt(i));
+      Tagged<HeapObject> key = HeapObject::cast(table->KeyAt(i));
 #ifdef VERIFY_HEAP
       if (v8_flags.verify_heap) {
-        Object value = table->ValueAt(i);
+        Tagged<Object> value = table->ValueAt(i);
         if (IsHeapObject(value)) {
-          HeapObject heap_object = HeapObject::cast(value);
+          Tagged<HeapObject> heap_object = HeapObject::cast(value);
           CHECK_IMPLIES(!ShouldMarkObject(key) ||
                             non_atomic_marking_state_->IsMarked(key),
                         !ShouldMarkObject(heap_object) ||
@@ -3336,11 +3357,11 @@ void MarkCompactCollector::ClearWeakCollections() {
 
 void MarkCompactCollector::ClearWeakReferences() {
   TRACE_GC(heap_->tracer(), GCTracer::Scope::MC_CLEAR_WEAK_REFERENCES);
-  std::pair<HeapObject, HeapObjectSlot> slot;
+  HeapObjectAndSlot slot;
   HeapObjectReference cleared_weak_ref =
       HeapObjectReference::ClearedValue(heap_->isolate());
   while (local_weak_objects()->weak_references_local.Pop(&slot)) {
-    HeapObject value;
+    Tagged<HeapObject> value;
     // The slot could have been overwritten, so we have to treat it
     // as MaybeObjectSlot.
     MaybeObjectSlot location(slot.second);
@@ -3362,10 +3383,10 @@ void MarkCompactCollector::ClearWeakReferences() {
 }
 
 void MarkCompactCollector::ClearJSWeakRefs() {
-  JSWeakRef weak_ref;
+  Tagged<JSWeakRef> weak_ref;
   Isolate* const isolate = heap_->isolate();
   while (local_weak_objects()->js_weak_refs_local.Pop(&weak_ref)) {
-    HeapObject target = HeapObject::cast(weak_ref->target());
+    Tagged<HeapObject> target = HeapObject::cast(weak_ref->target());
     if (!target.InReadOnlySpace() &&
         !non_atomic_marking_state_->IsMarked(target)) {
       weak_ref->set_target(ReadOnlyRoots(isolate).undefined_value());
@@ -3375,20 +3396,20 @@ void MarkCompactCollector::ClearJSWeakRefs() {
       RecordSlot(weak_ref, slot, target);
     }
   }
-  WeakCell weak_cell;
+  Tagged<WeakCell> weak_cell;
   while (local_weak_objects()->weak_cells_local.Pop(&weak_cell)) {
-    auto gc_notify_updated_slot = [](HeapObject object, ObjectSlot slot,
-                                     Object target) {
+    auto gc_notify_updated_slot = [](Tagged<HeapObject> object, ObjectSlot slot,
+                                     Tagged<Object> target) {
       if (IsHeapObject(target)) {
         RecordSlot(object, slot, HeapObject::cast(target));
       }
     };
-    HeapObject target = HeapObject::cast(weak_cell->target());
+    Tagged<HeapObject> target = HeapObject::cast(weak_cell->target());
     if (!target.InReadOnlySpace() &&
         !non_atomic_marking_state_->IsMarked(target)) {
       DCHECK(Object::CanBeHeldWeakly(target));
       // The value of the WeakCell is dead.
-      JSFinalizationRegistry finalization_registry =
+      Tagged<JSFinalizationRegistry> finalization_registry =
           JSFinalizationRegistry::cast(weak_cell->finalization_registry());
       if (!finalization_registry->scheduled_for_cleanup()) {
         heap_->EnqueueDirtyJSFinalizationRegistry(finalization_registry,
@@ -3406,7 +3427,7 @@ void MarkCompactCollector::ClearJSWeakRefs() {
       RecordSlot(weak_cell, slot, HeapObject::cast(*slot));
     }
 
-    HeapObject unregister_token = weak_cell->unregister_token();
+    Tagged<HeapObject> unregister_token = weak_cell->unregister_token();
     if (!unregister_token.InReadOnlySpace() &&
         !non_atomic_marking_state_->IsMarked(unregister_token)) {
       DCHECK(Object::CanBeHeldWeakly(unregister_token));
@@ -3415,7 +3436,7 @@ void MarkCompactCollector::ClearJSWeakRefs() {
       // unregister_token field set to undefined when processing the first
       // WeakCell. Like above, we're modifying pointers during GC, so record the
       // slots.
-      JSFinalizationRegistry finalization_registry =
+      Tagged<JSFinalizationRegistry> finalization_registry =
           JSFinalizationRegistry::cast(weak_cell->finalization_registry());
       finalization_registry->RemoveUnregisterToken(
           unregister_token, isolate,
@@ -3435,9 +3456,9 @@ bool MarkCompactCollector::IsOnEvacuationCandidate(MaybeObject obj) {
 }
 
 // static
-bool MarkCompactCollector::ShouldRecordRelocSlot(InstructionStream host,
+bool MarkCompactCollector::ShouldRecordRelocSlot(Tagged<InstructionStream> host,
                                                  RelocInfo* rinfo,
-                                                 HeapObject target) {
+                                                 Tagged<HeapObject> target) {
   MemoryChunk* source_chunk = MemoryChunk::FromHeapObject(host);
   BasicMemoryChunk* target_chunk = BasicMemoryChunk::FromHeapObject(target);
   return target_chunk->IsEvacuationCandidate() &&
@@ -3446,8 +3467,9 @@ bool MarkCompactCollector::ShouldRecordRelocSlot(InstructionStream host,
 
 // static
 MarkCompactCollector::RecordRelocSlotInfo
-MarkCompactCollector::ProcessRelocInfo(InstructionStream host, RelocInfo* rinfo,
-                                       HeapObject target) {
+MarkCompactCollector::ProcessRelocInfo(Tagged<InstructionStream> host,
+                                       RelocInfo* rinfo,
+                                       Tagged<HeapObject> target) {
   RecordRelocSlotInfo result;
   const RelocInfo::Mode rmode = rinfo->rmode();
   Address addr;
@@ -3488,9 +3510,9 @@ MarkCompactCollector::ProcessRelocInfo(InstructionStream host, RelocInfo* rinfo,
 }
 
 // static
-void MarkCompactCollector::RecordRelocSlot(InstructionStream host,
+void MarkCompactCollector::RecordRelocSlot(Tagged<InstructionStream> host,
                                            RelocInfo* rinfo,
-                                           HeapObject target) {
+                                           Tagged<HeapObject> target) {
   if (!ShouldRecordRelocSlot(host, rinfo, target)) return;
   RecordRelocSlotInfo info = ProcessRelocInfo(host, rinfo, target);
 
@@ -3509,49 +3531,51 @@ namespace {
 // Missing specialization MakeSlotValue<FullObjectSlot, WEAK>() will turn
 // attempt to store a weak reference to strong-only slot to a compilation error.
 template <typename TSlot, HeapObjectReferenceType reference_type>
-typename TSlot::TObject MakeSlotValue(HeapObject heap_object);
+typename TSlot::TObject MakeSlotValue(Tagged<HeapObject> heap_object);
 
 template <>
-Object MakeSlotValue<ObjectSlot, HeapObjectReferenceType::STRONG>(
-    HeapObject heap_object) {
+Tagged<Object> MakeSlotValue<ObjectSlot, HeapObjectReferenceType::STRONG>(
+    Tagged<HeapObject> heap_object) {
   return heap_object;
 }
 
 template <>
 MaybeObject MakeSlotValue<MaybeObjectSlot, HeapObjectReferenceType::STRONG>(
-    HeapObject heap_object) {
+    Tagged<HeapObject> heap_object) {
   return HeapObjectReference::Strong(heap_object);
 }
 
 template <>
 MaybeObject MakeSlotValue<MaybeObjectSlot, HeapObjectReferenceType::WEAK>(
-    HeapObject heap_object) {
+    Tagged<HeapObject> heap_object) {
   return HeapObjectReference::Weak(heap_object);
 }
 
 template <>
-Object MakeSlotValue<OffHeapObjectSlot, HeapObjectReferenceType::STRONG>(
-    HeapObject heap_object) {
+Tagged<Object>
+MakeSlotValue<OffHeapObjectSlot, HeapObjectReferenceType::STRONG>(
+    Tagged<HeapObject> heap_object) {
   return heap_object;
 }
 
 #ifdef V8_COMPRESS_POINTERS
 template <>
-Object MakeSlotValue<FullObjectSlot, HeapObjectReferenceType::STRONG>(
-    HeapObject heap_object) {
+Tagged<Object> MakeSlotValue<FullObjectSlot, HeapObjectReferenceType::STRONG>(
+    Tagged<HeapObject> heap_object) {
   return heap_object;
 }
 
 template <>
 MaybeObject MakeSlotValue<FullMaybeObjectSlot, HeapObjectReferenceType::STRONG>(
-    HeapObject heap_object) {
+    Tagged<HeapObject> heap_object) {
   return HeapObjectReference::Strong(heap_object);
 }
 
 #ifdef V8_EXTERNAL_CODE_SPACE
 template <>
-Object MakeSlotValue<InstructionStreamSlot, HeapObjectReferenceType::STRONG>(
-    HeapObject heap_object) {
+Tagged<Object>
+MakeSlotValue<InstructionStreamSlot, HeapObjectReferenceType::STRONG>(
+    Tagged<HeapObject> heap_object) {
   return heap_object;
 }
 #endif  // V8_EXTERNAL_CODE_SPACE
@@ -3565,7 +3589,7 @@ template <AccessMode access_mode, HeapObjectReferenceType reference_type,
           typename TSlot>
 static inline void UpdateSlot(PtrComprCageBase cage_base, TSlot slot,
                               typename TSlot::TObject old,
-                              HeapObject heap_obj) {
+                              Tagged<HeapObject> heap_obj) {
   static_assert(std::is_same<TSlot, FullObjectSlot>::value ||
                     std::is_same<TSlot, ObjectSlot>::value ||
                     std::is_same<TSlot, FullMaybeObjectSlot>::value ||
@@ -3600,7 +3624,7 @@ static inline void UpdateSlot(PtrComprCageBase cage_base, TSlot slot,
 template <AccessMode access_mode, typename TSlot>
 static inline void UpdateSlot(PtrComprCageBase cage_base, TSlot slot) {
   typename TSlot::TObject obj = slot.Relaxed_Load(cage_base);
-  HeapObject heap_obj;
+  Tagged<HeapObject> heap_obj;
   if (TSlot::kCanBeWeak && obj->GetHeapObjectIfWeak(&heap_obj)) {
     UpdateSlot<access_mode, HeapObjectReferenceType::WEAK>(cage_base, slot, obj,
                                                            heap_obj);
@@ -3613,7 +3637,7 @@ static inline void UpdateSlot(PtrComprCageBase cage_base, TSlot slot) {
 static inline SlotCallbackResult UpdateOldToSharedSlot(
     PtrComprCageBase cage_base, MaybeObjectSlot slot) {
   MaybeObject obj = slot.Relaxed_Load(cage_base);
-  HeapObject heap_obj;
+  Tagged<HeapObject> heap_obj;
 
   if (obj.GetHeapObject(&heap_obj)) {
     if (obj.IsWeak()) {
@@ -3634,7 +3658,7 @@ template <AccessMode access_mode, typename TSlot>
 static inline void UpdateStrongSlot(PtrComprCageBase cage_base, TSlot slot) {
   typename TSlot::TObject obj = slot.Relaxed_Load(cage_base);
   DCHECK(!HAS_WEAK_HEAP_OBJECT_TAG(obj.ptr()));
-  HeapObject heap_obj;
+  Tagged<HeapObject> heap_obj;
   if (obj.GetHeapObject(&heap_obj)) {
     UpdateSlot<access_mode, HeapObjectReferenceType::STRONG>(cage_base, slot,
                                                              obj, heap_obj);
@@ -3645,7 +3669,7 @@ static inline SlotCallbackResult UpdateStrongOldToSharedSlot(
     PtrComprCageBase cage_base, FullMaybeObjectSlot slot) {
   MaybeObject obj = slot.Relaxed_Load(cage_base);
   DCHECK(!HAS_WEAK_HEAP_OBJECT_TAG(obj.ptr()));
-  HeapObject heap_obj;
+  Tagged<HeapObject> heap_obj;
   if (obj.GetHeapObject(&heap_obj)) {
     UpdateSlot<AccessMode::NON_ATOMIC, HeapObjectReferenceType::STRONG>(
         cage_base, slot, obj, heap_obj);
@@ -3656,20 +3680,20 @@ static inline SlotCallbackResult UpdateStrongOldToSharedSlot(
 }
 
 template <AccessMode access_mode>
-static inline void UpdateStrongCodeSlot(HeapObject host,
+static inline void UpdateStrongCodeSlot(Tagged<HeapObject> host,
                                         PtrComprCageBase cage_base,
                                         PtrComprCageBase code_cage_base,
                                         InstructionStreamSlot slot) {
-  Object obj = slot.Relaxed_Load(code_cage_base);
+  Tagged<Object> obj = slot.Relaxed_Load(code_cage_base);
   DCHECK(!HAS_WEAK_HEAP_OBJECT_TAG(obj.ptr()));
-  HeapObject heap_obj;
+  Tagged<HeapObject> heap_obj;
   if (obj.GetHeapObject(&heap_obj)) {
     UpdateSlot<access_mode, HeapObjectReferenceType::STRONG>(cage_base, slot,
                                                              obj, heap_obj);
 
-    Code code = Code::cast(HeapObject::FromAddress(
+    Tagged<Code> code = Code::cast(HeapObject::FromAddress(
         slot.address() - Code::kInstructionStreamOffset));
-    InstructionStream instruction_stream =
+    Tagged<InstructionStream> instruction_stream =
         code->instruction_stream(code_cage_base);
     Isolate* isolate_for_sandbox = GetIsolateForSandbox(host);
     code->UpdateInstructionStart(isolate_for_sandbox, instruction_stream);
@@ -3686,29 +3710,29 @@ class PointersUpdatingVisitor final : public ObjectVisitorWithCageBases,
   explicit PointersUpdatingVisitor(Heap* heap)
       : ObjectVisitorWithCageBases(heap) {}
 
-  void VisitPointer(HeapObject host, ObjectSlot p) override {
+  void VisitPointer(Tagged<HeapObject> host, ObjectSlot p) override {
     UpdateStrongSlotInternal(cage_base(), p);
   }
 
-  void VisitPointer(HeapObject host, MaybeObjectSlot p) override {
+  void VisitPointer(Tagged<HeapObject> host, MaybeObjectSlot p) override {
     UpdateSlotInternal(cage_base(), p);
   }
 
-  void VisitPointers(HeapObject host, ObjectSlot start,
+  void VisitPointers(Tagged<HeapObject> host, ObjectSlot start,
                      ObjectSlot end) override {
     for (ObjectSlot p = start; p < end; ++p) {
       UpdateStrongSlotInternal(cage_base(), p);
     }
   }
 
-  void VisitPointers(HeapObject host, MaybeObjectSlot start,
+  void VisitPointers(Tagged<HeapObject> host, MaybeObjectSlot start,
                      MaybeObjectSlot end) final {
     for (MaybeObjectSlot p = start; p < end; ++p) {
       UpdateSlotInternal(cage_base(), p);
     }
   }
 
-  void VisitInstructionStreamPointer(Code host,
+  void VisitInstructionStreamPointer(Tagged<Code> host,
                                      InstructionStreamSlot slot) override {
     UpdateStrongCodeSlot<AccessMode::NON_ATOMIC>(host, cage_base(),
                                                  code_cage_base(), slot);
@@ -3735,12 +3759,14 @@ class PointersUpdatingVisitor final : public ObjectVisitorWithCageBases,
     }
   }
 
-  void VisitCodeTarget(InstructionStream host, RelocInfo* rinfo) override {
+  void VisitCodeTarget(Tagged<InstructionStream> host,
+                       RelocInfo* rinfo) override {
     // This visitor nevers visits code objects.
     UNREACHABLE();
   }
 
-  void VisitEmbeddedPointer(InstructionStream host, RelocInfo* rinfo) override {
+  void VisitEmbeddedPointer(Tagged<InstructionStream> host,
+                            RelocInfo* rinfo) override {
     // This visitor nevers visits code objects.
     UNREACHABLE();
   }
@@ -4005,7 +4031,7 @@ class LiveObjectVisitor final : AllStatic {
   // returned false.
   template <class Visitor>
   static bool VisitMarkedObjects(Page* page, Visitor* visitor,
-                                 HeapObject* failed_object);
+                                 Tagged<HeapObject>* failed_object);
 
   // Visits marked objects using `bool Visitor::Visit(HeapObject object, size_t
   // size)` as long as the return value is true. Assumes that the return value
@@ -4016,7 +4042,7 @@ class LiveObjectVisitor final : AllStatic {
 
 template <class Visitor>
 bool LiveObjectVisitor::VisitMarkedObjects(Page* page, Visitor* visitor,
-                                           HeapObject* failed_object) {
+                                           Tagged<HeapObject>* failed_object) {
   TRACE_EVENT0(TRACE_DISABLED_BY_DEFAULT("v8.gc"),
                "LiveObjectVisitor::VisitMarkedObjects");
   for (auto [object, size] : LiveObjectRange(page)) {
@@ -4073,7 +4099,7 @@ bool Evacuator::RawEvacuatePage(MemoryChunk* chunk) {
 #if DEBUG
       old_space_visitor_.SetUpAbortEvacuationAtAddress(chunk);
 #endif  // DEBUG
-      HeapObject failed_object;
+      Tagged<HeapObject> failed_object;
       if (LiveObjectVisitor::VisitMarkedObjects(
               Page::cast(chunk), &old_space_visitor_, &failed_object)) {
         chunk->ClearLiveness();
@@ -4310,7 +4336,7 @@ void MarkCompactCollector::EvacuatePagesInParallel() {
   if (auto* new_lo_space = heap_->new_lo_space()) {
     for (auto it = new_lo_space->begin(); it != new_lo_space->end();) {
       LargePage* current = *(it++);
-      HeapObject object = current->GetObject();
+      Tagged<HeapObject> object = current->GetObject();
       if (marking_state_->IsMarked(object)) {
         heap_->lo_space()->PromoteNewLargeObject(current);
         current->SetFlag(Page::PAGE_NEW_OLD_PROMOTION);
@@ -4396,7 +4422,7 @@ void MarkCompactCollector::Evacuate() {
     for (LargePage* p : promoted_large_pages_) {
       DCHECK(p->IsFlagSet(Page::PAGE_NEW_OLD_PROMOTION));
       p->ClearFlag(Page::PAGE_NEW_OLD_PROMOTION);
-      HeapObject object = p->GetObject();
+      Tagged<HeapObject> object = p->GetObject();
       MarkBit::From(object).Clear();
       p->ProgressBar().ResetIfEnabled();
       p->SetLiveBytes(0);
@@ -4521,7 +4547,7 @@ class RememberedSetUpdatingItem : public UpdatingItem {
   template <typename TSlot>
   inline void CheckSlotForOldToSharedUntyped(PtrComprCageBase cage_base,
                                              MemoryChunk* chunk, TSlot slot) {
-    HeapObject heap_object;
+    Tagged<HeapObject> heap_object;
 
     if (!slot.load(cage_base)->GetHeapObject(&heap_object)) {
       return;
@@ -4535,7 +4561,7 @@ class RememberedSetUpdatingItem : public UpdatingItem {
 
   inline void CheckSlotForOldToSharedTyped(MemoryChunk* chunk,
                                            SlotType slot_type, Address addr) {
-    HeapObject heap_object =
+    Tagged<HeapObject> heap_object =
         UpdateTypedSlotHelper::GetTargetObject(chunk->heap(), slot_type, addr);
 
 #if DEBUG
@@ -4561,7 +4587,7 @@ class RememberedSetUpdatingItem : public UpdatingItem {
         std::is_same<TSlot, FullMaybeObjectSlot>::value ||
             std::is_same<TSlot, MaybeObjectSlot>::value,
         "Only FullMaybeObjectSlot and MaybeObjectSlot are expected here");
-    HeapObject heap_object;
+    Tagged<HeapObject> heap_object;
     if (!(*slot).GetHeapObject(&heap_object)) return;
     if (!Heap::InYoungGeneration(heap_object)) return;
 
@@ -4649,7 +4675,7 @@ class RememberedSetUpdatingItem : public UpdatingItem {
       RememberedSet<OLD_TO_CODE>::Iterate(
           chunk_,
           [=](MaybeObjectSlot slot) {
-            HeapObject host = HeapObject::FromAddress(
+            Tagged<HeapObject> host = HeapObject::FromAddress(
                 slot.address() - Code::kInstructionStreamOffset);
             DCHECK(IsCode(host, cage_base));
             UpdateStrongCodeSlot<AccessMode::NON_ATOMIC>(
@@ -4760,7 +4786,7 @@ class EphemeronTableUpdatingItem : public UpdatingItem {
 
     auto* table_map = heap_->ephemeron_remembered_set()->tables();
     for (auto it = table_map->begin(); it != table_map->end();) {
-      EphemeronHashTable table = it->first;
+      Tagged<EphemeronHashTable> table = it->first;
       auto& indices = it->second;
       if (table->map_word(cage_base, kRelaxedLoad).IsForwardingAddress()) {
         // The table has moved, and RecordMigratedSlotVisitor::VisitEphemeron
@@ -4774,7 +4800,7 @@ class EphemeronTableUpdatingItem : public UpdatingItem {
         // EphemeronHashTable keys must be heap objects.
         HeapObjectSlot key_slot(table->RawFieldOfElementAt(
             EphemeronHashTable::EntryToIndex(InternalIndex(*iti))));
-        HeapObject key = key_slot.ToHeapObject();
+        Tagged<HeapObject> key = key_slot.ToHeapObject();
         MapWord map_word = key->map_word(cage_base, kRelaxedLoad);
         if (map_word.IsForwardingAddress()) {
           key = map_word.ToForwardingAddress(key);
@@ -5108,7 +5134,7 @@ void MarkCompactCollector::SweepLargeSpace(LargeObjectSpace* space) {
   size_t surviving_object_size = 0;
   for (auto it = space->begin(); it != space->end();) {
     LargePage* current = *(it++);
-    HeapObject object = current->GetObject();
+    Tagged<HeapObject> object = current->GetObject();
     if (!marking_state_->IsMarked(object)) {
       // Object is dead and page can be released.
       space->RemovePage(current);
diff --git a/src/heap/mark-compact.h b/src/heap/mark-compact.h
index a42f39fc396..ceeb596edac 100644
--- a/src/heap/mark-compact.h
+++ b/src/heap/mark-compact.h
@@ -75,7 +75,7 @@ class MarkCompactCollector final {
 
   void StartMarking();
 
-  static inline bool IsOnEvacuationCandidate(Object obj) {
+  static inline bool IsOnEvacuationCandidate(Tagged<Object> obj) {
     return Page::FromAddress(obj.ptr())->IsEvacuationCandidate();
   }
 
@@ -87,24 +87,27 @@ class MarkCompactCollector final {
     uint32_t offset;
   };
 
-  static bool ShouldRecordRelocSlot(InstructionStream host, RelocInfo* rinfo,
-                                    HeapObject target);
-  static RecordRelocSlotInfo ProcessRelocInfo(InstructionStream host,
+  static bool ShouldRecordRelocSlot(Tagged<InstructionStream> host,
+                                    RelocInfo* rinfo,
+                                    Tagged<HeapObject> target);
+  static RecordRelocSlotInfo ProcessRelocInfo(Tagged<InstructionStream> host,
                                               RelocInfo* rinfo,
-                                              HeapObject target);
-
-  static void RecordRelocSlot(InstructionStream host, RelocInfo* rinfo,
-                              HeapObject target);
-  V8_INLINE static void RecordSlot(HeapObject object, ObjectSlot slot,
-                                   HeapObject target);
-  V8_INLINE static void RecordSlot(HeapObject object, HeapObjectSlot slot,
-                                   HeapObject target);
+                                              Tagged<HeapObject> target);
+
+  static void RecordRelocSlot(Tagged<InstructionStream> host, RelocInfo* rinfo,
+                              Tagged<HeapObject> target);
+  V8_INLINE static void RecordSlot(Tagged<HeapObject> object, ObjectSlot slot,
+                                   Tagged<HeapObject> target);
+  V8_INLINE static void RecordSlot(Tagged<HeapObject> object,
+                                   HeapObjectSlot slot,
+                                   Tagged<HeapObject> target);
   V8_INLINE static void RecordSlot(MemoryChunk* source_page,
-                                   HeapObjectSlot slot, HeapObject target);
+                                   HeapObjectSlot slot,
+                                   Tagged<HeapObject> target);
 
   bool is_compacting() const { return compacting_; }
 
-  V8_INLINE void AddTransitionArray(TransitionArray array);
+  V8_INLINE void AddTransitionArray(Tagged<TransitionArray> array);
 
   void RecordStrongDescriptorArraysForWeakening(
       GlobalHandleVector<DescriptorArray> strong_descriptor_arrays);
@@ -138,7 +141,7 @@ class MarkCompactCollector final {
   WeakObjects* weak_objects() { return &weak_objects_; }
   WeakObjects::Local* local_weak_objects() { return local_weak_objects_.get(); }
 
-  void AddNewlyDiscovered(HeapObject object) {
+  void AddNewlyDiscovered(Tagged<HeapObject> object) {
     if (ephemeron_marking_.newly_discovered_overflowed) return;
 
     if (ephemeron_marking_.newly_discovered.size() <
@@ -180,11 +183,11 @@ class MarkCompactCollector final {
 
   // Marks the object grey and adds it to the marking work list.
   // This is for non-incremental marking only.
-  V8_INLINE void MarkObject(HeapObject host, HeapObject obj);
+  V8_INLINE void MarkObject(Tagged<HeapObject> host, Tagged<HeapObject> obj);
 
   // Marks the object grey and adds it to the marking work list.
   // This is for non-incremental marking only.
-  V8_INLINE void MarkRootObject(Root root, HeapObject obj);
+  V8_INLINE void MarkRootObject(Root root, Tagged<HeapObject> obj);
 
   // Mark the heap roots and all objects reachable from them.
   void MarkRoots(RootVisitor* root_visitor);
@@ -216,7 +219,7 @@ class MarkCompactCollector final {
 
   // Implements ephemeron semantics: Marks value if key is already reachable.
   // Returns true if value was actually marked.
-  bool ProcessEphemeron(HeapObject key, HeapObject value);
+  bool ProcessEphemeron(Tagged<HeapObject> key, Tagged<HeapObject> value);
 
   // Marks the transitive closure by draining the marking worklist iteratively,
   // applying ephemerons semantics and invoking embedder tracing until a
@@ -247,19 +250,21 @@ class MarkCompactCollector final {
   // Checks if the given weak cell is a simple transition from the parent map
   // of the given dead target. If so it clears the transition and trims
   // the descriptor array of the parent if needed.
-  void ClearPotentialSimpleMapTransition(Map dead_target);
-  void ClearPotentialSimpleMapTransition(Map map, Map dead_target);
+  void ClearPotentialSimpleMapTransition(Tagged<Map> dead_target);
+  void ClearPotentialSimpleMapTransition(Tagged<Map> map,
+                                         Tagged<Map> dead_target);
 
   // Flushes a weakly held bytecode array from a shared function info.
-  void FlushBytecodeFromSFI(SharedFunctionInfo shared_info);
+  void FlushBytecodeFromSFI(Tagged<SharedFunctionInfo> shared_info);
 
   // Clears bytecode arrays / baseline code that have not been executed for
   // multiple collections.
   void ProcessOldCodeCandidates();
 
-  bool ProcessOldBytecodeSFI(SharedFunctionInfo flushing_candidate);
-  bool ProcessOldBaselineSFI(SharedFunctionInfo flushing_candidate);
-  void FlushSFI(SharedFunctionInfo sfi, bool bytecode_already_decompiled);
+  bool ProcessOldBytecodeSFI(Tagged<SharedFunctionInfo> flushing_candidate);
+  bool ProcessOldBaselineSFI(Tagged<SharedFunctionInfo> flushing_candidate);
+  void FlushSFI(Tagged<SharedFunctionInfo> sfi,
+                bool bytecode_already_decompiled);
 
   void ProcessFlushedBaselineCandidates();
 
@@ -269,11 +274,13 @@ class MarkCompactCollector final {
   // Compact every array in the global list of transition arrays and
   // trim the corresponding descriptor array if a transition target is non-live.
   void ClearFullMapTransitions();
-  void TrimDescriptorArray(Map map, DescriptorArray descriptors);
-  void TrimEnumCache(Map map, DescriptorArray descriptors);
-  bool CompactTransitionArray(Map map, TransitionArray transitions,
-                              DescriptorArray descriptors);
-  bool TransitionArrayNeedsCompaction(TransitionArray transitions,
+  void TrimDescriptorArray(Tagged<Map> map,
+                           Tagged<DescriptorArray> descriptors);
+  void TrimEnumCache(Tagged<Map> map, Tagged<DescriptorArray> descriptors);
+  bool CompactTransitionArray(Tagged<Map> map,
+                              Tagged<TransitionArray> transitions,
+                              Tagged<DescriptorArray> descriptors);
+  bool TransitionArrayNeedsCompaction(Tagged<TransitionArray> transitions,
                                       int num_transitions);
   void WeakenStrongDescriptorArrays();
 
@@ -315,9 +322,10 @@ class MarkCompactCollector final {
 
   int NumberOfParallelEphemeronVisitingTasks(size_t elements);
 
-  void RightTrimDescriptorArray(DescriptorArray array, int descriptors_to_trim);
+  void RightTrimDescriptorArray(Tagged<DescriptorArray> array,
+                                int descriptors_to_trim);
 
-  V8_INLINE bool ShouldMarkObject(HeapObject) const;
+  V8_INLINE bool ShouldMarkObject(Tagged<HeapObject>) const;
 
   void StartSweepNewSpace();
   void SweepLargeSpace(LargeObjectSpace* space);
diff --git a/src/heap/mark-sweep-utilities.cc b/src/heap/mark-sweep-utilities.cc
index 2e410704f85..4ada3534fc0 100644
--- a/src/heap/mark-sweep-utilities.cc
+++ b/src/heap/mark-sweep-utilities.cc
@@ -24,7 +24,7 @@ static_assert(Heap::kMinObjectSizeInTaggedWords >= 2);
 MarkingVerifierBase::MarkingVerifierBase(Heap* heap)
     : ObjectVisitorWithCageBases(heap), heap_(heap) {}
 
-void MarkingVerifierBase::VisitMapPointer(HeapObject object) {
+void MarkingVerifierBase::VisitMapPointer(Tagged<HeapObject> object) {
   VerifyMap(object->map(cage_base()));
 }
 
@@ -89,7 +89,7 @@ void MarkingVerifierBase::VerifyMarking(PagedSpaceBase* space) {
 void MarkingVerifierBase::VerifyMarking(LargeObjectSpace* lo_space) {
   if (!lo_space) return;
   LargeObjectSpaceObjectIterator it(lo_space);
-  for (HeapObject obj = it.Next(); !obj.is_null(); obj = it.Next()) {
+  for (Tagged<HeapObject> obj = it.Next(); !obj.is_null(); obj = it.Next()) {
     if (IsMarked(obj)) {
       obj->Iterate(cage_base(), this);
     }
@@ -105,11 +105,11 @@ void ExternalStringTableCleanerVisitor<mode>::VisitRootPointers(
   DCHECK_EQ(static_cast<int>(root),
             static_cast<int>(Root::kExternalStringsTable));
   NonAtomicMarkingState* marking_state = heap_->non_atomic_marking_state();
-  Object the_hole = ReadOnlyRoots(heap_).the_hole_value();
+  Tagged<Object> the_hole = ReadOnlyRoots(heap_).the_hole_value();
   for (FullObjectSlot p = start; p < end; ++p) {
-    Object o = *p;
+    Tagged<Object> o = *p;
     if (!IsHeapObject(o)) continue;
-    HeapObject heap_object = HeapObject::cast(o);
+    Tagged<HeapObject> heap_object = HeapObject::cast(o);
     // MinorMS doesn't update the young strings set and so it may contain
     // strings that are already in old space.
     if (!marking_state->IsUnmarked(heap_object)) continue;
diff --git a/src/heap/mark-sweep-utilities.h b/src/heap/mark-sweep-utilities.h
index c399823ac8b..bdbb6762ab1 100644
--- a/src/heap/mark-sweep-utilities.h
+++ b/src/heap/mark-sweep-utilities.h
@@ -30,25 +30,25 @@ class MarkingVerifierBase : public ObjectVisitorWithCageBases,
 
   virtual const MarkingBitmap* bitmap(const MemoryChunk* chunk) = 0;
 
-  virtual void VerifyMap(Map map) = 0;
+  virtual void VerifyMap(Tagged<Map> map) = 0;
   virtual void VerifyPointers(ObjectSlot start, ObjectSlot end) = 0;
   virtual void VerifyPointers(MaybeObjectSlot start, MaybeObjectSlot end) = 0;
   virtual void VerifyCodePointer(InstructionStreamSlot slot) = 0;
   virtual void VerifyRootPointers(FullObjectSlot start, FullObjectSlot end) = 0;
 
-  virtual bool IsMarked(HeapObject object) = 0;
+  virtual bool IsMarked(Tagged<HeapObject> object) = 0;
 
-  void VisitPointers(HeapObject host, ObjectSlot start,
+  void VisitPointers(Tagged<HeapObject> host, ObjectSlot start,
                      ObjectSlot end) override {
     VerifyPointers(start, end);
   }
 
-  void VisitPointers(HeapObject host, MaybeObjectSlot start,
+  void VisitPointers(Tagged<HeapObject> host, MaybeObjectSlot start,
                      MaybeObjectSlot end) override {
     VerifyPointers(start, end);
   }
 
-  void VisitInstructionStreamPointer(Code host,
+  void VisitInstructionStreamPointer(Tagged<Code> host,
                                      InstructionStreamSlot slot) override {
     VerifyCodePointer(slot);
   }
@@ -58,7 +58,7 @@ class MarkingVerifierBase : public ObjectVisitorWithCageBases,
     VerifyRootPointers(start, end);
   }
 
-  void VisitMapPointer(HeapObject object) override;
+  void VisitMapPointer(Tagged<HeapObject> object) override;
 
   void VerifyRoots();
   void VerifyMarkingOnPage(const Page* page, Address start, Address end);
diff --git a/src/heap/marking-barrier-inl.h b/src/heap/marking-barrier-inl.h
index 62613dad845..1db1a1c0e9c 100644
--- a/src/heap/marking-barrier-inl.h
+++ b/src/heap/marking-barrier-inl.h
@@ -13,7 +13,8 @@
 namespace v8 {
 namespace internal {
 
-void MarkingBarrier::MarkValue(HeapObject host, HeapObject value) {
+void MarkingBarrier::MarkValue(Tagged<HeapObject> host,
+                               Tagged<HeapObject> value) {
   if (value.InReadOnlySpace()) return;
 
   DCHECK(IsCurrentMarkingBarrier(host));
@@ -45,7 +46,7 @@ void MarkingBarrier::MarkValue(HeapObject host, HeapObject value) {
   MarkValueLocal(value);
 }
 
-void MarkingBarrier::MarkValueShared(HeapObject value) {
+void MarkingBarrier::MarkValueShared(Tagged<HeapObject> value) {
   // Value is either in read-only space or shared heap.
   DCHECK(value.InAnySharedSpace());
 
@@ -59,7 +60,7 @@ void MarkingBarrier::MarkValueShared(HeapObject value) {
   }
 }
 
-void MarkingBarrier::MarkValueLocal(HeapObject value) {
+void MarkingBarrier::MarkValueLocal(Tagged<HeapObject> value) {
   DCHECK(!value.InReadOnlySpace());
   if (is_minor()) {
     // We do not need to insert into RememberedSet<OLD_TO_NEW> here because the
@@ -80,14 +81,15 @@ void MarkingBarrier::MarkValueLocal(HeapObject value) {
 }
 
 template <typename TSlot>
-inline void MarkingBarrier::MarkRange(HeapObject host, TSlot start, TSlot end) {
+inline void MarkingBarrier::MarkRange(Tagged<HeapObject> host, TSlot start,
+                                      TSlot end) {
   auto* isolate = heap_->isolate();
   const bool record_slots =
       IsCompacting(host) &&
       !MemoryChunk::FromHeapObject(host)->ShouldSkipEvacuationSlotRecording();
   for (TSlot slot = start; slot < end; ++slot) {
     typename TSlot::TObject object = slot.Relaxed_Load();
-    HeapObject heap_object;
+    Tagged<HeapObject> heap_object;
     // Mark both, weak and strong edges.
     if (object.GetHeapObject(isolate, &heap_object)) {
       MarkValue(host, heap_object);
@@ -98,7 +100,7 @@ inline void MarkingBarrier::MarkRange(HeapObject host, TSlot start, TSlot end) {
   }
 }
 
-bool MarkingBarrier::IsCompacting(HeapObject object) const {
+bool MarkingBarrier::IsCompacting(Tagged<HeapObject> object) const {
   if (is_compacting_) {
     DCHECK(is_major());
     return true;
@@ -107,7 +109,7 @@ bool MarkingBarrier::IsCompacting(HeapObject object) const {
   return shared_heap_worklist_.has_value() && object.InWritableSharedSpace();
 }
 
-bool MarkingBarrier::WhiteToGreyAndPush(HeapObject obj) {
+bool MarkingBarrier::WhiteToGreyAndPush(Tagged<HeapObject> obj) {
   if (marking_state_.TryMark(obj)) {
     current_worklist_->Push(obj);
     return true;
diff --git a/src/heap/marking-barrier.cc b/src/heap/marking-barrier.cc
index e5c0b4e1656..b634d90489e 100644
--- a/src/heap/marking-barrier.cc
+++ b/src/heap/marking-barrier.cc
@@ -39,8 +39,8 @@ MarkingBarrier::MarkingBarrier(LocalHeap* local_heap)
 
 MarkingBarrier::~MarkingBarrier() { DCHECK(typed_slots_map_.empty()); }
 
-void MarkingBarrier::Write(HeapObject host, HeapObjectSlot slot,
-                           HeapObject value) {
+void MarkingBarrier::Write(Tagged<HeapObject> host, HeapObjectSlot slot,
+                           Tagged<HeapObject> value) {
   DCHECK(IsCurrentMarkingBarrier(host));
   DCHECK(is_activated_ || shared_heap_worklist_.has_value());
   DCHECK(MemoryChunk::FromHeapObject(host)->IsMarking());
@@ -56,7 +56,7 @@ void MarkingBarrier::Write(HeapObject host, HeapObjectSlot slot,
   }
 }
 
-void MarkingBarrier::Write(HeapObject host, IndirectPointerSlot slot) {
+void MarkingBarrier::Write(Tagged<HeapObject> host, IndirectPointerSlot slot) {
   DCHECK(IsCurrentMarkingBarrier(host));
   DCHECK(is_activated_ || shared_heap_worklist_.has_value());
   DCHECK(MemoryChunk::FromHeapObject(host)->IsMarking());
@@ -66,7 +66,7 @@ void MarkingBarrier::Write(HeapObject host, IndirectPointerSlot slot) {
   // An indirect pointer slot can only contain a Smi if it is uninitialized (in
   // which case the vaue will be Smi::zero()). However, at this point the slot
   // must have been initialized because it was just written to.
-  HeapObject value = HeapObject::cast(slot.load());
+  Tagged<HeapObject> value = HeapObject::cast(slot.load());
   MarkValue(host, value);
 
   // We don't emit generational- and shared write barriers for indirect
@@ -83,7 +83,7 @@ void MarkingBarrier::Write(HeapObject host, IndirectPointerSlot slot) {
   // take care of updating the pointer to itself if it is relocated.
 }
 
-void MarkingBarrier::WriteWithoutHost(HeapObject value) {
+void MarkingBarrier::WriteWithoutHost(Tagged<HeapObject> value) {
   DCHECK(is_main_thread_barrier_);
   DCHECK(is_activated_);
 
@@ -99,8 +99,8 @@ void MarkingBarrier::WriteWithoutHost(HeapObject value) {
   MarkValueLocal(value);
 }
 
-void MarkingBarrier::Write(InstructionStream host, RelocInfo* reloc_info,
-                           HeapObject value) {
+void MarkingBarrier::Write(Tagged<InstructionStream> host,
+                           RelocInfo* reloc_info, Tagged<HeapObject> value) {
   DCHECK(IsCurrentMarkingBarrier(host));
   DCHECK(!host.InWritableSharedSpace());
   DCHECK(is_activated_ || shared_heap_worklist_.has_value());
@@ -123,7 +123,7 @@ void MarkingBarrier::Write(InstructionStream host, RelocInfo* reloc_info,
   }
 }
 
-void MarkingBarrier::Write(JSArrayBuffer host,
+void MarkingBarrier::Write(Tagged<JSArrayBuffer> host,
                            ArrayBufferExtension* extension) {
   DCHECK(IsCurrentMarkingBarrier(host));
   DCHECK(!host.InWritableSharedSpace());
@@ -142,7 +142,7 @@ void MarkingBarrier::Write(JSArrayBuffer host,
   }
 }
 
-void MarkingBarrier::Write(DescriptorArray descriptor_array,
+void MarkingBarrier::Write(Tagged<DescriptorArray> descriptor_array,
                            int number_of_own_descriptors) {
   DCHECK(IsCurrentMarkingBarrier(descriptor_array));
   DCHECK(IsReadOnlyHeapObject(descriptor_array->map()));
@@ -186,8 +186,9 @@ void MarkingBarrier::Write(DescriptorArray descriptor_array,
   }
 }
 
-void MarkingBarrier::RecordRelocSlot(InstructionStream host, RelocInfo* rinfo,
-                                     HeapObject target) {
+void MarkingBarrier::RecordRelocSlot(Tagged<InstructionStream> host,
+                                     RelocInfo* rinfo,
+                                     Tagged<HeapObject> target) {
   DCHECK(IsCurrentMarkingBarrier(host));
   if (!MarkCompactCollector::ShouldRecordRelocSlot(host, rinfo, target)) return;
 
@@ -448,7 +449,7 @@ void MarkingBarrier::PublishSharedIfNeeded() {
 }
 
 bool MarkingBarrier::IsCurrentMarkingBarrier(
-    HeapObject verification_candidate) {
+    Tagged<HeapObject> verification_candidate) {
   return WriteBarrier::CurrentMarkingBarrier(verification_candidate) == this;
 }
 
diff --git a/src/heap/marking-barrier.h b/src/heap/marking-barrier.h
index b36e2843441..959c794667d 100644
--- a/src/heap/marking-barrier.h
+++ b/src/heap/marking-barrier.h
@@ -42,16 +42,17 @@ class MarkingBarrier {
   static void DeactivateYoung(Heap* heap);
   V8_EXPORT_PRIVATE static void PublishYoung(Heap* heap);
 
-  void Write(HeapObject host, HeapObjectSlot, HeapObject value);
-  void Write(HeapObject host, IndirectPointerSlot slot);
-  void Write(InstructionStream host, RelocInfo*, HeapObject value);
-  void Write(JSArrayBuffer host, ArrayBufferExtension*);
-  void Write(DescriptorArray, int number_of_own_descriptors);
+  void Write(Tagged<HeapObject> host, HeapObjectSlot, Tagged<HeapObject> value);
+  void Write(Tagged<HeapObject> host, IndirectPointerSlot slot);
+  void Write(Tagged<InstructionStream> host, RelocInfo*,
+             Tagged<HeapObject> value);
+  void Write(Tagged<JSArrayBuffer> host, ArrayBufferExtension*);
+  void Write(Tagged<DescriptorArray>, int number_of_own_descriptors);
   // Only usable when there's no valid JS host object for this write, e.g., when
   // value is held alive from a global handle.
-  void WriteWithoutHost(HeapObject value);
+  void WriteWithoutHost(Tagged<HeapObject> value);
 
-  inline void MarkValue(HeapObject host, HeapObject value);
+  inline void MarkValue(Tagged<HeapObject> host, Tagged<HeapObject> value);
 
   bool is_minor() const { return marking_mode_ == MarkingMode::kMinorMarking; }
 
@@ -63,20 +64,20 @@ class MarkingBarrier {
 #endif  // DEBUG
 
  private:
-  inline void MarkValueShared(HeapObject value);
-  inline void MarkValueLocal(HeapObject value);
+  inline void MarkValueShared(Tagged<HeapObject> value);
+  inline void MarkValueLocal(Tagged<HeapObject> value);
 
-  inline bool WhiteToGreyAndPush(HeapObject value);
+  inline bool WhiteToGreyAndPush(Tagged<HeapObject> value);
 
-  void RecordRelocSlot(InstructionStream host, RelocInfo* rinfo,
-                       HeapObject target);
+  void RecordRelocSlot(Tagged<InstructionStream> host, RelocInfo* rinfo,
+                       Tagged<HeapObject> target);
 
-  bool IsCurrentMarkingBarrier(HeapObject verification_candidate);
+  bool IsCurrentMarkingBarrier(Tagged<HeapObject> verification_candidate);
 
   template <typename TSlot>
-  inline void MarkRange(HeapObject value, TSlot start, TSlot end);
+  inline void MarkRange(Tagged<HeapObject> value, TSlot start, TSlot end);
 
-  inline bool IsCompacting(HeapObject object) const;
+  inline bool IsCompacting(Tagged<HeapObject> object) const;
 
   bool is_major() const { return marking_mode_ == MarkingMode::kMajorMarking; }
 
diff --git a/src/heap/marking-inl.h b/src/heap/marking-inl.h
index aa420327372..528c1479d4c 100644
--- a/src/heap/marking-inl.h
+++ b/src/heap/marking-inl.h
@@ -253,7 +253,7 @@ MarkBit MarkBit::From(Address address) {
 }
 
 // static
-MarkBit MarkBit::From(HeapObject heap_object) {
+MarkBit MarkBit::From(Tagged<HeapObject> heap_object) {
   return MarkingBitmap::MarkBitFromAddress(heap_object.ptr());
 }
 
diff --git a/src/heap/marking-state-inl.h b/src/heap/marking-state-inl.h
index 79793752263..e97d8108185 100644
--- a/src/heap/marking-state-inl.h
+++ b/src/heap/marking-state-inl.h
@@ -14,27 +14,28 @@ namespace internal {
 
 template <typename ConcreteState, AccessMode access_mode>
 bool MarkingStateBase<ConcreteState, access_mode>::IsMarked(
-    const HeapObject obj) const {
+    const Tagged<HeapObject> obj) const {
   return MarkBit::From(obj).template Get<access_mode>();
 }
 
 template <typename ConcreteState, AccessMode access_mode>
 bool MarkingStateBase<ConcreteState, access_mode>::IsUnmarked(
-    const HeapObject obj) const {
+    const Tagged<HeapObject> obj) const {
   return !IsMarked(obj);
 }
 
 template <typename ConcreteState, AccessMode access_mode>
-bool MarkingStateBase<ConcreteState, access_mode>::TryMark(HeapObject obj) {
+bool MarkingStateBase<ConcreteState, access_mode>::TryMark(
+    Tagged<HeapObject> obj) {
   return MarkBit::From(obj).template Set<access_mode>();
 }
 
 template <typename ConcreteState, AccessMode access_mode>
 bool MarkingStateBase<ConcreteState, access_mode>::TryMarkAndAccountLiveBytes(
-    HeapObject obj) {
+    Tagged<HeapObject> obj) {
   if (TryMark(obj)) {
     MemoryChunk::FromHeapObject(obj)->IncrementLiveBytesAtomically(
-        ALIGN_TO_ALLOCATION_ALIGNMENT(obj.Size(cage_base())));
+        ALIGN_TO_ALLOCATION_ALIGNMENT(obj->Size(cage_base())));
     return true;
   }
   return false;
diff --git a/src/heap/marking-state.h b/src/heap/marking-state.h
index 6d960aea991..2efeaeb5fd5 100644
--- a/src/heap/marking-state.h
+++ b/src/heap/marking-state.h
@@ -35,12 +35,12 @@ class MarkingStateBase {
 #endif  // V8_COMPRESS_POINTERS
   }
 
-  V8_INLINE bool TryMark(HeapObject obj);
+  V8_INLINE bool TryMark(Tagged<HeapObject> obj);
   // Helper method for fully marking an object and accounting its live bytes.
   // Should be used to mark individual objects in one-off cases.
-  V8_INLINE bool TryMarkAndAccountLiveBytes(HeapObject obj);
-  V8_INLINE bool IsMarked(const HeapObject obj) const;
-  V8_INLINE bool IsUnmarked(const HeapObject obj) const;
+  V8_INLINE bool TryMarkAndAccountLiveBytes(Tagged<HeapObject> obj);
+  V8_INLINE bool IsMarked(const Tagged<HeapObject> obj) const;
+  V8_INLINE bool IsUnmarked(const Tagged<HeapObject> obj) const;
 
  private:
 #if V8_COMPRESS_POINTERS
diff --git a/src/heap/marking-visitor-inl.h b/src/heap/marking-visitor-inl.h
index 95bff41873b..2092aaebf83 100644
--- a/src/heap/marking-visitor-inl.h
+++ b/src/heap/marking-visitor-inl.h
@@ -31,8 +31,8 @@ namespace internal {
 // ===========================================================================
 
 template <typename ConcreteVisitor>
-void MarkingVisitorBase<ConcreteVisitor>::MarkObject(HeapObject host,
-                                                     HeapObject object) {
+void MarkingVisitorBase<ConcreteVisitor>::MarkObject(
+    Tagged<HeapObject> host, Tagged<HeapObject> object) {
   DCHECK(ReadOnlyHeap::Contains(object) || heap_->Contains(object));
   SynchronizePageAccess(object);
   concrete_visitor()->AddStrongReferenceForReferenceSummarizer(host, object);
@@ -50,7 +50,8 @@ template <typename ConcreteVisitor>
 // method template arguments
 template <typename THeapObjectSlot>
 void MarkingVisitorBase<ConcreteVisitor>::ProcessStrongHeapObject(
-    HeapObject host, THeapObjectSlot slot, HeapObject heap_object) {
+    Tagged<HeapObject> host, THeapObjectSlot slot,
+    Tagged<HeapObject> heap_object) {
   SynchronizePageAccess(heap_object);
   if (!ShouldMarkObject(heap_object)) return;
   MarkObject(host, heap_object);
@@ -62,7 +63,8 @@ template <typename ConcreteVisitor>
 // method template arguments
 template <typename THeapObjectSlot>
 void MarkingVisitorBase<ConcreteVisitor>::ProcessWeakHeapObject(
-    HeapObject host, THeapObjectSlot slot, HeapObject heap_object) {
+    Tagged<HeapObject> host, THeapObjectSlot slot,
+    Tagged<HeapObject> heap_object) {
   SynchronizePageAccess(heap_object);
   if (!ShouldMarkObject(heap_object)) return;
   if (concrete_visitor()->IsMarked(heap_object)) {
@@ -85,18 +87,18 @@ template <typename ConcreteVisitor>
 // method template arguments
 template <typename TSlot>
 V8_INLINE void MarkingVisitorBase<ConcreteVisitor>::VisitPointersImpl(
-    HeapObject host, TSlot start, TSlot end) {
+    Tagged<HeapObject> host, TSlot start, TSlot end) {
   using THeapObjectSlot = typename TSlot::THeapObjectSlot;
   for (TSlot slot = start; slot < end; ++slot) {
     typename TSlot::TObject object =
         slot.Relaxed_Load(ObjectVisitorWithCageBases::cage_base());
-    HeapObject heap_object;
-    if (object.GetHeapObjectIfStrong(&heap_object)) {
+    Tagged<HeapObject> heap_object;
+    if (object->GetHeapObjectIfStrong(&heap_object)) {
       // If the reference changes concurrently from strong to weak, the write
       // barrier will treat the weak reference as strong, so we won't miss the
       // weak reference.
       ProcessStrongHeapObject(host, THeapObjectSlot(slot), heap_object);
-    } else if (TSlot::kCanBeWeak && object.GetHeapObjectIfWeak(&heap_object)) {
+    } else if (TSlot::kCanBeWeak && object->GetHeapObjectIfWeak(&heap_object)) {
       ProcessWeakHeapObject(host, THeapObjectSlot(slot), heap_object);
     }
   }
@@ -105,11 +107,11 @@ V8_INLINE void MarkingVisitorBase<ConcreteVisitor>::VisitPointersImpl(
 template <typename ConcreteVisitor>
 V8_INLINE void
 MarkingVisitorBase<ConcreteVisitor>::VisitInstructionStreamPointerImpl(
-    Code host, InstructionStreamSlot slot) {
-  Object object =
+    Tagged<Code> host, InstructionStreamSlot slot) {
+  Tagged<Object> object =
       slot.Relaxed_Load(ObjectVisitorWithCageBases::code_cage_base());
-  HeapObject heap_object;
-  if (object.GetHeapObjectIfStrong(&heap_object)) {
+  Tagged<HeapObject> heap_object;
+  if (object->GetHeapObjectIfStrong(&heap_object)) {
     // If the reference changes concurrently from strong to weak, the write
     // barrier will treat the weak reference as strong, so we won't miss the
     // weak reference.
@@ -119,14 +121,14 @@ MarkingVisitorBase<ConcreteVisitor>::VisitInstructionStreamPointerImpl(
 
 template <typename ConcreteVisitor>
 void MarkingVisitorBase<ConcreteVisitor>::VisitEmbeddedPointer(
-    InstructionStream host, RelocInfo* rinfo) {
+    Tagged<InstructionStream> host, RelocInfo* rinfo) {
   DCHECK(RelocInfo::IsEmbeddedObjectMode(rinfo->rmode()));
-  HeapObject object =
+  Tagged<HeapObject> object =
       rinfo->target_object(ObjectVisitorWithCageBases::cage_base());
   if (!ShouldMarkObject(object)) return;
 
   if (!concrete_visitor()->IsMarked(object)) {
-    Code code = Code::unchecked_cast(host->raw_code(kAcquireLoad));
+    Tagged<Code> code = Code::unchecked_cast(host->raw_code(kAcquireLoad));
     if (code->IsWeakObject(object)) {
       local_weak_objects_->weak_objects_in_code_local.Push(
           std::make_pair(object, code));
@@ -140,9 +142,9 @@ void MarkingVisitorBase<ConcreteVisitor>::VisitEmbeddedPointer(
 
 template <typename ConcreteVisitor>
 void MarkingVisitorBase<ConcreteVisitor>::VisitCodeTarget(
-    InstructionStream host, RelocInfo* rinfo) {
+    Tagged<InstructionStream> host, RelocInfo* rinfo) {
   DCHECK(RelocInfo::IsCodeTargetMode(rinfo->rmode()));
-  InstructionStream target =
+  Tagged<InstructionStream> target =
       InstructionStream::FromTargetAddress(rinfo->target_address());
 
   if (!ShouldMarkObject(target)) return;
@@ -152,7 +154,7 @@ void MarkingVisitorBase<ConcreteVisitor>::VisitCodeTarget(
 
 template <typename ConcreteVisitor>
 void MarkingVisitorBase<ConcreteVisitor>::VisitExternalPointer(
-    HeapObject host, ExternalPointerSlot slot, ExternalPointerTag tag) {
+    Tagged<HeapObject> host, ExternalPointerSlot slot, ExternalPointerTag tag) {
 #ifdef V8_ENABLE_SANDBOX
   DCHECK_NE(tag, kExternalPointerNullTag);
   ExternalPointerHandle handle = slot.Relaxed_LoadHandle();
@@ -168,16 +170,17 @@ void MarkingVisitorBase<ConcreteVisitor>::VisitExternalPointer(
 
 template <typename ConcreteVisitor>
 void MarkingVisitorBase<ConcreteVisitor>::VisitIndirectPointer(
-    HeapObject host, IndirectPointerSlot slot, IndirectPointerMode mode) {
+    Tagged<HeapObject> host, IndirectPointerSlot slot,
+    IndirectPointerMode mode) {
 #ifdef V8_CODE_POINTER_SANDBOXING
   if (mode == IndirectPointerMode::kStrong) {
     // Load the referenced object (if the slot is initialized) and mark it as
     // alive if necessary. Indirect pointers never have to be added to a
     // remembered set because the referenced object will update the pointer
     // table entry when it is relocated.
-    Object value = slot.Relaxed_Load();
+    Tagged<Object> value = slot.Relaxed_Load();
     if (IsHeapObject(value)) {
-      HeapObject obj = HeapObject::cast(value);
+      Tagged<HeapObject> obj = HeapObject::cast(value);
       SynchronizePageAccess(obj);
       if (ShouldMarkObject(obj)) {
         MarkObject(host, obj);
@@ -191,7 +194,7 @@ void MarkingVisitorBase<ConcreteVisitor>::VisitIndirectPointer(
 
 template <typename ConcreteVisitor>
 void MarkingVisitorBase<ConcreteVisitor>::VisitIndirectPointerTableEntry(
-    HeapObject host, IndirectPointerSlot slot) {
+    Tagged<HeapObject> host, IndirectPointerSlot slot) {
 #ifdef V8_CODE_POINTER_SANDBOXING
   static_assert(kAllIndirectPointerObjectsAreCode);
   CodePointerTable* table = GetProcessWideCodePointerTable();
@@ -209,7 +212,7 @@ void MarkingVisitorBase<ConcreteVisitor>::VisitIndirectPointerTableEntry(
 
 template <typename ConcreteVisitor>
 int MarkingVisitorBase<ConcreteVisitor>::VisitBytecodeArray(
-    Map map, BytecodeArray object) {
+    Tagged<Map> map, Tagged<BytecodeArray> object) {
   int size = BytecodeArray::BodyDescriptor::SizeOf(map, object);
   this->VisitMapPointer(object);
   BytecodeArray::BodyDescriptor::IterateBody(map, object, size, this);
@@ -218,7 +221,7 @@ int MarkingVisitorBase<ConcreteVisitor>::VisitBytecodeArray(
 
 template <typename ConcreteVisitor>
 int MarkingVisitorBase<ConcreteVisitor>::VisitJSFunction(
-    Map map, JSFunction js_function) {
+    Tagged<Map> map, Tagged<JSFunction> js_function) {
   int size = concrete_visitor()->VisitJSObjectSubclass(map, js_function);
   if (ShouldFlushBaselineCode(js_function)) {
     DCHECK(IsBaselineCodeFlushingEnabled(code_flush_mode_));
@@ -245,7 +248,7 @@ int MarkingVisitorBase<ConcreteVisitor>::VisitJSFunction(
 
 template <typename ConcreteVisitor>
 int MarkingVisitorBase<ConcreteVisitor>::VisitSharedFunctionInfo(
-    Map map, SharedFunctionInfo shared_info) {
+    Tagged<Map> map, Tagged<SharedFunctionInfo> shared_info) {
   int size = SharedFunctionInfo::BodyDescriptor::SizeOf(map, shared_info);
   this->VisitMapPointer(shared_info);
   SharedFunctionInfo::BodyDescriptor::IterateBody(map, shared_info, size, this);
@@ -266,7 +269,8 @@ int MarkingVisitorBase<ConcreteVisitor>::VisitSharedFunctionInfo(
     // If bytecode flushing is disabled but baseline code flushing is enabled
     // then we have to visit the bytecode but not the baseline code.
     DCHECK(IsBaselineCodeFlushingEnabled(code_flush_mode_));
-    Code baseline_code = Code::cast(shared_info->function_data(kAcquireLoad));
+    Tagged<Code> baseline_code =
+        Code::cast(shared_info->function_data(kAcquireLoad));
     // Visit the bytecode hanging off baseline code.
     VisitPointer(baseline_code,
                  baseline_code->RawField(
@@ -282,7 +286,7 @@ int MarkingVisitorBase<ConcreteVisitor>::VisitSharedFunctionInfo(
 
 template <typename ConcreteVisitor>
 bool MarkingVisitorBase<ConcreteVisitor>::HasBytecodeArrayForFlushing(
-    SharedFunctionInfo sfi) const {
+    Tagged<SharedFunctionInfo> sfi) const {
   if (IsFlushingDisabled(code_flush_mode_)) return false;
 
   // TODO(rmcilroy): Enable bytecode flushing for resumable functions.
@@ -293,9 +297,9 @@ bool MarkingVisitorBase<ConcreteVisitor>::HasBytecodeArrayForFlushing(
   // Get a snapshot of the function data field, and if it is a bytecode array,
   // check if it is old. Note, this is done this way since this function can be
   // called by the concurrent marker.
-  Object data = sfi->function_data(kAcquireLoad);
+  Tagged<Object> data = sfi->function_data(kAcquireLoad);
   if (IsCode(data)) {
-    Code baseline_code = Code::cast(data);
+    Tagged<Code> baseline_code = Code::cast(data);
     DCHECK_EQ(baseline_code->kind(), CodeKind::BASELINE);
     // If baseline code flushing isn't enabled and we have baseline data on SFI
     // we cannot flush baseline / bytecode.
@@ -312,12 +316,13 @@ bool MarkingVisitorBase<ConcreteVisitor>::HasBytecodeArrayForFlushing(
 
 template <typename ConcreteVisitor>
 bool MarkingVisitorBase<ConcreteVisitor>::ShouldFlushCode(
-    SharedFunctionInfo sfi) const {
+    Tagged<SharedFunctionInfo> sfi) const {
   return IsStressFlushingEnabled(code_flush_mode_) || IsOld(sfi);
 }
 
 template <typename ConcreteVisitor>
-bool MarkingVisitorBase<ConcreteVisitor>::IsOld(SharedFunctionInfo sfi) const {
+bool MarkingVisitorBase<ConcreteVisitor>::IsOld(
+    Tagged<SharedFunctionInfo> sfi) const {
   if (v8_flags.flush_code_based_on_time) {
     return sfi->age() >= v8_flags.bytecode_old_time;
   } else if (v8_flags.flush_code_based_on_tab_visibility) {
@@ -330,7 +335,7 @@ bool MarkingVisitorBase<ConcreteVisitor>::IsOld(SharedFunctionInfo sfi) const {
 
 template <typename ConcreteVisitor>
 void MarkingVisitorBase<ConcreteVisitor>::MakeOlder(
-    SharedFunctionInfo sfi) const {
+    Tagged<SharedFunctionInfo> sfi) const {
   if (v8_flags.flush_code_based_on_time) {
     DCHECK_NE(code_flushing_increase_, 0);
     uint16_t current_age;
@@ -361,20 +366,20 @@ void MarkingVisitorBase<ConcreteVisitor>::MakeOlder(
 
 template <typename ConcreteVisitor>
 bool MarkingVisitorBase<ConcreteVisitor>::ShouldFlushBaselineCode(
-    JSFunction js_function) const {
+    Tagged<JSFunction> js_function) const {
   if (!IsBaselineCodeFlushingEnabled(code_flush_mode_)) return false;
   // Do a raw read for shared and code fields here since this function may be
   // called on a concurrent thread. JSFunction itself should be fully
   // initialized here but the SharedFunctionInfo, InstructionStream objects may
   // not be initialized. We read using acquire loads to defend against that.
-  Object maybe_shared =
+  Tagged<Object> maybe_shared =
       ACQUIRE_READ_FIELD(js_function, JSFunction::kSharedFunctionInfoOffset);
   if (!IsSharedFunctionInfo(maybe_shared)) return false;
 
   // See crbug.com/v8/11972 for more details on acquire / release semantics for
   // code field. We don't use release stores when copying code pointers from
   // SFI / FV to JSFunction but it is safe in practice.
-  Object maybe_code = js_function.raw_code(kAcquireLoad);
+  Tagged<Object> maybe_code = js_function->raw_code(kAcquireLoad);
 
 #ifdef THREAD_SANITIZER
   // This is needed because TSAN does not process the memory fence
@@ -382,10 +387,10 @@ bool MarkingVisitorBase<ConcreteVisitor>::ShouldFlushBaselineCode(
   BasicMemoryChunk::FromAddress(maybe_code.ptr())->SynchronizedHeapLoad();
 #endif
   if (!IsCode(maybe_code)) return false;
-  Code code = Code::cast(maybe_code);
+  Tagged<Code> code = Code::cast(maybe_code);
   if (code->kind() != CodeKind::BASELINE) return false;
 
-  SharedFunctionInfo shared = SharedFunctionInfo::cast(maybe_shared);
+  Tagged<SharedFunctionInfo> shared = SharedFunctionInfo::cast(maybe_shared);
   return HasBytecodeArrayForFlushing(shared) && ShouldFlushCode(shared);
 }
 
@@ -395,7 +400,7 @@ bool MarkingVisitorBase<ConcreteVisitor>::ShouldFlushBaselineCode(
 
 template <typename ConcreteVisitor>
 int MarkingVisitorBase<ConcreteVisitor>::VisitFixedArrayWithProgressBar(
-    Map map, FixedArray object, ProgressBar& progress_bar) {
+    Tagged<Map> map, Tagged<FixedArray> object, ProgressBar& progress_bar) {
   const int kProgressBarScanningChunk = kMaxRegularHeapObjectSize;
   static_assert(kMaxRegularHeapObjectSize % kTaggedSize == 0);
   DCHECK(concrete_visitor()->IsMarked(object));
@@ -423,7 +428,7 @@ int MarkingVisitorBase<ConcreteVisitor>::VisitFixedArrayWithProgressBar(
 
 template <typename ConcreteVisitor>
 int MarkingVisitorBase<ConcreteVisitor>::VisitFixedArrayRegularly(
-    Map map, FixedArray object) {
+    Tagged<Map> map, Tagged<FixedArray> object) {
   int size = FixedArray::BodyDescriptor::SizeOf(map, object);
   concrete_visitor()
       ->template VisitMapPointerIfNeeded<VisitorId::kVisitFixedArray>(object);
@@ -433,8 +438,8 @@ int MarkingVisitorBase<ConcreteVisitor>::VisitFixedArrayRegularly(
 }
 
 template <typename ConcreteVisitor>
-int MarkingVisitorBase<ConcreteVisitor>::VisitFixedArray(Map map,
-                                                         FixedArray object) {
+int MarkingVisitorBase<ConcreteVisitor>::VisitFixedArray(
+    Tagged<Map> map, Tagged<FixedArray> object) {
   ProgressBar& progress_bar =
       MemoryChunk::FromHeapObject(object)->ProgressBar();
   return concrete_visitor()->CanUpdateValuesInHeap() && progress_bar.IsEnabled()
@@ -448,16 +453,17 @@ int MarkingVisitorBase<ConcreteVisitor>::VisitFixedArray(Map map,
 
 template <typename ConcreteVisitor>
 template <typename T>
-inline int MarkingVisitorBase<
-    ConcreteVisitor>::VisitEmbedderTracingSubClassNoEmbedderTracing(Map map,
-                                                                    T object) {
+inline int MarkingVisitorBase<ConcreteVisitor>::
+    VisitEmbedderTracingSubClassNoEmbedderTracing(Tagged<Map> map,
+                                                  Tagged<T> object) {
   return concrete_visitor()->VisitJSObjectSubclass(map, object);
 }
 
 template <typename ConcreteVisitor>
 template <typename T>
 inline int MarkingVisitorBase<ConcreteVisitor>::
-    VisitEmbedderTracingSubClassWithEmbedderTracing(Map map, T object) {
+    VisitEmbedderTracingSubClassWithEmbedderTracing(Tagged<Map> map,
+                                                    Tagged<T> object) {
   const bool requires_snapshot =
       local_marking_worklists_->SupportsExtractWrapper();
   MarkingWorklists::Local::WrapperSnapshot wrapper_snapshot;
@@ -474,8 +480,8 @@ inline int MarkingVisitorBase<ConcreteVisitor>::
 template <typename ConcreteVisitor>
 template <typename T>
 int MarkingVisitorBase<ConcreteVisitor>::VisitEmbedderTracingSubclass(
-    Map map, T object) {
-  DCHECK(object.MayHaveEmbedderFields());
+    Tagged<Map> map, Tagged<T> object) {
+  DCHECK(object->MayHaveEmbedderFields());
   if (V8_LIKELY(trace_embedder_fields_)) {
     return VisitEmbedderTracingSubClassWithEmbedderTracing(map, object);
   }
@@ -483,27 +489,27 @@ int MarkingVisitorBase<ConcreteVisitor>::VisitEmbedderTracingSubclass(
 }
 
 template <typename ConcreteVisitor>
-int MarkingVisitorBase<ConcreteVisitor>::VisitJSApiObject(Map map,
-                                                          JSObject object) {
+int MarkingVisitorBase<ConcreteVisitor>::VisitJSApiObject(
+    Tagged<Map> map, Tagged<JSObject> object) {
   return VisitEmbedderTracingSubclass(map, object);
 }
 
 template <typename ConcreteVisitor>
 int MarkingVisitorBase<ConcreteVisitor>::VisitJSArrayBuffer(
-    Map map, JSArrayBuffer object) {
+    Tagged<Map> map, Tagged<JSArrayBuffer> object) {
   object->MarkExtension();
   return VisitEmbedderTracingSubclass(map, object);
 }
 
 template <typename ConcreteVisitor>
 int MarkingVisitorBase<ConcreteVisitor>::VisitJSDataViewOrRabGsabDataView(
-    Map map, JSDataViewOrRabGsabDataView object) {
+    Tagged<Map> map, Tagged<JSDataViewOrRabGsabDataView> object) {
   return VisitEmbedderTracingSubclass(map, object);
 }
 
 template <typename ConcreteVisitor>
 int MarkingVisitorBase<ConcreteVisitor>::VisitJSTypedArray(
-    Map map, JSTypedArray object) {
+    Tagged<Map> map, Tagged<JSTypedArray> object) {
   return VisitEmbedderTracingSubclass(map, object);
 }
 
@@ -513,13 +519,13 @@ int MarkingVisitorBase<ConcreteVisitor>::VisitJSTypedArray(
 
 template <typename ConcreteVisitor>
 int MarkingVisitorBase<ConcreteVisitor>::VisitEphemeronHashTable(
-    Map map, EphemeronHashTable table) {
+    Tagged<Map> map, Tagged<EphemeronHashTable> table) {
   local_weak_objects_->ephemeron_hash_tables_local.Push(table);
 
   for (InternalIndex i : table->IterateEntries()) {
     ObjectSlot key_slot =
         table->RawFieldOfElementAt(EphemeronHashTable::EntryToIndex(i));
-    HeapObject key = HeapObject::cast(table->KeyAt(i));
+    Tagged<HeapObject> key = HeapObject::cast(table->KeyAt(i));
 
     SynchronizePageAccess(key);
     concrete_visitor()->RecordSlot(table, key_slot, key);
@@ -535,10 +541,10 @@ int MarkingVisitorBase<ConcreteVisitor>::VisitEphemeronHashTable(
     if (key.InReadOnlySpace() || concrete_visitor()->IsMarked(key)) {
       VisitPointer(table, value_slot);
     } else {
-      Object value_obj = table->ValueAt(i);
+      Tagged<Object> value_obj = table->ValueAt(i);
 
       if (IsHeapObject(value_obj)) {
-        HeapObject value = HeapObject::cast(value_obj);
+        Tagged<HeapObject> value = HeapObject::cast(value_obj);
         SynchronizePageAccess(value);
         concrete_visitor()->RecordSlot(table, value_slot, value);
         concrete_visitor()->AddWeakReferenceForReferenceSummarizer(table,
@@ -559,12 +565,12 @@ int MarkingVisitorBase<ConcreteVisitor>::VisitEphemeronHashTable(
 }
 
 template <typename ConcreteVisitor>
-int MarkingVisitorBase<ConcreteVisitor>::VisitJSWeakRef(Map map,
-                                                        JSWeakRef weak_ref) {
+int MarkingVisitorBase<ConcreteVisitor>::VisitJSWeakRef(
+    Tagged<Map> map, Tagged<JSWeakRef> weak_ref) {
   int size = concrete_visitor()->VisitJSObjectSubclass(map, weak_ref);
   if (size == 0) return 0;
   if (IsHeapObject(weak_ref->target())) {
-    HeapObject target = HeapObject::cast(weak_ref->target());
+    Tagged<HeapObject> target = HeapObject::cast(weak_ref->target());
     SynchronizePageAccess(target);
     if (target.InReadOnlySpace() || concrete_visitor()->IsMarked(target)) {
       // Record the slot inside the JSWeakRef, since the
@@ -583,13 +589,13 @@ int MarkingVisitorBase<ConcreteVisitor>::VisitJSWeakRef(Map map,
 }
 
 template <typename ConcreteVisitor>
-int MarkingVisitorBase<ConcreteVisitor>::VisitWeakCell(Map map,
-                                                       WeakCell weak_cell) {
+int MarkingVisitorBase<ConcreteVisitor>::VisitWeakCell(
+    Tagged<Map> map, Tagged<WeakCell> weak_cell) {
   int size = WeakCell::BodyDescriptor::SizeOf(map, weak_cell);
   this->VisitMapPointer(weak_cell);
   WeakCell::BodyDescriptor::IterateBody(map, weak_cell, size, this);
-  HeapObject target = weak_cell->relaxed_target();
-  HeapObject unregister_token = weak_cell->relaxed_unregister_token();
+  Tagged<HeapObject> target = weak_cell->relaxed_target();
+  Tagged<HeapObject> unregister_token = weak_cell->relaxed_unregister_token();
   SynchronizePageAccess(target);
   SynchronizePageAccess(unregister_token);
   if ((target.InReadOnlySpace() || concrete_visitor()->IsMarked(target)) &&
@@ -620,7 +626,7 @@ int MarkingVisitorBase<ConcreteVisitor>::VisitWeakCell(Map map,
 
 template <typename ConcreteVisitor>
 int MarkingVisitorBase<ConcreteVisitor>::VisitDescriptorArrayStrongly(
-    Map map, DescriptorArray array) {
+    Tagged<Map> map, Tagged<DescriptorArray> array) {
   this->VisitMapPointer(array);
   int size = DescriptorArray::BodyDescriptor::SizeOf(map, array);
   VisitPointers(array, array->GetFirstPointerSlot(),
@@ -633,7 +639,7 @@ int MarkingVisitorBase<ConcreteVisitor>::VisitDescriptorArrayStrongly(
 
 template <typename ConcreteVisitor>
 int MarkingVisitorBase<ConcreteVisitor>::VisitDescriptorArray(
-    Map map, DescriptorArray array) {
+    Tagged<Map> map, Tagged<DescriptorArray> array) {
   if (!concrete_visitor()->CanUpdateValuesInHeap()) {
     // If we cannot update the values in the heap, we just treat the array
     // strongly.
@@ -667,7 +673,8 @@ int MarkingVisitorBase<ConcreteVisitor>::VisitDescriptorArray(
 }
 
 template <typename ConcreteVisitor>
-void MarkingVisitorBase<ConcreteVisitor>::VisitDescriptorsForMap(Map map) {
+void MarkingVisitorBase<ConcreteVisitor>::VisitDescriptorsForMap(
+    Tagged<Map> map) {
   if (!concrete_visitor()->CanUpdateValuesInHeap() || !map->CanTransition())
     return;
 
@@ -678,7 +685,7 @@ void MarkingVisitorBase<ConcreteVisitor>::VisitDescriptorsForMap(Map map) {
   // non-empty descriptor array is marked, its header is also visited. The
   // slot holding the descriptor array will be implicitly recorded when the
   // pointer fields of this map are visited.
-  Object maybe_descriptors =
+  Tagged<Object> maybe_descriptors =
       TaggedField<Object, Map::kInstanceDescriptorsOffset>::Acquire_Load(
           heap_->isolate(), map);
 
@@ -689,7 +696,8 @@ void MarkingVisitorBase<ConcreteVisitor>::VisitDescriptorsForMap(Map map) {
     return;
   }
 
-  DescriptorArray descriptors = DescriptorArray::cast(maybe_descriptors);
+  Tagged<DescriptorArray> descriptors =
+      DescriptorArray::cast(maybe_descriptors);
   // Synchronize reading of page flags for tsan.
   SynchronizePageAccess(descriptors);
   // Normal processing of descriptor arrays through the pointers iteration that
@@ -719,7 +727,8 @@ void MarkingVisitorBase<ConcreteVisitor>::VisitDescriptorsForMap(Map map) {
 }
 
 template <typename ConcreteVisitor>
-int MarkingVisitorBase<ConcreteVisitor>::VisitMap(Map meta_map, Map map) {
+int MarkingVisitorBase<ConcreteVisitor>::VisitMap(Tagged<Map> meta_map,
+                                                  Tagged<Map> map) {
   int size = Map::BodyDescriptor::SizeOf(meta_map, map);
   VisitDescriptorsForMap(map);
 
@@ -732,7 +741,7 @@ int MarkingVisitorBase<ConcreteVisitor>::VisitMap(Map meta_map, Map map) {
 
 template <typename ConcreteVisitor>
 int MarkingVisitorBase<ConcreteVisitor>::VisitTransitionArray(
-    Map map, TransitionArray array) {
+    Tagged<Map> map, Tagged<TransitionArray> array) {
   this->VisitMapPointer(array);
   int size = TransitionArray::BodyDescriptor::SizeOf(map, array);
   TransitionArray::BodyDescriptor::IterateBody(map, array, size, this);
diff --git a/src/heap/marking-visitor.h b/src/heap/marking-visitor.h
index 7f19eb167a8..7cead5f2830 100644
--- a/src/heap/marking-visitor.h
+++ b/src/heap/marking-visitor.h
@@ -76,65 +76,75 @@ class MarkingVisitorBase : public ConcurrentHeapVisitor<int, ConcreteVisitor> {
   {
   }
 
-  V8_INLINE int VisitBytecodeArray(Map map, BytecodeArray object);
-  V8_INLINE int VisitDescriptorArrayStrongly(Map map, DescriptorArray object);
-  V8_INLINE int VisitDescriptorArray(Map map, DescriptorArray object);
-  V8_INLINE int VisitEphemeronHashTable(Map map, EphemeronHashTable object);
-  V8_INLINE int VisitFixedArray(Map map, FixedArray object);
-  V8_INLINE int VisitJSApiObject(Map map, JSObject object);
-  V8_INLINE int VisitJSArrayBuffer(Map map, JSArrayBuffer object);
+  V8_INLINE int VisitBytecodeArray(Tagged<Map> map,
+                                   Tagged<BytecodeArray> object);
+  V8_INLINE int VisitDescriptorArrayStrongly(Tagged<Map> map,
+                                             Tagged<DescriptorArray> object);
+  V8_INLINE int VisitDescriptorArray(Tagged<Map> map,
+                                     Tagged<DescriptorArray> object);
+  V8_INLINE int VisitEphemeronHashTable(Tagged<Map> map,
+                                        Tagged<EphemeronHashTable> object);
+  V8_INLINE int VisitFixedArray(Tagged<Map> map, Tagged<FixedArray> object);
+  V8_INLINE int VisitJSApiObject(Tagged<Map> map, Tagged<JSObject> object);
+  V8_INLINE int VisitJSArrayBuffer(Tagged<Map> map,
+                                   Tagged<JSArrayBuffer> object);
   V8_INLINE int VisitJSDataViewOrRabGsabDataView(
-      Map map, JSDataViewOrRabGsabDataView object);
-  V8_INLINE int VisitJSFunction(Map map, JSFunction object);
-  V8_INLINE int VisitJSTypedArray(Map map, JSTypedArray object);
-  V8_INLINE int VisitJSWeakRef(Map map, JSWeakRef object);
-  V8_INLINE int VisitMap(Map map, Map object);
-  V8_INLINE int VisitSharedFunctionInfo(Map map, SharedFunctionInfo object);
-  V8_INLINE int VisitTransitionArray(Map map, TransitionArray object);
-  V8_INLINE int VisitWeakCell(Map map, WeakCell object);
+      Tagged<Map> map, Tagged<JSDataViewOrRabGsabDataView> object);
+  V8_INLINE int VisitJSFunction(Tagged<Map> map, Tagged<JSFunction> object);
+  V8_INLINE int VisitJSTypedArray(Tagged<Map> map, Tagged<JSTypedArray> object);
+  V8_INLINE int VisitJSWeakRef(Tagged<Map> map, Tagged<JSWeakRef> object);
+  V8_INLINE int VisitMap(Tagged<Map> map, Tagged<Map> object);
+  V8_INLINE int VisitSharedFunctionInfo(Tagged<Map> map,
+                                        Tagged<SharedFunctionInfo> object);
+  V8_INLINE int VisitTransitionArray(Tagged<Map> map,
+                                     Tagged<TransitionArray> object);
+  V8_INLINE int VisitWeakCell(Tagged<Map> map, Tagged<WeakCell> object);
 
   // ObjectVisitor overrides.
-  void VisitMapPointer(HeapObject host) final {
-    Map map = host.map(ObjectVisitorWithCageBases::cage_base());
+  void VisitMapPointer(Tagged<HeapObject> host) final {
+    Tagged<Map> map = host->map(ObjectVisitorWithCageBases::cage_base());
     ProcessStrongHeapObject(host, host->map_slot(), map);
   }
-  V8_INLINE void VisitPointer(HeapObject host, ObjectSlot p) final {
+  V8_INLINE void VisitPointer(Tagged<HeapObject> host, ObjectSlot p) final {
     VisitPointersImpl(host, p, p + 1);
   }
-  V8_INLINE void VisitPointer(HeapObject host, MaybeObjectSlot p) final {
+  V8_INLINE void VisitPointer(Tagged<HeapObject> host,
+                              MaybeObjectSlot p) final {
     VisitPointersImpl(host, p, p + 1);
   }
-  V8_INLINE void VisitPointers(HeapObject host, ObjectSlot start,
+  V8_INLINE void VisitPointers(Tagged<HeapObject> host, ObjectSlot start,
                                ObjectSlot end) final {
     VisitPointersImpl(host, start, end);
   }
-  V8_INLINE void VisitPointers(HeapObject host, MaybeObjectSlot start,
+  V8_INLINE void VisitPointers(Tagged<HeapObject> host, MaybeObjectSlot start,
                                MaybeObjectSlot end) final {
     VisitPointersImpl(host, start, end);
   }
   V8_INLINE void VisitInstructionStreamPointer(
-      Code host, InstructionStreamSlot slot) final {
+      Tagged<Code> host, InstructionStreamSlot slot) final {
     VisitInstructionStreamPointerImpl(host, slot);
   }
-  V8_INLINE void VisitEmbeddedPointer(InstructionStream host,
+  V8_INLINE void VisitEmbeddedPointer(Tagged<InstructionStream> host,
                                       RelocInfo* rinfo) final;
-  V8_INLINE void VisitCodeTarget(InstructionStream host,
+  V8_INLINE void VisitCodeTarget(Tagged<InstructionStream> host,
                                  RelocInfo* rinfo) final;
-  void VisitCustomWeakPointers(HeapObject host, ObjectSlot start,
+  void VisitCustomWeakPointers(Tagged<HeapObject> host, ObjectSlot start,
                                ObjectSlot end) final {
     // Weak list pointers should be ignored during marking. The lists are
     // reconstructed after GC.
   }
 
-  V8_INLINE void VisitExternalPointer(HeapObject host, ExternalPointerSlot slot,
+  V8_INLINE void VisitExternalPointer(Tagged<HeapObject> host,
+                                      ExternalPointerSlot slot,
                                       ExternalPointerTag tag) final;
-  V8_INLINE void VisitIndirectPointer(HeapObject host, IndirectPointerSlot slot,
+  V8_INLINE void VisitIndirectPointer(Tagged<HeapObject> host,
+                                      IndirectPointerSlot slot,
                                       IndirectPointerMode mode) final;
 
-  void VisitIndirectPointerTableEntry(HeapObject host,
+  void VisitIndirectPointerTableEntry(Tagged<HeapObject> host,
                                       IndirectPointerSlot slot) final;
 
-  void SynchronizePageAccess(HeapObject heap_object) {
+  void SynchronizePageAccess(Tagged<HeapObject> heap_object) {
 #ifdef THREAD_SANITIZER
     // This is needed because TSAN does not process the memory fence
     // emitted after page initialization.
@@ -142,21 +152,21 @@ class MarkingVisitorBase : public ConcurrentHeapVisitor<int, ConcreteVisitor> {
 #endif
   }
 
-  bool ShouldMarkObject(HeapObject object) const {
+  bool ShouldMarkObject(Tagged<HeapObject> object) const {
     if (object.InReadOnlySpace()) return false;
     if (should_mark_shared_heap_) return true;
     return !object.InAnySharedSpace();
   }
 
   // Marks the object grey and pushes it on the marking work list.
-  V8_INLINE void MarkObject(HeapObject host, HeapObject obj);
+  V8_INLINE void MarkObject(Tagged<HeapObject> host, Tagged<HeapObject> obj);
 
   V8_INLINE static constexpr bool ShouldVisitReadOnlyMapPointer() {
     return false;
   }
 
   // Convenience method.
-  bool IsUnmarked(HeapObject obj) const {
+  bool IsUnmarked(Tagged<HeapObject> obj) const {
     return !concrete_visitor()->IsMarked(obj);
   }
 
@@ -164,43 +174,48 @@ class MarkingVisitorBase : public ConcurrentHeapVisitor<int, ConcreteVisitor> {
   using ConcurrentHeapVisitor<int, ConcreteVisitor>::concrete_visitor;
 
   template <typename THeapObjectSlot>
-  void ProcessStrongHeapObject(HeapObject host, THeapObjectSlot slot,
-                               HeapObject heap_object);
+  void ProcessStrongHeapObject(Tagged<HeapObject> host, THeapObjectSlot slot,
+                               Tagged<HeapObject> heap_object);
   template <typename THeapObjectSlot>
-  void ProcessWeakHeapObject(HeapObject host, THeapObjectSlot slot,
-                             HeapObject heap_object);
+  void ProcessWeakHeapObject(Tagged<HeapObject> host, THeapObjectSlot slot,
+                             Tagged<HeapObject> heap_object);
 
   template <typename TSlot>
-  V8_INLINE void VisitPointerImpl(HeapObject host, TSlot p);
+  V8_INLINE void VisitPointerImpl(Tagged<HeapObject> host, TSlot p);
 
   template <typename TSlot>
-  V8_INLINE void VisitPointersImpl(HeapObject host, TSlot start, TSlot end);
+  V8_INLINE void VisitPointersImpl(Tagged<HeapObject> host, TSlot start,
+                                   TSlot end);
 
   // Similar to VisitPointersImpl() but using code cage base for loading from
   // the slot.
-  V8_INLINE void VisitInstructionStreamPointerImpl(Code host,
+  V8_INLINE void VisitInstructionStreamPointerImpl(Tagged<Code> host,
                                                    InstructionStreamSlot slot);
 
-  V8_INLINE void VisitDescriptorsForMap(Map map);
+  V8_INLINE void VisitDescriptorsForMap(Tagged<Map> map);
 
   template <typename T>
-  int VisitEmbedderTracingSubclass(Map map, T object);
+  int VisitEmbedderTracingSubclass(Tagged<Map> map, Tagged<T> object);
   template <typename T>
-  int VisitEmbedderTracingSubClassWithEmbedderTracing(Map map, T object);
+  int VisitEmbedderTracingSubClassWithEmbedderTracing(Tagged<Map> map,
+                                                      Tagged<T> object);
   template <typename T>
-  int VisitEmbedderTracingSubClassNoEmbedderTracing(Map map, T object);
+  int VisitEmbedderTracingSubClassNoEmbedderTracing(Tagged<Map> map,
+                                                    Tagged<T> object);
 
-  V8_INLINE int VisitFixedArrayWithProgressBar(Map map, FixedArray object,
+  V8_INLINE int VisitFixedArrayWithProgressBar(Tagged<Map> map,
+                                               Tagged<FixedArray> object,
                                                ProgressBar& progress_bar);
-  V8_INLINE int VisitFixedArrayRegularly(Map map, FixedArray object);
+  V8_INLINE int VisitFixedArrayRegularly(Tagged<Map> map,
+                                         Tagged<FixedArray> object);
 
   // Methods needed for supporting code flushing.
-  bool ShouldFlushCode(SharedFunctionInfo sfi) const;
-  bool ShouldFlushBaselineCode(JSFunction js_function) const;
+  bool ShouldFlushCode(Tagged<SharedFunctionInfo> sfi) const;
+  bool ShouldFlushBaselineCode(Tagged<JSFunction> js_function) const;
 
-  bool HasBytecodeArrayForFlushing(SharedFunctionInfo sfi) const;
-  bool IsOld(SharedFunctionInfo sfi) const;
-  void MakeOlder(SharedFunctionInfo sfi) const;
+  bool HasBytecodeArrayForFlushing(Tagged<SharedFunctionInfo> sfi) const;
+  bool IsOld(Tagged<SharedFunctionInfo> sfi) const;
+  void MakeOlder(Tagged<SharedFunctionInfo> sfi) const;
 
   MarkingWorklists::Local* const local_marking_worklists_;
   WeakObjects::Local* const local_weak_objects_;
@@ -237,18 +252,18 @@ class FullMarkingVisitorBase : public MarkingVisitorBase<ConcreteVisitor> {
             mark_compact_epoch, code_flush_mode, trace_embedder_fields,
             should_keep_ages_unchanged, code_flushing_increase) {}
 
-  V8_INLINE void AddStrongReferenceForReferenceSummarizer(HeapObject host,
-                                                          HeapObject obj) {}
+  V8_INLINE void AddStrongReferenceForReferenceSummarizer(
+      Tagged<HeapObject> host, Tagged<HeapObject> obj) {}
 
-  V8_INLINE void AddWeakReferenceForReferenceSummarizer(HeapObject host,
-                                                        HeapObject obj) {}
+  V8_INLINE void AddWeakReferenceForReferenceSummarizer(
+      Tagged<HeapObject> host, Tagged<HeapObject> obj) {}
 
   constexpr bool CanUpdateValuesInHeap() { return true; }
 
-  bool TryMark(HeapObject obj) {
+  bool TryMark(Tagged<HeapObject> obj) {
     return MarkBit::From(obj).Set<AccessMode::ATOMIC>();
   }
-  bool IsMarked(HeapObject obj) const {
+  bool IsMarked(Tagged<HeapObject> obj) const {
     return MarkBit::From(obj).Get<AccessMode::ATOMIC>();
   }
 };
diff --git a/src/heap/marking-worklist-inl.h b/src/heap/marking-worklist-inl.h
index b34f83ec500..b8093d6d371 100644
--- a/src/heap/marking-worklist-inl.h
+++ b/src/heap/marking-worklist-inl.h
@@ -24,9 +24,11 @@ void MarkingWorklists::Update(Callback callback) {
   }
 }
 
-void MarkingWorklists::Local::Push(HeapObject object) { active_->Push(object); }
+void MarkingWorklists::Local::Push(Tagged<HeapObject> object) {
+  active_->Push(object);
+}
 
-bool MarkingWorklists::Local::Pop(HeapObject* object) {
+bool MarkingWorklists::Local::Pop(Tagged<HeapObject>* object) {
   if (active_->Pop(object)) return true;
   if (!is_per_context_mode_) return false;
   // The active worklist is empty. Find any other non-empty worklist and
@@ -34,11 +36,11 @@ bool MarkingWorklists::Local::Pop(HeapObject* object) {
   return PopContext(object);
 }
 
-void MarkingWorklists::Local::PushOnHold(HeapObject object) {
+void MarkingWorklists::Local::PushOnHold(Tagged<HeapObject> object) {
   on_hold_.Push(object);
 }
 
-bool MarkingWorklists::Local::PopOnHold(HeapObject* object) {
+bool MarkingWorklists::Local::PopOnHold(Tagged<HeapObject>* object) {
   return on_hold_.Pop(object);
 }
 
@@ -46,7 +48,8 @@ bool MarkingWorklists::Local::SupportsExtractWrapper() {
   return cpp_marking_state_.get();
 }
 
-bool MarkingWorklists::Local::ExtractWrapper(Map map, JSObject object,
+bool MarkingWorklists::Local::ExtractWrapper(Tagged<Map> map,
+                                             Tagged<JSObject> object,
                                              WrapperSnapshot& snapshot) {
   DCHECK_NOT_NULL(cpp_marking_state_);
   return cpp_marking_state_->ExtractEmbedderDataSnapshot(map, object, snapshot);
diff --git a/src/heap/marking-worklist.cc b/src/heap/marking-worklist.cc
index 519884ca019..aa0208e7f0a 100644
--- a/src/heap/marking-worklist.cc
+++ b/src/heap/marking-worklist.cc
@@ -55,7 +55,7 @@ void MarkingWorklists::PrintWorklist(const char* worklist_name,
 #ifdef DEBUG
   std::map<InstanceType, int> count;
   int total_count = 0;
-  worklist->Iterate([&count, &total_count](HeapObject obj) {
+  worklist->Iterate([&count, &total_count](Tagged<HeapObject> obj) {
     ++total_count;
     count[obj->map()->instance_type()]++;
   });
@@ -171,7 +171,7 @@ void MarkingWorklists::Local::PublishWork() {
 
 void MarkingWorklists::Local::MergeOnHold() { shared_.Merge(on_hold_); }
 
-bool MarkingWorklists::Local::PopContext(HeapObject* object) {
+bool MarkingWorklists::Local::PopContext(Tagged<HeapObject>* object) {
   DCHECK(is_per_context_mode_);
   // As an optimization we first check only the local segments to avoid locks.
   for (auto& cw : worklist_by_context_) {
diff --git a/src/heap/marking-worklist.h b/src/heap/marking-worklist.h
index e0711cf03a2..d550de2e4fe 100644
--- a/src/heap/marking-worklist.h
+++ b/src/heap/marking-worklist.h
@@ -24,7 +24,7 @@ class JSObject;
 // The index of the main thread task used by concurrent/parallel GC.
 const int kMainThreadTask = 0;
 
-using MarkingWorklist = ::heap::base::Worklist<HeapObject, 64>;
+using MarkingWorklist = ::heap::base::Worklist<Tagged<HeapObject>, 64>;
 
 // We piggyback on marking to compute object sizes per native context that is
 // needed for the new memory measurement API. The algorithm works as follows:
@@ -149,14 +149,14 @@ class V8_EXPORT_PRIVATE MarkingWorklists::Local final {
   // Local worklists implicitly check for emptiness on destruction.
   ~Local() = default;
 
-  inline void Push(HeapObject object);
-  inline bool Pop(HeapObject* object);
+  inline void Push(Tagged<HeapObject> object);
+  inline bool Pop(Tagged<HeapObject>* object);
 
-  inline void PushOnHold(HeapObject object);
-  inline bool PopOnHold(HeapObject* object);
+  inline void PushOnHold(Tagged<HeapObject> object);
+  inline bool PopOnHold(Tagged<HeapObject>* object);
 
   using WrapperSnapshot = CppMarkingState::EmbedderDataSnapshot;
-  inline bool ExtractWrapper(Map map, JSObject object,
+  inline bool ExtractWrapper(Tagged<Map> map, Tagged<JSObject> object,
                              WrapperSnapshot& snapshot);
   inline void PushExtractedWrapper(const WrapperSnapshot& snapshot);
   inline bool SupportsExtractWrapper();
@@ -193,7 +193,7 @@ class V8_EXPORT_PRIVATE MarkingWorklists::Local final {
   inline void SwitchToContextImpl(Address context,
                                   MarkingWorklist::Local* worklist);
 
-  bool PopContext(HeapObject* object);
+  bool PopContext(Tagged<HeapObject>* object);
   Address SwitchToContextSlow(Address context);
 
   // Points to either `shared_`, `other_` or to a per-context worklist.
diff --git a/src/heap/marking.cc b/src/heap/marking.cc
index b208eea5854..87a566bc095 100644
--- a/src/heap/marking.cc
+++ b/src/heap/marking.cc
@@ -142,7 +142,7 @@ MarkBit MarkBit::FromForTesting(Address address) {
 }
 
 // static
-MarkBit MarkBit::FromForTesting(HeapObject heap_object) {
+MarkBit MarkBit::FromForTesting(Tagged<HeapObject> heap_object) {
   return MarkingBitmap::MarkBitFromAddress(heap_object.ptr());
 }
 
diff --git a/src/heap/marking.h b/src/heap/marking.h
index 92aa6148155..e3c66972777 100644
--- a/src/heap/marking.h
+++ b/src/heap/marking.h
@@ -23,12 +23,12 @@ class MarkBit final {
   static_assert(sizeof(CellType) == sizeof(base::AtomicWord));
 
   V8_ALLOW_UNUSED static inline MarkBit From(Address);
-  V8_ALLOW_UNUSED static inline MarkBit From(HeapObject);
+  V8_ALLOW_UNUSED static inline MarkBit From(Tagged<HeapObject>);
 
   // These methods are meant to be used from the debugger and therefore
   // intentionally not inlined such that they are always available.
   V8_ALLOW_UNUSED static MarkBit FromForTesting(Address);
-  V8_ALLOW_UNUSED static MarkBit FromForTesting(HeapObject);
+  V8_ALLOW_UNUSED static MarkBit FromForTesting(Tagged<HeapObject>);
 
   // The function returns true if it succeeded to
   // transition the bit from 0 to 1.
diff --git a/src/heap/memory-chunk.h b/src/heap/memory-chunk.h
index 788b83cd478..ea57a08ddd0 100644
--- a/src/heap/memory-chunk.h
+++ b/src/heap/memory-chunk.h
@@ -60,7 +60,7 @@ class MemoryChunk : public BasicMemoryChunk {
   }
 
   // Only works if the object is in the first kPageSize of the MemoryChunk.
-  static MemoryChunk* FromHeapObject(HeapObject o) {
+  static MemoryChunk* FromHeapObject(Tagged<HeapObject> o) {
     return cast(BasicMemoryChunk::FromHeapObject(o));
   }
 
diff --git a/src/heap/memory-measurement-inl.h b/src/heap/memory-measurement-inl.h
index a37fe1c8a16..0810ab40b57 100644
--- a/src/heap/memory-measurement-inl.h
+++ b/src/heap/memory-measurement-inl.h
@@ -16,7 +16,8 @@
 namespace v8 {
 namespace internal {
 
-bool NativeContextInferrer::Infer(Isolate* isolate, Map map, HeapObject object,
+bool NativeContextInferrer::Infer(Isolate* isolate, Tagged<Map> map,
+                                  Tagged<HeapObject> object,
                                   Address* native_context) {
   switch (map->visitor_id()) {
     case kVisitContext:
@@ -41,14 +42,15 @@ bool NativeContextInferrer::Infer(Isolate* isolate, Map map, HeapObject object,
   }
 }
 
-V8_INLINE bool NativeContextStats::HasExternalBytes(Map map) {
+V8_INLINE bool NativeContextStats::HasExternalBytes(Tagged<Map> map) {
   InstanceType instance_type = map->instance_type();
   return (instance_type == JS_ARRAY_BUFFER_TYPE ||
           InstanceTypeChecker::IsExternalString(instance_type));
 }
 
-V8_INLINE void NativeContextStats::IncrementSize(Address context, Map map,
-                                                 HeapObject object,
+V8_INLINE void NativeContextStats::IncrementSize(Address context,
+                                                 Tagged<Map> map,
+                                                 Tagged<HeapObject> object,
                                                  size_t size) {
   size_by_context_[context] += size;
   if (HasExternalBytes(map)) {
diff --git a/src/heap/memory-measurement.cc b/src/heap/memory-measurement.cc
index 02f75dd1e68..5fcec4bc783 100644
--- a/src/heap/memory-measurement.cc
+++ b/src/heap/memory-measurement.cc
@@ -220,7 +220,7 @@ std::vector<Address> MemoryMeasurement::StartProcessing() {
   for (const auto& request : processing_) {
     Handle<WeakFixedArray> contexts = request.contexts;
     for (int i = 0; i < contexts->length(); i++) {
-      HeapObject context;
+      Tagged<HeapObject> context;
       if (contexts->Get(i).GetHeapObject(&context)) {
         unique_contexts.insert(context.ptr());
       }
@@ -243,7 +243,7 @@ void MemoryMeasurement::FinishProcessing(const NativeContextStats& stats) {
     Request request = std::move(processing_.front());
     processing_.pop_front();
     for (int i = 0; i < static_cast<int>(request.sizes.size()); i++) {
-      HeapObject context;
+      Tagged<HeapObject> context;
       if (!request.contexts->Get(i).GetHeapObject(&context)) {
         continue;
       }
@@ -340,7 +340,7 @@ void MemoryMeasurement::ReportResults() {
     DCHECK_EQ(request.sizes.size(),
               static_cast<size_t>(request.contexts->length()));
     for (int i = 0; i < request.contexts->length(); i++) {
-      HeapObject raw_context;
+      Tagged<HeapObject> raw_context;
       if (!request.contexts->Get(i).GetHeapObject(&raw_context)) {
         continue;
       }
@@ -366,11 +366,12 @@ std::unique_ptr<v8::MeasureMemoryDelegate> MemoryMeasurement::DefaultDelegate(
                                                  mode);
 }
 
-bool NativeContextInferrer::InferForContext(Isolate* isolate, Context context,
+bool NativeContextInferrer::InferForContext(Isolate* isolate,
+                                            Tagged<Context> context,
                                             Address* native_context) {
   PtrComprCageBase cage_base(isolate);
-  Map context_map = context->map(cage_base, kAcquireLoad);
-  Object maybe_native_context =
+  Tagged<Map> context_map = context->map(cage_base, kAcquireLoad);
+  Tagged<Object> maybe_native_context =
       TaggedField<Object, Map::kConstructorOrBackPointerOrNativeContextOffset>::
           Acquire_Load(cage_base, context_map);
   if (IsNativeContext(maybe_native_context, cage_base)) {
@@ -381,9 +382,9 @@ bool NativeContextInferrer::InferForContext(Isolate* isolate, Context context,
 }
 
 bool NativeContextInferrer::InferForJSFunction(Isolate* isolate,
-                                               JSFunction function,
+                                               Tagged<JSFunction> function,
                                                Address* native_context) {
-  Object maybe_context =
+  Tagged<Object> maybe_context =
       TaggedField<Object, JSFunction::kContextOffset>::Acquire_Load(isolate,
                                                                     function);
   // The context may be a smi during deserialization.
@@ -398,11 +399,11 @@ bool NativeContextInferrer::InferForJSFunction(Isolate* isolate,
   return InferForContext(isolate, Context::cast(maybe_context), native_context);
 }
 
-bool NativeContextInferrer::InferForJSObject(Isolate* isolate, Map map,
-                                             JSObject object,
+bool NativeContextInferrer::InferForJSObject(Isolate* isolate, Tagged<Map> map,
+                                             Tagged<JSObject> object,
                                              Address* native_context) {
   if (map->instance_type() == JS_GLOBAL_OBJECT_TYPE) {
-    Object maybe_context =
+    Tagged<Object> maybe_context =
         JSGlobalObject::cast(object)->native_context_unchecked(isolate);
     if (IsNativeContext(maybe_context)) {
       *native_context = maybe_context.ptr();
@@ -411,7 +412,7 @@ bool NativeContextInferrer::InferForJSObject(Isolate* isolate, Map map,
   }
   // The maximum number of steps to perform when looking for the context.
   const int kMaxSteps = 3;
-  Object maybe_constructor = map->TryGetConstructor(isolate, kMaxSteps);
+  Tagged<Object> maybe_constructor = map->TryGetConstructor(isolate, kMaxSteps);
   if (IsJSFunction(maybe_constructor)) {
     return InferForJSFunction(isolate, JSFunction::cast(maybe_constructor),
                               native_context);
@@ -427,8 +428,8 @@ void NativeContextStats::Merge(const NativeContextStats& other) {
   }
 }
 
-void NativeContextStats::IncrementExternalSize(Address context, Map map,
-                                               HeapObject object) {
+void NativeContextStats::IncrementExternalSize(Address context, Tagged<Map> map,
+                                               Tagged<HeapObject> object) {
   InstanceType instance_type = map->instance_type();
   size_t external_size = 0;
   if (instance_type == JS_ARRAY_BUFFER_TYPE) {
diff --git a/src/heap/memory-measurement.h b/src/heap/memory-measurement.h
index 10b97974a42..9a01fafe9b5 100644
--- a/src/heap/memory-measurement.h
+++ b/src/heap/memory-measurement.h
@@ -73,23 +73,23 @@ class V8_EXPORT_PRIVATE NativeContextInferrer {
   // It should be initialized to the context that will be used for the object
   // if the inference is not successful. The function performs more work if the
   // context is the shared context.
-  V8_INLINE bool Infer(Isolate* isolate, Map map, HeapObject object,
-                       Address* native_context);
+  V8_INLINE bool Infer(Isolate* isolate, Tagged<Map> map,
+                       Tagged<HeapObject> object, Address* native_context);
 
  private:
-  bool InferForContext(Isolate* isolate, Context context,
+  bool InferForContext(Isolate* isolate, Tagged<Context> context,
                        Address* native_context);
-  bool InferForJSFunction(Isolate* isolate, JSFunction function,
+  bool InferForJSFunction(Isolate* isolate, Tagged<JSFunction> function,
                           Address* native_context);
-  bool InferForJSObject(Isolate* isolate, Map map, JSObject object,
-                        Address* native_context);
+  bool InferForJSObject(Isolate* isolate, Tagged<Map> map,
+                        Tagged<JSObject> object, Address* native_context);
 };
 
 // Maintains mapping from native contexts to their sizes.
 class V8_EXPORT_PRIVATE NativeContextStats {
  public:
-  V8_INLINE void IncrementSize(Address context, Map map, HeapObject object,
-                               size_t size);
+  V8_INLINE void IncrementSize(Address context, Tagged<Map> map,
+                               Tagged<HeapObject> object, size_t size);
 
   size_t Get(Address context) const {
     const auto it = size_by_context_.find(context);
@@ -102,8 +102,9 @@ class V8_EXPORT_PRIVATE NativeContextStats {
   bool Empty() const { return size_by_context_.empty(); }
 
  private:
-  V8_INLINE bool HasExternalBytes(Map map);
-  void IncrementExternalSize(Address context, Map map, HeapObject object);
+  V8_INLINE bool HasExternalBytes(Tagged<Map> map);
+  void IncrementExternalSize(Address context, Tagged<Map> map,
+                             Tagged<HeapObject> object);
   std::unordered_map<Address, size_t> size_by_context_;
 };
 
diff --git a/src/heap/minor-mark-sweep-inl.h b/src/heap/minor-mark-sweep-inl.h
index d5b2fa58133..e8d71c1d02b 100644
--- a/src/heap/minor-mark-sweep-inl.h
+++ b/src/heap/minor-mark-sweep-inl.h
@@ -93,7 +93,7 @@ void YoungGenerationRememberedSetsMarkingWorklist::MarkingItem::Process(
 void YoungGenerationRememberedSetsMarkingWorklist::MarkingItem::
     CheckOldToNewSlotForSharedUntyped(MemoryChunk* chunk, Address slot_address,
                                       MaybeObject object) {
-  HeapObject heap_object;
+  Tagged<HeapObject> heap_object;
   if (!object.GetHeapObject(&heap_object)) return;
 #ifdef THREAD_SANITIZER
   BasicMemoryChunk::FromHeapObject(heap_object)->SynchronizedHeapLoad();
@@ -108,7 +108,7 @@ void YoungGenerationRememberedSetsMarkingWorklist::MarkingItem::
     CheckOldToNewSlotForSharedTyped(MemoryChunk* chunk, SlotType slot_type,
                                     Address slot_address,
                                     MaybeObject new_target) {
-  HeapObject heap_object;
+  Tagged<HeapObject> heap_object;
   if (!new_target.GetHeapObject(&heap_object)) return;
 #ifdef THREAD_SANITIZER
   BasicMemoryChunk::FromHeapObject(heap_object)->SynchronizedHeapLoad();
diff --git a/src/heap/minor-mark-sweep.cc b/src/heap/minor-mark-sweep.cc
index 165c1db4661..0060edf396b 100644
--- a/src/heap/minor-mark-sweep.cc
+++ b/src/heap/minor-mark-sweep.cc
@@ -73,7 +73,7 @@ class YoungGenerationMarkingVerifier : public MarkingVerifierBase {
     return chunk->marking_bitmap();
   }
 
-  bool IsMarked(HeapObject object) override {
+  bool IsMarked(Tagged<HeapObject> object) override {
     return marking_state_->IsMarked(object);
   }
 
@@ -87,7 +87,7 @@ class YoungGenerationMarkingVerifier : public MarkingVerifierBase {
   }
 
  protected:
-  void VerifyMap(Map map) override { VerifyHeapObjectImpl(map); }
+  void VerifyMap(Tagged<Map> map) override { VerifyHeapObjectImpl(map); }
 
   void VerifyPointers(ObjectSlot start, ObjectSlot end) override {
     VerifyPointersImpl(start, end);
@@ -103,12 +103,14 @@ class YoungGenerationMarkingVerifier : public MarkingVerifierBase {
     UNREACHABLE();
   }
 
-  void VisitCodeTarget(InstructionStream host, RelocInfo* rinfo) override {
-    InstructionStream target =
+  void VisitCodeTarget(Tagged<InstructionStream> host,
+                       RelocInfo* rinfo) override {
+    Tagged<InstructionStream> target =
         InstructionStream::FromTargetAddress(rinfo->target_address());
     VerifyHeapObjectImpl(target);
   }
-  void VisitEmbeddedPointer(InstructionStream host, RelocInfo* rinfo) override {
+  void VisitEmbeddedPointer(Tagged<InstructionStream> host,
+                            RelocInfo* rinfo) override {
     VerifyHeapObjectImpl(rinfo->target_object(cage_base()));
   }
   void VerifyRootPointers(FullObjectSlot start, FullObjectSlot end) override {
@@ -116,7 +118,7 @@ class YoungGenerationMarkingVerifier : public MarkingVerifierBase {
   }
 
  private:
-  V8_INLINE void VerifyHeapObjectImpl(HeapObject heap_object) {
+  V8_INLINE void VerifyHeapObjectImpl(Tagged<HeapObject> heap_object) {
     CHECK_IMPLIES(Heap::InYoungGeneration(heap_object), IsMarked(heap_object));
   }
 
@@ -126,7 +128,7 @@ class YoungGenerationMarkingVerifier : public MarkingVerifierBase {
         GetPtrComprCageBaseFromOnHeapAddress(start.address());
     for (TSlot slot = start; slot < end; ++slot) {
       typename TSlot::TObject object = slot.load(cage_base);
-      HeapObject heap_object;
+      Tagged<HeapObject> heap_object;
       // Minor MS treats weak references as strong.
       if (object.GetHeapObject(&heap_object)) {
         VerifyHeapObjectImpl(heap_object);
@@ -409,12 +411,12 @@ class YoungStringForwardingTableCleaner final
 
  private:
   void ClearNonLiveYoungObjects(StringForwardingTable::Record* record) {
-    Object original = record->OriginalStringObject(isolate_);
+    Tagged<Object> original = record->OriginalStringObject(isolate_);
     if (!IsHeapObject(original)) {
       DCHECK_EQ(original, StringForwardingTable::deleted_element());
       return;
     }
-    String original_string = String::cast(original);
+    Tagged<String> original_string = String::cast(original);
     if (!Heap::InYoungGeneration(original_string)) return;
     if (!marking_state_->IsMarked(original_string)) {
       DisposeExternalResource(record);
@@ -480,13 +482,13 @@ void MinorMarkSweepCollector::ClearNonLiveReferences() {
     DCHECK_NOT_NULL(ephemeron_table_list_);
     EphemeronRememberedSet::TableList::Local local_ephemeron_table_list(
         *ephemeron_table_list_);
-    EphemeronHashTable table;
+    Tagged<EphemeronHashTable> table;
     while (local_ephemeron_table_list.Pop(&table)) {
       for (InternalIndex i : table->IterateEntries()) {
         // Keys in EphemeronHashTables must be heap objects.
         HeapObjectSlot key_slot(
             table->RawFieldOfElementAt(EphemeronHashTable::EntryToIndex(i)));
-        HeapObject key = key_slot.ToHeapObject();
+        Tagged<HeapObject> key = key_slot.ToHeapObject();
         if (Heap::InYoungGeneration(key) &&
             non_atomic_marking_state_->IsUnmarked(key)) {
           table->RemoveEntry(i);
@@ -504,13 +506,13 @@ void MinorMarkSweepCollector::ClearNonLiveReferences() {
   // the sweeper during promoted pages iteration.
   auto* table_map = heap_->ephemeron_remembered_set()->tables();
   for (auto it = table_map->begin(); it != table_map->end();) {
-    EphemeronHashTable table = it->first;
+    Tagged<EphemeronHashTable> table = it->first;
     auto& indices = it->second;
     for (auto iti = indices.begin(); iti != indices.end();) {
       // Keys in EphemeronHashTables must be heap objects.
       HeapObjectSlot key_slot(table->RawFieldOfElementAt(
           EphemeronHashTable::EntryToIndex(InternalIndex(*iti))));
-      HeapObject key = key_slot.ToHeapObject();
+      Tagged<HeapObject> key = key_slot.ToHeapObject();
       // There may be old generation entries left in the remembered set as
       // MinorMS only promotes pages after clearing non-live references.
       if (!Heap::InYoungGeneration(key)) {
@@ -532,7 +534,7 @@ void MinorMarkSweepCollector::ClearNonLiveReferences() {
 }
 
 namespace {
-void VisitObjectWithEmbedderFields(JSObject object,
+void VisitObjectWithEmbedderFields(Tagged<JSObject> object,
                                    MarkingWorklists::Local& worklist) {
   DCHECK(object->MayHaveEmbedderFields());
   DCHECK(!Heap::InYoungGeneration(object));
@@ -555,7 +557,7 @@ void MinorMarkSweepCollector::MarkRootsFromTracedHandles(
     heap_->isolate()->traced_handles()->IterateAndMarkYoungRootsWithOldHosts(
         &root_visitor);
     // Visit the V8-to-Oilpan remembered set.
-    cpp_heap->VisitCrossHeapRememberedSetIfNeeded([this](JSObject obj) {
+    cpp_heap->VisitCrossHeapRememberedSetIfNeeded([this](Tagged<JSObject> obj) {
       VisitObjectWithEmbedderFields(obj, *local_marking_worklists());
     });
   } else {
@@ -681,7 +683,7 @@ void MinorMarkSweepCollector::DrainMarkingWorklist() {
 
     PerformWrapperTracing();
 
-    HeapObject heap_object;
+    Tagged<HeapObject> heap_object;
     while (marking_worklists_local->Pop(&heap_object)) {
       DCHECK(!IsFreeSpaceOrFiller(heap_object, cage_base));
       DCHECK(IsHeapObject(heap_object));
@@ -689,7 +691,7 @@ void MinorMarkSweepCollector::DrainMarkingWorklist() {
       DCHECK(!marking_state_->IsUnmarked(heap_object));
       // Maps won't change in the atomic pause, so the map can be read without
       // atomics.
-      Map map = Map::cast(*heap_object->map_slot());
+      Tagged<Map> map = Map::cast(*heap_object->map_slot());
       const auto visited_size = main_marking_visitor_->Visit(map, heap_object);
       if (visited_size) {
         main_marking_visitor_->IncrementLiveBytesCached(
@@ -851,7 +853,7 @@ bool MinorMarkSweepCollector::SweepNewLargeSpace() {
   for (auto it = new_lo_space->begin(); it != new_lo_space->end();) {
     LargePage* current = *it;
     it++;
-    HeapObject object = current->GetObject();
+    Tagged<HeapObject> object = current->GetObject();
     if (!non_atomic_marking_state_->IsMarked(object)) {
       // Object is dead and page can be released.
       new_lo_space->RemovePage(current);
diff --git a/src/heap/new-spaces-inl.h b/src/heap/new-spaces-inl.h
index 3f9a8e54874..00ccc934790 100644
--- a/src/heap/new-spaces-inl.h
+++ b/src/heap/new-spaces-inl.h
@@ -21,14 +21,14 @@ namespace internal {
 // -----------------------------------------------------------------------------
 // SemiSpace
 
-bool SemiSpace::Contains(HeapObject o) const {
+bool SemiSpace::Contains(Tagged<HeapObject> o) const {
   BasicMemoryChunk* memory_chunk = BasicMemoryChunk::FromHeapObject(o);
   if (memory_chunk->IsLargePage()) return false;
   return id_ == kToSpace ? memory_chunk->IsToPage()
                          : memory_chunk->IsFromPage();
 }
 
-bool SemiSpace::Contains(Object o) const {
+bool SemiSpace::Contains(Tagged<Object> o) const {
   return IsHeapObject(o) && Contains(HeapObject::cast(o));
 }
 
@@ -48,20 +48,14 @@ bool SemiSpace::ContainsSlow(Address a) const {
 // --------------------------------------------------------------------------
 // NewSpace
 
-bool NewSpace::Contains(Object o) const {
+bool NewSpace::Contains(Tagged<Object> o) const {
   return IsHeapObject(o) && Contains(HeapObject::cast(o));
 }
 
-bool NewSpace::Contains(HeapObject o) const {
+bool NewSpace::Contains(Tagged<HeapObject> o) const {
   return BasicMemoryChunk::FromHeapObject(o)->InNewSpace();
 }
 
-template <typename T>
-inline bool NewSpace::Contains(Tagged<T> o) const {
-  static_assert(kTaggedCanConvertToRawObjects);
-  return Contains(*o);
-}
-
 V8_WARN_UNUSED_RESULT inline AllocationResult NewSpace::AllocateRawSynchronized(
     int size_in_bytes, AllocationAlignment alignment, AllocationOrigin origin) {
   base::MutexGuard guard(&mutex_);
diff --git a/src/heap/new-spaces.cc b/src/heap/new-spaces.cc
index 997e2cdb91e..e4a4b46d632 100644
--- a/src/heap/new-spaces.cc
+++ b/src/heap/new-spaces.cc
@@ -723,7 +723,7 @@ void SemiSpaceNewSpace::VerifyObjects(Isolate* isolate,
     Address current_address = page->area_start();
 
     while (!Page::IsAlignedToPageSize(current_address)) {
-      HeapObject object = HeapObject::FromAddress(current_address);
+      Tagged<HeapObject> object = HeapObject::FromAddress(current_address);
 
       // The first word should be a map, and we expect all map pointers to
       // be in map space or read-only space.
@@ -732,7 +732,7 @@ void SemiSpaceNewSpace::VerifyObjects(Isolate* isolate,
       visitor->VerifyObject(object);
 
       if (IsExternalString(object, cage_base)) {
-        ExternalString external_string = ExternalString::cast(object);
+        Tagged<ExternalString> external_string = ExternalString::cast(object);
         size_t string_size = external_string->ExternalPayloadSize();
         external_space_bytes[static_cast<int>(
             ExternalBackingStoreType::kExternalString)] += string_size;
diff --git a/src/heap/new-spaces.h b/src/heap/new-spaces.h
index b094230d496..916d9238361 100644
--- a/src/heap/new-spaces.h
+++ b/src/heap/new-spaces.h
@@ -46,8 +46,8 @@ class SemiSpace final : public Space {
   SemiSpace(Heap* heap, SemiSpaceId semispace)
       : Space(heap, NEW_SPACE, nullptr, allocation_counter_), id_(semispace) {}
 
-  inline bool Contains(HeapObject o) const;
-  inline bool Contains(Object o) const;
+  inline bool Contains(Tagged<HeapObject> o) const;
+  inline bool Contains(Tagged<Object> o) const;
   template <typename T>
   inline bool Contains(Tagged<T> o) const;
   inline bool ContainsSlow(Address a) const;
@@ -227,10 +227,8 @@ class NewSpace : NON_EXPORTED_BASE(public SpaceWithLinearArea) {
 
   NewSpace(Heap* heap, LinearAllocationArea& allocation_info);
 
-  inline bool Contains(Object o) const;
-  inline bool Contains(HeapObject o) const;
-  template <typename T>
-  inline bool Contains(Tagged<T> o) const;
+  inline bool Contains(Tagged<Object> o) const;
+  inline bool Contains(Tagged<HeapObject> o) const;
   virtual bool ContainsSlow(Address a) const = 0;
 
 #if DEBUG
diff --git a/src/heap/object-lock.h b/src/heap/object-lock.h
index 7263e47296f..4da0dd491ad 100644
--- a/src/heap/object-lock.h
+++ b/src/heap/object-lock.h
@@ -12,20 +12,20 @@ namespace v8::internal {
 
 class ExclusiveObjectLock final {
  public:
-  static void Lock(HeapObject heap_object) {
+  static void Lock(Tagged<HeapObject> heap_object) {
     MemoryChunk::FromHeapObject(heap_object)->shared_mutex()->LockExclusive();
   }
-  static void Unlock(HeapObject heap_object) {
+  static void Unlock(Tagged<HeapObject> heap_object) {
     MemoryChunk::FromHeapObject(heap_object)->shared_mutex()->UnlockExclusive();
   }
 };
 
 class SharedObjectLock final {
  public:
-  static void Lock(HeapObject heap_object) {
+  static void Lock(Tagged<HeapObject> heap_object) {
     MemoryChunk::FromHeapObject(heap_object)->shared_mutex()->LockShared();
   }
-  static void Unlock(HeapObject heap_object) {
+  static void Unlock(Tagged<HeapObject> heap_object) {
     MemoryChunk::FromHeapObject(heap_object)->shared_mutex()->UnlockShared();
   }
 };
@@ -33,7 +33,7 @@ class SharedObjectLock final {
 template <typename LockType>
 class ObjectLockGuard final {
  public:
-  explicit ObjectLockGuard(HeapObject object) : raw_object_(object) {
+  explicit ObjectLockGuard(Tagged<HeapObject> object) : raw_object_(object) {
     LockType::Lock(object);
   }
   ~ObjectLockGuard() { LockType::Unlock(raw_object_); }
diff --git a/src/heap/object-stats.cc b/src/heap/object-stats.cc
index 1152ac4775c..7ee99db45a6 100644
--- a/src/heap/object-stats.cc
+++ b/src/heap/object-stats.cc
@@ -49,7 +49,7 @@ class FieldStatsCollector : public ObjectVisitorWithCageBases {
         string_data_count_(string_data_count),
         raw_fields_count_(raw_fields_count) {}
 
-  void RecordStats(HeapObject host) {
+  void RecordStats(Tagged<HeapObject> host) {
     size_t old_pointer_fields_count = *tagged_fields_count_;
     host->Iterate(cage_base(), this);
     size_t tagged_fields_count_in_object =
@@ -88,30 +88,32 @@ class FieldStatsCollector : public ObjectVisitorWithCageBases {
     *raw_fields_count_ += raw_fields_count_in_object;
   }
 
-  void VisitPointers(HeapObject host, ObjectSlot start,
+  void VisitPointers(Tagged<HeapObject> host, ObjectSlot start,
                      ObjectSlot end) override {
     *tagged_fields_count_ += (end - start);
   }
-  void VisitPointers(HeapObject host, MaybeObjectSlot start,
+  void VisitPointers(Tagged<HeapObject> host, MaybeObjectSlot start,
                      MaybeObjectSlot end) override {
     *tagged_fields_count_ += (end - start);
   }
 
   V8_INLINE void VisitInstructionStreamPointer(
-      Code host, InstructionStreamSlot slot) override {
+      Tagged<Code> host, InstructionStreamSlot slot) override {
     *tagged_fields_count_ += 1;
   }
 
-  void VisitCodeTarget(InstructionStream host, RelocInfo* rinfo) override {
+  void VisitCodeTarget(Tagged<InstructionStream> host,
+                       RelocInfo* rinfo) override {
     // InstructionStream target is most likely encoded as a relative 32-bit
     // offset and not as a full tagged value, so there's nothing to count.
   }
 
-  void VisitEmbeddedPointer(InstructionStream host, RelocInfo* rinfo) override {
+  void VisitEmbeddedPointer(Tagged<InstructionStream> host,
+                            RelocInfo* rinfo) override {
     *tagged_fields_count_ += 1;
   }
 
-  void VisitMapPointer(HeapObject host) override {
+  void VisitMapPointer(Tagged<HeapObject> host) override {
     // Just do nothing, but avoid the inherited UNREACHABLE implementation.
   }
 
@@ -125,7 +127,7 @@ class FieldStatsCollector : public ObjectVisitorWithCageBases {
   std::unordered_map<Map, JSObjectFieldStats, Object::Hasher>
       field_stats_cache_;
 
-  JSObjectFieldStats GetInobjectFieldStats(Map map);
+  JSObjectFieldStats GetInobjectFieldStats(Tagged<Map> map);
 
   size_t* const tagged_fields_count_;
   size_t* const embedder_fields_count_;
@@ -136,7 +138,7 @@ class FieldStatsCollector : public ObjectVisitorWithCageBases {
 };
 
 FieldStatsCollector::JSObjectFieldStats
-FieldStatsCollector::GetInobjectFieldStats(Map map) {
+FieldStatsCollector::GetInobjectFieldStats(Tagged<Map> map) {
   auto iter = field_stats_cache_.find(map);
   if (iter != field_stats_cache_.end()) {
     return iter->second;
@@ -145,7 +147,7 @@ FieldStatsCollector::GetInobjectFieldStats(Map map) {
   JSObjectFieldStats stats;
   stats.embedded_fields_count_ = JSObject::GetEmbedderFieldCount(map);
   if (!map->is_dictionary_map()) {
-    DescriptorArray descriptors = map->instance_descriptors();
+    Tagged<DescriptorArray> descriptors = map->instance_descriptors();
     for (InternalIndex descriptor : map->IterateOwnDescriptors()) {
       PropertyDetails details = descriptors->GetDetails(descriptor);
       if (details.location() == PropertyLocation::kField) {
@@ -381,7 +383,7 @@ class ObjectStatsCollectorImpl {
   void CollectGlobalStatistics();
 
   enum class CollectFieldStats { kNo, kYes };
-  void CollectStatistics(HeapObject obj, Phase phase,
+  void CollectStatistics(Tagged<HeapObject> obj, Phase phase,
                          CollectFieldStats collect_field_stats);
 
  private:
@@ -392,7 +394,8 @@ class ObjectStatsCollectorImpl {
 
   Isolate* isolate() { return heap_->isolate(); }
 
-  bool RecordVirtualObjectStats(HeapObject parent, HeapObject obj,
+  bool RecordVirtualObjectStats(Tagged<HeapObject> parent,
+                                Tagged<HeapObject> obj,
                                 ObjectStats::VirtualInstanceType type,
                                 size_t size, size_t over_allocated,
                                 CowMode check_cow_array = kCheckCow);
@@ -400,7 +403,8 @@ class ObjectStatsCollectorImpl {
                                    ObjectStats::VirtualInstanceType type,
                                    size_t size);
   // Gets size from |ob| and assumes no over allocating.
-  bool RecordSimpleVirtualObjectStats(HeapObject parent, HeapObject obj,
+  bool RecordSimpleVirtualObjectStats(Tagged<HeapObject> parent,
+                                      Tagged<HeapObject> obj,
                                       ObjectStats::VirtualInstanceType type);
   // For HashTable it is possible to compute over allocated memory.
   template <typename Dictionary>
@@ -408,42 +412,43 @@ class ObjectStatsCollectorImpl {
                                          Tagged<Dictionary> hash_table,
                                          ObjectStats::VirtualInstanceType type);
 
-  bool SameLiveness(HeapObject obj1, HeapObject obj2);
-  bool CanRecordFixedArray(FixedArrayBase array);
-  bool IsCowArray(FixedArrayBase array);
+  bool SameLiveness(Tagged<HeapObject> obj1, Tagged<HeapObject> obj2);
+  bool CanRecordFixedArray(Tagged<FixedArrayBase> array);
+  bool IsCowArray(Tagged<FixedArrayBase> array);
 
   // Blocklist for objects that should not be recorded using
   // VirtualObjectStats and RecordSimpleVirtualObjectStats. For recording those
   // objects dispatch to the low level ObjectStats::RecordObjectStats manually.
-  bool ShouldRecordObject(HeapObject object, CowMode check_cow_array);
+  bool ShouldRecordObject(Tagged<HeapObject> object, CowMode check_cow_array);
 
   void RecordObjectStats(
-      HeapObject obj, InstanceType type, size_t size,
+      Tagged<HeapObject> obj, InstanceType type, size_t size,
       size_t over_allocated = ObjectStats::kNoOverAllocation);
 
   // Specific recursion into constant pool or embedded code objects. Records
   // FixedArrays and Tuple2.
   void RecordVirtualObjectsForConstantPoolOrEmbeddedObjects(
-      HeapObject parent, HeapObject object,
+      Tagged<HeapObject> parent, Tagged<HeapObject> object,
       ObjectStats::VirtualInstanceType type);
 
   // Details.
-  void RecordVirtualAllocationSiteDetails(AllocationSite site);
-  void RecordVirtualBytecodeArrayDetails(BytecodeArray bytecode);
-  void RecordVirtualCodeDetails(InstructionStream code);
-  void RecordVirtualContext(Context context);
-  void RecordVirtualFeedbackVectorDetails(FeedbackVector vector);
-  void RecordVirtualFixedArrayDetails(FixedArray array);
-  void RecordVirtualFunctionTemplateInfoDetails(FunctionTemplateInfo fti);
+  void RecordVirtualAllocationSiteDetails(Tagged<AllocationSite> site);
+  void RecordVirtualBytecodeArrayDetails(Tagged<BytecodeArray> bytecode);
+  void RecordVirtualCodeDetails(Tagged<InstructionStream> code);
+  void RecordVirtualContext(Tagged<Context> context);
+  void RecordVirtualFeedbackVectorDetails(Tagged<FeedbackVector> vector);
+  void RecordVirtualFixedArrayDetails(Tagged<FixedArray> array);
+  void RecordVirtualFunctionTemplateInfoDetails(
+      Tagged<FunctionTemplateInfo> fti);
   void RecordVirtualJSGlobalObjectDetails(Tagged<JSGlobalObject> object);
-  void RecordVirtualJSObjectDetails(JSObject object);
-  void RecordVirtualMapDetails(Map map);
-  void RecordVirtualScriptDetails(Script script);
-  void RecordVirtualExternalStringDetails(ExternalString script);
-  void RecordVirtualSharedFunctionInfoDetails(SharedFunctionInfo info);
+  void RecordVirtualJSObjectDetails(Tagged<JSObject> object);
+  void RecordVirtualMapDetails(Tagged<Map> map);
+  void RecordVirtualScriptDetails(Tagged<Script> script);
+  void RecordVirtualExternalStringDetails(Tagged<ExternalString> script);
+  void RecordVirtualSharedFunctionInfoDetails(Tagged<SharedFunctionInfo> info);
 
   void RecordVirtualArrayBoilerplateDescription(
-      ArrayBoilerplateDescription description);
+      Tagged<ArrayBoilerplateDescription> description);
 
   PtrComprCageBase cage_base() const {
     return field_stats_collector_.cage_base();
@@ -469,10 +474,10 @@ ObjectStatsCollectorImpl::ObjectStatsCollectorImpl(Heap* heap,
           &stats->boxed_double_fields_count_, &stats->string_data_count_,
           &stats->raw_fields_count_) {}
 
-bool ObjectStatsCollectorImpl::ShouldRecordObject(HeapObject obj,
+bool ObjectStatsCollectorImpl::ShouldRecordObject(Tagged<HeapObject> obj,
                                                   CowMode check_cow_array) {
   if (IsFixedArrayExact(obj)) {
-    FixedArray fixed_array = FixedArray::cast(obj);
+    Tagged<FixedArray> fixed_array = FixedArray::cast(obj);
     bool cow_check = check_cow_array == kIgnoreCow || !IsCowArray(fixed_array);
     return CanRecordFixedArray(fixed_array) && cow_check;
   }
@@ -493,14 +498,16 @@ void ObjectStatsCollectorImpl::RecordHashTableVirtualObjectStats(
 }
 
 bool ObjectStatsCollectorImpl::RecordSimpleVirtualObjectStats(
-    HeapObject parent, HeapObject obj, ObjectStats::VirtualInstanceType type) {
+    Tagged<HeapObject> parent, Tagged<HeapObject> obj,
+    ObjectStats::VirtualInstanceType type) {
   return RecordVirtualObjectStats(parent, obj, type, obj->Size(cage_base()),
                                   ObjectStats::kNoOverAllocation, kCheckCow);
 }
 
 bool ObjectStatsCollectorImpl::RecordVirtualObjectStats(
-    HeapObject parent, HeapObject obj, ObjectStats::VirtualInstanceType type,
-    size_t size, size_t over_allocated, CowMode check_cow_array) {
+    Tagged<HeapObject> parent, Tagged<HeapObject> obj,
+    ObjectStats::VirtualInstanceType type, size_t size, size_t over_allocated,
+    CowMode check_cow_array) {
   CHECK_LT(over_allocated, size);
   if (!SameLiveness(parent, obj) || !ShouldRecordObject(obj, check_cow_array)) {
     return false;
@@ -523,9 +530,9 @@ void ObjectStatsCollectorImpl::RecordExternalResourceStats(
 }
 
 void ObjectStatsCollectorImpl::RecordVirtualAllocationSiteDetails(
-    AllocationSite site) {
+    Tagged<AllocationSite> site) {
   if (!site->PointsToLiteral()) return;
-  JSObject boilerplate = site->boilerplate();
+  Tagged<JSObject> boilerplate = site->boilerplate();
   if (IsJSArray(boilerplate)) {
     RecordSimpleVirtualObjectStats(site, boilerplate,
                                    ObjectStats::JS_ARRAY_BOILERPLATE_TYPE);
@@ -537,25 +544,25 @@ void ObjectStatsCollectorImpl::RecordVirtualAllocationSiteDetails(
     if (boilerplate->HasFastProperties()) {
       // We'll mis-classify the empty_property_array here. Given that there is a
       // single instance, this is negligible.
-      PropertyArray properties = boilerplate->property_array();
+      Tagged<PropertyArray> properties = boilerplate->property_array();
       RecordSimpleVirtualObjectStats(
           site, properties, ObjectStats::BOILERPLATE_PROPERTY_ARRAY_TYPE);
     } else {
-      NameDictionary properties = boilerplate->property_dictionary();
+      Tagged<NameDictionary> properties = boilerplate->property_dictionary();
       RecordSimpleVirtualObjectStats(
           site, properties, ObjectStats::BOILERPLATE_PROPERTY_DICTIONARY_TYPE);
     }
   }
-  FixedArrayBase elements = boilerplate->elements();
+  Tagged<FixedArrayBase> elements = boilerplate->elements();
   RecordSimpleVirtualObjectStats(site, elements,
                                  ObjectStats::BOILERPLATE_ELEMENTS_TYPE);
 }
 
 void ObjectStatsCollectorImpl::RecordVirtualFunctionTemplateInfoDetails(
-    FunctionTemplateInfo fti) {
+    Tagged<FunctionTemplateInfo> fti) {
   // named_property_handler and indexed_property_handler are recorded as
   // INTERCEPTOR_INFO_TYPE.
-  HeapObject call_code = fti->call_code(kAcquireLoad);
+  Tagged<HeapObject> call_code = fti->call_code(kAcquireLoad);
   if (!IsUndefined(call_code, isolate())) {
     RecordSimpleVirtualObjectStats(
         fti, CallHandlerInfo::cast(call_code),
@@ -575,12 +582,13 @@ void ObjectStatsCollectorImpl::RecordVirtualJSGlobalObjectDetails(
   RecordHashTableVirtualObjectStats(object, properties,
                                     ObjectStats::GLOBAL_PROPERTIES_TYPE);
   // Elements.
-  FixedArrayBase elements = object->elements();
+  Tagged<FixedArrayBase> elements = object->elements();
   RecordSimpleVirtualObjectStats(object, elements,
                                  ObjectStats::GLOBAL_ELEMENTS_TYPE);
 }
 
-void ObjectStatsCollectorImpl::RecordVirtualJSObjectDetails(JSObject object) {
+void ObjectStatsCollectorImpl::RecordVirtualJSObjectDetails(
+    Tagged<JSObject> object) {
   // JSGlobalObject is recorded separately.
   if (IsJSGlobalObject(object)) return;
 
@@ -592,7 +600,7 @@ void ObjectStatsCollectorImpl::RecordVirtualJSObjectDetails(JSObject object) {
 
   // Properties.
   if (object->HasFastProperties()) {
-    PropertyArray properties = object->property_array();
+    Tagged<PropertyArray> properties = object->property_array();
     if (properties != ReadOnlyRoots(heap_).empty_property_array()) {
       size_t over_allocated =
           object->map()->UnusedPropertyFields() * kTaggedSize;
@@ -612,7 +620,7 @@ void ObjectStatsCollectorImpl::RecordVirtualJSObjectDetails(JSObject object) {
   }
 
   // Elements.
-  FixedArrayBase elements = object->elements();
+  Tagged<FixedArrayBase> elements = object->elements();
   if (object->HasDictionaryElements()) {
     RecordHashTableVirtualObjectStats(
         object, NumberDictionary::cast(elements),
@@ -635,7 +643,7 @@ void ObjectStatsCollectorImpl::RecordVirtualJSObjectDetails(JSObject object) {
 
   // JSCollections.
   if (IsJSCollection(object)) {
-    Object maybe_table = JSCollection::cast(object)->table();
+    Tagged<Object> maybe_table = JSCollection::cast(object)->table();
     if (!IsUndefined(maybe_table, isolate())) {
       DCHECK(IsFixedArray(maybe_table, isolate()));
       // TODO(bmeurer): Properly compute over-allocation here.
@@ -649,7 +657,7 @@ static ObjectStats::VirtualInstanceType GetFeedbackSlotType(
     MaybeObject maybe_obj, FeedbackSlotKind kind, Isolate* isolate) {
   if (maybe_obj->IsCleared())
     return ObjectStats::FEEDBACK_VECTOR_SLOT_OTHER_TYPE;
-  Object obj = maybe_obj->GetHeapObjectOrSmi();
+  Tagged<Object> obj = maybe_obj->GetHeapObjectOrSmi();
   switch (kind) {
     case FeedbackSlotKind::kCall:
       if (obj == *isolate->factory()->uninitialized_symbol()) {
@@ -689,7 +697,7 @@ static ObjectStats::VirtualInstanceType GetFeedbackSlotType(
 }
 
 void ObjectStatsCollectorImpl::RecordVirtualFeedbackVectorDetails(
-    FeedbackVector vector) {
+    Tagged<FeedbackVector> vector) {
   if (virtual_objects_.find(vector) != virtual_objects_.end()) return;
   // Manually insert the feedback vector into the virtual object list, since
   // we're logging its component parts separately.
@@ -719,7 +727,7 @@ void ObjectStatsCollectorImpl::RecordVirtualFeedbackVectorDetails(
     // Log the monomorphic/polymorphic helper objects that this slot owns.
     for (int i = 0; i < it.entry_size(); i++) {
       MaybeObject raw_object = vector->Get(slot.WithOffset(i));
-      HeapObject object;
+      Tagged<HeapObject> object;
       if (raw_object->GetHeapObject(&object)) {
         if (IsCell(object, cage_base()) ||
             IsWeakFixedArray(object, cage_base())) {
@@ -734,7 +742,7 @@ void ObjectStatsCollectorImpl::RecordVirtualFeedbackVectorDetails(
 }
 
 void ObjectStatsCollectorImpl::RecordVirtualFixedArrayDetails(
-    FixedArray array) {
+    Tagged<FixedArray> array) {
   if (IsCowArray(array)) {
     RecordVirtualObjectStats(HeapObject(), array, ObjectStats::COW_ARRAY_TYPE,
                              array->Size(), ObjectStats::kNoOverAllocation,
@@ -743,9 +751,10 @@ void ObjectStatsCollectorImpl::RecordVirtualFixedArrayDetails(
 }
 
 void ObjectStatsCollectorImpl::CollectStatistics(
-    HeapObject obj, Phase phase, CollectFieldStats collect_field_stats) {
+    Tagged<HeapObject> obj, Phase phase,
+    CollectFieldStats collect_field_stats) {
   DisallowGarbageCollection no_gc;
-  Map map = obj->map(cage_base());
+  Tagged<Map> map = obj->map(cage_base());
   InstanceType instance_type = map->instance_type();
   switch (phase) {
     case kPhase1:
@@ -802,9 +811,9 @@ void ObjectStatsCollectorImpl::CollectStatistics(
 
 void ObjectStatsCollectorImpl::CollectGlobalStatistics() {
   // Iterate boilerplates first to disambiguate them from regular JS objects.
-  Object list = heap_->allocation_sites_list();
+  Tagged<Object> list = heap_->allocation_sites_list();
   while (IsAllocationSite(list, cage_base())) {
-    AllocationSite site = AllocationSite::cast(list);
+    Tagged<AllocationSite> site = AllocationSite::cast(list);
     RecordVirtualAllocationSiteDetails(site);
     list = site->weak_next();
   }
@@ -828,7 +837,7 @@ void ObjectStatsCollectorImpl::CollectGlobalStatistics() {
                                  ObjectStats::SCRIPT_LIST_TYPE);
 }
 
-void ObjectStatsCollectorImpl::RecordObjectStats(HeapObject obj,
+void ObjectStatsCollectorImpl::RecordObjectStats(Tagged<HeapObject> obj,
                                                  InstanceType type, size_t size,
                                                  size_t over_allocated) {
   if (virtual_objects_.find(obj) == virtual_objects_.end()) {
@@ -836,18 +845,20 @@ void ObjectStatsCollectorImpl::RecordObjectStats(HeapObject obj,
   }
 }
 
-bool ObjectStatsCollectorImpl::CanRecordFixedArray(FixedArrayBase array) {
+bool ObjectStatsCollectorImpl::CanRecordFixedArray(
+    Tagged<FixedArrayBase> array) {
   ReadOnlyRoots roots(heap_);
   return array != roots.empty_fixed_array() &&
          array != roots.empty_slow_element_dictionary() &&
          array != roots.empty_property_dictionary();
 }
 
-bool ObjectStatsCollectorImpl::IsCowArray(FixedArrayBase array) {
+bool ObjectStatsCollectorImpl::IsCowArray(Tagged<FixedArrayBase> array) {
   return array->map(cage_base()) == ReadOnlyRoots(heap_).fixed_cow_array_map();
 }
 
-bool ObjectStatsCollectorImpl::SameLiveness(HeapObject obj1, HeapObject obj2) {
+bool ObjectStatsCollectorImpl::SameLiveness(Tagged<HeapObject> obj1,
+                                            Tagged<HeapObject> obj2) {
   if (obj1.is_null() || obj2.is_null()) return true;
   const auto obj1_marked =
       obj1.InReadOnlySpace() || marking_state_->IsMarked(obj1);
@@ -856,7 +867,7 @@ bool ObjectStatsCollectorImpl::SameLiveness(HeapObject obj1, HeapObject obj2) {
   return obj1_marked == obj2_marked;
 }
 
-void ObjectStatsCollectorImpl::RecordVirtualMapDetails(Map map) {
+void ObjectStatsCollectorImpl::RecordVirtualMapDetails(Tagged<Map> map) {
   // TODO(mlippautz): map->dependent_code(): DEPENDENT_CODE_TYPE.
 
   // For Map we want to distinguish between various different states
@@ -887,7 +898,7 @@ void ObjectStatsCollectorImpl::RecordVirtualMapDetails(Map map) {
     // This will be logged as MAP_TYPE in Phase2.
   }
 
-  DescriptorArray array = map->instance_descriptors(cage_base());
+  Tagged<DescriptorArray> array = map->instance_descriptors(cage_base());
   if (map->owns_descriptors() &&
       array != ReadOnlyRoots(heap_).empty_descriptor_array()) {
     // Generally DescriptorArrays have their own instance type already
@@ -902,7 +913,7 @@ void ObjectStatsCollectorImpl::RecordVirtualMapDetails(Map map) {
           map, array, ObjectStats::DEPRECATED_DESCRIPTOR_ARRAY_TYPE);
     }
 
-    EnumCache enum_cache = array->enum_cache();
+    Tagged<EnumCache> enum_cache = array->enum_cache();
     RecordSimpleVirtualObjectStats(array, enum_cache->keys(),
                                    ObjectStats::ENUM_KEYS_CACHE_TYPE);
     RecordSimpleVirtualObjectStats(array, enum_cache->indices(),
@@ -910,9 +921,9 @@ void ObjectStatsCollectorImpl::RecordVirtualMapDetails(Map map) {
   }
 
   if (map->is_prototype_map()) {
-    PrototypeInfo prototype_info;
+    Tagged<PrototypeInfo> prototype_info;
     if (map->TryGetPrototypeInfo(&prototype_info)) {
-      Object users = prototype_info->prototype_users();
+      Tagged<Object> users = prototype_info->prototype_users();
       if (IsWeakFixedArray(users, cage_base())) {
         RecordSimpleVirtualObjectStats(map, WeakArrayList::cast(users),
                                        ObjectStats::PROTOTYPE_USERS_TYPE);
@@ -921,18 +932,19 @@ void ObjectStatsCollectorImpl::RecordVirtualMapDetails(Map map) {
   }
 }
 
-void ObjectStatsCollectorImpl::RecordVirtualScriptDetails(Script script) {
+void ObjectStatsCollectorImpl::RecordVirtualScriptDetails(
+    Tagged<Script> script) {
   RecordSimpleVirtualObjectStats(
       script, script->shared_function_infos(),
       ObjectStats::SCRIPT_SHARED_FUNCTION_INFOS_TYPE);
 
   // Log the size of external source code.
-  Object raw_source = script->source();
+  Tagged<Object> raw_source = script->source();
   if (IsExternalString(raw_source, cage_base())) {
     // The contents of external strings aren't on the heap, so we have to record
     // them manually. The on-heap String object is recorded independently in
     // the normal pass.
-    ExternalString string = ExternalString::cast(raw_source);
+    Tagged<ExternalString> string = ExternalString::cast(raw_source);
     Address resource = string->resource_as_address();
     size_t off_heap_size = string->ExternalPayloadSize();
     RecordExternalResourceStats(
@@ -942,7 +954,7 @@ void ObjectStatsCollectorImpl::RecordVirtualScriptDetails(Script script) {
             : ObjectStats::SCRIPT_SOURCE_EXTERNAL_TWO_BYTE_TYPE,
         off_heap_size);
   } else if (IsString(raw_source, cage_base())) {
-    String source = String::cast(raw_source);
+    Tagged<String> source = String::cast(raw_source);
     RecordSimpleVirtualObjectStats(
         script, source,
         source->IsOneByteRepresentation()
@@ -952,7 +964,7 @@ void ObjectStatsCollectorImpl::RecordVirtualScriptDetails(Script script) {
 }
 
 void ObjectStatsCollectorImpl::RecordVirtualExternalStringDetails(
-    ExternalString string) {
+    Tagged<ExternalString> string) {
   // Track the external string resource size in a separate category.
 
   Address resource = string->resource_as_address();
@@ -966,7 +978,7 @@ void ObjectStatsCollectorImpl::RecordVirtualExternalStringDetails(
 }
 
 void ObjectStatsCollectorImpl::RecordVirtualSharedFunctionInfoDetails(
-    SharedFunctionInfo info) {
+    Tagged<SharedFunctionInfo> info) {
   // Uncompiled SharedFunctionInfo gets its own category.
   if (!info->is_compiled()) {
     RecordSimpleVirtualObjectStats(
@@ -975,7 +987,7 @@ void ObjectStatsCollectorImpl::RecordVirtualSharedFunctionInfoDetails(
 }
 
 void ObjectStatsCollectorImpl::RecordVirtualArrayBoilerplateDescription(
-    ArrayBoilerplateDescription description) {
+    Tagged<ArrayBoilerplateDescription> description) {
   RecordVirtualObjectsForConstantPoolOrEmbeddedObjects(
       description, description->constant_elements(),
       ObjectStats::ARRAY_BOILERPLATE_DESCRIPTION_ELEMENTS_TYPE);
@@ -983,13 +995,13 @@ void ObjectStatsCollectorImpl::RecordVirtualArrayBoilerplateDescription(
 
 void ObjectStatsCollectorImpl::
     RecordVirtualObjectsForConstantPoolOrEmbeddedObjects(
-        HeapObject parent, HeapObject object,
+        Tagged<HeapObject> parent, Tagged<HeapObject> object,
         ObjectStats::VirtualInstanceType type) {
   if (!RecordSimpleVirtualObjectStats(parent, object, type)) return;
   if (IsFixedArrayExact(object, cage_base())) {
-    FixedArray array = FixedArray::cast(object);
+    Tagged<FixedArray> array = FixedArray::cast(object);
     for (int i = 0; i < array->length(); i++) {
-      Object entry = array->get(i);
+      Tagged<Object> entry = array->get(i);
       if (!IsHeapObject(entry)) continue;
       RecordVirtualObjectsForConstantPoolOrEmbeddedObjects(
           array, HeapObject::cast(entry), type);
@@ -998,15 +1010,16 @@ void ObjectStatsCollectorImpl::
 }
 
 void ObjectStatsCollectorImpl::RecordVirtualBytecodeArrayDetails(
-    BytecodeArray bytecode) {
+    Tagged<BytecodeArray> bytecode) {
   RecordSimpleVirtualObjectStats(
       bytecode, bytecode->constant_pool(),
       ObjectStats::BYTECODE_ARRAY_CONSTANT_POOL_TYPE);
   // FixedArrays on constant pool are used for holding descriptor information.
   // They are shared with optimized code.
-  FixedArray constant_pool = FixedArray::cast(bytecode->constant_pool());
+  Tagged<FixedArray> constant_pool =
+      FixedArray::cast(bytecode->constant_pool());
   for (int i = 0; i < constant_pool->length(); i++) {
-    Object entry = constant_pool->get(i);
+    Tagged<Object> entry = constant_pool->get(i);
     if (IsFixedArrayExact(entry, cage_base())) {
       RecordVirtualObjectsForConstantPoolOrEmbeddedObjects(
           constant_pool, HeapObject::cast(entry),
@@ -1038,15 +1051,15 @@ ObjectStats::VirtualInstanceType CodeKindToVirtualInstanceType(CodeKind kind) {
 }  // namespace
 
 void ObjectStatsCollectorImpl::RecordVirtualCodeDetails(
-    InstructionStream istream) {
-  Code code;
+    Tagged<InstructionStream> istream) {
+  Tagged<Code> code;
   if (!istream->TryGetCode(&code, kAcquireLoad)) return;
   RecordSimpleVirtualObjectStats(HeapObject(), istream,
                                  CodeKindToVirtualInstanceType(code->kind()));
   RecordSimpleVirtualObjectStats(istream, istream->relocation_info(),
                                  ObjectStats::RELOC_INFO_TYPE);
   if (CodeKindIsOptimizedJSFunction(code->kind())) {
-    Object source_position_table = code->source_position_table();
+    Tagged<Object> source_position_table = code->source_position_table();
     if (IsHeapObject(source_position_table)) {
       RecordSimpleVirtualObjectStats(istream,
                                      HeapObject::cast(source_position_table),
@@ -1054,7 +1067,7 @@ void ObjectStatsCollectorImpl::RecordVirtualCodeDetails(
     }
     RecordSimpleVirtualObjectStats(istream, code->deoptimization_data(),
                                    ObjectStats::DEOPTIMIZATION_DATA_TYPE);
-    DeoptimizationData input_data =
+    Tagged<DeoptimizationData> input_data =
         DeoptimizationData::cast(code->deoptimization_data());
     if (input_data->length() > 0) {
       RecordSimpleVirtualObjectStats(code->deoptimization_data(),
@@ -1065,7 +1078,7 @@ void ObjectStatsCollectorImpl::RecordVirtualCodeDetails(
   int const mode_mask = RelocInfo::EmbeddedObjectModeMask();
   for (RelocIterator it(code, mode_mask); !it.done(); it.next()) {
     DCHECK(RelocInfo::IsEmbeddedObjectMode(it.rinfo()->rmode()));
-    Object target = it.rinfo()->target_object(cage_base());
+    Tagged<Object> target = it.rinfo()->target_object(cage_base());
     if (IsFixedArrayExact(target, cage_base())) {
       RecordVirtualObjectsForConstantPoolOrEmbeddedObjects(
           istream, HeapObject::cast(target), ObjectStats::EMBEDDED_OBJECT_TYPE);
@@ -1073,7 +1086,7 @@ void ObjectStatsCollectorImpl::RecordVirtualCodeDetails(
   }
 }
 
-void ObjectStatsCollectorImpl::RecordVirtualContext(Context context) {
+void ObjectStatsCollectorImpl::RecordVirtualContext(Tagged<Context> context) {
   if (IsNativeContext(context)) {
     RecordObjectStats(context, NATIVE_CONTEXT_TYPE, context->Size());
     if (IsWeakArrayList(context->retained_maps(), cage_base())) {
@@ -1100,7 +1113,7 @@ class ObjectStatsVisitor {
         marking_state_(heap->non_atomic_marking_state()),
         phase_(phase) {}
 
-  void Visit(HeapObject obj) {
+  void Visit(Tagged<HeapObject> obj) {
     if (obj.InReadOnlySpace() || marking_state_->IsMarked(obj)) {
       live_collector_->CollectStatistics(
           obj, phase_, ObjectStatsCollectorImpl::CollectFieldStats::kYes);
@@ -1124,7 +1137,7 @@ void IterateHeap(Heap* heap, ObjectStatsVisitor* visitor) {
   // the nested SafepointScope inside CombinedHeapObjectIterator.
   AllowGarbageCollection allow_gc;
   CombinedHeapObjectIterator iterator(heap);
-  for (HeapObject obj = iterator.Next(); !obj.is_null();
+  for (Tagged<HeapObject> obj = iterator.Next(); !obj.is_null();
        obj = iterator.Next()) {
     visitor->Visit(obj);
   }
diff --git a/src/heap/objects-visiting-inl.h b/src/heap/objects-visiting-inl.h
index 41aa9ec6715..87de1218134 100644
--- a/src/heap/objects-visiting-inl.h
+++ b/src/heap/objects-visiting-inl.h
@@ -32,7 +32,7 @@ namespace v8 {
 namespace internal {
 
 template <VisitorId visitor_id>
-inline bool ContainsReadOnlyMap(PtrComprCageBase, HeapObject) {
+inline bool ContainsReadOnlyMap(PtrComprCageBase, Tagged<HeapObject>) {
   return false;
 }
 
@@ -82,7 +82,7 @@ inline bool ContainsReadOnlyMap(PtrComprCageBase, HeapObject) {
 #define DEFINE_READ_ONLY_MAP_SPECIALIZATION(VisitorIdType)                    \
   template <>                                                                 \
   inline bool ContainsReadOnlyMap<VisitorId::kVisit##VisitorIdType>(          \
-      PtrComprCageBase cage_base, HeapObject object) {                        \
+      PtrComprCageBase cage_base, Tagged<HeapObject> object) {                \
     /* If you see this DCHECK fail we encountered a Map with a VisitorId that \
      * should have only ever appeared in read-only space. */                  \
     DCHECK(object->map(cage_base).InReadOnlySpace());                         \
@@ -107,18 +107,20 @@ HeapVisitor<ResultType, ConcreteVisitor>::HeapVisitor(Heap* heap)
 
 template <typename ResultType, typename ConcreteVisitor>
 template <typename T>
-T HeapVisitor<ResultType, ConcreteVisitor>::Cast(HeapObject object) {
+Tagged<T> HeapVisitor<ResultType, ConcreteVisitor>::Cast(
+    Tagged<HeapObject> object) {
   return T::cast(object);
 }
 
 template <typename ResultType, typename ConcreteVisitor>
-ResultType HeapVisitor<ResultType, ConcreteVisitor>::Visit(HeapObject object) {
-  return Visit(object.map(cage_base()), object);
+ResultType HeapVisitor<ResultType, ConcreteVisitor>::Visit(
+    Tagged<HeapObject> object) {
+  return Visit(object->map(cage_base()), object);
 }
 
 template <typename ResultType, typename ConcreteVisitor>
-ResultType HeapVisitor<ResultType, ConcreteVisitor>::Visit(Map map,
-                                                           HeapObject object) {
+ResultType HeapVisitor<ResultType, ConcreteVisitor>::Visit(
+    Tagged<Map> map, Tagged<HeapObject> object) {
   ConcreteVisitor* visitor = static_cast<ConcreteVisitor*>(this);
   switch (map->visitor_id()) {
 #define CASE(TypeName)               \
@@ -155,8 +157,8 @@ ResultType HeapVisitor<ResultType, ConcreteVisitor>::Visit(Map map,
 template <typename ResultType, typename ConcreteVisitor>
 template <VisitorId visitor_id>
 void HeapVisitor<ResultType, ConcreteVisitor>::VisitMapPointerIfNeeded(
-    HeapObject host) {
-  DCHECK(!host.map_word(cage_base(), kRelaxedLoad).IsForwardingAddress());
+    Tagged<HeapObject> host) {
+  DCHECK(!host->map_word(cage_base(), kRelaxedLoad).IsForwardingAddress());
   if constexpr (!ConcreteVisitor::ShouldVisitMapPointer()) {
     return;
   }
@@ -192,13 +194,13 @@ TORQUE_VISITOR_ID_LIST(VISIT)
 
 template <typename ResultType, typename ConcreteVisitor>
 ResultType HeapVisitor<ResultType, ConcreteVisitor>::VisitShortcutCandidate(
-    Map map, ConsString object) {
+    Tagged<Map> map, Tagged<ConsString> object) {
   return static_cast<ConcreteVisitor*>(this)->VisitConsString(map, object);
 }
 
 template <typename ResultType, typename ConcreteVisitor>
 ResultType HeapVisitor<ResultType, ConcreteVisitor>::VisitDataObject(
-    Map map, HeapObject object) {
+    Tagged<Map> map, Tagged<HeapObject> object) {
   ConcreteVisitor* visitor = static_cast<ConcreteVisitor*>(this);
   int size = map->instance_size();
   visitor->template VisitMapPointerIfNeeded<VisitorId::kVisitDataObject>(
@@ -216,20 +218,20 @@ ResultType HeapVisitor<ResultType, ConcreteVisitor>::VisitDataObject(
 
 template <typename ResultType, typename ConcreteVisitor>
 ResultType HeapVisitor<ResultType, ConcreteVisitor>::VisitJSObjectFast(
-    Map map, JSObject object) {
+    Tagged<Map> map, Tagged<JSObject> object) {
   return VisitJSObjectSubclass<JSObject, JSObject::FastBodyDescriptor>(map,
                                                                        object);
 }
 
 template <typename ResultType, typename ConcreteVisitor>
 ResultType HeapVisitor<ResultType, ConcreteVisitor>::VisitJSApiObject(
-    Map map, JSObject object) {
+    Tagged<Map> map, Tagged<JSObject> object) {
   return VisitJSObjectSubclass<JSObject, JSObject::BodyDescriptor>(map, object);
 }
 
 template <typename ResultType, typename ConcreteVisitor>
 ResultType HeapVisitor<ResultType, ConcreteVisitor>::VisitStruct(
-    Map map, HeapObject object) {
+    Tagged<Map> map, Tagged<HeapObject> object) {
   ConcreteVisitor* visitor = static_cast<ConcreteVisitor*>(this);
   int size = map->instance_size();
   visitor->template VisitMapPointerIfNeeded<VisitorId::kVisitStruct>(object);
@@ -239,7 +241,7 @@ ResultType HeapVisitor<ResultType, ConcreteVisitor>::VisitStruct(
 
 template <typename ResultType, typename ConcreteVisitor>
 ResultType HeapVisitor<ResultType, ConcreteVisitor>::VisitFreeSpace(
-    Map map, FreeSpace object) {
+    Tagged<Map> map, Tagged<FreeSpace> object) {
   ConcreteVisitor* visitor = static_cast<ConcreteVisitor*>(this);
   visitor->template VisitMapPointerIfNeeded<VisitorId::kVisitFreeSpace>(object);
   return static_cast<ResultType>(object->size(kRelaxedLoad));
@@ -248,7 +250,7 @@ ResultType HeapVisitor<ResultType, ConcreteVisitor>::VisitFreeSpace(
 template <typename ResultType, typename ConcreteVisitor>
 template <typename T, typename TBodyDescriptor>
 ResultType HeapVisitor<ResultType, ConcreteVisitor>::VisitJSObjectSubclass(
-    Map map, T object) {
+    Tagged<Map> map, Tagged<T> object) {
   ConcreteVisitor* visitor = static_cast<ConcreteVisitor*>(this);
   visitor->template VisitMapPointerIfNeeded<VisitorId::kVisitJSObject>(object);
   const int size = TBodyDescriptor::SizeOf(map, object);
@@ -270,14 +272,16 @@ ConcurrentHeapVisitor<ResultType, ConcreteVisitor>::ConcurrentHeapVisitor(
 
 template <typename T>
 struct ConcurrentVisitorCastHelper {
-  static V8_INLINE T Cast(HeapObject object) { return T::cast(object); }
+  static V8_INLINE Tagged<T> Cast(Tagged<HeapObject> object) {
+    return T::cast(object);
+  }
 };
 
-#define UNCHECKED_CAST(VisitorId, TypeName)                       \
-  template <>                                                     \
-  V8_INLINE TypeName ConcurrentVisitorCastHelper<TypeName>::Cast( \
-      HeapObject object) {                                        \
-    return TypeName::unchecked_cast(object);                      \
+#define UNCHECKED_CAST(VisitorId, TypeName)                               \
+  template <>                                                             \
+  V8_INLINE Tagged<TypeName> ConcurrentVisitorCastHelper<TypeName>::Cast( \
+      Tagged<HeapObject> object) {                                        \
+    return TypeName::unchecked_cast(object);                              \
   }
 SAFE_STRING_TRANSITION_SOURCES(UNCHECKED_CAST)
 // Casts are also needed for unsafe ones for the initial dispatch in
@@ -287,7 +291,8 @@ UNSAFE_STRING_TRANSITION_SOURCES(UNCHECKED_CAST)
 
 template <typename ResultType, typename ConcreteVisitor>
 template <typename T>
-T ConcurrentHeapVisitor<ResultType, ConcreteVisitor>::Cast(HeapObject object) {
+Tagged<T> ConcurrentHeapVisitor<ResultType, ConcreteVisitor>::Cast(
+    Tagged<HeapObject> object) {
   if constexpr (ConcreteVisitor::EnableConcurrentVisitation()) {
     return ConcurrentVisitorCastHelper<T>::Cast(object);
   }
@@ -298,7 +303,7 @@ T ConcurrentHeapVisitor<ResultType, ConcreteVisitor>::Cast(HeapObject object) {
   template <typename ResultType, typename ConcreteVisitor>                    \
   ResultType                                                                  \
       ConcurrentHeapVisitor<ResultType, ConcreteVisitor>::Visit##TypeName(    \
-          Map map, TypeName object) {                                         \
+          Tagged<Map> map, Tagged<TypeName> object) {                         \
     if constexpr (ConcreteVisitor::EnableConcurrentVisitation()) {            \
       return VisitStringLocked(object);                                       \
     }                                                                         \
@@ -311,14 +316,15 @@ UNSAFE_STRING_TRANSITION_SOURCES(VISIT_AS_LOCKED_STRING)
 
 template <typename ResultType, typename ConcreteVisitor>
 template <typename T>
-ResultType ConcurrentHeapVisitor<ResultType,
-                                 ConcreteVisitor>::VisitStringLocked(T object) {
+ResultType
+ConcurrentHeapVisitor<ResultType, ConcreteVisitor>::VisitStringLocked(
+    Tagged<T> object) {
   ConcreteVisitor* visitor = static_cast<ConcreteVisitor*>(this);
   SharedObjectLockGuard guard(object);
   // The object has been locked. At this point shared read access is
   // guaranteed but we must re-read the map and check whether the string has
   // transitioned.
-  Map map = object.map(visitor->cage_base());
+  Tagged<Map> map = object->map(visitor->cage_base());
   int size;
   switch (map->visitor_id()) {
 #define UNSAFE_STRING_TRANSITION_TARGET_CASE(VisitorIdType, TypeName)         \
diff --git a/src/heap/objects-visiting.cc b/src/heap/objects-visiting.cc
index 213fb0e0002..08655597634 100644
--- a/src/heap/objects-visiting.cc
+++ b/src/heap/objects-visiting.cc
@@ -25,9 +25,10 @@ template <class T>
 struct WeakListVisitor;
 
 template <class T>
-Object VisitWeakList(Heap* heap, Object list, WeakObjectRetainer* retainer) {
-  HeapObject undefined = ReadOnlyRoots(heap).undefined_value();
-  Object head = undefined;
+Tagged<Object> VisitWeakList(Heap* heap, Tagged<Object> list,
+                             WeakObjectRetainer* retainer) {
+  Tagged<HeapObject> undefined = ReadOnlyRoots(heap).undefined_value();
+  Tagged<Object> head = undefined;
   T tail;
   bool record_slots = MustRecordSlots(heap);
 
@@ -35,7 +36,7 @@ Object VisitWeakList(Heap* heap, Object list, WeakObjectRetainer* retainer) {
     // Check whether to keep the candidate in the list.
     T candidate = T::cast(list);
 
-    Object retained = retainer->RetainAs(list);
+    Tagged<Object> retained = retainer->RetainAs(list);
 
     // Move to the next element before the WeakNext is cleared.
     list = WeakListVisitor<T>::WeakNext(candidate);
@@ -49,7 +50,8 @@ Object VisitWeakList(Heap* heap, Object list, WeakObjectRetainer* retainer) {
         DCHECK(!tail.is_null());
         WeakListVisitor<T>::SetWeakNext(tail, HeapObject::cast(retained));
         if (record_slots) {
-          HeapObject slot_holder = WeakListVisitor<T>::WeakNextHolder(tail);
+          Tagged<HeapObject> slot_holder =
+              WeakListVisitor<T>::WeakNextHolder(tail);
           int slot_offset = WeakListVisitor<T>::WeakNextOffset();
           ObjectSlot slot = slot_holder->RawField(slot_offset);
           MarkCompactCollector::RecordSlot(slot_holder, slot,
@@ -75,8 +77,8 @@ Object VisitWeakList(Heap* heap, Object list, WeakObjectRetainer* retainer) {
 }
 
 template <class T>
-static void ClearWeakList(Heap* heap, Object list) {
-  Object undefined = ReadOnlyRoots(heap).undefined_value();
+static void ClearWeakList(Heap* heap, Tagged<Object> list) {
+  Tagged<Object> undefined = ReadOnlyRoots(heap).undefined_value();
   while (list != undefined) {
     T candidate = T::cast(list);
     list = WeakListVisitor<T>::WeakNext(candidate);
@@ -86,21 +88,23 @@ static void ClearWeakList(Heap* heap, Object list) {
 
 template <>
 struct WeakListVisitor<Context> {
-  static void SetWeakNext(Context context, Object next) {
+  static void SetWeakNext(Tagged<Context> context, Tagged<Object> next) {
     context->set(Context::NEXT_CONTEXT_LINK, next, UPDATE_WRITE_BARRIER);
   }
 
-  static Object WeakNext(Context context) {
+  static Tagged<Object> WeakNext(Tagged<Context> context) {
     return context->next_context_link();
   }
 
-  static HeapObject WeakNextHolder(Context context) { return context; }
+  static Tagged<HeapObject> WeakNextHolder(Tagged<Context> context) {
+    return context;
+  }
 
   static int WeakNextOffset() {
     return FixedArray::SizeFor(Context::NEXT_CONTEXT_LINK);
   }
 
-  static void VisitLiveObject(Heap* heap, Context context,
+  static void VisitLiveObject(Heap* heap, Tagged<Context> context,
                               WeakObjectRetainer* retainer) {
     if (heap->gc_state() == Heap::MARK_COMPACT) {
       // Record the slots of the weak entries in the native context.
@@ -114,10 +118,11 @@ struct WeakListVisitor<Context> {
   }
 
   template <class T>
-  static void DoWeakList(Heap* heap, Context context,
+  static void DoWeakList(Heap* heap, Tagged<Context> context,
                          WeakObjectRetainer* retainer, int index) {
     // Visit the weak list, removing dead intermediate elements.
-    Object list_head = VisitWeakList<T>(heap, context->get(index), retainer);
+    Tagged<Object> list_head =
+        VisitWeakList<T>(heap, context->get(index), retainer);
 
     // Update the list head.
     context->set(index, list_head, UPDATE_WRITE_BARRIER);
@@ -130,58 +135,66 @@ struct WeakListVisitor<Context> {
     }
   }
 
-  static void VisitPhantomObject(Heap* heap, Context context) {}
+  static void VisitPhantomObject(Heap* heap, Tagged<Context> context) {}
 };
 
 
 template <>
 struct WeakListVisitor<AllocationSite> {
-  static void SetWeakNext(AllocationSite obj, Object next) {
+  static void SetWeakNext(Tagged<AllocationSite> obj, Tagged<Object> next) {
     obj->set_weak_next(next, UPDATE_WRITE_BARRIER);
   }
 
-  static Object WeakNext(AllocationSite obj) { return obj->weak_next(); }
+  static Tagged<Object> WeakNext(Tagged<AllocationSite> obj) {
+    return obj->weak_next();
+  }
 
-  static HeapObject WeakNextHolder(AllocationSite obj) { return obj; }
+  static Tagged<HeapObject> WeakNextHolder(Tagged<AllocationSite> obj) {
+    return obj;
+  }
 
   static int WeakNextOffset() { return AllocationSite::kWeakNextOffset; }
 
-  static void VisitLiveObject(Heap*, AllocationSite, WeakObjectRetainer*) {}
+  static void VisitLiveObject(Heap*, Tagged<AllocationSite>,
+                              WeakObjectRetainer*) {}
 
-  static void VisitPhantomObject(Heap*, AllocationSite) {}
+  static void VisitPhantomObject(Heap*, Tagged<AllocationSite>) {}
 };
 
 template <>
 struct WeakListVisitor<JSFinalizationRegistry> {
-  static void SetWeakNext(JSFinalizationRegistry obj, HeapObject next) {
+  static void SetWeakNext(Tagged<JSFinalizationRegistry> obj,
+                          Tagged<HeapObject> next) {
     obj->set_next_dirty(next, UPDATE_WRITE_BARRIER);
   }
 
-  static Object WeakNext(JSFinalizationRegistry obj) {
+  static Tagged<Object> WeakNext(Tagged<JSFinalizationRegistry> obj) {
     return obj->next_dirty();
   }
 
-  static HeapObject WeakNextHolder(JSFinalizationRegistry obj) { return obj; }
+  static Tagged<HeapObject> WeakNextHolder(Tagged<JSFinalizationRegistry> obj) {
+    return obj;
+  }
 
   static int WeakNextOffset() {
     return JSFinalizationRegistry::kNextDirtyOffset;
   }
 
-  static void VisitLiveObject(Heap* heap, JSFinalizationRegistry obj,
+  static void VisitLiveObject(Heap* heap, Tagged<JSFinalizationRegistry> obj,
                               WeakObjectRetainer*) {
     heap->set_dirty_js_finalization_registries_list_tail(obj);
   }
 
-  static void VisitPhantomObject(Heap*, JSFinalizationRegistry) {}
+  static void VisitPhantomObject(Heap*, Tagged<JSFinalizationRegistry>) {}
 };
 
-template Object VisitWeakList<Context>(Heap* heap, Object list,
-                                       WeakObjectRetainer* retainer);
+template Tagged<Object> VisitWeakList<Context>(Heap* heap, Tagged<Object> list,
+                                               WeakObjectRetainer* retainer);
 
-template Object VisitWeakList<AllocationSite>(Heap* heap, Object list,
-                                              WeakObjectRetainer* retainer);
+template Tagged<Object> VisitWeakList<AllocationSite>(
+    Heap* heap, Tagged<Object> list, WeakObjectRetainer* retainer);
 
-template Object VisitWeakList<JSFinalizationRegistry>(
-    Heap* heap, Object list, WeakObjectRetainer* retainer);
+template Tagged<Object> VisitWeakList<JSFinalizationRegistry>(
+    Heap* heap, Tagged<Object> list, WeakObjectRetainer* retainer);
 }  // namespace internal
 }  // namespace v8
diff --git a/src/heap/objects-visiting.h b/src/heap/objects-visiting.h
index 5f77843a16f..a81fadb7ce9 100644
--- a/src/heap/objects-visiting.h
+++ b/src/heap/objects-visiting.h
@@ -108,8 +108,8 @@ class HeapVisitor : public ObjectVisitorWithCageBases {
   inline explicit HeapVisitor(Isolate* isolate);
   inline explicit HeapVisitor(Heap* heap);
 
-  V8_INLINE ResultType Visit(HeapObject object);
-  V8_INLINE ResultType Visit(Map map, HeapObject object);
+  V8_INLINE ResultType Visit(Tagged<HeapObject> object);
+  V8_INLINE ResultType Visit(Tagged<Map> map, Tagged<HeapObject> object);
 
  protected:
   // If this predicate returns false the default implementations of Visit*
@@ -123,7 +123,7 @@ class HeapVisitor : public ObjectVisitorWithCageBases {
 
   // Only visits the Map pointer if `ShouldVisitMapPointer()` returns true.
   template <VisitorId visitor_id>
-  V8_INLINE void VisitMapPointerIfNeeded(HeapObject host);
+  V8_INLINE void VisitMapPointerIfNeeded(Tagged<HeapObject> host);
 
   ConcreteVisitor* concrete_visitor() {
     return static_cast<ConcreteVisitor*>(this);
@@ -138,18 +138,23 @@ class HeapVisitor : public ObjectVisitorWithCageBases {
   TYPED_VISITOR_ID_LIST(VISIT)
   TORQUE_VISITOR_ID_LIST(VISIT)
 #undef VISIT
-  V8_INLINE ResultType VisitShortcutCandidate(Map map, ConsString object);
-  V8_INLINE ResultType VisitDataObject(Map map, HeapObject object);
-  V8_INLINE ResultType VisitJSObjectFast(Map map, JSObject object);
-  V8_INLINE ResultType VisitJSApiObject(Map map, JSObject object);
-  V8_INLINE ResultType VisitStruct(Map map, HeapObject object);
-  V8_INLINE ResultType VisitFreeSpace(Map map, FreeSpace object);
+  V8_INLINE ResultType VisitShortcutCandidate(Tagged<Map> map,
+                                              Tagged<ConsString> object);
+  V8_INLINE ResultType VisitDataObject(Tagged<Map> map,
+                                       Tagged<HeapObject> object);
+  V8_INLINE ResultType VisitJSObjectFast(Tagged<Map> map,
+                                         Tagged<JSObject> object);
+  V8_INLINE ResultType VisitJSApiObject(Tagged<Map> map,
+                                        Tagged<JSObject> object);
+  V8_INLINE ResultType VisitStruct(Tagged<Map> map, Tagged<HeapObject> object);
+  V8_INLINE ResultType VisitFreeSpace(Tagged<Map> map,
+                                      Tagged<FreeSpace> object);
 
   template <typename T, typename TBodyDescriptor = typename T::BodyDescriptor>
-  V8_INLINE ResultType VisitJSObjectSubclass(Map map, T object);
+  V8_INLINE ResultType VisitJSObjectSubclass(Tagged<Map> map, Tagged<T> object);
 
   template <typename T>
-  static V8_INLINE T Cast(HeapObject object);
+  static V8_INLINE Tagged<T> Cast(Tagged<HeapObject> object);
 };
 
 // These strings can be sources of safe string transitions. Transitions are safe
@@ -186,18 +191,19 @@ class ConcurrentHeapVisitor : public HeapVisitor<ResultType, ConcreteVisitor> {
   V8_INLINE static constexpr bool EnableConcurrentVisitation() { return false; }
 
  protected:
-#define VISIT_AS_LOCKED_STRING(VisitorId, TypeName) \
-  V8_INLINE ResultType Visit##TypeName(Map map, TypeName object);
+#define VISIT_AS_LOCKED_STRING(VisitorId, TypeName)     \
+  V8_INLINE ResultType Visit##TypeName(Tagged<Map> map, \
+                                       Tagged<TypeName> object);
 
   UNSAFE_STRING_TRANSITION_SOURCES(VISIT_AS_LOCKED_STRING)
 #undef VISIT_AS_LOCKED_STRING
 
   template <typename T>
-  static V8_INLINE T Cast(HeapObject object);
+  static V8_INLINE Tagged<T> Cast(Tagged<HeapObject> object);
 
  private:
   template <typename T>
-  V8_INLINE ResultType VisitStringLocked(T object);
+  V8_INLINE ResultType VisitStringLocked(Tagged<T> object);
 
   friend class HeapVisitor<ResultType, ConcreteVisitor>;
 };
@@ -209,26 +215,29 @@ class NewSpaceVisitor : public ConcurrentHeapVisitor<int, ConcreteVisitor> {
 
   // Special cases: Unreachable visitors for objects that are never found in the
   // young generation.
-  void VisitInstructionStreamPointer(Code, InstructionStreamSlot) final {
+  void VisitInstructionStreamPointer(Tagged<Code>,
+                                     InstructionStreamSlot) final {
     UNREACHABLE();
   }
-  void VisitCodeTarget(InstructionStream host, RelocInfo*) final {
+  void VisitCodeTarget(Tagged<InstructionStream> host, RelocInfo*) final {
     UNREACHABLE();
   }
-  void VisitEmbeddedPointer(InstructionStream host, RelocInfo*) final {
+  void VisitEmbeddedPointer(Tagged<InstructionStream> host, RelocInfo*) final {
     UNREACHABLE();
   }
-  void VisitMapPointer(HeapObject) override { UNREACHABLE(); }
+  void VisitMapPointer(Tagged<HeapObject>) override { UNREACHABLE(); }
 
  protected:
   V8_INLINE static constexpr bool ShouldVisitMapPointer() { return false; }
 
   // Special cases: Unreachable visitors for objects that are never found in the
   // young generation.
-  int VisitNativeContext(Map, NativeContext) { UNREACHABLE(); }
-  int VisitBytecodeArray(Map, BytecodeArray) { UNREACHABLE(); }
-  int VisitSharedFunctionInfo(Map map, SharedFunctionInfo) { UNREACHABLE(); }
-  int VisitWeakCell(Map, WeakCell) { UNREACHABLE(); }
+  int VisitNativeContext(Tagged<Map>, Tagged<NativeContext>) { UNREACHABLE(); }
+  int VisitBytecodeArray(Tagged<Map>, Tagged<BytecodeArray>) { UNREACHABLE(); }
+  int VisitSharedFunctionInfo(Tagged<Map> map, Tagged<SharedFunctionInfo>) {
+    UNREACHABLE();
+  }
+  int VisitWeakCell(Tagged<Map>, Tagged<WeakCell>) { UNREACHABLE(); }
 
   friend class HeapVisitor<int, ConcreteVisitor>;
 };
@@ -241,7 +250,8 @@ class WeakObjectRetainer;
 // pointers. The template parameter T is a WeakListVisitor that defines how to
 // access the next-element pointers.
 template <class T>
-Object VisitWeakList(Heap* heap, Object list, WeakObjectRetainer* retainer);
+Tagged<Object> VisitWeakList(Heap* heap, Tagged<Object> list,
+                             WeakObjectRetainer* retainer);
 }  // namespace internal
 }  // namespace v8
 
diff --git a/src/heap/page.cc b/src/heap/page.cc
index 54e1812bada..fe3e68732d5 100644
--- a/src/heap/page.cc
+++ b/src/heap/page.cc
@@ -89,7 +89,7 @@ void Page::MarkNeverAllocateForTesting() {
 namespace {
 // Skips filler starting from the given filler until the end address.
 // Returns the first address after the skipped fillers.
-Address SkipFillers(PtrComprCageBase cage_base, HeapObject filler,
+Address SkipFillers(PtrComprCageBase cage_base, Tagged<HeapObject> filler,
                     Address end) {
   Address addr = filler.address();
   while (addr < end) {
@@ -110,7 +110,7 @@ size_t Page::ShrinkToHighWaterMark() {
 
   // Shrink pages to high water mark. The water mark points either to a filler
   // or the area_end.
-  HeapObject filler = HeapObject::FromAddress(HighWaterMark());
+  Tagged<HeapObject> filler = HeapObject::FromAddress(HighWaterMark());
   if (filler.address() == area_end()) return 0;
   PtrComprCageBase cage_base(heap()->isolate());
   CHECK(IsFreeSpaceOrFiller(filler, cage_base));
diff --git a/src/heap/page.h b/src/heap/page.h
index 103a980d1e8..aaa0ca4b61a 100644
--- a/src/heap/page.h
+++ b/src/heap/page.h
@@ -39,7 +39,7 @@ class Page : public MemoryChunk {
     DCHECK(!V8_ENABLE_THIRD_PARTY_HEAP_BOOL);
     return reinterpret_cast<Page*>(addr & ~kPageAlignmentMask);
   }
-  static Page* FromHeapObject(HeapObject o) {
+  static Page* FromHeapObject(Tagged<HeapObject> o) {
     DCHECK(!V8_ENABLE_THIRD_PARTY_HEAP_BOOL);
     return reinterpret_cast<Page*>(o.ptr() & ~kAlignmentMask);
   }
diff --git a/src/heap/paged-spaces-inl.h b/src/heap/paged-spaces-inl.h
index 9c1e9091ddc..c555d84ae87 100644
--- a/src/heap/paged-spaces-inl.h
+++ b/src/heap/paged-spaces-inl.h
@@ -44,10 +44,10 @@ void HeapObjectRange::iterator::AdvanceToNextObject() {
     Tagged<HeapObject> obj = HeapObject::FromAddress(cur_addr_);
     cur_size_ = ALIGN_TO_ALLOCATION_ALIGNMENT(obj->Size(cage_base()));
     DCHECK_LE(cur_addr_ + cur_size_, cur_end_);
-    if (IsFreeSpaceOrFiller(*obj, cage_base())) {
+    if (IsFreeSpaceOrFiller(obj, cage_base())) {
       cur_addr_ += cur_size_;
     } else {
-      if (IsInstructionStream(*obj, cage_base())) {
+      if (IsInstructionStream(obj, cage_base())) {
         DCHECK_EQ(Page::FromHeapObject(obj)->owner_identity(), CODE_SPACE);
         DCHECK_CODEOBJECT_SIZE(cur_size_);
       } else {
@@ -79,7 +79,7 @@ bool PagedSpaceBase::Contains(Address addr) const {
   return Page::FromAddress(addr)->owner() == this;
 }
 
-bool PagedSpaceBase::Contains(Object o) const {
+bool PagedSpaceBase::Contains(Tagged<Object> o) const {
   if (!IsHeapObject(o)) return false;
   return Page::FromAddress(o.ptr())->owner() == this;
 }
diff --git a/src/heap/paged-spaces.cc b/src/heap/paged-spaces.cc
index ed641cb27c9..b3394dcec84 100644
--- a/src/heap/paged-spaces.cc
+++ b/src/heap/paged-spaces.cc
@@ -565,7 +565,7 @@ bool PagedSpaceBase::TryAllocationFromFreeListMain(size_t size_in_bytes,
   FreeLinearAllocationArea();
 
   size_t new_node_size = 0;
-  FreeSpace new_node =
+  Tagged<FreeSpace> new_node =
       free_list_->Allocate(size_in_bytes, &new_node_size, origin);
   if (new_node.is_null()) return false;
   DCHECK_GE(new_node_size, size_in_bytes);
@@ -611,7 +611,7 @@ PagedSpaceBase::TryAllocationFromFreeListBackground(size_t min_size_in_bytes,
          identity() == SHARED_SPACE);
 
   size_t new_node_size = 0;
-  FreeSpace new_node =
+  Tagged<FreeSpace> new_node =
       free_list_->Allocate(min_size_in_bytes, &new_node_size, origin);
   if (new_node.is_null()) return {};
   DCHECK_GE(new_node_size, min_size_in_bytes);
@@ -670,7 +670,7 @@ void PagedSpaceBase::Verify(Isolate* isolate,
     Address end_of_previous_object = page->area_start();
     Address top = page->area_end();
 
-    for (HeapObject object : HeapObjectRange(page)) {
+    for (Tagged<HeapObject> object : HeapObjectRange(page)) {
       CHECK(end_of_previous_object <= object.address());
 
       // Invoke verification method for each object.
@@ -682,7 +682,7 @@ void PagedSpaceBase::Verify(Isolate* isolate,
       end_of_previous_object = object.address() + size;
 
       if (IsExternalString(object, cage_base)) {
-        ExternalString external_string = ExternalString::cast(object);
+        Tagged<ExternalString> external_string = ExternalString::cast(object);
         size_t payload_size = external_string->ExternalPayloadSize();
         external_page_bytes[static_cast<int>(
             ExternalBackingStoreType::kExternalString)] += payload_size;
@@ -731,7 +731,7 @@ void PagedSpaceBase::VerifyLiveBytes() const {
   for (const Page* page : *this) {
     CHECK(page->SweepingDone());
     int black_size = 0;
-    for (HeapObject object : HeapObjectRange(page)) {
+    for (Tagged<HeapObject> object : HeapObjectRange(page)) {
       // All the interior pointers should be contained in the heap.
       if (marking_state->IsMarked(object)) {
         black_size += object->Size(cage_base);
diff --git a/src/heap/paged-spaces.h b/src/heap/paged-spaces.h
index 962d3d1ec57..a7901aab660 100644
--- a/src/heap/paged-spaces.h
+++ b/src/heap/paged-spaces.h
@@ -128,7 +128,7 @@ class V8_EXPORT_PRIVATE PagedSpaceBase
 
   // Checks whether an object/address is in this space.
   inline bool Contains(Address a) const;
-  inline bool Contains(Object o) const;
+  inline bool Contains(Tagged<Object> o) const;
   bool ContainsSlow(Address addr) const;
 
   // Does the space need executable memory?
diff --git a/src/heap/pretenuring-handler-inl.h b/src/heap/pretenuring-handler-inl.h
index a5dda61230f..5ed6522fd76 100644
--- a/src/heap/pretenuring-handler-inl.h
+++ b/src/heap/pretenuring-handler-inl.h
@@ -17,7 +17,8 @@ namespace v8 {
 namespace internal {
 
 void PretenuringHandler::UpdateAllocationSite(
-    Map map, HeapObject object, PretenuringFeedbackMap* pretenuring_feedback) {
+    Tagged<Map> map, Tagged<HeapObject> object,
+    PretenuringFeedbackMap* pretenuring_feedback) {
   DCHECK_NE(pretenuring_feedback, &global_pretenuring_feedback_);
 #ifdef DEBUG
   BasicMemoryChunk* chunk = BasicMemoryChunk::FromHeapObject(object);
@@ -29,7 +30,7 @@ void PretenuringHandler::UpdateAllocationSite(
       !AllocationSite::CanTrack(map->instance_type())) {
     return;
   }
-  AllocationMemento memento_candidate =
+  Tagged<AllocationMemento> memento_candidate =
       FindAllocationMemento<kForGC>(map, object);
   if (memento_candidate.is_null()) return;
   DCHECK(IsJSObjectMap(map));
@@ -42,8 +43,8 @@ void PretenuringHandler::UpdateAllocationSite(
 }
 
 template <PretenuringHandler::FindMementoMode mode>
-AllocationMemento PretenuringHandler::FindAllocationMemento(Map map,
-                                                            HeapObject object) {
+Tagged<AllocationMemento> PretenuringHandler::FindAllocationMemento(
+    Tagged<Map> map, Tagged<HeapObject> object) {
   Address object_address = object.address();
   Address memento_address =
       object_address + ALIGN_TO_ALLOCATION_ALIGNMENT(object->SizeFromMap(map));
@@ -59,7 +60,7 @@ AllocationMemento PretenuringHandler::FindAllocationMemento(Map map,
   if (mode != FindMementoMode::kForGC && !object_page->SweepingDone())
     return AllocationMemento();
 
-  HeapObject candidate = HeapObject::FromAddress(memento_address);
+  Tagged<HeapObject> candidate = HeapObject::FromAddress(memento_address);
   ObjectSlot candidate_map_slot = candidate->map_slot();
   // This fast check may peek at an uninitialized word. However, the slow check
   // below (memento_address == top) ensures that this is safe. Mark the word as
@@ -84,7 +85,8 @@ AllocationMemento PretenuringHandler::FindAllocationMemento(Map map,
     }
   }
 
-  AllocationMemento memento_candidate = AllocationMemento::cast(candidate);
+  Tagged<AllocationMemento> memento_candidate =
+      AllocationMemento::cast(candidate);
 
   // Depending on what the memento is used for, we might need to perform
   // additional checks.
diff --git a/src/heap/pretenuring-handler.cc b/src/heap/pretenuring-handler.cc
index b413baaac00..4e3ee77773b 100644
--- a/src/heap/pretenuring-handler.cc
+++ b/src/heap/pretenuring-handler.cc
@@ -38,8 +38,9 @@ double GetPretenuringRatioThreshold(size_t new_space_capacity) {
 }
 
 inline bool MakePretenureDecision(
-    AllocationSite site, AllocationSite::PretenureDecision current_decision,
-    double ratio, bool new_space_capacity_was_above_pretenuring_threshold,
+    Tagged<AllocationSite> site,
+    AllocationSite::PretenureDecision current_decision, double ratio,
+    bool new_space_capacity_was_above_pretenuring_threshold,
     size_t new_space_capacity) {
   // Here we just allow state transitions from undecided or maybe tenure
   // to don't tenure, maybe tenure, or tenure.
@@ -64,13 +65,13 @@ inline bool MakePretenureDecision(
 }
 
 // Clear feedback calculation fields until the next gc.
-inline void ResetPretenuringFeedback(AllocationSite site) {
+inline void ResetPretenuringFeedback(Tagged<AllocationSite> site) {
   site->set_memento_found_count(0);
   site->set_memento_create_count(0);
 }
 
 inline bool DigestPretenuringFeedback(
-    Isolate* isolate, AllocationSite site,
+    Isolate* isolate, Tagged<AllocationSite> site,
     bool new_space_capacity_was_above_pretenuring_threshold,
     size_t new_space_capacity) {
   bool deopt = false;
@@ -103,7 +104,8 @@ inline bool DigestPretenuringFeedback(
   return deopt;
 }
 
-bool PretenureAllocationSiteManually(Isolate* isolate, AllocationSite site) {
+bool PretenureAllocationSiteManually(Isolate* isolate,
+                                     Tagged<AllocationSite> site) {
   AllocationSite::PretenureDecision current_decision =
       site->pretenure_decision();
   bool deopt = true;
@@ -137,7 +139,7 @@ int PretenuringHandler::GetMinMementoCountForTesting() {
 void PretenuringHandler::MergeAllocationSitePretenuringFeedback(
     const PretenuringFeedbackMap& local_pretenuring_feedback) {
   PtrComprCageBase cage_base(heap_->isolate());
-  AllocationSite site;
+  Tagged<AllocationSite> site;
   for (auto& site_and_count : local_pretenuring_feedback) {
     site = site_and_count.first;
     MapWord map_word = site->map_word(cage_base, kRelaxedLoad);
@@ -160,7 +162,7 @@ void PretenuringHandler::MergeAllocationSitePretenuringFeedback(
 }
 
 void PretenuringHandler::RemoveAllocationSitePretenuringFeedback(
-    AllocationSite site) {
+    Tagged<AllocationSite> site) {
   global_pretenuring_feedback_.erase(site);
 }
 
@@ -188,7 +190,7 @@ void PretenuringHandler::ProcessPretenuringFeedback(
   int allocation_sites = 0;
   int active_allocation_sites = 0;
 
-  AllocationSite site;
+  Tagged<AllocationSite> site;
 
   // Step 1: Digest feedback for recorded allocation sites.
   // This is the pretenuring trigger for allocation sites that are in maybe
@@ -242,16 +244,16 @@ void PretenuringHandler::ProcessPretenuringFeedback(
                               min_new_space_capacity_for_pretenuring) &&
                              !new_space_was_above_pretenuring_threshold;
   if (deopt_maybe_tenured) {
-    heap_->ForeachAllocationSite(
-        heap_->allocation_sites_list(),
-        [&allocation_sites, &trigger_deoptimization](AllocationSite site) {
-          DCHECK(IsAllocationSite(site));
-          allocation_sites++;
-          if (site->IsMaybeTenure()) {
-            site->set_deopt_dependent_code(true);
-            trigger_deoptimization = true;
-          }
-        });
+    heap_->ForeachAllocationSite(heap_->allocation_sites_list(),
+                                 [&allocation_sites, &trigger_deoptimization](
+                                     Tagged<AllocationSite> site) {
+                                   DCHECK(IsAllocationSite(site));
+                                   allocation_sites++;
+                                   if (site->IsMaybeTenure()) {
+                                     site->set_deopt_dependent_code(true);
+                                     trigger_deoptimization = true;
+                                   }
+                                 });
   }
 
   if (trigger_deoptimization) {
@@ -276,7 +278,7 @@ void PretenuringHandler::ProcessPretenuringFeedback(
 }
 
 void PretenuringHandler::PretenureAllocationSiteOnNextCollection(
-    AllocationSite site) {
+    Tagged<AllocationSite> site) {
   if (!allocation_sites_to_pretenure_) {
     allocation_sites_to_pretenure_.reset(
         new GlobalHandleVector<AllocationSite>(heap_));
diff --git a/src/heap/pretenuring-handler.h b/src/heap/pretenuring-handler.h
index 7728ba81be5..87a1d4d01ff 100644
--- a/src/heap/pretenuring-handler.h
+++ b/src/heap/pretenuring-handler.h
@@ -34,7 +34,8 @@ class PretenuringHandler final {
   // If an object has an AllocationMemento trailing it, return it, otherwise
   // return a null AllocationMemento.
   template <FindMementoMode mode>
-  inline AllocationMemento FindAllocationMemento(Map map, HeapObject object);
+  inline Tagged<AllocationMemento> FindAllocationMemento(
+      Tagged<Map> map, Tagged<HeapObject> object);
 
   // ===========================================================================
   // Allocation site tracking. =================================================
@@ -43,7 +44,8 @@ class PretenuringHandler final {
   // Updates the AllocationSite of a given {object}. The entry (including the
   // count) is cached on the local pretenuring feedback.
   inline void UpdateAllocationSite(
-      Map map, HeapObject object, PretenuringFeedbackMap* pretenuring_feedback);
+      Tagged<Map> map, Tagged<HeapObject> object,
+      PretenuringFeedbackMap* pretenuring_feedback);
 
   // Merges local pretenuring feedback into the global one. Note that this
   // method needs to be called after evacuation, as allocation sites may be
@@ -55,7 +57,7 @@ class PretenuringHandler final {
   // next collection. Added allocation sites are pretenured independent of
   // their feedback.
   V8_EXPORT_PRIVATE void PretenureAllocationSiteOnNextCollection(
-      AllocationSite site);
+      Tagged<AllocationSite> site);
 
   // ===========================================================================
   // Pretenuring. ==============================================================
@@ -67,7 +69,7 @@ class PretenuringHandler final {
   void ProcessPretenuringFeedback(size_t new_space_capacity_before_gc);
 
   // Removes an entry from the global pretenuring storage.
-  void RemoveAllocationSitePretenuringFeedback(AllocationSite site);
+  void RemoveAllocationSitePretenuringFeedback(Tagged<AllocationSite> site);
 
   bool HasPretenuringFeedback() const {
     return !global_pretenuring_feedback_.empty();
diff --git a/src/heap/read-only-heap-inl.h b/src/heap/read-only-heap-inl.h
index ad027a7a0d1..81d4b73a484 100644
--- a/src/heap/read-only-heap-inl.h
+++ b/src/heap/read-only-heap-inl.h
@@ -13,7 +13,7 @@ namespace v8 {
 namespace internal {
 
 // static
-ReadOnlyRoots ReadOnlyHeap::EarlyGetReadOnlyRoots(HeapObject object) {
+ReadOnlyRoots ReadOnlyHeap::EarlyGetReadOnlyRoots(Tagged<HeapObject> object) {
 #ifdef V8_SHARED_RO_HEAP
   auto* shared_ro_heap = SoleReadOnlyHeap::shared_ro_heap_;
   if (shared_ro_heap && shared_ro_heap->roots_init_complete()) {
@@ -26,7 +26,7 @@ ReadOnlyRoots ReadOnlyHeap::EarlyGetReadOnlyRoots(HeapObject object) {
 }
 
 // static
-ReadOnlyRoots ReadOnlyHeap::GetReadOnlyRoots(HeapObject object) {
+ReadOnlyRoots ReadOnlyHeap::GetReadOnlyRoots(Tagged<HeapObject> object) {
 #ifdef V8_SHARED_RO_HEAP
   auto* shared_ro_heap = SoleReadOnlyHeap::shared_ro_heap_;
   // If this check fails in code that runs during initialization use
diff --git a/src/heap/read-only-heap.cc b/src/heap/read-only-heap.cc
index 80663c09b0d..d39e5c3c711 100644
--- a/src/heap/read-only-heap.cc
+++ b/src/heap/read-only-heap.cc
@@ -282,7 +282,7 @@ bool ReadOnlyHeap::Contains(Address address) {
 }
 
 // static
-bool ReadOnlyHeap::Contains(HeapObject object) {
+bool ReadOnlyHeap::Contains(Tagged<HeapObject> object) {
   if (V8_ENABLE_THIRD_PARTY_HEAP_BOOL) {
     return third_party_heap::Heap::InReadOnlySpace(object.address());
   } else {
@@ -334,7 +334,7 @@ ReadOnlyPageObjectIterator::ReadOnlyPageObjectIterator(
   DCHECK_LT(current_addr, page->GetAreaStart() + page->area_size());
 }
 
-HeapObject ReadOnlyPageObjectIterator::Next() {
+Tagged<HeapObject> ReadOnlyPageObjectIterator::Next() {
   if (page_ == nullptr) return HeapObject();
 
   Address end = page_->GetAreaStart() + page_->area_size();
@@ -342,7 +342,7 @@ HeapObject ReadOnlyPageObjectIterator::Next() {
     DCHECK_LE(current_addr_, end);
     if (current_addr_ == end) return HeapObject();
 
-    HeapObject object = HeapObject::FromAddress(current_addr_);
+    Tagged<HeapObject> object = HeapObject::FromAddress(current_addr_);
     const int object_size = object->Size();
     current_addr_ += ALIGN_TO_ALLOCATION_ALIGNMENT(object_size);
 
diff --git a/src/heap/read-only-heap.h b/src/heap/read-only-heap.h
index ddf0098bb58..0906ad8f5d0 100644
--- a/src/heap/read-only-heap.h
+++ b/src/heap/read-only-heap.h
@@ -72,15 +72,15 @@ class ReadOnlyHeap {
   // Returns whether the address is within the read-only space.
   V8_EXPORT_PRIVATE static bool Contains(Address address);
   // Returns whether the object resides in the read-only space.
-  V8_EXPORT_PRIVATE static bool Contains(HeapObject object);
+  V8_EXPORT_PRIVATE static bool Contains(Tagged<HeapObject> object);
   // Gets read-only roots from an appropriate root list. Shared read only root
   // must be initialized
   V8_EXPORT_PRIVATE inline static ReadOnlyRoots GetReadOnlyRoots(
-      HeapObject object);
+      Tagged<HeapObject> object);
   // Returns the current isolates roots table during initialization as opposed
   // to the shared one in case the latter is not initialized yet.
   V8_EXPORT_PRIVATE inline static ReadOnlyRoots EarlyGetReadOnlyRoots(
-      HeapObject object);
+      Tagged<HeapObject> object);
 
   ReadOnlySpace* read_only_space() const { return read_only_space_; }
 
@@ -171,7 +171,7 @@ class V8_EXPORT_PRIVATE ReadOnlyPageObjectIterator final {
                              SkipFreeSpaceOrFiller skip_free_space_or_filler =
                                  SkipFreeSpaceOrFiller::kYes);
 
-  HeapObject Next();
+  Tagged<HeapObject> Next();
 
  private:
   void Reset(const ReadOnlyPage* page);
diff --git a/src/heap/read-only-promotion.cc b/src/heap/read-only-promotion.cc
index 6d0696a0936..c9e373d49f1 100644
--- a/src/heap/read-only-promotion.cc
+++ b/src/heap/read-only-promotion.cc
@@ -20,10 +20,14 @@ namespace {
 // Convenience aliases:
 using HeapObjectSet =
     std::unordered_set<HeapObject, Object::Hasher, Object::KeyEqualSafe>;
-using HeapObjectMap = std::unordered_map<HeapObject, HeapObject, Object::Hasher,
-                                         Object::KeyEqualSafe>;
-bool Contains(const HeapObjectSet& s, HeapObject o) { return s.count(o) != 0; }
-bool Contains(const HeapObjectMap& s, HeapObject o) { return s.count(o) != 0; }
+using HeapObjectMap = std::unordered_map<Tagged<HeapObject>, Tagged<HeapObject>,
+                                         Object::Hasher, Object::KeyEqualSafe>;
+bool Contains(const HeapObjectSet& s, Tagged<HeapObject> o) {
+  return s.count(o) != 0;
+}
+bool Contains(const HeapObjectMap& s, Tagged<HeapObject> o) {
+  return s.count(o) != 0;
+}
 
 class Committee final {
  public:
@@ -44,7 +48,7 @@ class Committee final {
     // We assume that a full and precise GC has reclaimed all dead objects
     // and therefore that no filtering of unreachable objects is required here.
     HeapObjectIterator it(isolate_->heap(), safepoint_scope);
-    for (HeapObject o = it.Next(); !o.is_null(); o = it.Next()) {
+    for (Tagged<HeapObject> o = it.Next(); !o.is_null(); o = it.Next()) {
       DCHECK(!o.InReadOnlySpace());
 
       // Note that cycles prevent us from promoting/rejecting each subgraph as
@@ -77,7 +81,7 @@ class Committee final {
   // Returns `false` if the subgraph rooted at `o` is rejected.
   // Returns `true` if it is accepted, or if we've reached a cycle and `o`
   // will be processed further up the callchain.
-  bool EvaluateSubgraph(HeapObject o, HeapObjectSet* accepted_subgraph,
+  bool EvaluateSubgraph(Tagged<HeapObject> o, HeapObjectSet* accepted_subgraph,
                         HeapObjectSet* visited) {
     if (o.InReadOnlySpace()) return true;
     if (Contains(promo_rejected_, o)) return false;
@@ -93,7 +97,7 @@ class Committee final {
     }
     // Recurse into outgoing pointers.
     CandidateVisitor v(this, accepted_subgraph, visited);
-    o.Iterate(isolate_, &v);
+    o->Iterate(isolate_, &v);
     if (!v.all_slots_are_promo_candidates()) {
       const auto& [it, inserted] = promo_rejected_.insert(o);
       if (V8_UNLIKELY(v8_flags.trace_read_only_promotion) && inserted) {
@@ -119,8 +123,8 @@ class Committee final {
   // TODO(jgruber): Don't forget to extend ReadOnlyPromotionImpl::Verify when
   // adding new object types here.
 
-  static bool IsPromoCandidate(Isolate* isolate, HeapObject o) {
-    const InstanceType itype = o.map(isolate)->instance_type();
+  static bool IsPromoCandidate(Isolate* isolate, Tagged<HeapObject> o) {
+    const InstanceType itype = o->map(isolate)->instance_type();
 #define V(TYPE)                                            \
   if (InstanceTypeChecker::Is##TYPE(itype)) {              \
     return IsPromoCandidate##TYPE(isolate, TYPE::cast(o)); \
@@ -141,15 +145,15 @@ class Committee final {
   DEF_PROMO_CANDIDATE(AccessCheckInfo)
   DEF_PROMO_CANDIDATE(AccessorInfo)
   DEF_PROMO_CANDIDATE(CallHandlerInfo)
-  static bool IsPromoCandidateCode(Isolate* isolate, Code o) {
-    return Builtins::kCodeObjectsAreInROSpace && o.is_builtin();
+  static bool IsPromoCandidateCode(Isolate* isolate, Tagged<Code> o) {
+    return Builtins::kCodeObjectsAreInROSpace && o->is_builtin();
   }
   DEF_PROMO_CANDIDATE(InterceptorInfo)
   DEF_PROMO_CANDIDATE(ScopeInfo)
   static bool IsPromoCandidateSharedFunctionInfo(Isolate* isolate,
-                                                 SharedFunctionInfo o) {
+                                                 Tagged<SharedFunctionInfo> o) {
     // Only internal SFIs are guaranteed to remain immutable.
-    if (o.has_script(kAcquireLoad)) return false;
+    if (o->has_script(kAcquireLoad)) return false;
     // kIllegal is used for js_global_object_function, which is created during
     // bootstrapping but never rooted. We currently assumed that all objects in
     // the snapshot are live. But RO space is 1) not GC'd and 2) serialized
@@ -158,7 +162,7 @@ class Committee final {
     // TODO(jgruber): A better solution. Remove the liveness assumption (see
     // test-heap-profiler.cc)? Overwrite dead RO objects with fillers
     // pre-serialization? Implement a RO GC pass pre-serialization?
-    return o.HasBuiltinId() && o.builtin_id() != Builtin::kIllegal;
+    return o->HasBuiltinId() && o->builtin_id() != Builtin::kIllegal;
   }
   DEF_PROMO_CANDIDATE(Symbol)
 
@@ -181,12 +185,12 @@ class Committee final {
       return first_rejected_slot_offset_ == -1;
     }
 
-    void VisitPointers(HeapObject host, MaybeObjectSlot start,
+    void VisitPointers(Tagged<HeapObject> host, MaybeObjectSlot start,
                        MaybeObjectSlot end) final {
       if (!all_slots_are_promo_candidates()) return;
       for (MaybeObjectSlot slot = start; slot < end; slot++) {
         MaybeObject maybe_object = slot.load(committee_->isolate_);
-        HeapObject heap_object;
+        Tagged<HeapObject> heap_object;
         if (!maybe_object.GetHeapObject(&heap_object)) continue;
         if (!committee_->EvaluateSubgraph(heap_object, accepted_subgraph_,
                                           visited_)) {
@@ -197,16 +201,16 @@ class Committee final {
         }
       }
     }
-    void VisitPointers(HeapObject host, ObjectSlot start,
+    void VisitPointers(Tagged<HeapObject> host, ObjectSlot start,
                        ObjectSlot end) final {
       VisitPointers(host, MaybeObjectSlot(start), MaybeObjectSlot(end));
     }
-    void VisitInstructionStreamPointer(Code host,
+    void VisitInstructionStreamPointer(Tagged<Code> host,
                                        InstructionStreamSlot slot) final {
-      DCHECK(host.is_builtin());
+      DCHECK(host->is_builtin());
     }
-    void VisitMapPointer(HeapObject host) final {
-      MaybeObjectSlot slot = host.RawMaybeWeakField(HeapObject::kMapOffset);
+    void VisitMapPointer(Tagged<HeapObject> host) final {
+      MaybeObjectSlot slot = host->RawMaybeWeakField(HeapObject::kMapOffset);
       VisitPointers(host, slot, slot + 1);
     }
 
@@ -219,32 +223,32 @@ class Committee final {
 
   static void LogAcceptedPromotionSet(const HeapObjectSet& os) {
     std::cout << "ro-promotion: accepted set {";
-    for (HeapObject o : os) {
+    for (Tagged<HeapObject> o : os) {
       std::cout << reinterpret_cast<void*>(o.ptr()) << ", ";
     }
     std::cout << "}\n";
   }
 
-  static void LogRejectedPromotionForFailedPredicate(HeapObject o) {
+  static void LogRejectedPromotionForFailedPredicate(Tagged<HeapObject> o) {
     std::cout << "ro-promotion: rejected due to failed predicate "
               << reinterpret_cast<void*>(o.ptr()) << " ("
-              << o.map()->instance_type() << ")"
+              << o->map()->instance_type() << ")"
               << "\n";
   }
 
-  void LogRejectedPromotionForInvalidSubgraph(HeapObject o,
+  void LogRejectedPromotionForInvalidSubgraph(Tagged<HeapObject> o,
                                               int first_rejected_slot_offset) {
     std::cout << "ro-promotion: rejected due to rejected subgraph "
               << reinterpret_cast<void*>(o.ptr()) << " ("
-              << o.map()->instance_type() << ")"
+              << o->map()->instance_type() << ")"
               << " at slot offset " << first_rejected_slot_offset << " ";
 
-    MaybeObjectSlot slot = o.RawMaybeWeakField(first_rejected_slot_offset);
+    MaybeObjectSlot slot = o->RawMaybeWeakField(first_rejected_slot_offset);
     MaybeObject maybe_object = slot.load(isolate_);
-    HeapObject heap_object;
+    Tagged<HeapObject> heap_object;
     if (maybe_object.GetHeapObject(&heap_object)) {
       std::cout << reinterpret_cast<void*>(heap_object.ptr()) << " ("
-                << heap_object.map()->instance_type() << ")"
+                << heap_object->map()->instance_type() << ")"
                 << "\n";
     } else {
       std::cout << "<cleared weak object>\n";
@@ -262,8 +266,8 @@ class ReadOnlyPromotionImpl final : public AllStatic {
                                  const std::vector<HeapObject>& promotees,
                                  HeapObjectMap* moves) {
     ReadOnlySpace* rospace = isolate->heap()->read_only_space();
-    for (HeapObject src : promotees) {
-      const int size = src.Size(isolate);
+    for (Tagged<HeapObject> src : promotees) {
+      const int size = src->Size(isolate);
       Tagged<HeapObject> dst =
           rospace->AllocateRaw(size, kTaggedAligned).ToObjectChecked();
       Heap::CopyBlock(dst.address(), src.address(), size);
@@ -291,13 +295,13 @@ class ReadOnlyPromotionImpl final : public AllStatic {
     // We assume that a full and precise GC has reclaimed all dead objects
     // and therefore that no filtering of unreachable objects is required here.
     HeapObjectIterator it(heap, safepoint_scope);
-    for (HeapObject o = it.Next(); !o.is_null(); o = it.Next()) {
-      o.Iterate(isolate, &v);
+    for (Tagged<HeapObject> o = it.Next(); !o.is_null(); o = it.Next()) {
+      o->Iterate(isolate, &v);
     }
 
     // Iterate all objects we just copied into RO space.
     for (auto [src, dst] : moves) {
-      dst.Iterate(isolate, &v);
+      dst->Iterate(isolate, &v);
     }
   }
 
@@ -340,24 +344,24 @@ class ReadOnlyPromotionImpl final : public AllStatic {
     }
 
     // The ObjectVisitor interface.
-    void VisitPointers(HeapObject host, MaybeObjectSlot start,
+    void VisitPointers(Tagged<HeapObject> host, MaybeObjectSlot start,
                        MaybeObjectSlot end) final {
       for (MaybeObjectSlot slot = start; slot < end; slot++) {
         ProcessSlot(host, slot);
       }
     }
-    void VisitPointers(HeapObject host, ObjectSlot start,
+    void VisitPointers(Tagged<HeapObject> host, ObjectSlot start,
                        ObjectSlot end) final {
       VisitPointers(host, MaybeObjectSlot(start), MaybeObjectSlot(end));
     }
-    void VisitInstructionStreamPointer(Code host,
+    void VisitInstructionStreamPointer(Tagged<Code> host,
                                        InstructionStreamSlot slot) final {
       // InstructionStream objects never move to RO space.
     }
-    void VisitMapPointer(HeapObject host) final {
-      ProcessSlot(host, host.RawMaybeWeakField(HeapObject::kMapOffset));
+    void VisitMapPointer(Tagged<HeapObject> host) final {
+      ProcessSlot(host, host->RawMaybeWeakField(HeapObject::kMapOffset));
     }
-    void VisitExternalPointer(HeapObject host, ExternalPointerSlot slot,
+    void VisitExternalPointer(Tagged<HeapObject> host, ExternalPointerSlot slot,
                               ExternalPointerTag tag) final {
 #ifdef V8_ENABLE_SANDBOX
       auto it = moves_reverse_lookup_.find(host);
@@ -376,9 +380,9 @@ class ReadOnlyPromotionImpl final : public AllStatic {
       }
 #endif  // V8_ENABLE_SANDBOX
     }
-    void VisitIndirectPointer(HeapObject host, IndirectPointerSlot slot,
+    void VisitIndirectPointer(Tagged<HeapObject> host, IndirectPointerSlot slot,
                               IndirectPointerMode mode) final {}
-    void VisitIndirectPointerTableEntry(HeapObject host,
+    void VisitIndirectPointerTableEntry(Tagged<HeapObject> host,
                                         IndirectPointerSlot slot) final {
 #ifdef V8_CODE_POINTER_SANDBOXING
       // When an object owning an indirect pointer table entry is relocated, it
@@ -398,7 +402,7 @@ class ReadOnlyPromotionImpl final : public AllStatic {
       CHECK(host.InReadOnlySpace());
       RecordProcessedSlotIfDebug(slot.address());
 
-      Code dead_code = Code::cast(it->second);
+      Tagged<Code> dead_code = Code::cast(it->second);
       CHECK(IsCode(dead_code));
       CHECK(!dead_code.InReadOnlySpace());
 
@@ -435,23 +439,23 @@ class ReadOnlyPromotionImpl final : public AllStatic {
 
    private:
     void ProcessSlot(Root root, FullObjectSlot slot) {
-      Object old_slot_value_obj = slot.load(isolate_);
+      Tagged<Object> old_slot_value_obj = slot.load(isolate_);
       if (!IsHeapObject(old_slot_value_obj)) return;
-      HeapObject old_slot_value = HeapObject::cast(old_slot_value_obj);
+      Tagged<HeapObject> old_slot_value = HeapObject::cast(old_slot_value_obj);
       auto it = moves_->find(old_slot_value);
       if (it == moves_->end()) return;
-      HeapObject new_slot_value = it->second;
+      Tagged<HeapObject> new_slot_value = it->second;
       slot.store(new_slot_value);
       if (V8_UNLIKELY(v8_flags.trace_read_only_promotion_verbose)) {
         LogUpdatedPointer(root, slot, old_slot_value, new_slot_value);
       }
     }
-    void ProcessSlot(HeapObject host, MaybeObjectSlot slot) {
-      HeapObject old_slot_value;
+    void ProcessSlot(Tagged<HeapObject> host, MaybeObjectSlot slot) {
+      Tagged<HeapObject> old_slot_value;
       if (!slot.load(isolate_).GetHeapObject(&old_slot_value)) return;
       auto it = moves_->find(old_slot_value);
       if (it == moves_->end()) return;
-      HeapObject new_slot_value = it->second;
+      Tagged<HeapObject> new_slot_value = it->second;
       slot.store(MaybeObject::FromObject(new_slot_value));
       if (V8_UNLIKELY(v8_flags.trace_read_only_promotion_verbose)) {
         LogUpdatedPointer(host, slot, old_slot_value, new_slot_value);
@@ -459,24 +463,24 @@ class ReadOnlyPromotionImpl final : public AllStatic {
     }
 
     void LogUpdatedPointer(Root root, FullObjectSlot slot,
-                           HeapObject old_slot_value,
-                           HeapObject new_slot_value) {
+                           Tagged<HeapObject> old_slot_value,
+                           Tagged<HeapObject> new_slot_value) {
       std::cout << "ro-promotion: updated pointer {root "
                 << static_cast<int>(root) << " slot "
                 << reinterpret_cast<void*>(slot.address()) << " from "
                 << reinterpret_cast<void*>(old_slot_value.ptr()) << " to "
                 << reinterpret_cast<void*>(new_slot_value.ptr()) << "}\n";
     }
-    void LogUpdatedPointer(HeapObject host, MaybeObjectSlot slot,
-                           HeapObject old_slot_value,
-                           HeapObject new_slot_value) {
+    void LogUpdatedPointer(Tagged<HeapObject> host, MaybeObjectSlot slot,
+                           Tagged<HeapObject> old_slot_value,
+                           Tagged<HeapObject> new_slot_value) {
       std::cout << "ro-promotion: updated pointer {host "
                 << reinterpret_cast<void*>(host.address()) << " slot "
                 << reinterpret_cast<void*>(slot.address()) << " from "
                 << reinterpret_cast<void*>(old_slot_value.ptr()) << " to "
                 << reinterpret_cast<void*>(new_slot_value.ptr()) << "}\n";
     }
-    void LogUpdatedExternalPointerTableEntry(HeapObject host,
+    void LogUpdatedExternalPointerTableEntry(Tagged<HeapObject> host,
                                              ExternalPointerSlot slot,
                                              Address slot_value) {
       std::cout << "ro-promotion: updated external pointer slot {host "
@@ -484,9 +488,9 @@ class ReadOnlyPromotionImpl final : public AllStatic {
                 << reinterpret_cast<void*>(slot.address()) << " slot_value "
                 << reinterpret_cast<void*>(slot_value) << "}\n";
     }
-    void LogUpdatedCodePointerTableEntry(HeapObject host,
+    void LogUpdatedCodePointerTableEntry(Tagged<HeapObject> host,
                                          IndirectPointerSlot slot,
-                                         Code old_code_object) {
+                                         Tagged<Code> old_code_object) {
       std::cout << "ro-promotion: updated code pointer table entry {host "
                 << reinterpret_cast<void*>(host.address()) << " slot "
                 << reinterpret_cast<void*>(slot.address()) << " from "
diff --git a/src/heap/read-only-spaces.cc b/src/heap/read-only-spaces.cc
index f4ba58cf9b0..5526988963a 100644
--- a/src/heap/read-only-spaces.cc
+++ b/src/heap/read-only-spaces.cc
@@ -419,7 +419,7 @@ class ReadOnlySpaceObjectIterator : public ObjectIterator {
         cur_addr_ = space_->limit();
         continue;
       }
-      HeapObject obj = HeapObject::FromAddress(cur_addr_);
+      Tagged<HeapObject> obj = HeapObject::FromAddress(cur_addr_);
       const int obj_size = obj->Size();
       cur_addr_ += ALIGN_TO_ALLOCATION_ALIGNMENT(obj_size);
       DCHECK_LE(cur_addr_, cur_end_);
@@ -458,7 +458,8 @@ void ReadOnlySpace::Verify(Isolate* isolate,
     Address end_of_previous_object = page->area_start();
     Address top = page->area_end();
 
-    for (HeapObject object = it.Next(); !object.is_null(); object = it.Next()) {
+    for (Tagged<HeapObject> object = it.Next(); !object.is_null();
+         object = it.Next()) {
       CHECK(end_of_previous_object <= object.address());
 
       visitor->VerifyObject(object);
@@ -486,7 +487,8 @@ void ReadOnlySpace::VerifyCounters(Heap* heap) const {
     total_capacity += page->area_size();
     ReadOnlySpaceObjectIterator it(heap, this, page);
     size_t real_allocated = 0;
-    for (HeapObject object = it.Next(); !object.is_null(); object = it.Next()) {
+    for (Tagged<HeapObject> object = it.Next(); !object.is_null();
+         object = it.Next()) {
       if (!IsFreeSpaceOrFiller(object)) {
         real_allocated += object->Size();
       }
@@ -564,7 +566,7 @@ void ReadOnlySpace::EnsureSpaceForAllocation(int size_in_bytes) {
   limit_ = chunk->area_end();
 }
 
-HeapObject ReadOnlySpace::TryAllocateLinearlyAligned(
+Tagged<HeapObject> ReadOnlySpace::TryAllocateLinearlyAligned(
     int size_in_bytes, AllocationAlignment alignment) {
   size_in_bytes = ALIGN_TO_ALLOCATION_ALIGNMENT(size_in_bytes);
   Address current_top = top_;
@@ -595,7 +597,8 @@ AllocationResult ReadOnlySpace::AllocateRawAligned(
   size_in_bytes = ALIGN_TO_ALLOCATION_ALIGNMENT(size_in_bytes);
   int allocation_size = size_in_bytes;
 
-  HeapObject object = TryAllocateLinearlyAligned(allocation_size, alignment);
+  Tagged<HeapObject> object =
+      TryAllocateLinearlyAligned(allocation_size, alignment);
   if (object.is_null()) {
     // We don't know exactly how much filler we need to align until space is
     // allocated, so assume the worst case.
@@ -618,7 +621,7 @@ AllocationResult ReadOnlySpace::AllocateRawUnaligned(int size_in_bytes) {
   Address new_top = current_top + size_in_bytes;
   DCHECK_LE(new_top, limit_);
   top_ = new_top;
-  HeapObject object = HeapObject::FromAddress(current_top);
+  Tagged<HeapObject> object = HeapObject::FromAddress(current_top);
 
   DCHECK(!object.is_null());
   MSAN_ALLOCATED_UNINITIALIZED_MEMORY(object.address(), size_in_bytes);
@@ -641,7 +644,7 @@ AllocationResult ReadOnlySpace::AllocateRaw(int size_in_bytes,
 size_t ReadOnlyPage::ShrinkToHighWaterMark() {
   // Shrink pages to high water mark. The water mark points either to a filler
   // or the area_end.
-  HeapObject filler = HeapObject::FromAddress(HighWaterMark());
+  Tagged<HeapObject> filler = HeapObject::FromAddress(HighWaterMark());
   if (filler.address() == area_end()) return 0;
   CHECK(IsFreeSpaceOrFiller(filler));
   DCHECK_EQ(filler.address() + filler->Size(), area_end());
diff --git a/src/heap/read-only-spaces.h b/src/heap/read-only-spaces.h
index 4ae1afd18fc..61fa11e0f92 100644
--- a/src/heap/read-only-spaces.h
+++ b/src/heap/read-only-spaces.h
@@ -218,7 +218,7 @@ class ReadOnlySpace : public BaseSpace {
   bool writable() const { return !is_marked_read_only_; }
 
   bool Contains(Address a) = delete;
-  bool Contains(Object o) = delete;
+  bool Contains(Tagged<Object> o) = delete;
 
   V8_EXPORT_PRIVATE
   AllocationResult AllocateRaw(int size_in_bytes,
@@ -294,8 +294,8 @@ class ReadOnlySpace : public BaseSpace {
   AllocationResult AllocateRawUnaligned(int size_in_bytes);
   AllocationResult AllocateRawAligned(int size_in_bytes,
                                       AllocationAlignment alignment);
-  HeapObject TryAllocateLinearlyAligned(int size_in_bytes,
-                                        AllocationAlignment alignment);
+  Tagged<HeapObject> TryAllocateLinearlyAligned(int size_in_bytes,
+                                                AllocationAlignment alignment);
 
   // Return the index within pages_ of the newly allocated page.
   size_t AllocateNextPage();
diff --git a/src/heap/reference-summarizer.cc b/src/heap/reference-summarizer.cc
index 7b6e7b2937f..f0913fec6cb 100644
--- a/src/heap/reference-summarizer.cc
+++ b/src/heap/reference-summarizer.cc
@@ -20,7 +20,7 @@ namespace {
 // marking visitor.
 class ReferenceSummarizerMarkingState final {
  public:
-  explicit ReferenceSummarizerMarkingState(HeapObject object)
+  explicit ReferenceSummarizerMarkingState(Tagged<HeapObject> object)
       : primary_object_(object),
         local_marking_worklists_(&marking_worklists_),
         local_weak_objects_(&weak_objects_) {}
@@ -43,20 +43,21 @@ class ReferenceSummarizerMarkingState final {
   }
 
   // Standard marking visitor functions:
-  bool TryMark(HeapObject obj) { return true; }
-  bool IsUnmarked(HeapObject obj) const { return true; }
-  bool IsMarked(HeapObject obj) const { return false; }
+  bool TryMark(Tagged<HeapObject> obj) { return true; }
+  bool IsUnmarked(Tagged<HeapObject> obj) const { return true; }
+  bool IsMarked(Tagged<HeapObject> obj) const { return false; }
 
   // Adds a retaining relationship found by the marking visitor.
-  void AddStrongReferenceForReferenceSummarizer(HeapObject host,
-                                                HeapObject obj) {
+  void AddStrongReferenceForReferenceSummarizer(Tagged<HeapObject> host,
+                                                Tagged<HeapObject> obj) {
     AddReference(host, obj, references_.strong_references());
   }
 
   // Adds a non-retaining weak reference found by the marking visitor. The value
   // in an ephemeron hash table entry is also included here, since it is not
   // known to be strong without further information about the key.
-  void AddWeakReferenceForReferenceSummarizer(HeapObject host, HeapObject obj) {
+  void AddWeakReferenceForReferenceSummarizer(Tagged<HeapObject> host,
+                                              Tagged<HeapObject> obj) {
     AddReference(host, obj, references_.weak_references());
   }
 
@@ -68,7 +69,7 @@ class ReferenceSummarizerMarkingState final {
   WeakObjects::Local* local_weak_objects() { return &local_weak_objects_; }
 
  private:
-  void AddReference(HeapObject host, HeapObject obj,
+  void AddReference(Tagged<HeapObject> host, Tagged<HeapObject> obj,
                     ReferenceSummary::UnorderedHeapObjectSet& references) {
     // It's possible that the marking visitor handles multiple objects at once,
     // such as a Map and its DescriptorArray, but we're only interested in
@@ -99,18 +100,19 @@ class ReferenceSummarizerMarkingVisitor
         marking_state_(marking_state) {}
 
   template <typename TSlot>
-  void RecordSlot(HeapObject object, TSlot slot, HeapObject target) {}
+  void RecordSlot(Tagged<HeapObject> object, TSlot slot,
+                  Tagged<HeapObject> target) {}
 
-  void RecordRelocSlot(InstructionStream host, RelocInfo* rinfo,
-                       HeapObject target) {}
+  void RecordRelocSlot(Tagged<InstructionStream> host, RelocInfo* rinfo,
+                       Tagged<HeapObject> target) {}
 
-  V8_INLINE void AddStrongReferenceForReferenceSummarizer(HeapObject host,
-                                                          HeapObject obj) {
+  V8_INLINE void AddStrongReferenceForReferenceSummarizer(
+      Tagged<HeapObject> host, Tagged<HeapObject> obj) {
     marking_state_->AddStrongReferenceForReferenceSummarizer(host, obj);
   }
 
-  V8_INLINE void AddWeakReferenceForReferenceSummarizer(HeapObject host,
-                                                        HeapObject obj) {
+  V8_INLINE void AddWeakReferenceForReferenceSummarizer(
+      Tagged<HeapObject> host, Tagged<HeapObject> obj) {
     marking_state_->AddWeakReferenceForReferenceSummarizer(host, obj);
   }
 
@@ -121,8 +123,8 @@ class ReferenceSummarizerMarkingVisitor
   constexpr bool CanUpdateValuesInHeap() { return false; }
 
   // Standard marking visitor functions:
-  bool TryMark(HeapObject obj) { return true; }
-  bool IsMarked(HeapObject obj) const { return false; }
+  bool TryMark(Tagged<HeapObject> obj) { return true; }
+  bool IsMarked(Tagged<HeapObject> obj) const { return false; }
 
  private:
   ReferenceSummarizerMarkingState* marking_state_;
@@ -130,8 +132,8 @@ class ReferenceSummarizerMarkingVisitor
 
 }  // namespace
 
-ReferenceSummary ReferenceSummary::SummarizeReferencesFrom(Heap* heap,
-                                                           HeapObject obj) {
+ReferenceSummary ReferenceSummary::SummarizeReferencesFrom(
+    Heap* heap, Tagged<HeapObject> obj) {
   ReferenceSummarizerMarkingState marking_state(obj);
 
   ReferenceSummarizerMarkingVisitor visitor(heap, &marking_state);
diff --git a/src/heap/reference-summarizer.h b/src/heap/reference-summarizer.h
index 2aa09619d16..6e5071b7cba 100644
--- a/src/heap/reference-summarizer.h
+++ b/src/heap/reference-summarizer.h
@@ -24,7 +24,8 @@ class ReferenceSummary {
   // Produces a set of objects referred to by the object. This function uses a
   // realistic marking visitor, so its results are likely to match real GC
   // behavior. Intended only for verification.
-  static ReferenceSummary SummarizeReferencesFrom(Heap* heap, HeapObject obj);
+  static ReferenceSummary SummarizeReferencesFrom(Heap* heap,
+                                                  Tagged<HeapObject> obj);
 
   using UnorderedHeapObjectSet =
       std::unordered_set<HeapObject, Object::Hasher, Object::KeyEqualSafe>;
diff --git a/src/heap/remembered-set-inl.h b/src/heap/remembered-set-inl.h
index e17fe7439af..09f7608c758 100644
--- a/src/heap/remembered-set-inl.h
+++ b/src/heap/remembered-set-inl.h
@@ -35,10 +35,10 @@ SlotCallbackResult UpdateTypedSlotHelper::UpdateTypedSlot(Heap* heap,
       return UpdateEmbeddedPointer(heap, &rinfo, callback);
     }
     case SlotType::kConstPoolEmbeddedObjectCompressed: {
-      HeapObject old_target =
+      Tagged<HeapObject> old_target =
           HeapObject::cast(Object(V8HeapCompressionScheme::DecompressTagged(
               heap->isolate(), base::Memory<Tagged_t>(addr))));
-      HeapObject new_target = old_target;
+      Tagged<HeapObject> new_target = old_target;
       SlotCallbackResult result = callback(FullMaybeObjectSlot(&new_target));
       DCHECK(!HasWeakHeapObjectTag(new_target));
       if (new_target != old_target) {
@@ -56,9 +56,9 @@ SlotCallbackResult UpdateTypedSlotHelper::UpdateTypedSlot(Heap* heap,
   UNREACHABLE();
 }
 
-HeapObject UpdateTypedSlotHelper::GetTargetObject(Heap* heap,
-                                                  SlotType slot_type,
-                                                  Address addr) {
+Tagged<HeapObject> UpdateTypedSlotHelper::GetTargetObject(Heap* heap,
+                                                          SlotType slot_type,
+                                                          Address addr) {
   switch (slot_type) {
     case SlotType::kCodeEntry: {
       RelocInfo rinfo(addr, RelocInfo::CODE_TARGET);
diff --git a/src/heap/remembered-set.h b/src/heap/remembered-set.h
index b125b0fa167..30b805026fb 100644
--- a/src/heap/remembered-set.h
+++ b/src/heap/remembered-set.h
@@ -305,8 +305,9 @@ class UpdateTypedSlotHelper {
                                             Address addr, Callback callback);
 
   // Returns the HeapObject referenced by the given typed slot entry.
-  inline static HeapObject GetTargetObject(Heap* heap, SlotType slot_type,
-                                           Address addr);
+  inline static Tagged<HeapObject> GetTargetObject(Heap* heap,
+                                                   SlotType slot_type,
+                                                   Address addr);
 
  private:
   // Updates a code entry slot using an untyped slot callback.
@@ -314,8 +315,9 @@ class UpdateTypedSlotHelper {
   template <typename Callback>
   static SlotCallbackResult UpdateCodeEntry(Address entry_address,
                                             Callback callback) {
-    InstructionStream code = InstructionStream::FromEntryAddress(entry_address);
-    InstructionStream old_code = code;
+    Tagged<InstructionStream> code =
+        InstructionStream::FromEntryAddress(entry_address);
+    Tagged<InstructionStream> old_code = code;
     SlotCallbackResult result = callback(FullMaybeObjectSlot(&code));
     DCHECK(!HasWeakHeapObjectTag(code));
     if (code != old_code) {
@@ -330,9 +332,9 @@ class UpdateTypedSlotHelper {
   static SlotCallbackResult UpdateCodeTarget(RelocInfo* rinfo,
                                              Callback callback) {
     DCHECK(RelocInfo::IsCodeTargetMode(rinfo->rmode()));
-    InstructionStream old_target =
+    Tagged<InstructionStream> old_target =
         InstructionStream::FromTargetAddress(rinfo->target_address());
-    InstructionStream new_target = old_target;
+    Tagged<InstructionStream> new_target = old_target;
     SlotCallbackResult result = callback(FullMaybeObjectSlot(&new_target));
     DCHECK(!HasWeakHeapObjectTag(new_target));
     if (new_target != old_target) {
@@ -348,8 +350,8 @@ class UpdateTypedSlotHelper {
   static SlotCallbackResult UpdateEmbeddedPointer(Heap* heap, RelocInfo* rinfo,
                                                   Callback callback) {
     DCHECK(RelocInfo::IsEmbeddedObjectMode(rinfo->rmode()));
-    HeapObject old_target = rinfo->target_object(heap->isolate());
-    HeapObject new_target = old_target;
+    Tagged<HeapObject> old_target = rinfo->target_object(heap->isolate());
+    Tagged<HeapObject> new_target = old_target;
     SlotCallbackResult result = callback(FullMaybeObjectSlot(&new_target));
     DCHECK(!HasWeakHeapObjectTag(new_target));
     if (new_target != old_target) {
diff --git a/src/heap/scavenger-inl.h b/src/heap/scavenger-inl.h
index 2163cc7a930..a542ff83752 100644
--- a/src/heap/scavenger-inl.h
+++ b/src/heap/scavenger-inl.h
@@ -22,13 +22,14 @@
 namespace v8 {
 namespace internal {
 
-void Scavenger::PromotionList::Local::PushRegularObject(HeapObject object,
-                                                        int size) {
+void Scavenger::PromotionList::Local::PushRegularObject(
+    Tagged<HeapObject> object, int size) {
   regular_object_promotion_list_local_.Push({object, size});
 }
 
-void Scavenger::PromotionList::Local::PushLargeObject(HeapObject object,
-                                                      Map map, int size) {
+void Scavenger::PromotionList::Local::PushLargeObject(Tagged<HeapObject> object,
+                                                      Tagged<Map> map,
+                                                      int size) {
   large_object_promotion_list_local_.Push({object, map, size});
 }
 
@@ -81,15 +82,15 @@ void Scavenger::PageMemoryFence(MaybeObject object) {
 #ifdef THREAD_SANITIZER
   // Perform a dummy acquire load to tell TSAN that there is no data race
   // with  page initialization.
-  HeapObject heap_object;
+  Tagged<HeapObject> heap_object;
   if (object->GetHeapObject(&heap_object)) {
     BasicMemoryChunk::FromHeapObject(heap_object)->SynchronizedHeapLoad();
   }
 #endif
 }
 
-bool Scavenger::MigrateObject(Map map, HeapObject source, HeapObject target,
-                              int size,
+bool Scavenger::MigrateObject(Tagged<Map> map, Tagged<HeapObject> source,
+                              Tagged<HeapObject> target, int size,
                               PromotionHeapChoice promotion_heap_choice) {
   // Copy the content of source to target.
   target->set_map_word(map, kRelaxedStore);
@@ -119,8 +120,8 @@ bool Scavenger::MigrateObject(Map map, HeapObject source, HeapObject target,
 
 template <typename THeapObjectSlot>
 CopyAndForwardResult Scavenger::SemiSpaceCopyObject(
-    Map map, THeapObjectSlot slot, HeapObject object, int object_size,
-    ObjectFields object_fields) {
+    Tagged<Map> map, THeapObjectSlot slot, Tagged<HeapObject> object,
+    int object_size, ObjectFields object_fields) {
   static_assert(std::is_same<THeapObjectSlot, FullHeapObjectSlot>::value ||
                     std::is_same<THeapObjectSlot, HeapObjectSlot>::value,
                 "Only FullHeapObjectSlot and HeapObjectSlot are expected here");
@@ -129,7 +130,7 @@ CopyAndForwardResult Scavenger::SemiSpaceCopyObject(
   AllocationResult allocation = allocator_.Allocate(
       NEW_SPACE, object_size, AllocationOrigin::kGC, alignment);
 
-  HeapObject target;
+  Tagged<HeapObject> target;
   if (allocation.To(&target)) {
     DCHECK(heap()->marking_state()->IsUnmarked(target));
     const bool self_success =
@@ -155,8 +156,9 @@ CopyAndForwardResult Scavenger::SemiSpaceCopyObject(
 
 template <typename THeapObjectSlot,
           Scavenger::PromotionHeapChoice promotion_heap_choice>
-CopyAndForwardResult Scavenger::PromoteObject(Map map, THeapObjectSlot slot,
-                                              HeapObject object,
+CopyAndForwardResult Scavenger::PromoteObject(Tagged<Map> map,
+                                              THeapObjectSlot slot,
+                                              Tagged<HeapObject> object,
                                               int object_size,
                                               ObjectFields object_fields) {
   static_assert(std::is_same<THeapObjectSlot, FullHeapObjectSlot>::value ||
@@ -177,7 +179,7 @@ CopyAndForwardResult Scavenger::PromoteObject(Map map, THeapObjectSlot slot,
       break;
   }
 
-  HeapObject target;
+  Tagged<HeapObject> target;
   if (allocation.To(&target)) {
     DCHECK(heap()->non_atomic_marking_state()->IsUnmarked(target));
     const bool self_success =
@@ -219,8 +221,8 @@ SlotCallbackResult Scavenger::RememberedSetEntryNeeded(
                                                                   : REMOVE_SLOT;
 }
 
-bool Scavenger::HandleLargeObject(Map map, HeapObject object, int object_size,
-                                  ObjectFields object_fields) {
+bool Scavenger::HandleLargeObject(Tagged<Map> map, Tagged<HeapObject> object,
+                                  int object_size, ObjectFields object_fields) {
   // TODO(hpayer): Make this check size based, i.e.
   // object_size > kMaxRegularHeapObjectSize
   if (V8_UNLIKELY(
@@ -243,12 +245,12 @@ bool Scavenger::HandleLargeObject(Map map, HeapObject object, int object_size,
 template <typename THeapObjectSlot,
           Scavenger::PromotionHeapChoice promotion_heap_choice>
 SlotCallbackResult Scavenger::EvacuateObjectDefault(
-    Map map, THeapObjectSlot slot, HeapObject object, int object_size,
-    ObjectFields object_fields) {
+    Tagged<Map> map, THeapObjectSlot slot, Tagged<HeapObject> object,
+    int object_size, ObjectFields object_fields) {
   static_assert(std::is_same<THeapObjectSlot, FullHeapObjectSlot>::value ||
                     std::is_same<THeapObjectSlot, HeapObjectSlot>::value,
                 "Only FullHeapObjectSlot and HeapObjectSlot are expected here");
-  SLOW_DCHECK(object.SizeFromMap(map) == object_size);
+  SLOW_DCHECK(object->SizeFromMap(map) == object_size);
   CopyAndForwardResult result;
 
   if (HandleLargeObject(map, object, object_size, object_fields)) {
@@ -288,8 +290,9 @@ SlotCallbackResult Scavenger::EvacuateObjectDefault(
 }
 
 template <typename THeapObjectSlot>
-SlotCallbackResult Scavenger::EvacuateThinString(Map map, THeapObjectSlot slot,
-                                                 ThinString object,
+SlotCallbackResult Scavenger::EvacuateThinString(Tagged<Map> map,
+                                                 THeapObjectSlot slot,
+                                                 Tagged<ThinString> object,
                                                  int object_size) {
   static_assert(std::is_same<THeapObjectSlot, FullHeapObjectSlot>::value ||
                     std::is_same<THeapObjectSlot, HeapObjectSlot>::value,
@@ -298,7 +301,7 @@ SlotCallbackResult Scavenger::EvacuateThinString(Map map, THeapObjectSlot slot,
     // The ThinString should die after Scavenge, so avoid writing the proper
     // forwarding pointer and instead just signal the actual object as forwarded
     // reference.
-    String actual = object->actual();
+    Tagged<String> actual = object->actual();
     // ThinStrings always refer to internalized strings, which are always in old
     // space.
     DCHECK(!Heap::InYoungGeneration(actual));
@@ -313,10 +316,9 @@ SlotCallbackResult Scavenger::EvacuateThinString(Map map, THeapObjectSlot slot,
 }
 
 template <typename THeapObjectSlot>
-SlotCallbackResult Scavenger::EvacuateShortcutCandidate(Map map,
-                                                        THeapObjectSlot slot,
-                                                        ConsString object,
-                                                        int object_size) {
+SlotCallbackResult Scavenger::EvacuateShortcutCandidate(
+    Tagged<Map> map, THeapObjectSlot slot, Tagged<ConsString> object,
+    int object_size) {
   static_assert(std::is_same<THeapObjectSlot, FullHeapObjectSlot>::value ||
                     std::is_same<THeapObjectSlot, HeapObjectSlot>::value,
                 "Only FullHeapObjectSlot and HeapObjectSlot are expected here");
@@ -324,7 +326,7 @@ SlotCallbackResult Scavenger::EvacuateShortcutCandidate(Map map,
 
   if (shortcut_strings_ &&
       object->unchecked_second() == ReadOnlyRoots(heap()).empty_string()) {
-    HeapObject first = HeapObject::cast(object->unchecked_first());
+    Tagged<HeapObject> first = HeapObject::cast(object->unchecked_first());
 
     HeapObjectReference::Update(slot, first);
 
@@ -335,17 +337,17 @@ SlotCallbackResult Scavenger::EvacuateShortcutCandidate(Map map,
 
     MapWord first_word = first->map_word(kAcquireLoad);
     if (first_word.IsForwardingAddress()) {
-      HeapObject target = first_word.ToForwardingAddress(first);
+      Tagged<HeapObject> target = first_word.ToForwardingAddress(first);
 
       HeapObjectReference::Update(slot, target);
       object->set_map_word_forwarded(target, kReleaseStore);
       return Heap::InYoungGeneration(target) ? KEEP_SLOT : REMOVE_SLOT;
     }
-    Map first_map = first_word.ToMap();
+    Tagged<Map> first_map = first_word.ToMap();
     SlotCallbackResult result = EvacuateObjectDefault(
         first_map, slot, first, first->SizeFromMap(first_map),
         Map::ObjectFieldsFrom(first_map->visitor_id()));
-    object.set_map_word_forwarded(slot.ToHeapObject(), kReleaseStore);
+    object->set_map_word_forwarded(slot.ToHeapObject(), kReleaseStore);
     return result;
   }
   DCHECK_EQ(ObjectFields::kMaybePointers,
@@ -356,8 +358,8 @@ SlotCallbackResult Scavenger::EvacuateShortcutCandidate(Map map,
 
 template <typename THeapObjectSlot>
 SlotCallbackResult Scavenger::EvacuateInPlaceInternalizableString(
-    Map map, THeapObjectSlot slot, String object, int object_size,
-    ObjectFields object_fields) {
+    Tagged<Map> map, THeapObjectSlot slot, Tagged<String> object,
+    int object_size, ObjectFields object_fields) {
   DCHECK(String::IsInPlaceInternalizable(map->instance_type()));
   DCHECK_EQ(object_fields, Map::ObjectFieldsFrom(map->visitor_id()));
   if (shared_string_table_) {
@@ -368,8 +370,9 @@ SlotCallbackResult Scavenger::EvacuateInPlaceInternalizableString(
 }
 
 template <typename THeapObjectSlot>
-SlotCallbackResult Scavenger::EvacuateObject(THeapObjectSlot slot, Map map,
-                                             HeapObject source) {
+SlotCallbackResult Scavenger::EvacuateObject(THeapObjectSlot slot,
+                                             Tagged<Map> map,
+                                             Tagged<HeapObject> source) {
   static_assert(std::is_same<THeapObjectSlot, FullHeapObjectSlot>::value ||
                     std::is_same<THeapObjectSlot, HeapObjectSlot>::value,
                 "Only FullHeapObjectSlot and HeapObjectSlot are expected here");
@@ -414,7 +417,7 @@ SlotCallbackResult Scavenger::EvacuateObject(THeapObjectSlot slot, Map map,
 
 template <typename THeapObjectSlot>
 SlotCallbackResult Scavenger::ScavengeObject(THeapObjectSlot p,
-                                             HeapObject object) {
+                                             Tagged<HeapObject> object) {
   static_assert(std::is_same<THeapObjectSlot, FullHeapObjectSlot>::value ||
                     std::is_same<THeapObjectSlot, HeapObjectSlot>::value,
                 "Only FullHeapObjectSlot and HeapObjectSlot are expected here");
@@ -428,7 +431,7 @@ SlotCallbackResult Scavenger::ScavengeObject(THeapObjectSlot p,
   // If the first word is a forwarding address, the object has already been
   // copied.
   if (first_word.IsForwardingAddress()) {
-    HeapObject dest = first_word.ToForwardingAddress(object);
+    Tagged<HeapObject> dest = first_word.ToForwardingAddress(object);
     HeapObjectReference::Update(p, dest);
     DCHECK_IMPLIES(Heap::InYoungGeneration(dest),
                    Heap::InToPage(dest) || Heap::IsLargeObject(dest));
@@ -438,7 +441,7 @@ SlotCallbackResult Scavenger::ScavengeObject(THeapObjectSlot p,
     return Heap::InYoungGeneration(dest) ? KEEP_SLOT : REMOVE_SLOT;
   }
 
-  Map map = first_word.ToMap();
+  Tagged<Map> map = first_word.ToMap();
   // AllocationMementos are unrooted and shouldn't survive a scavenge
   DCHECK_NE(ReadOnlyRoots(heap()).allocation_memento_map(), map);
   // Call the slow part of scavenge object.
@@ -454,7 +457,7 @@ SlotCallbackResult Scavenger::CheckAndScavengeObject(Heap* heap, TSlot slot) {
   using THeapObjectSlot = typename TSlot::THeapObjectSlot;
   MaybeObject object = *slot;
   if (Heap::InFromPage(object)) {
-    HeapObject heap_object = object->GetHeapObject();
+    Tagged<HeapObject> heap_object = object->GetHeapObject();
 
     SlotCallbackResult result =
         ScavengeObject(THeapObjectSlot(slot), heap_object);
@@ -475,37 +478,43 @@ class ScavengeVisitor final : public NewSpaceVisitor<ScavengeVisitor> {
  public:
   explicit ScavengeVisitor(Scavenger* scavenger);
 
-  V8_INLINE void VisitPointers(HeapObject host, ObjectSlot start,
+  V8_INLINE void VisitPointers(Tagged<HeapObject> host, ObjectSlot start,
                                ObjectSlot end) final;
 
-  V8_INLINE void VisitPointers(HeapObject host, MaybeObjectSlot start,
+  V8_INLINE void VisitPointers(Tagged<HeapObject> host, MaybeObjectSlot start,
                                MaybeObjectSlot end) final;
-  V8_INLINE int VisitEphemeronHashTable(Map map, EphemeronHashTable object);
-  V8_INLINE int VisitJSArrayBuffer(Map map, JSArrayBuffer object);
-  V8_INLINE int VisitJSApiObject(Map map, JSObject object);
+  V8_INLINE int VisitEphemeronHashTable(Tagged<Map> map,
+                                        Tagged<EphemeronHashTable> object);
+  V8_INLINE int VisitJSArrayBuffer(Tagged<Map> map,
+                                   Tagged<JSArrayBuffer> object);
+  V8_INLINE int VisitJSApiObject(Tagged<Map> map, Tagged<JSObject> object);
 
  private:
   template <typename TSlot>
-  V8_INLINE void VisitHeapObjectImpl(TSlot slot, HeapObject heap_object);
+  V8_INLINE void VisitHeapObjectImpl(TSlot slot,
+                                     Tagged<HeapObject> heap_object);
 
   template <typename TSlot>
-  V8_INLINE void VisitPointersImpl(HeapObject host, TSlot start, TSlot end);
+  V8_INLINE void VisitPointersImpl(Tagged<HeapObject> host, TSlot start,
+                                   TSlot end);
 
   Scavenger* const scavenger_;
 };
 
-void ScavengeVisitor::VisitPointers(HeapObject host, ObjectSlot start,
+void ScavengeVisitor::VisitPointers(Tagged<HeapObject> host, ObjectSlot start,
                                     ObjectSlot end) {
   return VisitPointersImpl(host, start, end);
 }
 
-void ScavengeVisitor::VisitPointers(HeapObject host, MaybeObjectSlot start,
+void ScavengeVisitor::VisitPointers(Tagged<HeapObject> host,
+                                    MaybeObjectSlot start,
                                     MaybeObjectSlot end) {
   return VisitPointersImpl(host, start, end);
 }
 
 template <typename TSlot>
-void ScavengeVisitor::VisitHeapObjectImpl(TSlot slot, HeapObject heap_object) {
+void ScavengeVisitor::VisitHeapObjectImpl(TSlot slot,
+                                          Tagged<HeapObject> heap_object) {
   if (Heap::InYoungGeneration(heap_object)) {
     using THeapObjectSlot = typename TSlot::THeapObjectSlot;
     scavenger_->ScavengeObject(THeapObjectSlot(slot), heap_object);
@@ -513,11 +522,11 @@ void ScavengeVisitor::VisitHeapObjectImpl(TSlot slot, HeapObject heap_object) {
 }
 
 template <typename TSlot>
-void ScavengeVisitor::VisitPointersImpl(HeapObject host, TSlot start,
+void ScavengeVisitor::VisitPointersImpl(Tagged<HeapObject> host, TSlot start,
                                         TSlot end) {
   for (TSlot slot = start; slot < end; ++slot) {
     typename TSlot::TObject object = *slot;
-    HeapObject heap_object;
+    Tagged<HeapObject> heap_object;
     // Treat weak references as strong.
     if (object.GetHeapObject(&heap_object)) {
       VisitHeapObjectImpl(slot, heap_object);
@@ -525,19 +534,21 @@ void ScavengeVisitor::VisitPointersImpl(HeapObject host, TSlot start,
   }
 }
 
-int ScavengeVisitor::VisitJSArrayBuffer(Map map, JSArrayBuffer object) {
+int ScavengeVisitor::VisitJSArrayBuffer(Tagged<Map> map,
+                                        Tagged<JSArrayBuffer> object) {
   object->YoungMarkExtension();
   int size = JSArrayBuffer::BodyDescriptor::SizeOf(map, object);
   JSArrayBuffer::BodyDescriptor::IterateBody(map, object, size, this);
   return size;
 }
 
-int ScavengeVisitor::VisitJSApiObject(Map map, JSObject object) {
+int ScavengeVisitor::VisitJSApiObject(Tagged<Map> map,
+                                      Tagged<JSObject> object) {
   return VisitJSObject(map, object);
 }
 
-int ScavengeVisitor::VisitEphemeronHashTable(Map map,
-                                             EphemeronHashTable table) {
+int ScavengeVisitor::VisitEphemeronHashTable(Tagged<Map> map,
+                                             Tagged<EphemeronHashTable> table) {
   // Register table with the scavenger, so it can take care of the weak keys
   // later. This allows to only iterate the tables' values, which are treated
   // as strong independently of whether the key is live.
diff --git a/src/heap/scavenger.cc b/src/heap/scavenger.cc
index 4b2f012da14..ac00ceafd9a 100644
--- a/src/heap/scavenger.cc
+++ b/src/heap/scavenger.cc
@@ -42,7 +42,7 @@ class IterateAndScavengePromotedObjectsVisitor final : public ObjectVisitor {
                                            bool record_slots)
       : scavenger_(scavenger), record_slots_(record_slots) {}
 
-  V8_INLINE void VisitMapPointer(HeapObject host) final {
+  V8_INLINE void VisitMapPointer(Tagged<HeapObject> host) final {
     if (!record_slots_) return;
     MapWord map_word = host->map_word(kRelaxedLoad);
     if (map_word.IsForwardingAddress()) {
@@ -53,17 +53,17 @@ class IterateAndScavengePromotedObjectsVisitor final : public ObjectVisitor {
     HandleSlot(host, HeapObjectSlot(host->map_slot()), map_word.ToMap());
   }
 
-  V8_INLINE void VisitPointers(HeapObject host, ObjectSlot start,
+  V8_INLINE void VisitPointers(Tagged<HeapObject> host, ObjectSlot start,
                                ObjectSlot end) final {
     VisitPointersImpl(host, start, end);
   }
 
-  V8_INLINE void VisitPointers(HeapObject host, MaybeObjectSlot start,
+  V8_INLINE void VisitPointers(Tagged<HeapObject> host, MaybeObjectSlot start,
                                MaybeObjectSlot end) final {
     VisitPointersImpl(host, start, end);
   }
 
-  inline void VisitEphemeron(HeapObject obj, int entry, ObjectSlot key,
+  inline void VisitEphemeron(Tagged<HeapObject> obj, int entry, ObjectSlot key,
                              ObjectSlot value) override {
     DCHECK(Heap::IsLargeObject(obj) || IsEphemeronHashTable(obj));
     VisitPointer(obj, value);
@@ -79,23 +79,27 @@ class IterateAndScavengePromotedObjectsVisitor final : public ObjectVisitor {
 
   // Special cases: Unreachable visitors for objects that are never found in the
   // young generation and thus cannot be found when iterating promoted objects.
-  void VisitInstructionStreamPointer(Code, InstructionStreamSlot) final {
+  void VisitInstructionStreamPointer(Tagged<Code>,
+                                     InstructionStreamSlot) final {
     UNREACHABLE();
   }
-  void VisitCodeTarget(InstructionStream, RelocInfo*) final { UNREACHABLE(); }
-  void VisitEmbeddedPointer(InstructionStream, RelocInfo*) final {
+  void VisitCodeTarget(Tagged<InstructionStream>, RelocInfo*) final {
+    UNREACHABLE();
+  }
+  void VisitEmbeddedPointer(Tagged<InstructionStream>, RelocInfo*) final {
     UNREACHABLE();
   }
 
  private:
   template <typename TSlot>
-  V8_INLINE void VisitPointersImpl(HeapObject host, TSlot start, TSlot end) {
+  V8_INLINE void VisitPointersImpl(Tagged<HeapObject> host, TSlot start,
+                                   TSlot end) {
     using THeapObjectSlot = typename TSlot::THeapObjectSlot;
     // Treat weak references as strong.
     // TODO(marja): Proper weakness handling in the young generation.
     for (TSlot slot = start; slot < end; ++slot) {
       typename TSlot::TObject object = *slot;
-      HeapObject heap_object;
+      Tagged<HeapObject> heap_object;
       if (object.GetHeapObject(&heap_object)) {
         HandleSlot(host, THeapObjectSlot(slot), heap_object);
       }
@@ -103,8 +107,8 @@ class IterateAndScavengePromotedObjectsVisitor final : public ObjectVisitor {
   }
 
   template <typename THeapObjectSlot>
-  V8_INLINE void HandleSlot(HeapObject host, THeapObjectSlot slot,
-                            HeapObject target) {
+  V8_INLINE void HandleSlot(Tagged<HeapObject> host, THeapObjectSlot slot,
+                            Tagged<HeapObject> target) {
     static_assert(
         std::is_same<THeapObjectSlot, FullHeapObjectSlot>::value ||
             std::is_same<THeapObjectSlot, HeapObjectSlot>::value,
@@ -158,14 +162,15 @@ class IterateAndScavengePromotedObjectsVisitor final : public ObjectVisitor {
 
 namespace {
 
-V8_INLINE bool IsUnscavengedHeapObject(Heap* heap, Object object) {
+V8_INLINE bool IsUnscavengedHeapObject(Heap* heap, Tagged<Object> object) {
   return Heap::InFromPage(object) && !HeapObject::cast(object)
                                           ->map_word(kRelaxedLoad)
                                           .IsForwardingAddress();
 }
 
 // Same as IsUnscavengedHeapObject() above but specialized for HeapObjects.
-V8_INLINE bool IsUnscavengedHeapObject(Heap* heap, HeapObject heap_object) {
+V8_INLINE bool IsUnscavengedHeapObject(Heap* heap,
+                                       Tagged<HeapObject> heap_object) {
   return Heap::InFromPage(heap_object) &&
          !heap_object->map_word(kRelaxedLoad).IsForwardingAddress();
 }
@@ -278,20 +283,20 @@ class GlobalHandlesWeakRootsUpdatingVisitor final : public RootVisitor {
 
  private:
   void UpdatePointer(FullObjectSlot p) {
-    Object object = *p;
+    Tagged<Object> object = *p;
     DCHECK(!HasWeakHeapObjectTag(object));
     // The object may be in the old generation as global handles over
     // approximates the list of young nodes. This checks also bails out for
     // Smis.
     if (!Heap::InYoungGeneration(object)) return;
 
-    HeapObject heap_object = HeapObject::cast(object);
+    Tagged<HeapObject> heap_object = HeapObject::cast(object);
     // TODO(chromium:1336158): Turn the following CHECKs into DCHECKs after
     // flushing out potential issues.
     CHECK(Heap::InFromPage(heap_object));
     MapWord first_word = heap_object->map_word(kRelaxedLoad);
     CHECK(first_word.IsForwardingAddress());
-    HeapObject dest = first_word.ToForwardingAddress(heap_object);
+    Tagged<HeapObject> dest = first_word.ToForwardingAddress(heap_object);
     HeapObjectReference::Update(FullHeapObjectSlot(p), dest);
     CHECK_IMPLIES(Heap::InYoungGeneration(dest),
                   Heap::InToPage(dest) || Heap::IsLargeObject(dest));
@@ -459,7 +464,8 @@ void ScavengerCollector::CollectGarbage() {
   // Since we promote all surviving large objects immediately, all remaining
   // large objects must be dead.
   // TODO(hpayer): Don't free all as soon as we have an intermediate generation.
-  heap_->new_lo_space()->FreeDeadObjects([](HeapObject) { return true; });
+  heap_->new_lo_space()->FreeDeadObjects(
+      [](Tagged<HeapObject>) { return true; });
 
   {
     TRACE_GC(heap_->tracer(), GCTracer::Scope::SCAVENGER_FREE_REMEMBERED_SET);
@@ -548,8 +554,8 @@ void ScavengerCollector::HandleSurvivingNewLargeObjects() {
 
   for (SurvivingNewLargeObjectMapEntry update_info :
        surviving_new_large_objects_) {
-    HeapObject object = update_info.first;
-    Map map = update_info.second;
+    Tagged<HeapObject> object = update_info.first;
+    Tagged<Map> map = update_info.second;
     // Order is important here. We have to re-install the map to have access
     // to meta-data like size during page promotion.
     object->set_map_word(map, kRelaxedStore);
@@ -638,8 +644,8 @@ Scavenger::Scavenger(ScavengerCollector* collector, Heap* heap, bool is_logging,
                  heap->incremental_marking()->IsMajorMarking());
 }
 
-void Scavenger::IterateAndScavengePromotedObject(HeapObject target, Map map,
-                                                 int size) {
+void Scavenger::IterateAndScavengePromotedObject(Tagged<HeapObject> target,
+                                                 Tagged<Map> map, int size) {
   // We are not collecting slots on new space objects during mutation thus we
   // have to scan for pointers to evacuation candidates when we promote
   // objects. But we should not record any slots in non-black objects. Grey
@@ -660,7 +666,8 @@ void Scavenger::IterateAndScavengePromotedObject(HeapObject target, Map map,
   }
 }
 
-void Scavenger::RememberPromotedEphemeron(EphemeronHashTable table, int index) {
+void Scavenger::RememberPromotedEphemeron(Tagged<EphemeronHashTable> table,
+                                          int index) {
   auto indices =
       ephemeron_remembered_set_.insert({table, std::unordered_set<int>()});
   indices.first->second.insert(index);
@@ -740,7 +747,7 @@ void Scavenger::Process(JobDelegate* delegate) {
 
     struct PromotionListEntry entry;
     while (promotion_list_local_.Pop(&entry)) {
-      HeapObject target = entry.heap_object;
+      Tagged<HeapObject> target = entry.heap_object;
       IterateAndScavengePromotedObject(target, entry.map, entry.size);
       done = false;
       if (delegate && ((++objects % kInterruptThreshold) == 0)) {
@@ -762,16 +769,16 @@ void ScavengerCollector::ProcessWeakReferences(
 // entry has a dead new-space key.
 void ScavengerCollector::ClearYoungEphemerons(
     EphemeronRememberedSet::TableList* ephemeron_table_list) {
-  ephemeron_table_list->Iterate([this](EphemeronHashTable table) {
+  ephemeron_table_list->Iterate([this](Tagged<EphemeronHashTable> table) {
     for (InternalIndex i : table->IterateEntries()) {
       // Keys in EphemeronHashTables must be heap objects.
       HeapObjectSlot key_slot(
           table->RawFieldOfElementAt(EphemeronHashTable::EntryToIndex(i)));
-      HeapObject key = key_slot.ToHeapObject();
+      Tagged<HeapObject> key = key_slot.ToHeapObject();
       if (IsUnscavengedHeapObject(heap_, key)) {
         table->RemoveEntry(i);
       } else {
-        HeapObject forwarded = ForwardingAddress(key);
+        Tagged<HeapObject> forwarded = ForwardingAddress(key);
         key_slot.StoreHeapObject(forwarded);
       }
     }
@@ -784,18 +791,18 @@ void ScavengerCollector::ClearYoungEphemerons(
 void ScavengerCollector::ClearOldEphemerons() {
   auto* table_map = heap_->ephemeron_remembered_set_->tables();
   for (auto it = table_map->begin(); it != table_map->end();) {
-    EphemeronHashTable table = it->first;
+    Tagged<EphemeronHashTable> table = it->first;
     auto& indices = it->second;
     for (auto iti = indices.begin(); iti != indices.end();) {
       // Keys in EphemeronHashTables must be heap objects.
       HeapObjectSlot key_slot(table->RawFieldOfElementAt(
           EphemeronHashTable::EntryToIndex(InternalIndex(*iti))));
-      HeapObject key = key_slot.ToHeapObject();
+      Tagged<HeapObject> key = key_slot.ToHeapObject();
       if (IsUnscavengedHeapObject(heap_, key)) {
         table->RemoveEntry(InternalIndex(*iti));
         iti = indices.erase(iti);
       } else {
-        HeapObject forwarded = ForwardingAddress(key);
+        Tagged<HeapObject> forwarded = ForwardingAddress(key);
         key_slot.StoreHeapObject(forwarded);
         if (!Heap::InYoungGeneration(forwarded)) {
           iti = indices.erase(iti);
@@ -837,7 +844,7 @@ void Scavenger::Publish() {
   promotion_list_local_.Publish();
 }
 
-void Scavenger::AddEphemeronHashTable(EphemeronHashTable table) {
+void Scavenger::AddEphemeronHashTable(Tagged<EphemeronHashTable> table) {
   ephemeron_table_list_local_.Push(table);
 }
 
@@ -845,7 +852,7 @@ template <typename TSlot>
 void Scavenger::CheckOldToNewSlotForSharedUntyped(MemoryChunk* chunk,
                                                   TSlot slot) {
   MaybeObject object = *slot;
-  HeapObject heap_object;
+  Tagged<HeapObject> heap_object;
 
   if (object.GetHeapObject(&heap_object) &&
       heap_object.InWritableSharedSpace()) {
@@ -858,7 +865,7 @@ void Scavenger::CheckOldToNewSlotForSharedTyped(MemoryChunk* chunk,
                                                 SlotType slot_type,
                                                 Address slot_address,
                                                 MaybeObject new_target) {
-  HeapObject heap_object;
+  Tagged<HeapObject> heap_object;
 
   if (new_target.GetHeapObject(&heap_object) &&
       heap_object.InWritableSharedSpace()) {
@@ -888,7 +895,7 @@ void RootScavengeVisitor::VisitRootPointers(Root root, const char* description,
 }
 
 void RootScavengeVisitor::ScavengePointer(FullObjectSlot p) {
-  Object object = *p;
+  Tagged<Object> object = *p;
   DCHECK(!HasWeakHeapObjectTag(object));
   DCHECK(!MapWord::IsPacked(object.ptr()));
   if (Heap::InYoungGeneration(object)) {
diff --git a/src/heap/scavenger.h b/src/heap/scavenger.h
index d0078c24a46..80f4cbf7a4b 100644
--- a/src/heap/scavenger.h
+++ b/src/heap/scavenger.h
@@ -39,7 +39,7 @@ class ScavengerCollector;
 class Scavenger {
  public:
   struct PromotionListEntry {
-    HeapObject heap_object;
+    Tagged<HeapObject> heap_object;
     Map map;
     int size;
   };
@@ -60,8 +60,9 @@ class Scavenger {
      public:
       explicit Local(PromotionList* promotion_list);
 
-      inline void PushRegularObject(HeapObject object, int size);
-      inline void PushLargeObject(HeapObject object, Map map, int size);
+      inline void PushRegularObject(Tagged<HeapObject> object, int size);
+      inline void PushLargeObject(Tagged<HeapObject> object, Tagged<Map> map,
+                                  int size);
       inline size_t LocalPushSegmentSize() const;
       inline bool Pop(struct PromotionListEntry* entry);
       inline bool IsGlobalPoolEmpty() const;
@@ -105,7 +106,7 @@ class Scavenger {
   void Finalize();
   void Publish();
 
-  void AddEphemeronHashTable(EphemeronHashTable table);
+  void AddEphemeronHashTable(Tagged<EphemeronHashTable> table);
 
   size_t bytes_copied() const { return copied_size_; }
   size_t bytes_promoted() const { return promoted_size_; }
@@ -139,60 +140,64 @@ class Scavenger {
   // to be in from space.
   template <typename THeapObjectSlot>
   inline SlotCallbackResult ScavengeObject(THeapObjectSlot p,
-                                           HeapObject object);
+                                           Tagged<HeapObject> object);
 
   // Copies |source| to |target| and sets the forwarding pointer in |source|.
-  V8_INLINE bool MigrateObject(Map map, HeapObject source, HeapObject target,
-                               int size,
+  V8_INLINE bool MigrateObject(Tagged<Map> map, Tagged<HeapObject> source,
+                               Tagged<HeapObject> target, int size,
                                PromotionHeapChoice promotion_heap_choice);
 
   V8_INLINE SlotCallbackResult
   RememberedSetEntryNeeded(CopyAndForwardResult result);
 
   template <typename THeapObjectSlot>
-  V8_INLINE CopyAndForwardResult
-  SemiSpaceCopyObject(Map map, THeapObjectSlot slot, HeapObject object,
-                      int object_size, ObjectFields object_fields);
+  V8_INLINE CopyAndForwardResult SemiSpaceCopyObject(
+      Tagged<Map> map, THeapObjectSlot slot, Tagged<HeapObject> object,
+      int object_size, ObjectFields object_fields);
 
   template <typename THeapObjectSlot,
             PromotionHeapChoice promotion_heap_choice = kPromoteIntoLocalHeap>
-  V8_INLINE CopyAndForwardResult PromoteObject(Map map, THeapObjectSlot slot,
-                                               HeapObject object,
+  V8_INLINE CopyAndForwardResult PromoteObject(Tagged<Map> map,
+                                               THeapObjectSlot slot,
+                                               Tagged<HeapObject> object,
                                                int object_size,
                                                ObjectFields object_fields);
 
   template <typename THeapObjectSlot>
-  V8_INLINE SlotCallbackResult EvacuateObject(THeapObjectSlot slot, Map map,
-                                              HeapObject source);
+  V8_INLINE SlotCallbackResult EvacuateObject(THeapObjectSlot slot,
+                                              Tagged<Map> map,
+                                              Tagged<HeapObject> source);
 
-  V8_INLINE bool HandleLargeObject(Map map, HeapObject object, int object_size,
-                                   ObjectFields object_fields);
+  V8_INLINE bool HandleLargeObject(Tagged<Map> map, Tagged<HeapObject> object,
+                                   int object_size, ObjectFields object_fields);
 
   // Different cases for object evacuation.
   template <typename THeapObjectSlot,
             PromotionHeapChoice promotion_heap_choice = kPromoteIntoLocalHeap>
-  V8_INLINE SlotCallbackResult
-  EvacuateObjectDefault(Map map, THeapObjectSlot slot, HeapObject object,
-                        int object_size, ObjectFields object_fields);
+  V8_INLINE SlotCallbackResult EvacuateObjectDefault(
+      Tagged<Map> map, THeapObjectSlot slot, Tagged<HeapObject> object,
+      int object_size, ObjectFields object_fields);
 
   template <typename THeapObjectSlot>
-  inline SlotCallbackResult EvacuateThinString(Map map, THeapObjectSlot slot,
-                                               ThinString object,
+  inline SlotCallbackResult EvacuateThinString(Tagged<Map> map,
+                                               THeapObjectSlot slot,
+                                               Tagged<ThinString> object,
                                                int object_size);
 
   template <typename THeapObjectSlot>
-  inline SlotCallbackResult EvacuateShortcutCandidate(Map map,
+  inline SlotCallbackResult EvacuateShortcutCandidate(Tagged<Map> map,
                                                       THeapObjectSlot slot,
-                                                      ConsString object,
+                                                      Tagged<ConsString> object,
                                                       int object_size);
 
   template <typename THeapObjectSlot>
   inline SlotCallbackResult EvacuateInPlaceInternalizableString(
-      Map map, THeapObjectSlot slot, String string, int object_size,
-      ObjectFields object_fields);
+      Tagged<Map> map, THeapObjectSlot slot, Tagged<String> string,
+      int object_size, ObjectFields object_fields);
 
-  void IterateAndScavengePromotedObject(HeapObject target, Map map, int size);
-  void RememberPromotedEphemeron(EphemeronHashTable table, int index);
+  void IterateAndScavengePromotedObject(Tagged<HeapObject> target,
+                                        Tagged<Map> map, int size);
+  void RememberPromotedEphemeron(Tagged<EphemeronHashTable> table, int index);
 
   ScavengerCollector* const collector_;
   Heap* const heap_;
diff --git a/src/heap/setup-heap-internal.cc b/src/heap/setup-heap-internal.cc
index 7d29c7ce2a2..9f51a70c46c 100644
--- a/src/heap/setup-heap-internal.cc
+++ b/src/heap/setup-heap-internal.cc
@@ -256,7 +256,7 @@ AllocationResult Heap::AllocateMap(AllocationType allocation_type,
                                    ElementsKind elements_kind,
                                    int inobject_properties) {
   static_assert(LAST_JS_OBJECT_TYPE == LAST_TYPE);
-  HeapObject result;
+  Tagged<HeapObject> result;
   DCHECK_EQ(allocation_type, IsMutableMap(instance_type, elements_kind)
                                  ? AllocationType::kMap
                                  : AllocationType::kReadOnly);
@@ -265,7 +265,7 @@ AllocationResult Heap::AllocateMap(AllocationType allocation_type,
 
   result->set_map_after_allocation(ReadOnlyRoots(this).meta_map(),
                                    SKIP_WRITE_BARRIER);
-  Map map = isolate()->factory()->InitializeMap(
+  Tagged<Map> map = isolate()->factory()->InitializeMap(
       Map::cast(result), instance_type, instance_size, elements_kind,
       inobject_properties, this);
 
@@ -274,12 +274,12 @@ AllocationResult Heap::AllocateMap(AllocationType allocation_type,
 
 AllocationResult Heap::AllocatePartialMap(InstanceType instance_type,
                                           int instance_size) {
-  Object result;
+  Tagged<Object> result;
   AllocationResult allocation =
       AllocateRaw(Map::kSize, AllocationType::kReadOnly);
   if (!allocation.To(&result)) return allocation;
   // Map::cast cannot be used due to uninitialized map field.
-  Map map = Map::unchecked_cast(result);
+  Tagged<Map> map = Map::unchecked_cast(result);
   map->set_map_after_allocation(
       Map::unchecked_cast(isolate()->root(RootIndex::kMetaMap)),
       SKIP_WRITE_BARRIER);
@@ -316,7 +316,7 @@ AllocationResult Heap::Allocate(Handle<Map> map,
                                 AllocationType allocation_type) {
   DCHECK(map->instance_type() != MAP_TYPE);
   int size = map->instance_size();
-  HeapObject result;
+  Tagged<HeapObject> result;
   AllocationResult allocation = AllocateRaw(size, allocation_type);
   if (!allocation.To(&result)) return allocation;
   // New space objects are allocated white.
@@ -330,13 +330,13 @@ AllocationResult Heap::Allocate(Handle<Map> map,
 bool Heap::CreateEarlyReadOnlyMaps() {
   // Setup maps which are used often, or used in CreateImportantReadOnlyObjects.
   ReadOnlyRoots roots(this);
-  HeapObject obj;
+  Tagged<HeapObject> obj;
   {
     AllocationResult allocation = AllocatePartialMap(MAP_TYPE, Map::kSize);
     if (!allocation.To(&obj)) return false;
   }
   // Map::cast cannot be used due to uninitialized map field.
-  Map new_meta_map = Map::unchecked_cast(obj);
+  Tagged<Map> new_meta_map = Map::unchecked_cast(obj);
   set_meta_map(new_meta_map);
   new_meta_map->set_map_after_allocation(new_meta_map);
 
@@ -367,7 +367,7 @@ bool Heap::CreateEarlyReadOnlyMaps() {
     // Some struct maps which we need for later dependencies
     for (const StructInit& entry : kStructTable) {
       if (!is_important_struct(entry.type)) continue;
-      Map map;
+      Tagged<Map> map;
       if (!AllocatePartialMap(entry.type, entry.size).To(&map)) return false;
       roots_table()[entry.index] = map.ptr();
     }
@@ -446,7 +446,7 @@ bool Heap::CreateEarlyReadOnlyMaps() {
     if (!AllocateRaw(size, AllocationType::kReadOnly).To(&obj)) return false;
     obj->set_map_after_allocation(roots.descriptor_array_map(),
                                   SKIP_WRITE_BARRIER);
-    DescriptorArray array = DescriptorArray::cast(obj);
+    Tagged<DescriptorArray> array = DescriptorArray::cast(obj);
     array->Initialize(roots.empty_enum_cache(), roots.undefined_value(), 0, 0,
                       DescriptorArrayMarkingState::kInitialGCState);
   }
@@ -517,7 +517,7 @@ bool Heap::CreateEarlyReadOnlyMaps() {
                            Context::SYMBOL_FUNCTION_INDEX)
 
     for (const StringTypeInit& entry : kStringTypeTable) {
-      Map map;
+      Tagged<Map> map;
       if (!AllocateMap(AllocationType::kReadOnly, entry.type, entry.size)
                .To(&map)) {
         return false;
@@ -546,7 +546,7 @@ bool Heap::CreateEarlyReadOnlyMaps() {
     ALLOCATE_MAP(CELL_TYPE, Cell::kSize, cell);
     {
       // The invalid_prototype_validity_cell is needed for JSObject maps.
-      Smi value = Smi::FromInt(Map::kPrototypeChainInvalid);
+      Tagged<Smi> value = Smi::FromInt(Map::kPrototypeChainInvalid);
       AllocationResult alloc =
           AllocateRaw(Cell::kSize, AllocationType::kReadOnly);
       if (!alloc.To(&obj)) return false;
@@ -600,7 +600,7 @@ bool Heap::CreateLateReadOnlyNonJSReceiverMaps() {
     // Setup the struct maps.
     for (const StructInit& entry : kStructTable) {
       if (is_important_struct(entry.type)) continue;
-      Map map;
+      Tagged<Map> map;
       if (!AllocateMap(AllocationType::kReadOnly, entry.type, entry.size)
                .To(&map))
         return false;
@@ -693,7 +693,7 @@ bool Heap::CreateLateReadOnlyJSReceiverMaps() {
 
   // Shared space object maps are immutable and can be in RO space.
   {
-    Map shared_array_map;
+    Tagged<Map> shared_array_map;
     if (!AllocateMap(AllocationType::kReadOnly, JS_SHARED_ARRAY_TYPE,
                      JSSharedArray::kSize, SHARED_ARRAY_ELEMENTS,
                      JSSharedArray::kInObjectFieldCount)
@@ -737,7 +737,7 @@ void Heap::StaticRootsEnsureAllocatedSize(Handle<HeapObject> obj,
     CHECK_LT(obj_size, required);
     int filler_size = required - obj_size;
 
-    HeapObject filler =
+    Tagged<HeapObject> filler =
         allocator()->AllocateRawWith<HeapAllocator::kRetryOrFail>(
             filler_size, AllocationType::kReadOnly, AllocationOrigin::kRuntime,
             AllocationAlignment::kTaggedAligned);
@@ -751,7 +751,7 @@ void Heap::StaticRootsEnsureAllocatedSize(Handle<HeapObject> obj,
 
 bool Heap::CreateImportantReadOnlyObjects() {
   // Allocate some objects early to get addresses to fit as arm64 immediates.
-  HeapObject obj;
+  Tagged<HeapObject> obj;
   ReadOnlyRoots roots(isolate());
 
   // Bools
@@ -876,7 +876,7 @@ bool Heap::CreateReadOnlyObjects() {
   HandleScope initial_objects_handle_scope(isolate());
   Factory* factory = isolate()->factory();
   ReadOnlyRoots roots(this);
-  HeapObject obj;
+  Tagged<HeapObject> obj;
 
   // Empty elements
   {
@@ -1172,7 +1172,7 @@ bool Heap::CreateReadOnlyObjects() {
     // TODO(v8:7748) Depending on where we end up this might actually not hold,
     // in which case we would need to use a one or two-word filler.
     CHECK(filler_size > 2 * kTaggedSize);
-    HeapObject filler =
+    Tagged<HeapObject> filler =
         allocator()->AllocateRawWith<HeapAllocator::kRetryOrFail>(
             filler_size, AllocationType::kReadOnly, AllocationOrigin::kRuntime,
             AllocationAlignment::kTaggedAligned);
@@ -1186,7 +1186,7 @@ bool Heap::CreateReadOnlyObjects() {
 
   // Finally, allocate the wasm-null object.
   {
-    HeapObject obj;
+    Tagged<HeapObject> obj;
     CHECK(AllocateRaw(WasmNull::kSize, AllocationType::kReadOnly).To(&obj));
     // No need to initialize the payload since it's either empty or unmapped.
     CHECK_IMPLIES(!(V8_STATIC_ROOTS_BOOL || V8_STATIC_ROOTS_GENERATION_BOOL),
diff --git a/src/heap/spaces-inl.h b/src/heap/spaces-inl.h
index 86074b90b22..2c4e9be31e7 100644
--- a/src/heap/spaces-inl.h
+++ b/src/heap/spaces-inl.h
@@ -131,7 +131,7 @@ AllocationResult LocalAllocationBuffer::AllocateRawAligned(
   if (!allocation_info_.CanIncrementTop(aligned_size)) {
     return AllocationResult::Failure();
   }
-  HeapObject object =
+  Tagged<HeapObject> object =
       HeapObject::FromAddress(allocation_info_.IncrementTop(aligned_size));
   return filler_size > 0 ? AllocationResult::FromObject(
                                heap_->PrecedeWithFiller(object, filler_size))
@@ -151,7 +151,7 @@ LocalAllocationBuffer LocalAllocationBuffer::FromResult(Heap* heap,
                                                         AllocationResult result,
                                                         intptr_t size) {
   if (result.IsFailure()) return InvalidBuffer();
-  HeapObject obj;
+  Tagged<HeapObject> obj;
   bool ok = result.To(&obj);
   USE(ok);
   DCHECK(ok);
@@ -163,7 +163,8 @@ bool LocalAllocationBuffer::TryMerge(LocalAllocationBuffer* other) {
   return allocation_info_.MergeIfAdjacent(other->allocation_info_);
 }
 
-bool LocalAllocationBuffer::TryFreeLast(HeapObject object, int object_size) {
+bool LocalAllocationBuffer::TryFreeLast(Tagged<HeapObject> object,
+                                        int object_size) {
   if (IsValid()) {
     const Address object_address = object.address();
     return allocation_info_.DecrementTopIfAdjacent(object_address, object_size);
@@ -195,7 +196,7 @@ AllocationResult SpaceWithLinearArea::AllocateFastUnaligned(
   if (!allocation_info_.CanIncrementTop(size_in_bytes)) {
     return AllocationResult::Failure();
   }
-  HeapObject obj =
+  Tagged<HeapObject> obj =
       HeapObject::FromAddress(allocation_info_.IncrementTop(size_in_bytes));
 
   MSAN_ALLOCATED_UNINITIALIZED_MEMORY(obj.address(), size_in_bytes);
@@ -213,7 +214,7 @@ AllocationResult SpaceWithLinearArea::AllocateFastAligned(
   if (!allocation_info_.CanIncrementTop(aligned_size_in_bytes)) {
     return AllocationResult::Failure();
   }
-  HeapObject obj = HeapObject::FromAddress(
+  Tagged<HeapObject> obj = HeapObject::FromAddress(
       allocation_info_.IncrementTop(aligned_size_in_bytes));
   if (result_aligned_size_in_bytes)
     *result_aligned_size_in_bytes = aligned_size_in_bytes;
diff --git a/src/heap/spaces.h b/src/heap/spaces.h
index 9be915bfe90..c9d8b0a3a99 100644
--- a/src/heap/spaces.h
+++ b/src/heap/spaces.h
@@ -280,7 +280,7 @@ class LocalAllocationBuffer {
   // Returns true if the merge was successful, false otherwise.
   inline bool TryMerge(LocalAllocationBuffer* other);
 
-  inline bool TryFreeLast(HeapObject object, int object_size);
+  inline bool TryFreeLast(Tagged<HeapObject> object, int object_size);
 
   // Close a LAB, effectively invalidating it. Returns the unused area.
   V8_EXPORT_PRIVATE LinearAllocationArea CloseAndMakeIterable();
diff --git a/src/heap/sweeper.cc b/src/heap/sweeper.cc
index 64be40a512d..c8649bfa21a 100644
--- a/src/heap/sweeper.cc
+++ b/src/heap/sweeper.cc
@@ -431,8 +431,8 @@ class PromotedPageRecordMigratedSlotVisitor final
            host_chunk->owner_identity() == LO_SPACE);
   }
 
-  void Process(HeapObject object) {
-    Map map = object->map(cage_base());
+  void Process(Tagged<HeapObject> object) {
+    Tagged<Map> map = object->map(cage_base());
     if (Map::ObjectFieldsFrom(map->visitor_id()) == ObjectFields::kDataOnly) {
       return;
     }
@@ -446,33 +446,36 @@ class PromotedPageRecordMigratedSlotVisitor final
   // more unsafe shape changes that happen concurrently.
   V8_INLINE static constexpr bool EnableConcurrentVisitation() { return true; }
 
-  V8_INLINE void VisitMapPointer(HeapObject host) final {
+  V8_INLINE void VisitMapPointer(Tagged<HeapObject> host) final {
     VerifyHost(host);
     VisitObjectImpl(host, host->map(cage_base()), host->map_slot().address());
   }
 
-  V8_INLINE void VisitPointer(HeapObject host, ObjectSlot p) final {
+  V8_INLINE void VisitPointer(Tagged<HeapObject> host, ObjectSlot p) final {
     VisitPointersImpl(host, p, p + 1);
   }
-  V8_INLINE void VisitPointer(HeapObject host, MaybeObjectSlot p) final {
+  V8_INLINE void VisitPointer(Tagged<HeapObject> host,
+                              MaybeObjectSlot p) final {
     VisitPointersImpl(host, p, p + 1);
   }
-  V8_INLINE void VisitPointers(HeapObject host, ObjectSlot start,
+  V8_INLINE void VisitPointers(Tagged<HeapObject> host, ObjectSlot start,
                                ObjectSlot end) final {
     VisitPointersImpl(host, start, end);
   }
-  V8_INLINE void VisitPointers(HeapObject host, MaybeObjectSlot start,
+  V8_INLINE void VisitPointers(Tagged<HeapObject> host, MaybeObjectSlot start,
                                MaybeObjectSlot end) final {
     VisitPointersImpl(host, start, end);
   }
 
-  V8_INLINE int VisitJSArrayBuffer(Map map, JSArrayBuffer object) {
+  V8_INLINE int VisitJSArrayBuffer(Tagged<Map> map,
+                                   Tagged<JSArrayBuffer> object) {
     object->YoungMarkExtensionPromoted();
     return NewSpaceVisitor<
         PromotedPageRecordMigratedSlotVisitor>::VisitJSArrayBuffer(map, object);
   }
 
-  V8_INLINE int VisitEphemeronHashTable(Map map, EphemeronHashTable table) {
+  V8_INLINE int VisitEphemeronHashTable(Tagged<Map> map,
+                                        Tagged<EphemeronHashTable> table) {
     NewSpaceVisitor<PromotedPageRecordMigratedSlotVisitor>::
         VisitMapPointerIfNeeded<VisitorId::kVisitEphemeronHashTable>(table);
     EphemeronRememberedSet::IndicesSet indices;
@@ -482,8 +485,8 @@ class PromotedPageRecordMigratedSlotVisitor final
       VisitPointer(table, value_slot);
       ObjectSlot key_slot =
           table->RawFieldOfElementAt(EphemeronHashTable::EntryToIndex(i));
-      Object key = key_slot.Acquire_Load();
-      HeapObject key_object;
+      Tagged<Object> key = key_slot.Acquire_Load();
+      Tagged<HeapObject> key_object;
       if (!key.GetHeapObject(&key_object)) continue;
 #ifdef THREAD_SANITIZER
       BasicMemoryChunk::FromHeapObject(key_object)->SynchronizedHeapLoad();
@@ -500,9 +503,11 @@ class PromotedPageRecordMigratedSlotVisitor final
   }
 
   // Entries that are skipped for recording.
-  void VisitExternalReference(InstructionStream host, RelocInfo* rinfo) final {}
-  void VisitInternalReference(InstructionStream host, RelocInfo* rinfo) final {}
-  void VisitExternalPointer(HeapObject host, ExternalPointerSlot slot,
+  void VisitExternalReference(Tagged<InstructionStream> host,
+                              RelocInfo* rinfo) final {}
+  void VisitInternalReference(Tagged<InstructionStream> host,
+                              RelocInfo* rinfo) final {}
+  void VisitExternalPointer(Tagged<HeapObject> host, ExternalPointerSlot slot,
                             ExternalPointerTag tag) final {}
 
   // Maps can be shared, so we need to visit them to record old to shared slots.
@@ -512,7 +517,7 @@ class PromotedPageRecordMigratedSlotVisitor final
   }
 
  private:
-  V8_INLINE void VerifyHost(HeapObject host) {
+  V8_INLINE void VerifyHost(Tagged<HeapObject> host) {
     DCHECK(!host.InWritableSharedSpace());
     DCHECK(!Heap::InYoungGeneration(host));
     DCHECK(!MemoryChunk::FromHeapObject(host)->SweepingDone());
@@ -520,9 +525,9 @@ class PromotedPageRecordMigratedSlotVisitor final
   }
 
   template <typename TObject>
-  V8_INLINE void VisitObjectImpl(HeapObject host, TObject object,
+  V8_INLINE void VisitObjectImpl(Tagged<HeapObject> host, TObject object,
                                  Address slot) {
-    HeapObject value_heap_object;
+    Tagged<HeapObject> value_heap_object;
     if (!object.GetHeapObject(&value_heap_object)) return;
 
     BasicMemoryChunk* value_chunk =
@@ -540,7 +545,8 @@ class PromotedPageRecordMigratedSlotVisitor final
   }
 
   template <typename TSlot>
-  V8_INLINE void VisitPointersImpl(HeapObject host, TSlot start, TSlot end) {
+  V8_INLINE void VisitPointersImpl(Tagged<HeapObject> host, TSlot start,
+                                   TSlot end) {
     VerifyHost(host);
     for (TSlot slot = start; slot < end; ++slot) {
       typename TSlot::TObject target =
diff --git a/src/heap/third-party/heap-api-stub.cc b/src/heap/third-party/heap-api-stub.cc
index bf8ba63ce64..7b1169a950c 100644
--- a/src/heap/third-party/heap-api-stub.cc
+++ b/src/heap/third-party/heap-api-stub.cc
@@ -39,7 +39,7 @@ const base::AddressRegion& Heap::GetCodeRange() {
   return no_region;
 }
 
-bool Heap::IsPendingAllocation(HeapObject) { return false; }
+bool Heap::IsPendingAllocation(Tagged<HeapObject>) { return false; }
 
 // static
 bool Heap::InSpace(Address, AllocationSpace) { return false; }
@@ -54,13 +54,13 @@ bool Heap::InReadOnlySpace(Address) { return false; }
 bool Heap::InLargeObjectSpace(Address address) { return false; }
 
 // static
-bool Heap::IsValidHeapObject(HeapObject) { return false; }
+bool Heap::IsValidHeapObject(Tagged<HeapObject>) { return false; }
 
 // static
-bool Heap::IsImmovable(HeapObject) { return false; }
+bool Heap::IsImmovable(Tagged<HeapObject>) { return false; }
 
 // static
-bool Heap::IsValidCodeObject(HeapObject) { return false; }
+bool Heap::IsValidCodeObject(Tagged<HeapObject>) { return false; }
 
 void Heap::ResetIterator() {}
 
diff --git a/src/heap/third-party/heap-api.h b/src/heap/third-party/heap-api.h
index 902ffc17d87..3816defe5d0 100644
--- a/src/heap/third-party/heap-api.h
+++ b/src/heap/third-party/heap-api.h
@@ -30,7 +30,7 @@ class Heap {
 
   const base::AddressRegion& GetCodeRange();
 
-  bool IsPendingAllocation(HeapObject object);
+  bool IsPendingAllocation(Tagged<HeapObject> object);
 
   static bool InSpace(Address address, AllocationSpace space);
 
@@ -40,11 +40,11 @@ class Heap {
 
   static bool InLargeObjectSpace(Address address);
 
-  static bool IsValidHeapObject(HeapObject object);
+  static bool IsValidHeapObject(Tagged<HeapObject> object);
 
-  static bool IsImmovable(HeapObject object);
+  static bool IsImmovable(Tagged<HeapObject> object);
 
-  static bool IsValidCodeObject(HeapObject object);
+  static bool IsValidCodeObject(Tagged<HeapObject> object);
 
   void ResetIterator();
   Tagged<HeapObject> NextObject();
diff --git a/src/heap/traced-handles-marking-visitor.cc b/src/heap/traced-handles-marking-visitor.cc
index 82fc1b1d7e6..83ad7f04328 100644
--- a/src/heap/traced-handles-marking-visitor.cc
+++ b/src/heap/traced-handles-marking-visitor.cc
@@ -45,7 +45,7 @@ void ConservativeTracedHandlesMarkingVisitor::VisitPointer(
       // object to mark.
       return;
     }
-    HeapObject heap_object = HeapObject::cast(object);
+    Tagged<HeapObject> heap_object = HeapObject::cast(object);
     if (heap_object.InReadOnlySpace()) return;
     if (marking_state_.TryMark(heap_object)) {
       local_marking_worklist_.Push(heap_object);
@@ -57,7 +57,7 @@ void ConservativeTracedHandlesMarkingVisitor::VisitPointer(
 }
 
 bool ConservativeTracedHandlesMarkingVisitor::ShouldMarkObject(
-    HeapObject object) const {
+    Tagged<HeapObject> object) const {
   // Keep up-to-date with MarkCompactCollector::ShouldMarkObject.
   if (V8_LIKELY(!has_shared_space_)) return true;
   if (is_shared_space_isolate_) return true;
diff --git a/src/heap/traced-handles-marking-visitor.h b/src/heap/traced-handles-marking-visitor.h
index 47022f72a39..174ba9150fb 100644
--- a/src/heap/traced-handles-marking-visitor.h
+++ b/src/heap/traced-handles-marking-visitor.h
@@ -25,7 +25,7 @@ class ConservativeTracedHandlesMarkingVisitor final
   ~ConservativeTracedHandlesMarkingVisitor() override = default;
 
   void VisitPointer(const void*) override;
-  bool ShouldMarkObject(HeapObject object) const;
+  bool ShouldMarkObject(Tagged<HeapObject> object) const;
 
  private:
   Heap& heap_;
diff --git a/src/heap/weak-object-worklists.cc b/src/heap/weak-object-worklists.cc
index ebe72bd47fd..a2c80c474c3 100644
--- a/src/heap/weak-object-worklists.cc
+++ b/src/heap/weak-object-worklists.cc
@@ -46,16 +46,17 @@ void WeakObjects::Clear() {
 
 // static
 void WeakObjects::UpdateTransitionArrays(
-    WeakObjectWorklist<TransitionArray>& transition_arrays) {
+    WeakObjectWorklist<Tagged<TransitionArray>>& transition_arrays) {
   DCHECK(!ContainsYoungObjects(transition_arrays));
 }
 
 // static
 void WeakObjects::UpdateEphemeronHashTables(
-    WeakObjectWorklist<EphemeronHashTable>& ephemeron_hash_tables) {
+    WeakObjectWorklist<Tagged<EphemeronHashTable>>& ephemeron_hash_tables) {
   ephemeron_hash_tables.Update(
-      [](EphemeronHashTable slot_in, EphemeronHashTable* slot_out) -> bool {
-        EphemeronHashTable forwarded = ForwardingAddress(slot_in);
+      [](Tagged<EphemeronHashTable> slot_in,
+         Tagged<EphemeronHashTable>* slot_out) -> bool {
+        Tagged<EphemeronHashTable> forwarded = ForwardingAddress(slot_in);
 
         if (!forwarded.is_null()) {
           *slot_out = forwarded;
@@ -68,10 +69,10 @@ void WeakObjects::UpdateEphemeronHashTables(
 
 namespace {
 bool EphemeronUpdater(Ephemeron slot_in, Ephemeron* slot_out) {
-  HeapObject key = slot_in.key;
-  HeapObject value = slot_in.value;
-  HeapObject forwarded_key = ForwardingAddress(key);
-  HeapObject forwarded_value = ForwardingAddress(value);
+  Tagged<HeapObject> key = slot_in.key;
+  Tagged<HeapObject> value = slot_in.value;
+  Tagged<HeapObject> forwarded_key = ForwardingAddress(key);
+  Tagged<HeapObject> forwarded_value = ForwardingAddress(value);
 
   if (!forwarded_key.is_null() && !forwarded_value.is_null()) {
     *slot_out = Ephemeron{forwarded_key, forwarded_value};
@@ -105,8 +106,8 @@ void WeakObjects::UpdateWeakReferences(
     WeakObjectWorklist<HeapObjectAndSlot>& weak_references) {
   weak_references.Update(
       [](HeapObjectAndSlot slot_in, HeapObjectAndSlot* slot_out) -> bool {
-        HeapObject heap_obj = slot_in.first;
-        HeapObject forwarded = ForwardingAddress(heap_obj);
+        Tagged<HeapObject> heap_obj = slot_in.first;
+        Tagged<HeapObject> forwarded = ForwardingAddress(heap_obj);
 
         if (!forwarded.is_null()) {
           ptrdiff_t distance_to_slot =
@@ -126,8 +127,8 @@ void WeakObjects::UpdateWeakObjectsInCode(
     WeakObjectWorklist<HeapObjectAndCode>& weak_objects_in_code) {
   weak_objects_in_code.Update(
       [](HeapObjectAndCode slot_in, HeapObjectAndCode* slot_out) -> bool {
-        HeapObject heap_obj = slot_in.first;
-        HeapObject forwarded = ForwardingAddress(heap_obj);
+        Tagged<HeapObject> heap_obj = slot_in.first;
+        Tagged<HeapObject> forwarded = ForwardingAddress(heap_obj);
 
         if (!forwarded.is_null()) {
           slot_out->first = forwarded;
@@ -141,38 +142,39 @@ void WeakObjects::UpdateWeakObjectsInCode(
 
 // static
 void WeakObjects::UpdateJSWeakRefs(
-    WeakObjectWorklist<JSWeakRef>& js_weak_refs) {
-  js_weak_refs.Update(
-      [](JSWeakRef js_weak_ref_in, JSWeakRef* js_weak_ref_out) -> bool {
-        JSWeakRef forwarded = ForwardingAddress(js_weak_ref_in);
-
-        if (!forwarded.is_null()) {
-          *js_weak_ref_out = forwarded;
-          return true;
-        }
+    WeakObjectWorklist<Tagged<JSWeakRef>>& js_weak_refs) {
+  js_weak_refs.Update([](Tagged<JSWeakRef> js_weak_ref_in,
+                         Tagged<JSWeakRef>* js_weak_ref_out) -> bool {
+    Tagged<JSWeakRef> forwarded = ForwardingAddress(js_weak_ref_in);
+
+    if (!forwarded.is_null()) {
+      *js_weak_ref_out = forwarded;
+      return true;
+    }
 
-        return false;
-      });
+    return false;
+  });
 }
 
 // static
-void WeakObjects::UpdateWeakCells(WeakObjectWorklist<WeakCell>& weak_cells) {
+void WeakObjects::UpdateWeakCells(
+    WeakObjectWorklist<Tagged<WeakCell>>& weak_cells) {
   // TODO(syg, marja): Support WeakCells in the young generation.
   DCHECK(!ContainsYoungObjects(weak_cells));
 }
 
 // static
 void WeakObjects::UpdateCodeFlushingCandidates(
-    WeakObjectWorklist<SharedFunctionInfo>& code_flushing_candidates) {
+    WeakObjectWorklist<Tagged<SharedFunctionInfo>>& code_flushing_candidates) {
   DCHECK(!ContainsYoungObjects(code_flushing_candidates));
 }
 
 // static
 void WeakObjects::UpdateFlushedJSFunctions(
-    WeakObjectWorklist<JSFunction>& flushed_js_functions) {
+    WeakObjectWorklist<Tagged<JSFunction>>& flushed_js_functions) {
   flushed_js_functions.Update(
-      [](JSFunction slot_in, JSFunction* slot_out) -> bool {
-        JSFunction forwarded = ForwardingAddress(slot_in);
+      [](Tagged<JSFunction> slot_in, Tagged<JSFunction>* slot_out) -> bool {
+        Tagged<JSFunction> forwarded = ForwardingAddress(slot_in);
 
         if (!forwarded.is_null()) {
           *slot_out = forwarded;
@@ -185,10 +187,10 @@ void WeakObjects::UpdateFlushedJSFunctions(
 
 // static
 void WeakObjects::UpdateBaselineFlushingCandidates(
-    WeakObjectWorklist<JSFunction>& baseline_flush_candidates) {
+    WeakObjectWorklist<Tagged<JSFunction>>& baseline_flush_candidates) {
   baseline_flush_candidates.Update(
-      [](JSFunction slot_in, JSFunction* slot_out) -> bool {
-        JSFunction forwarded = ForwardingAddress(slot_in);
+      [](Tagged<JSFunction> slot_in, Tagged<JSFunction>* slot_out) -> bool {
+        Tagged<JSFunction> forwarded = ForwardingAddress(slot_in);
 
         if (!forwarded.is_null()) {
           *slot_out = forwarded;
@@ -202,9 +204,10 @@ void WeakObjects::UpdateBaselineFlushingCandidates(
 #ifdef DEBUG
 // static
 template <typename Type>
-bool WeakObjects::ContainsYoungObjects(WeakObjectWorklist<Type>& worklist) {
+bool WeakObjects::ContainsYoungObjects(
+    WeakObjectWorklist<Tagged<Type>>& worklist) {
   bool result = false;
-  worklist.Iterate([&result](Type candidate) {
+  worklist.Iterate([&result](Tagged<Type> candidate) {
     if (Heap::InYoungGeneration(candidate)) {
       result = true;
     }
diff --git a/src/heap/weak-object-worklists.h b/src/heap/weak-object-worklists.h
index a921a415079..f29e2d07c12 100644
--- a/src/heap/weak-object-worklists.h
+++ b/src/heap/weak-object-worklists.h
@@ -14,12 +14,12 @@ namespace v8 {
 namespace internal {
 
 struct Ephemeron {
-  HeapObject key;
-  HeapObject value;
+  Tagged<HeapObject> key;
+  Tagged<HeapObject> value;
 };
 
-using HeapObjectAndSlot = std::pair<HeapObject, HeapObjectSlot>;
-using HeapObjectAndCode = std::pair<HeapObject, Code>;
+using HeapObjectAndSlot = std::pair<Tagged<HeapObject>, HeapObjectSlot>;
+using HeapObjectAndCode = std::pair<Tagged<HeapObject>, Tagged<Code>>;
 class EphemeronHashTable;
 class JSFunction;
 class SharedFunctionInfo;
@@ -35,10 +35,10 @@ class TransitionArray;
 // If you add a new entry, then you also need to implement the corresponding
 // Update*() function in the cc file for updating pointers after Scavenge.
 #define WEAK_OBJECT_WORKLISTS(F)                                             \
-  F(TransitionArray, transition_arrays, TransitionArrays)                    \
+  F(Tagged<TransitionArray>, transition_arrays, TransitionArrays)            \
   /* Keep track of all EphemeronHashTables in the heap to process            \
      them in the atomic pause. */                                            \
-  F(EphemeronHashTable, ephemeron_hash_tables, EphemeronHashTables)          \
+  F(Tagged<EphemeronHashTable>, ephemeron_hash_tables, EphemeronHashTables)  \
   /* Keep track of all ephemerons for concurrent marking tasks. Only store   \
      ephemerons in these worklists if both (key, value) are unreachable at   \
      the moment.                                                             \
@@ -55,11 +55,13 @@ class TransitionArray;
      Optimize this by adding a different storage for old space. */           \
   F(HeapObjectAndSlot, weak_references, WeakReferences)                      \
   F(HeapObjectAndCode, weak_objects_in_code, WeakObjectsInCode)              \
-  F(JSWeakRef, js_weak_refs, JSWeakRefs)                                     \
-  F(WeakCell, weak_cells, WeakCells)                                         \
-  F(SharedFunctionInfo, code_flushing_candidates, CodeFlushingCandidates)    \
-  F(JSFunction, baseline_flushing_candidates, BaselineFlushingCandidates)    \
-  F(JSFunction, flushed_js_functions, FlushedJSFunctions)
+  F(Tagged<JSWeakRef>, js_weak_refs, JSWeakRefs)                             \
+  F(Tagged<WeakCell>, weak_cells, WeakCells)                                 \
+  F(Tagged<SharedFunctionInfo>, code_flushing_candidates,                    \
+    CodeFlushingCandidates)                                                  \
+  F(Tagged<JSFunction>, baseline_flushing_candidates,                        \
+    BaselineFlushingCandidates)                                              \
+  F(Tagged<JSFunction>, flushed_js_functions, FlushedJSFunctions)
 
 class WeakObjects final {
  private:
@@ -96,7 +98,7 @@ class WeakObjects final {
 
 #ifdef DEBUG
   template <typename Type>
-  static bool ContainsYoungObjects(WeakObjectWorklist<Type>& worklist);
+  static bool ContainsYoungObjects(WeakObjectWorklist<Tagged<Type>>& worklist);
 #endif
 };
 
diff --git a/src/heap/young-generation-marking-visitor-inl.h b/src/heap/young-generation-marking-visitor-inl.h
index af3ed9f4085..f57d3bbb926 100644
--- a/src/heap/young-generation-marking-visitor-inl.h
+++ b/src/heap/young-generation-marking-visitor-inl.h
@@ -51,9 +51,9 @@ YoungGenerationMarkingVisitor<marking_mode>::~YoungGenerationMarkingVisitor() {
 
 template <YoungGenerationMarkingVisitationMode marking_mode>
 template <typename T>
-int YoungGenerationMarkingVisitor<
-    marking_mode>::VisitEmbedderTracingSubClassWithEmbedderTracing(Map map,
-                                                                   T object) {
+int YoungGenerationMarkingVisitor<marking_mode>::
+    VisitEmbedderTracingSubClassWithEmbedderTracing(Tagged<Map> map,
+                                                    Tagged<T> object) {
   const int size = VisitJSObjectSubclass(map, object);
   if (!marking_worklists_local_.SupportsExtractWrapper()) return size;
   MarkingWorklists::Local::WrapperSnapshot wrapper_snapshot;
@@ -68,33 +68,33 @@ int YoungGenerationMarkingVisitor<
 
 template <YoungGenerationMarkingVisitationMode marking_mode>
 int YoungGenerationMarkingVisitor<marking_mode>::VisitJSArrayBuffer(
-    Map map, JSArrayBuffer object) {
+    Tagged<Map> map, Tagged<JSArrayBuffer> object) {
   object->YoungMarkExtension();
   return VisitEmbedderTracingSubClassWithEmbedderTracing(map, object);
 }
 
 template <YoungGenerationMarkingVisitationMode marking_mode>
 int YoungGenerationMarkingVisitor<marking_mode>::VisitJSApiObject(
-    Map map, JSObject object) {
+    Tagged<Map> map, Tagged<JSObject> object) {
   return VisitEmbedderTracingSubClassWithEmbedderTracing(map, object);
 }
 
 template <YoungGenerationMarkingVisitationMode marking_mode>
 int YoungGenerationMarkingVisitor<marking_mode>::
-    VisitJSDataViewOrRabGsabDataView(Map map,
-                                     JSDataViewOrRabGsabDataView object) {
+    VisitJSDataViewOrRabGsabDataView(
+        Tagged<Map> map, Tagged<JSDataViewOrRabGsabDataView> object) {
   return VisitEmbedderTracingSubClassWithEmbedderTracing(map, object);
 }
 
 template <YoungGenerationMarkingVisitationMode marking_mode>
 int YoungGenerationMarkingVisitor<marking_mode>::VisitJSTypedArray(
-    Map map, JSTypedArray object) {
+    Tagged<Map> map, Tagged<JSTypedArray> object) {
   return VisitEmbedderTracingSubClassWithEmbedderTracing(map, object);
 }
 
 template <YoungGenerationMarkingVisitationMode marking_mode>
 int YoungGenerationMarkingVisitor<marking_mode>::VisitJSObject(
-    Map map, JSObject object) {
+    Tagged<Map> map, Tagged<JSObject> object) {
   int result = Parent::VisitJSObject(map, object);
   DCHECK_LT(0, result);
   pretenuring_handler_->UpdateAllocationSite(map, object,
@@ -104,7 +104,7 @@ int YoungGenerationMarkingVisitor<marking_mode>::VisitJSObject(
 
 template <YoungGenerationMarkingVisitationMode marking_mode>
 int YoungGenerationMarkingVisitor<marking_mode>::VisitJSObjectFast(
-    Map map, JSObject object) {
+    Tagged<Map> map, Tagged<JSObject> object) {
   int result = Parent::VisitJSObjectFast(map, object);
   DCHECK_LT(0, result);
   pretenuring_handler_->UpdateAllocationSite(map, object,
@@ -115,7 +115,7 @@ int YoungGenerationMarkingVisitor<marking_mode>::VisitJSObjectFast(
 template <YoungGenerationMarkingVisitationMode marking_mode>
 template <typename T, typename TBodyDescriptor>
 int YoungGenerationMarkingVisitor<marking_mode>::VisitJSObjectSubclass(
-    Map map, T object) {
+    Tagged<Map> map, Tagged<T> object) {
   int result =
       Parent::template VisitJSObjectSubclass<T, TBodyDescriptor>(map, object);
   DCHECK_LT(0, result);
@@ -126,7 +126,7 @@ int YoungGenerationMarkingVisitor<marking_mode>::VisitJSObjectSubclass(
 
 template <YoungGenerationMarkingVisitationMode marking_mode>
 int YoungGenerationMarkingVisitor<marking_mode>::VisitEphemeronHashTable(
-    Map map, EphemeronHashTable table) {
+    Tagged<Map> map, Tagged<EphemeronHashTable> table) {
   // Register table with Minor MC, so it can take care of the weak keys later.
   // This allows to only iterate the tables' values, which are treated as strong
   // independently of whether the key is live.
@@ -142,7 +142,7 @@ int YoungGenerationMarkingVisitor<marking_mode>::VisitEphemeronHashTable(
 template <YoungGenerationMarkingVisitationMode marking_mode>
 template <typename TSlot>
 void YoungGenerationMarkingVisitor<marking_mode>::VisitPointersImpl(
-    HeapObject host, TSlot start, TSlot end) {
+    Tagged<HeapObject> host, TSlot start, TSlot end) {
   for (TSlot slot = start; slot < end; ++slot) {
     if constexpr (marking_mode ==
                   YoungGenerationMarkingVisitationMode::kConcurrent) {
@@ -181,7 +181,7 @@ V8_INLINE bool YoungGenerationMarkingVisitor<marking_mode>::VisitObjectViaSlot(
   typename TSlot::TObject target =
       slot.Relaxed_Load(ObjectVisitorWithCageBases::cage_base());
 
-  HeapObject heap_object;
+  Tagged<HeapObject> heap_object;
   // Treat weak references as strong.
   if (!target.GetHeapObject(&heap_object)) {
     return false;
@@ -207,7 +207,7 @@ V8_INLINE bool YoungGenerationMarkingVisitor<marking_mode>::VisitObjectViaSlot(
   // Maps won't change in the atomic pause, so the map can be read without
   // atomics.
   if constexpr (visitation_mode == ObjectVisitationMode::kVisitDirectly) {
-    Map map = heap_object->map(isolate_);
+    Tagged<Map> map = heap_object->map(isolate_);
     const int visited_size = Parent::Visit(map, heap_object);
     if (visited_size) {
       IncrementLiveBytesCached(
diff --git a/src/heap/young-generation-marking-visitor.h b/src/heap/young-generation-marking-visitor.h
index e91e0830f26..c0b1f71ccc1 100644
--- a/src/heap/young-generation-marking-visitor.h
+++ b/src/heap/young-generation-marking-visitor.h
@@ -46,35 +46,38 @@ class YoungGenerationMarkingVisitor final
     return marking_mode == YoungGenerationMarkingVisitationMode::kConcurrent;
   }
 
-  V8_INLINE void VisitPointers(HeapObject host, ObjectSlot start,
+  V8_INLINE void VisitPointers(Tagged<HeapObject> host, ObjectSlot start,
                                ObjectSlot end) final {
     VisitPointersImpl(host, start, end);
   }
-  V8_INLINE void VisitPointers(HeapObject host, MaybeObjectSlot start,
+  V8_INLINE void VisitPointers(Tagged<HeapObject> host, MaybeObjectSlot start,
                                MaybeObjectSlot end) final {
     VisitPointersImpl(host, start, end);
   }
-  V8_INLINE void VisitPointer(HeapObject host, ObjectSlot p) final {
+  V8_INLINE void VisitPointer(Tagged<HeapObject> host, ObjectSlot p) final {
     VisitPointersImpl(host, p, p + 1);
   }
-  V8_INLINE void VisitPointer(HeapObject host, MaybeObjectSlot p) final {
+  V8_INLINE void VisitPointer(Tagged<HeapObject> host,
+                              MaybeObjectSlot p) final {
     VisitPointersImpl(host, p, p + 1);
   }
 
   // Visitation specializations used for unified heap young gen marking.
-  V8_INLINE int VisitJSApiObject(Map map, JSObject object);
-  V8_INLINE int VisitJSArrayBuffer(Map map, JSArrayBuffer object);
+  V8_INLINE int VisitJSApiObject(Tagged<Map> map, Tagged<JSObject> object);
+  V8_INLINE int VisitJSArrayBuffer(Tagged<Map> map,
+                                   Tagged<JSArrayBuffer> object);
   V8_INLINE int VisitJSDataViewOrRabGsabDataView(
-      Map map, JSDataViewOrRabGsabDataView object);
-  V8_INLINE int VisitJSTypedArray(Map map, JSTypedArray object);
+      Tagged<Map> map, Tagged<JSDataViewOrRabGsabDataView> object);
+  V8_INLINE int VisitJSTypedArray(Tagged<Map> map, Tagged<JSTypedArray> object);
 
   // Visitation specializations used for collecting pretenuring feedback.
-  V8_INLINE int VisitJSObject(Map map, JSObject object);
-  V8_INLINE int VisitJSObjectFast(Map map, JSObject object);
+  V8_INLINE int VisitJSObject(Tagged<Map> map, Tagged<JSObject> object);
+  V8_INLINE int VisitJSObjectFast(Tagged<Map> map, Tagged<JSObject> object);
   template <typename T, typename TBodyDescriptor = typename T::BodyDescriptor>
-  V8_INLINE int VisitJSObjectSubclass(Map map, T object);
+  V8_INLINE int VisitJSObjectSubclass(Tagged<Map> map, Tagged<T> object);
 
-  V8_INLINE int VisitEphemeronHashTable(Map map, EphemeronHashTable table);
+  V8_INLINE int VisitEphemeronHashTable(Tagged<Map> map,
+                                        Tagged<EphemeronHashTable> table);
 
   template <ObjectVisitationMode visitation_mode,
             SlotTreatmentMode slot_treatment_mode, typename TSlot>
@@ -97,19 +100,21 @@ class YoungGenerationMarkingVisitor final
  private:
   using Parent = NewSpaceVisitor<YoungGenerationMarkingVisitor<marking_mode>>;
 
-  bool TryMark(HeapObject obj) {
+  bool TryMark(Tagged<HeapObject> obj) {
     return MarkBit::From(obj).Set<AccessMode::ATOMIC>();
   }
 
   template <typename TSlot>
-  V8_INLINE void VisitPointersImpl(HeapObject host, TSlot start, TSlot end);
+  V8_INLINE void VisitPointersImpl(Tagged<HeapObject> host, TSlot start,
+                                   TSlot end);
 
 #ifdef V8_MINORMS_STRING_SHORTCUTTING
   V8_INLINE bool ShortCutStrings(HeapObjectSlot slot, HeapObject* heap_object);
 #endif  // V8_MINORMS_STRING_SHORTCUTTING
 
   template <typename T>
-  int VisitEmbedderTracingSubClassWithEmbedderTracing(Map map, T object);
+  int VisitEmbedderTracingSubClassWithEmbedderTracing(Tagged<Map> map,
+                                                      Tagged<T> object);
 
   static constexpr size_t kNumEntries = 128;
   static constexpr size_t kEntriesMask = kNumEntries - 1;
diff --git a/src/ic/call-optimization.cc b/src/ic/call-optimization.cc
index fd06f45ad0f..d0dfc92d342 100644
--- a/src/ic/call-optimization.cc
+++ b/src/ic/call-optimization.cc
@@ -24,13 +24,13 @@ template CallOptimization::CallOptimization(LocalIsolate* isolate,
                                             Handle<Object> function);
 
 base::Optional<NativeContext> CallOptimization::GetAccessorContext(
-    Map holder_map) const {
+    Tagged<Map> holder_map) const {
   if (is_constant_call()) {
     return constant_function_->native_context();
   }
-  Object maybe_constructor = holder_map->GetConstructor();
+  Tagged<Object> maybe_constructor = holder_map->GetConstructor();
   if (IsJSFunction(maybe_constructor)) {
-    JSFunction constructor = JSFunction::cast(maybe_constructor);
+    Tagged<JSFunction> constructor = JSFunction::cast(maybe_constructor);
     return constructor->native_context();
   }
   // |maybe_constructor| might theoretically be |null| for some objects but
@@ -42,7 +42,7 @@ base::Optional<NativeContext> CallOptimization::GetAccessorContext(
 }
 
 bool CallOptimization::IsCrossContextLazyAccessorPair(
-    NativeContext native_context, Map holder_map) const {
+    Tagged<NativeContext> native_context, Tagged<Map> holder_map) const {
   DCHECK(IsNativeContext(native_context));
   if (is_constant_call()) return false;
   base::Optional<NativeContext> maybe_context = GetAccessorContext(holder_map);
@@ -68,7 +68,7 @@ Handle<JSObject> CallOptimization::LookupHolderOfExpectedType(
     return Handle<JSObject>::null();
   }
   if (IsJSGlobalProxyMap(*object_map) && !IsNull(object_map->prototype())) {
-    JSObject raw_prototype = JSObject::cast(object_map->prototype());
+    Tagged<JSObject> raw_prototype = JSObject::cast(object_map->prototype());
     Handle<JSObject> prototype(raw_prototype, isolate);
     object_map = handle(prototype->map(), isolate);
     if (expected_receiver_type_->IsTemplateFor(*object_map)) {
@@ -101,9 +101,9 @@ bool CallOptimization::IsCompatibleReceiverMap(
       if (api_holder.is_identical_to(holder)) return true;
       // Check if holder is in prototype chain of api_holder.
       {
-        JSObject object = *api_holder;
+        Tagged<JSObject> object = *api_holder;
         while (true) {
-          Object prototype = object->map()->prototype();
+          Tagged<Object> prototype = object->map()->prototype();
           if (!IsJSObject(prototype)) return false;
           if (prototype == *holder) return true;
           object = JSObject::cast(prototype);
@@ -116,11 +116,12 @@ bool CallOptimization::IsCompatibleReceiverMap(
 template <class IsolateT>
 void CallOptimization::Initialize(
     IsolateT* isolate, Handle<FunctionTemplateInfo> function_template_info) {
-  HeapObject call_code = function_template_info->call_code(kAcquireLoad);
+  Tagged<HeapObject> call_code =
+      function_template_info->call_code(kAcquireLoad);
   if (IsUndefined(call_code, isolate)) return;
   api_call_info_ = handle(CallHandlerInfo::cast(call_code), isolate);
 
-  HeapObject signature = function_template_info->signature();
+  Tagged<HeapObject> signature = function_template_info->signature();
   if (!IsUndefined(signature, isolate)) {
     expected_receiver_type_ =
         handle(FunctionTemplateInfo::cast(signature), isolate);
@@ -146,7 +147,7 @@ void CallOptimization::AnalyzePossibleApiFunction(IsolateT* isolate,
                                     isolate);
 
   // Require a C++ callback.
-  HeapObject call_code = info->call_code(kAcquireLoad);
+  Tagged<HeapObject> call_code = info->call_code(kAcquireLoad);
   if (IsUndefined(call_code, isolate)) return;
   api_call_info_ = handle(CallHandlerInfo::cast(call_code), isolate);
 
diff --git a/src/ic/call-optimization.h b/src/ic/call-optimization.h
index 303b4b7e9df..ea6d4887ee0 100644
--- a/src/ic/call-optimization.h
+++ b/src/ic/call-optimization.h
@@ -21,12 +21,13 @@ class CallOptimization {
   // If the holder is a remote object returns empty optional.
   // This method must not be called for holder maps with null constructor
   // because they can't be holders for lazy accessor pairs anyway.
-  base::Optional<NativeContext> GetAccessorContext(Map holder_map) const;
+  base::Optional<NativeContext> GetAccessorContext(
+      Tagged<Map> holder_map) const;
 
   // Return true if the accessor context for given holder doesn't match
   // given native context of if the holder is a remote object.
-  bool IsCrossContextLazyAccessorPair(NativeContext native_context,
-                                      Map holder_map) const;
+  bool IsCrossContextLazyAccessorPair(Tagged<NativeContext> native_context,
+                                      Tagged<Map> holder_map) const;
 
   bool is_constant_call() const { return !constant_function_.is_null(); }
   bool accept_any_receiver() const { return accept_any_receiver_; }
diff --git a/src/ic/handler-configuration-inl.h b/src/ic/handler-configuration-inl.h
index 679e6ea95ce..fbdcf9f6f52 100644
--- a/src/ic/handler-configuration-inl.h
+++ b/src/ic/handler-configuration-inl.h
@@ -25,7 +25,7 @@ OBJECT_CONSTRUCTORS_IMPL(LoadHandler, DataHandler)
 CAST_ACCESSOR(LoadHandler)
 
 // Decodes kind from Smi-handler.
-LoadHandler::Kind LoadHandler::GetHandlerKind(Smi smi_handler) {
+LoadHandler::Kind LoadHandler::GetHandlerKind(Tagged<Smi> smi_handler) {
   return KindBits::decode(smi_handler.value());
 }
 
@@ -220,7 +220,7 @@ Handle<Smi> StoreHandler::StoreProxy(Isolate* isolate) {
   return handle(StoreProxy(), isolate);
 }
 
-Smi StoreHandler::StoreProxy() {
+Tagged<Smi> StoreHandler::StoreProxy() {
   int config = KindBits::encode(Kind::kProxy);
   return Smi::FromInt(config);
 }
diff --git a/src/ic/handler-configuration.cc b/src/ic/handler-configuration.cc
index e5ce9e73ae6..5b1bff88cc0 100644
--- a/src/ic/handler-configuration.cc
+++ b/src/ic/handler-configuration.cc
@@ -344,7 +344,7 @@ Handle<Object> StoreHandler::StoreProxy(Isolate* isolate,
                                MaybeObjectHandle::Weak(proxy));
 }
 
-bool LoadHandler::CanHandleHolderNotLookupStart(Object handler) {
+bool LoadHandler::CanHandleHolderNotLookupStart(Tagged<Object> handler) {
   if (IsSmi(handler)) {
     auto kind = LoadHandler::KindBits::decode(handler.ToSmi().value());
     return kind == LoadHandler::Kind::kSlow ||
@@ -519,7 +519,7 @@ void PrintSmiStoreHandler(int raw_handler, std::ostream& os) {
 }  // namespace
 
 // static
-void LoadHandler::PrintHandler(Object handler, std::ostream& os) {
+void LoadHandler::PrintHandler(Tagged<Object> handler, std::ostream& os) {
   DisallowGarbageCollection no_gc;
   if (IsSmi(handler)) {
     int raw_handler = handler.ToSmi().value();
@@ -559,7 +559,7 @@ void LoadHandler::PrintHandler(Object handler, std::ostream& os) {
   }
 }
 
-void StoreHandler::PrintHandler(Object handler, std::ostream& os) {
+void StoreHandler::PrintHandler(Tagged<Object> handler, std::ostream& os) {
   DisallowGarbageCollection no_gc;
   if (IsSmi(handler)) {
     int raw_handler = handler.ToSmi().value();
diff --git a/src/ic/handler-configuration.h b/src/ic/handler-configuration.h
index a7bc14be0f6..94b37e44d6a 100644
--- a/src/ic/handler-configuration.h
+++ b/src/ic/handler-configuration.h
@@ -148,7 +148,7 @@ class LoadHandler final : public DataHandler {
   static_assert(ExportsIndexBits::kLastUsedBit < kSmiValueSize);
 
   // Decodes kind from Smi-handler.
-  static inline Kind GetHandlerKind(Smi smi_handler);
+  static inline Kind GetHandlerKind(Tagged<Smi> smi_handler);
 
   // Creates a Smi-handler for loading a property from a slow object.
   static inline Handle<Smi> LoadNormal(Isolate* isolate);
@@ -230,10 +230,10 @@ class LoadHandler final : public DataHandler {
 
   // Returns true iff the handler can be used in the "holder != lookup start
   // object" case.
-  static bool CanHandleHolderNotLookupStart(Object handler);
+  static bool CanHandleHolderNotLookupStart(Tagged<Object> handler);
 
 #if defined(OBJECT_PRINT)
-  static void PrintHandler(Object handler, std::ostream& os);
+  static void PrintHandler(Tagged<Object> handler, std::ostream& os);
 #endif  // defined(OBJECT_PRINT)
 
   OBJECT_CONSTRUCTORS(LoadHandler, DataHandler);
@@ -371,13 +371,13 @@ class StoreHandler final : public DataHandler {
 
   // Creates a Smi-handler for storing a property on a proxy.
   static inline Handle<Smi> StoreProxy(Isolate* isolate);
-  static inline Smi StoreProxy();
+  static inline Tagged<Smi> StoreProxy();
 
   // Decodes the KeyedAccessStoreMode from a {handler}.
   static KeyedAccessStoreMode GetKeyedAccessStoreMode(MaybeObject handler);
 
 #if defined(OBJECT_PRINT)
-  static void PrintHandler(Object handler, std::ostream& os);
+  static void PrintHandler(Tagged<Object> handler, std::ostream& os);
 #endif  // defined(OBJECT_PRINT)
 
  private:
diff --git a/src/ic/ic-inl.h b/src/ic/ic-inl.h
index 239be3b8bce..bebde2da250 100644
--- a/src/ic/ic-inl.h
+++ b/src/ic/ic-inl.h
@@ -26,7 +26,7 @@ void IC::update_lookup_start_object_map(Handle<Object> object) {
 }
 
 bool IC::IsHandler(MaybeObject object) {
-  HeapObject heap_object;
+  Tagged<HeapObject> heap_object;
   return (IsSmi(object) && (object.ptr() != kNullAddress)) ||
          (object->GetHeapObjectIfWeak(&heap_object) &&
           (IsMap(heap_object) || IsPropertyCell(heap_object) ||
diff --git a/src/ic/ic-stats.cc b/src/ic/ic-stats.cc
index f062012a71a..fd3c1c838e9 100644
--- a/src/ic/ic-stats.cc
+++ b/src/ic/ic-stats.cc
@@ -54,14 +54,14 @@ void ICStats::Dump() {
   Reset();
 }
 
-const char* ICStats::GetOrCacheScriptName(Script script) {
+const char* ICStats::GetOrCacheScriptName(Tagged<Script> script) {
   Address script_ptr = script.ptr();
   if (script_name_map_.find(script_ptr) != script_name_map_.end()) {
     return script_name_map_[script_ptr].get();
   }
-  Object script_name_raw = script->name();
+  Tagged<Object> script_name_raw = script->name();
   if (IsString(script_name_raw)) {
-    String script_name = String::cast(script_name_raw);
+    Tagged<String> script_name = String::cast(script_name_raw);
     char* c_script_name =
         script_name->ToCString(DISALLOW_NULLS, ROBUST_STRING_TRAVERSAL)
             .release();
@@ -74,7 +74,7 @@ const char* ICStats::GetOrCacheScriptName(Script script) {
   return nullptr;
 }
 
-const char* ICStats::GetOrCacheFunctionName(JSFunction function) {
+const char* ICStats::GetOrCacheFunctionName(Tagged<JSFunction> function) {
   Address function_ptr = function.ptr();
   // Lookup the function name or add a null unique_ptr if no entry exists.
   std::unique_ptr<char[]>& function_name = function_name_map_[function_ptr];
diff --git a/src/ic/ic-stats.h b/src/ic/ic-stats.h
index fa7c7af464f..cdabf351893 100644
--- a/src/ic/ic-stats.h
+++ b/src/ic/ic-stats.h
@@ -24,6 +24,8 @@ namespace internal {
 
 class JSFunction;
 class Script;
+template <typename T>
+class Tagged;
 
 struct ICInfo {
   ICInfo();
@@ -60,8 +62,8 @@ class ICStats {
     DCHECK(pos_ >= 0 && pos_ < MAX_IC_INFO);
     return ic_infos_[pos_];
   }
-  const char* GetOrCacheScriptName(Script script);
-  const char* GetOrCacheFunctionName(JSFunction function);
+  const char* GetOrCacheScriptName(Tagged<Script> script);
+  const char* GetOrCacheFunctionName(Tagged<JSFunction> function);
   V8_INLINE static ICStats* instance() { return instance_.Pointer(); }
 
  private:
diff --git a/src/ic/ic.cc b/src/ic/ic.cc
index a70292d3181..3fe25e80974 100644
--- a/src/ic/ic.cc
+++ b/src/ic/ic.cc
@@ -142,7 +142,7 @@ void IC::TraceIC(const char* type, Handle<Object> name, State old_state,
   JavaScriptFrame* frame = it.frame();
 
   DisallowGarbageCollection no_gc;
-  JSFunction function = frame->function();
+  Tagged<JSFunction> function = frame->function();
 
   ICStats::instance()->Begin();
   ICInfo& ic_info = ICStats::instance()->Current();
@@ -150,7 +150,7 @@ void IC::TraceIC(const char* type, Handle<Object> name, State old_state,
   ic_info.type += type;
 
   int code_offset = 0;
-  AbstractCode code = function->abstract_code(isolate_);
+  Tagged<AbstractCode> code = function->abstract_code(isolate_);
   if (function->ActiveTierIsIgnition()) {
     code_offset = InterpretedFrame::GetBytecodeOffset(frame->fp());
   } else if (function->ActiveTierIsBaseline()) {
@@ -253,7 +253,7 @@ bool IC::ShouldRecomputeHandler(Handle<String> name) {
   // would transition to.
   if (maybe_handler.is_null()) {
     if (!IsJSObjectMap(*lookup_start_object_map())) return false;
-    Map first_map = FirstTargetMap();
+    Tagged<Map> first_map = FirstTargetMap();
     if (first_map.is_null()) return false;
     Handle<Map> old_map(first_map, isolate());
     if (old_map->is_deprecated()) return true;
@@ -268,7 +268,7 @@ bool IC::RecomputeHandlerForName(Handle<Object> name) {
   if (is_keyed()) {
     // Determine whether the failure is due to a name failure.
     if (!IsName(*name)) return false;
-    Name stub_name = nexus()->GetName();
+    Tagged<Name> stub_name = nexus()->GetName();
     if (*name != stub_name) return false;
   }
 
@@ -304,29 +304,29 @@ MaybeHandle<Object> IC::ReferenceError(Handle<Name> name) {
 
 void IC::OnFeedbackChanged(const char* reason) {
   vector_set_ = true;
-  FeedbackVector vector = nexus()->vector();
+  Tagged<FeedbackVector> vector = nexus()->vector();
   FeedbackSlot slot = nexus()->slot();
   OnFeedbackChanged(isolate(), vector, slot, reason);
 }
 
 // static
-void IC::OnFeedbackChanged(Isolate* isolate, FeedbackVector vector,
+void IC::OnFeedbackChanged(Isolate* isolate, Tagged<FeedbackVector> vector,
                            FeedbackSlot slot, const char* reason) {
 #ifdef V8_TRACE_FEEDBACK_UPDATES
   if (v8_flags.trace_feedback_updates) {
-    int slot_count = vector.metadata()->slot_count();
+    int slot_count = vector->metadata()->slot_count();
     StdoutStream os;
     if (slot.IsInvalid()) {
       os << "[Feedback slots in ";
     } else {
       os << "[Feedback slot " << slot.ToInt() << "/" << slot_count << " in ";
     }
-    ShortPrint(vector.shared_function_info(), os);
+    ShortPrint(vector->shared_function_info(), os);
     if (slot.IsInvalid()) {
       os << " updated - ";
     } else {
       os << " updated to ";
-      vector.FeedbackSlotPrint(os, slot);
+      vector->FeedbackSlotPrint(os, slot);
       os << " - ";
     }
     os << reason << "]" << std::endl;
@@ -565,7 +565,7 @@ bool AddOneReceiverMapIfMissing(
 }
 
 Handle<NativeContext> GetAccessorContext(
-    const CallOptimization& call_optimization, Map holder_map,
+    const CallOptimization& call_optimization, Tagged<Map> holder_map,
     Isolate* isolate) {
   base::Optional<NativeContext> maybe_context =
       call_optimization.GetAccessorContext(holder_map);
@@ -734,14 +734,15 @@ void IC::CopyICToMegamorphicCache(Handle<Name> name) {
   }
 }
 
-bool IC::IsTransitionOfMonomorphicTarget(Map source_map, Map target_map) {
+bool IC::IsTransitionOfMonomorphicTarget(Tagged<Map> source_map,
+                                         Tagged<Map> target_map) {
   if (source_map.is_null()) return true;
   if (target_map.is_null()) return false;
   if (source_map->is_abandoned_prototype_map()) return false;
   ElementsKind target_elements_kind = target_map->elements_kind();
   bool more_general_transition = IsMoreGeneralElementsKindTransition(
       source_map->elements_kind(), target_elements_kind);
-  Map transitioned_map;
+  Tagged<Map> transitioned_map;
   if (more_general_transition) {
     MapHandles map_list;
     map_list.push_back(handle(target_map, isolate_));
@@ -1350,7 +1351,7 @@ void KeyedLoadIC::LoadElementPolymorphicHandlers(
     // among receiver_maps as unstable because the optimizing compilers may
     // generate an elements kind transition for this kind of receivers.
     if (receiver_map->is_stable()) {
-      Map tmap = receiver_map->FindElementsKindTransitionedMap(
+      Tagged<Map> tmap = receiver_map->FindElementsKindTransitionedMap(
           isolate(), *receiver_maps, ConcurrencyMode::kSynchronous);
       if (!tmap.is_null()) {
         receiver_map->NotifyLeafMapLayoutChange(isolate());
@@ -1540,7 +1541,7 @@ bool StoreIC::LookupForWrite(LookupIterator* it, Handle<Object> value,
           return true;
         case LookupIterator::INTERCEPTOR: {
           Handle<JSObject> holder = it->GetHolder<JSObject>();
-          InterceptorInfo info = holder->GetNamedInterceptor();
+          Tagged<InterceptorInfo> info = holder->GetNamedInterceptor();
           if (it->HolderIsReceiverOrHiddenPrototype() ||
               !IsUndefined(info->getter(), isolate()) ||
               !IsUndefined(info->query(), isolate())) {
@@ -1921,7 +1922,7 @@ MaybeObjectHandle StoreIC::ComputeHandler(LookupIterator* lookup) {
 
     case LookupIterator::INTERCEPTOR: {
       Handle<JSObject> holder = lookup->GetHolder<JSObject>();
-      InterceptorInfo info = holder->GetNamedInterceptor();
+      Tagged<InterceptorInfo> info = holder->GetNamedInterceptor();
 
       // If the interceptor is on the receiver...
       if (lookup->HolderIsReceiverOrHiddenPrototype() && !info->non_masking()) {
@@ -2368,7 +2369,7 @@ void KeyedStoreIC::StoreElementPolymorphicHandlers(
 
     } else {
       {
-        Map tmap = receiver_map->FindElementsKindTransitionedMap(
+        Tagged<Map> tmap = receiver_map->FindElementsKindTransitionedMap(
             isolate(), receiver_maps, ConcurrencyMode::kSynchronous);
         if (!tmap.is_null()) {
           if (receiver_map->is_stable()) {
@@ -2379,7 +2380,7 @@ void KeyedStoreIC::StoreElementPolymorphicHandlers(
       }
 
       MaybeHandle<Object> validity_cell;
-      HeapObject old_handler_obj;
+      Tagged<HeapObject> old_handler_obj;
       if (!old_handler.is_null() &&
           old_handler->GetHeapObject(&old_handler_obj) &&
           IsDataHandler(old_handler_obj)) {
@@ -3161,10 +3162,10 @@ FastCloneObjectMode GetCloneModeForMap(Handle<Map> map, int flags,
     mode = FastCloneObjectMode::kDifferentMap;
   }
 
-  DescriptorArray descriptors = map->instance_descriptors();
+  Tagged<DescriptorArray> descriptors = map->instance_descriptors();
   for (InternalIndex i : map->IterateOwnDescriptors()) {
     PropertyDetails details = descriptors->GetDetails(i);
-    Name key = descriptors->GetKey(i);
+    Tagged<Name> key = descriptors->GetKey(i);
     if (details.kind() != PropertyKind::kData || !details.IsEnumerable() ||
         key->IsPrivateName()) {
       return FastCloneObjectMode::kNotSupported;
@@ -3229,8 +3230,9 @@ bool CanFastCloneObjectWithDifferentMaps(Handle<Map> source_map,
       return false;
     }
   }
-  DescriptorArray descriptors = source_map->instance_descriptors();
-  DescriptorArray target_descriptors = target_map->instance_descriptors();
+  Tagged<DescriptorArray> descriptors = source_map->instance_descriptors();
+  Tagged<DescriptorArray> target_descriptors =
+      target_map->instance_descriptors();
   for (InternalIndex i : target_map->IterateOwnDescriptors()) {
     PropertyDetails details = descriptors->GetDetails(i);
     PropertyDetails target_details = target_descriptors->GetDetails(i);
@@ -3254,7 +3256,7 @@ static MaybeHandle<JSObject> CloneObjectSlowPath(Isolate* isolate,
     new_object = isolate->factory()->NewJSObjectWithNullProto();
   } else if (IsJSObject(*source) &&
              JSObject::cast(*source)->map()->OnlyHasSimpleProperties()) {
-    Map source_map = JSObject::cast(*source)->map();
+    Tagged<Map> source_map = JSObject::cast(*source)->map();
     // TODO(olivf, chrome:1204540) It might be interesting to pick a map with
     // more properties, depending how many properties are added by the
     // surrounding literal.
diff --git a/src/ic/ic.h b/src/ic/ic.h
index 56f9b03f4e1..138938b9118 100644
--- a/src/ic/ic.h
+++ b/src/ic/ic.h
@@ -64,7 +64,7 @@ class IC {
   static inline bool IsHandler(MaybeObject object);
 
   // Nofity the IC system that a feedback has changed.
-  static void OnFeedbackChanged(Isolate* isolate, FeedbackVector vector,
+  static void OnFeedbackChanged(Isolate* isolate, Tagged<FeedbackVector> vector,
                                 FeedbackSlot slot, const char* reason);
 
   void OnFeedbackChanged(const char* reason);
@@ -110,7 +110,8 @@ class IC {
   StubCache* stub_cache();
 
   void CopyICToMegamorphicCache(Handle<Name> name);
-  bool IsTransitionOfMonomorphicTarget(Map source_map, Map target_map);
+  bool IsTransitionOfMonomorphicTarget(Tagged<Map> source_map,
+                                       Tagged<Map> target_map);
   void SetCache(Handle<Name> name, Handle<Object> handler);
   void SetCache(Handle<Name> name, const MaybeObjectHandle& handler);
   FeedbackSlotKind kind() const { return kind_; }
@@ -143,7 +144,7 @@ class IC {
     }
   }
 
-  Map FirstTargetMap() {
+  Tagged<Map> FirstTargetMap() {
     FindTargetMaps();
     return !target_maps_.empty() ? *target_maps_[0] : Tagged<Map>();
   }
diff --git a/src/ic/stub-cache.cc b/src/ic/stub-cache.cc
index aa9c5b83d01..8a5dfd2e447 100644
--- a/src/ic/stub-cache.cc
+++ b/src/ic/stub-cache.cc
@@ -29,7 +29,7 @@ void StubCache::Initialize() {
 // Hash algorithm for the primary table. This algorithm is replicated in
 // the AccessorAssembler.  Returns an index into the table that
 // is scaled by 1 << kCacheIndexShift.
-int StubCache::PrimaryOffset(Name name, Map map) {
+int StubCache::PrimaryOffset(Tagged<Name> name, Tagged<Map> map) {
   // Compute the hash of the name (use entire hash field).
   uint32_t field = name->RawHash();
   DCHECK(Name::IsHashFieldComputed(field));
@@ -47,7 +47,7 @@ int StubCache::PrimaryOffset(Name name, Map map) {
 // assembler. This hash should be sufficiently different from the primary one
 // in order to avoid collisions for minified code with short names.
 // Returns an index into the table that is scaled by 1 << kCacheIndexShift.
-int StubCache::SecondaryOffset(Name name, Map old_map) {
+int StubCache::SecondaryOffset(Tagged<Name> name, Tagged<Map> old_map) {
   uint32_t name_low32bits = static_cast<uint32_t>(name.ptr());
   uint32_t map_low32bits = static_cast<uint32_t>(old_map.ptr());
   uint32_t key = (map_low32bits + name_low32bits);
@@ -55,19 +55,19 @@ int StubCache::SecondaryOffset(Name name, Map old_map) {
   return key & ((kSecondaryTableSize - 1) << kCacheIndexShift);
 }
 
-int StubCache::PrimaryOffsetForTesting(Name name, Map map) {
+int StubCache::PrimaryOffsetForTesting(Tagged<Name> name, Tagged<Map> map) {
   return PrimaryOffset(name, map);
 }
 
-int StubCache::SecondaryOffsetForTesting(Name name, Map map) {
+int StubCache::SecondaryOffsetForTesting(Tagged<Name> name, Tagged<Map> map) {
   return SecondaryOffset(name, map);
 }
 
 #ifdef DEBUG
 namespace {
 
-bool CommonStubCacheChecks(StubCache* stub_cache, Name name, Map map,
-                           MaybeObject handler) {
+bool CommonStubCacheChecks(StubCache* stub_cache, Tagged<Name> name,
+                           Tagged<Map> map, MaybeObject handler) {
   // Validate that the name and handler do not move on scavenge, and that we
   // can use identity checks instead of structural equality checks.
   DCHECK(!Heap::InYoungGeneration(name));
@@ -80,7 +80,7 @@ bool CommonStubCacheChecks(StubCache* stub_cache, Name name, Map map,
 }  // namespace
 #endif
 
-void StubCache::Set(Name name, Map map, MaybeObject handler) {
+void StubCache::Set(Tagged<Name> name, Tagged<Map> map, MaybeObject handler) {
   DCHECK(CommonStubCacheChecks(this, name, map, handler));
 
   // Compute the primary entry.
@@ -93,9 +93,9 @@ void StubCache::Set(Name name, Map map, MaybeObject handler) {
   if (old_handler != MaybeObject::FromObject(
                          isolate()->builtins()->code(Builtin::kIllegal)) &&
       !primary->map.IsSmi()) {
-    Map old_map =
+    Tagged<Map> old_map =
         Map::cast(StrongTaggedValue::ToObject(isolate(), primary->map));
-    Name old_name =
+    Tagged<Name> old_name =
         Name::cast(StrongTaggedValue::ToObject(isolate(), primary->key));
     int secondary_offset = SecondaryOffset(old_name, old_map);
     Entry* secondary = entry(secondary_, secondary_offset);
@@ -109,7 +109,7 @@ void StubCache::Set(Name name, Map map, MaybeObject handler) {
   isolate()->counters()->megamorphic_stub_cache_updates()->Increment();
 }
 
-MaybeObject StubCache::Get(Name name, Map map) {
+MaybeObject StubCache::Get(Tagged<Name> name, Tagged<Map> map) {
   DCHECK(CommonStubCacheChecks(this, name, map, MaybeObject()));
   int primary_offset = PrimaryOffset(name, map);
   Entry* primary = entry(primary_, primary_offset);
@@ -127,7 +127,7 @@ MaybeObject StubCache::Get(Name name, Map map) {
 void StubCache::Clear() {
   MaybeObject empty =
       MaybeObject::FromObject(isolate_->builtins()->code(Builtin::kIllegal));
-  Name empty_string = ReadOnlyRoots(isolate()).empty_string();
+  Tagged<Name> empty_string = ReadOnlyRoots(isolate()).empty_string();
   for (int i = 0; i < kPrimaryTableSize; i++) {
     primary_[i].key = StrongTaggedValue(empty_string);
     primary_[i].map = StrongTaggedValue(Smi::zero());
diff --git a/src/ic/stub-cache.h b/src/ic/stub-cache.h
index 2bee68c8938..5c0b6e61fa1 100644
--- a/src/ic/stub-cache.h
+++ b/src/ic/stub-cache.h
@@ -44,8 +44,8 @@ class V8_EXPORT_PRIVATE StubCache {
 
   void Initialize();
   // Access cache for entry hash(name, map).
-  void Set(Name name, Map map, MaybeObject handler);
-  MaybeObject Get(Name name, Map map);
+  void Set(Tagged<Name> name, Tagged<Map> map, MaybeObject handler);
+  MaybeObject Get(Tagged<Name> name, Tagged<Map> map);
   // Clear the lookup table (@ mark compact collection).
   void Clear();
 
@@ -90,8 +90,8 @@ class V8_EXPORT_PRIVATE StubCache {
   static const int kSecondaryTableBits = 9;
   static const int kSecondaryTableSize = (1 << kSecondaryTableBits);
 
-  static int PrimaryOffsetForTesting(Name name, Map map);
-  static int SecondaryOffsetForTesting(Name name, Map map);
+  static int PrimaryOffsetForTesting(Tagged<Name> name, Tagged<Map> map);
+  static int SecondaryOffsetForTesting(Tagged<Name> name, Tagged<Map> map);
 
   // The constructor is made public only for the purposes of testing.
   explicit StubCache(Isolate* isolate);
@@ -109,12 +109,12 @@ class V8_EXPORT_PRIVATE StubCache {
   // Hash algorithm for the primary table.  This algorithm is replicated in
   // assembler for every architecture.  Returns an index into the table that
   // is scaled by 1 << kCacheIndexShift.
-  static int PrimaryOffset(Name name, Map map);
+  static int PrimaryOffset(Tagged<Name> name, Tagged<Map> map);
 
   // Hash algorithm for the secondary table.  This algorithm is replicated in
   // assembler for every architecture.  Returns an index into the table that
   // is scaled by 1 << kCacheIndexShift.
-  static int SecondaryOffset(Name name, Map map);
+  static int SecondaryOffset(Tagged<Name> name, Tagged<Map> map);
 
   // Compute the entry for a given offset in exactly the same way as
   // we do in generated code.  We generate an hash code that already
diff --git a/src/init/bootstrapper.cc b/src/init/bootstrapper.cc
index c85dfc9e36b..2beaf45b140 100644
--- a/src/init/bootstrapper.cc
+++ b/src/init/bootstrapper.cc
@@ -98,7 +98,7 @@ void SourceCodeCache::Iterate(RootVisitor* v) {
 bool SourceCodeCache::Lookup(Isolate* isolate, base::Vector<const char> name,
                              Handle<SharedFunctionInfo>* handle) {
   for (int i = 0; i < cache_->length(); i += 2) {
-    SeqOneByteString str = SeqOneByteString::cast(cache_->get(i));
+    Tagged<SeqOneByteString> str = SeqOneByteString::cast(cache_->get(i));
     if (str->IsOneByteEqualTo(name)) {
       *handle = Handle<SharedFunctionInfo>(
           SharedFunctionInfo::cast(cache_->get(i + 1)), isolate);
@@ -538,7 +538,7 @@ V8_NOINLINE void SetConstructorInstanceType(Isolate* isolate,
   DCHECK(InstanceTypeChecker::IsJSFunction(constructor_type));
   DCHECK_NE(constructor_type, JS_FUNCTION_TYPE);
 
-  Map map = constructor->map();
+  Tagged<Map> map = constructor->map();
 
   // Check we don't accidentally change one of the existing maps.
   DCHECK_NE(map, *isolate->strict_function_map());
@@ -758,7 +758,7 @@ Handle<JSFunction> Genesis::CreateEmptyFunction() {
   Handle<WeakFixedArray> infos = factory()->NewWeakFixedArray(2);
   script->set_shared_function_infos(*infos);
   ReadOnlyRoots roots{isolate()};
-  SharedFunctionInfo sfi = empty_function->shared();
+  Tagged<SharedFunctionInfo> sfi = empty_function->shared();
   sfi->set_raw_scope_info(roots.empty_function_scope_info());
   sfi->DontAdaptArguments();
   sfi->SetScript(roots, *script, 1);
@@ -890,7 +890,7 @@ void Genesis::CreateObjectFunction(Handle<JSFunction> empty_function) {
 
   {
     // Finish setting up Object function's initial map.
-    Map initial_map = object_fun->initial_map();
+    Tagged<Map> initial_map = object_fun->initial_map();
     initial_map->set_elements_kind(HOLEY_ELEMENTS);
   }
 
@@ -1233,7 +1233,7 @@ namespace {
 void ReplaceAccessors(Isolate* isolate, Handle<Map> map, Handle<String> name,
                       PropertyAttributes attributes,
                       Handle<AccessorPair> accessor_pair) {
-  DescriptorArray descriptors = map->instance_descriptors(isolate);
+  Tagged<DescriptorArray> descriptors = map->instance_descriptors(isolate);
   InternalIndex entry = descriptors->SearchWithCache(isolate, *name, *map);
   Descriptor d = Descriptor::AccessorConstant(name, accessor_pair, attributes);
   descriptors->Replace(entry, &d);
@@ -1255,7 +1255,7 @@ void InitializeJSArrayMaps(Isolate* isolate, Handle<Context> native_context,
        i < kFastElementsKindCount; ++i) {
     Handle<Map> new_map;
     ElementsKind next_kind = GetFastElementsKindFromSequenceIndex(i);
-    Map maybe_elements_transition = current_map->ElementsTransitionMap(
+    Tagged<Map> maybe_elements_transition = current_map->ElementsTransitionMap(
         isolate, ConcurrencyMode::kSynchronous);
     if (!maybe_elements_transition.is_null()) {
       new_map = handle(maybe_elements_transition, isolate);
@@ -1285,14 +1285,15 @@ void Genesis::AddRestrictedFunctionProperties(Handle<JSFunction> empty) {
                    accessors);
 }
 
-static void AddToWeakNativeContextList(Isolate* isolate, Context context) {
+static void AddToWeakNativeContextList(Isolate* isolate,
+                                       Tagged<Context> context) {
   DCHECK(IsNativeContext(context));
   Heap* heap = isolate->heap();
 #ifdef DEBUG
   {
     DCHECK(IsUndefined(context->next_context_link(), isolate));
     // Check that context is not in the list yet.
-    for (Object current = heap->native_contexts_list();
+    for (Tagged<Object> current = heap->native_contexts_list();
          !IsUndefined(current, isolate);
          current = Context::cast(current)->next_context_link()) {
       DCHECK(current != context);
@@ -5858,7 +5859,7 @@ bool Genesis::InstallABunchOfRandomThings() {
   // and the String function has been set up.
   Handle<JSFunction> string_function(native_context()->string_function(),
                                      isolate());
-  JSObject string_function_prototype =
+  Tagged<JSObject> string_function_prototype =
       JSObject::cast(string_function->initial_map()->prototype());
   DCHECK(string_function_prototype->HasFastProperties());
   native_context()->set_string_function_prototype_map(
@@ -5914,7 +5915,7 @@ bool Genesis::InstallABunchOfRandomThings() {
                           isolate());
 
     // Verification of important array prototype properties.
-    Object length = proto->length();
+    Tagged<Object> length = proto->length();
     CHECK(IsSmi(length));
     CHECK_EQ(Smi::ToInt(length), 0);
     CHECK(proto->HasSmiOrObjectElements());
@@ -6030,7 +6031,7 @@ bool Genesis::InstallABunchOfRandomThings() {
         Handle<JSArray>::cast(factory()->NewJSObjectFromMap(template_map));
     {
       DisallowGarbageCollection no_gc;
-      JSArray raw = *template_object;
+      Tagged<JSArray> raw = *template_object;
       raw->set_elements(ReadOnlyRoots(isolate()).empty_fixed_array());
       raw->set_length(Smi::FromInt(0));
     }
@@ -6065,7 +6066,8 @@ bool Genesis::InstallABunchOfRandomThings() {
         .ToChecked();
     {
       DisallowGarbageCollection no_gc;
-      DescriptorArray desc = template_object->map()->instance_descriptors();
+      Tagged<DescriptorArray> desc =
+          template_object->map()->instance_descriptors();
       {
         // Verify TemplateLiteralObject::kRawOffset
         InternalIndex descriptor_index = desc->Search(
@@ -6293,7 +6295,7 @@ void Genesis::InitializeMapCaches() {
       cache->Set(i, HeapObjectReference::ClearedValue(isolate()));
     }
     native_context()->set_map_cache(*cache);
-    Map initial = native_context()->object_function()->initial_map();
+    Tagged<Map> initial = native_context()->object_function()->initial_map();
     cache->Set(0, HeapObjectReference::Weak(initial));
     cache->Set(initial->GetInObjectProperties(),
                HeapObjectReference::Weak(initial));
@@ -6603,7 +6605,7 @@ void Genesis::TransferNamedProperties(Handle<JSObject> from,
         from->property_dictionary_swiss(), isolate());
     ReadOnlyRoots roots(isolate());
     for (InternalIndex entry : properties->IterateEntriesOrdered()) {
-      Object raw_key;
+      Tagged<Object> raw_key;
       if (!properties->ToKey(roots, entry, &raw_key)) continue;
 
       DCHECK(IsName(raw_key));
@@ -6628,7 +6630,7 @@ void Genesis::TransferNamedProperties(Handle<JSObject> from,
     ReadOnlyRoots roots(isolate());
     for (int i = 0; i < key_indices->length(); i++) {
       InternalIndex key_index(Smi::ToInt(key_indices->get(i)));
-      Object raw_key = properties->KeyAt(key_index);
+      Tagged<Object> raw_key = properties->KeyAt(key_index);
       DCHECK(properties->IsKey(roots, raw_key));
       DCHECK(IsName(raw_key));
       Handle<Name> key(Name::cast(raw_key), isolate());
@@ -6693,7 +6695,7 @@ Handle<Map> Genesis::CreateInitialMapForArraySubclass(int size,
 
   // length descriptor.
   {
-    JSFunction array_function = native_context()->array_function();
+    Tagged<JSFunction> array_function = native_context()->array_function();
     Handle<DescriptorArray> array_descriptors(
         array_function->initial_map()->instance_descriptors(isolate()),
         isolate());
@@ -6734,9 +6736,10 @@ Genesis::Genesis(
       // The global proxy function to reinitialize this global proxy is in the
       // context that is yet to be deserialized. We need to prepare a global
       // proxy of the correct size.
-      Object size = isolate->heap()->serialized_global_proxy_sizes()->get(
-          static_cast<int>(context_snapshot_index) -
-          SnapshotCreatorImpl::kFirstAddtlContextIndex);
+      Tagged<Object> size =
+          isolate->heap()->serialized_global_proxy_sizes()->get(
+              static_cast<int>(context_snapshot_index) -
+              SnapshotCreatorImpl::kFirstAddtlContextIndex);
       instance_size = Smi::ToInt(size);
     } else {
       instance_size = JSGlobalProxy::SizeWithEmbedderFields(
@@ -6830,7 +6833,7 @@ Genesis::Genesis(
     // experimental natives.
     Handle<JSFunction> string_function(native_context()->string_function(),
                                        isolate);
-    JSObject string_function_prototype =
+    Tagged<JSObject> string_function_prototype =
         JSObject::cast(string_function->initial_map()->prototype());
     DCHECK(string_function_prototype->HasFastProperties());
     native_context()->set_string_function_prototype_map(
diff --git a/src/init/setup-isolate.h b/src/init/setup-isolate.h
index 6f52a157e1c..c1166670ba1 100644
--- a/src/init/setup-isolate.h
+++ b/src/init/setup-isolate.h
@@ -12,6 +12,8 @@ namespace internal {
 
 class Builtins;
 enum class Builtin : int32_t;
+template <typename T>
+class Tagged;
 class Code;
 class Heap;
 class Isolate;
@@ -39,7 +41,8 @@ class V8_EXPORT_PRIVATE SetupIsolateDelegate {
 
  protected:
   static void SetupBuiltinsInternal(Isolate* isolate);
-  static void AddBuiltin(Builtins* builtins, Builtin builtin, Code code);
+  static void AddBuiltin(Builtins* builtins, Builtin builtin,
+                         Tagged<Code> code);
   static void PopulateWithPlaceholders(Isolate* isolate);
   static void ReplacePlaceholders(Isolate* isolate);
 
diff --git a/src/interpreter/bytecode-array-builder.cc b/src/interpreter/bytecode-array-builder.cc
index a66fcfe241c..573007d5cf2 100644
--- a/src/interpreter/bytecode-array-builder.cc
+++ b/src/interpreter/bytecode-array-builder.cc
@@ -108,7 +108,7 @@ template EXPORT_TEMPLATE_DEFINE(V8_EXPORT_PRIVATE)
         LocalIsolate* isolate);
 
 #ifdef DEBUG
-int BytecodeArrayBuilder::CheckBytecodeMatches(BytecodeArray bytecode) {
+int BytecodeArrayBuilder::CheckBytecodeMatches(Tagged<BytecodeArray> bytecode) {
   DisallowGarbageCollection no_gc;
   return bytecode_array_writer_.CheckBytecodeMatches(bytecode);
 }
@@ -438,7 +438,7 @@ BytecodeArrayBuilder& BytecodeArrayBuilder::BinaryOperation(Token::Value op,
 }
 
 BytecodeArrayBuilder& BytecodeArrayBuilder::BinaryOperationSmiLiteral(
-    Token::Value op, Smi literal, int feedback_slot) {
+    Token::Value op, Tagged<Smi> literal, int feedback_slot) {
   switch (op) {
     case Token::Value::ADD:
       OutputAddSmi(literal.value(), feedback_slot);
@@ -614,7 +614,7 @@ BytecodeArrayBuilder& BytecodeArrayBuilder::LoadConstantPoolEntry(
   return *this;
 }
 
-BytecodeArrayBuilder& BytecodeArrayBuilder::LoadLiteral(Smi smi) {
+BytecodeArrayBuilder& BytecodeArrayBuilder::LoadLiteral(Tagged<Smi> smi) {
   int32_t raw_smi = smi.value();
   if (raw_smi == 0) {
     OutputLdaZero();
diff --git a/src/interpreter/bytecode-array-builder.h b/src/interpreter/bytecode-array-builder.h
index a9bc0632582..b24467380a7 100644
--- a/src/interpreter/bytecode-array-builder.h
+++ b/src/interpreter/bytecode-array-builder.h
@@ -52,7 +52,7 @@ class V8_EXPORT_PRIVATE BytecodeArrayBuilder final {
   Handle<ByteArray> ToSourcePositionTable(IsolateT* isolate);
 
 #ifdef DEBUG
-  int CheckBytecodeMatches(BytecodeArray bytecode);
+  int CheckBytecodeMatches(Tagged<BytecodeArray> bytecode);
 #endif
 
   // Get the number of parameters expected by function.
@@ -83,7 +83,7 @@ class V8_EXPORT_PRIVATE BytecodeArrayBuilder final {
 
   // Constant loads to accumulator.
   BytecodeArrayBuilder& LoadConstantPoolEntry(size_t entry);
-  BytecodeArrayBuilder& LoadLiteral(Smi value);
+  BytecodeArrayBuilder& LoadLiteral(Tagged<Smi> value);
   BytecodeArrayBuilder& LoadLiteral(double value);
   BytecodeArrayBuilder& LoadLiteral(const AstRawString* raw_string);
   BytecodeArrayBuilder& LoadLiteral(const Scope* scope);
@@ -362,7 +362,7 @@ class V8_EXPORT_PRIVATE BytecodeArrayBuilder final {
                                         int feedback_slot);
   // Same as above, but lhs in the accumulator and rhs in |literal|.
   BytecodeArrayBuilder& BinaryOperationSmiLiteral(Token::Value binop,
-                                                  Smi literal,
+                                                  Tagged<Smi> literal,
                                                   int feedback_slot);
 
   // Unary and Count Operators (value stored in accumulator).
diff --git a/src/interpreter/bytecode-array-iterator.cc b/src/interpreter/bytecode-array-iterator.cc
index 791aad6519d..557e8114ef3 100644
--- a/src/interpreter/bytecode-array-iterator.cc
+++ b/src/interpreter/bytecode-array-iterator.cc
@@ -240,7 +240,7 @@ bool BytecodeArrayIterator::IsConstantAtIndexSmi(int index) const {
   return IsSmi(bytecode_array()->constant_pool()->get(index));
 }
 
-Smi BytecodeArrayIterator::GetConstantAtIndexAsSmi(int index) const {
+Tagged<Smi> BytecodeArrayIterator::GetConstantAtIndexAsSmi(int index) const {
   return Smi::cast(bytecode_array()->constant_pool()->get(index));
 }
 
@@ -265,7 +265,7 @@ int BytecodeArrayIterator::GetRelativeJumpTargetOffset() const {
     }
     return relative_offset;
   } else if (interpreter::Bytecodes::IsJumpConstant(bytecode)) {
-    Smi smi = GetConstantAtIndexAsSmi(GetIndexOperand(0));
+    Tagged<Smi> smi = GetConstantAtIndexAsSmi(GetIndexOperand(0));
     return smi.value();
   } else {
     UNREACHABLE();
diff --git a/src/interpreter/bytecode-array-iterator.h b/src/interpreter/bytecode-array-iterator.h
index 80913cc6e9d..3343e82a4ba 100644
--- a/src/interpreter/bytecode-array-iterator.h
+++ b/src/interpreter/bytecode-array-iterator.h
@@ -139,7 +139,7 @@ class V8_EXPORT_PRIVATE BytecodeArrayIterator {
   template <typename IsolateT>
   Handle<Object> GetConstantAtIndex(int offset, IsolateT* isolate) const;
   bool IsConstantAtIndexSmi(int offset) const;
-  Smi GetConstantAtIndexAsSmi(int offset) const;
+  Tagged<Smi> GetConstantAtIndexAsSmi(int offset) const;
   template <typename IsolateT>
   Handle<Object> GetConstantForIndexOperand(int operand_index,
                                             IsolateT* isolate) const;
diff --git a/src/interpreter/bytecode-array-writer.cc b/src/interpreter/bytecode-array-writer.cc
index 672367020fe..bb1bcf763c1 100644
--- a/src/interpreter/bytecode-array-writer.cc
+++ b/src/interpreter/bytecode-array-writer.cc
@@ -82,7 +82,7 @@ template EXPORT_TEMPLATE_DEFINE(V8_EXPORT_PRIVATE)
         LocalIsolate* isolate);
 
 #ifdef DEBUG
-int BytecodeArrayWriter::CheckBytecodeMatches(BytecodeArray bytecode) {
+int BytecodeArrayWriter::CheckBytecodeMatches(Tagged<BytecodeArray> bytecode) {
   int mismatches = false;
   int bytecode_size = static_cast<int>(bytecodes()->size());
   const uint8_t* bytecode_ptr = &bytecodes()->front();
diff --git a/src/interpreter/bytecode-array-writer.h b/src/interpreter/bytecode-array-writer.h
index 672d544b398..71b710e4ed6 100644
--- a/src/interpreter/bytecode-array-writer.h
+++ b/src/interpreter/bytecode-array-writer.h
@@ -66,7 +66,7 @@ class V8_EXPORT_PRIVATE BytecodeArrayWriter final {
 
 #ifdef DEBUG
   // Returns -1 if they match or the offset of the first mismatching byte.
-  int CheckBytecodeMatches(BytecodeArray bytecode);
+  int CheckBytecodeMatches(Tagged<BytecodeArray> bytecode);
 #endif
 
   bool RemainderOfBlockIsDead() const { return exit_seen_in_block_; }
diff --git a/src/interpreter/bytecode-generator.cc b/src/interpreter/bytecode-generator.cc
index 794473855bb..287cd17eec8 100644
--- a/src/interpreter/bytecode-generator.cc
+++ b/src/interpreter/bytecode-generator.cc
@@ -1362,7 +1362,7 @@ template Handle<ByteArray> BytecodeGenerator::FinalizeSourcePositionTable(
     LocalIsolate* isolate);
 
 #ifdef DEBUG
-int BytecodeGenerator::CheckBytecodeMatches(BytecodeArray bytecode) {
+int BytecodeGenerator::CheckBytecodeMatches(Tagged<BytecodeArray> bytecode) {
   return builder()->CheckBytecodeMatches(bytecode);
 }
 #endif
@@ -6703,7 +6703,7 @@ void BytecodeGenerator::VisitCompareOperation(CompareOperation* expr) {
 void BytecodeGenerator::VisitArithmeticExpression(BinaryOperation* expr) {
   FeedbackSlot slot = feedback_spec()->AddBinaryOpICSlot();
   Expression* subexpr;
-  Smi literal;
+  Tagged<Smi> literal;
   if (expr->IsSmiLiteralOperation(&subexpr, &literal)) {
     TypeHint type_hint = VisitForAccumulatorValue(subexpr);
     builder()->SetExpressionPosition(expr);
diff --git a/src/interpreter/bytecode-generator.h b/src/interpreter/bytecode-generator.h
index 0656693285a..9ec9e807504 100644
--- a/src/interpreter/bytecode-generator.h
+++ b/src/interpreter/bytecode-generator.h
@@ -62,7 +62,7 @@ class BytecodeGenerator final : public AstVisitor<BytecodeGenerator> {
   }
 
 #ifdef DEBUG
-  int CheckBytecodeMatches(BytecodeArray bytecode);
+  int CheckBytecodeMatches(Tagged<BytecodeArray> bytecode);
 #endif
 
 #define DECLARE_VISIT(type) void Visit##type(type* node);
diff --git a/src/interpreter/constant-array-builder.cc b/src/interpreter/constant-array-builder.cc
index 8591a37183d..e2cba36f6f5 100644
--- a/src/interpreter/constant-array-builder.cc
+++ b/src/interpreter/constant-array-builder.cc
@@ -220,7 +220,7 @@ template EXPORT_TEMPLATE_DEFINE(V8_EXPORT_PRIVATE)
     Handle<FixedArray> ConstantArrayBuilder::ToFixedArray(
         LocalIsolate* isolate);
 
-size_t ConstantArrayBuilder::Insert(Smi smi) {
+size_t ConstantArrayBuilder::Insert(Tagged<Smi> smi) {
   auto entry = smi_map_.find(smi);
   if (entry == smi_map_.end()) {
     return AllocateReservedEntry(smi);
@@ -321,7 +321,7 @@ void ConstantArrayBuilder::SetDeferredAt(size_t index, Handle<Object> object) {
   return slice->At(index).SetDeferred(object);
 }
 
-void ConstantArrayBuilder::SetJumpTableSmi(size_t index, Smi smi) {
+void ConstantArrayBuilder::SetJumpTableSmi(size_t index, Tagged<Smi> smi) {
   ConstantArraySlice* slice = IndexToSlice(index);
   // Allow others to reuse these Smis, but insert using emplace to avoid
   // overwriting existing values in the Smi map (which may have a smaller
@@ -341,14 +341,14 @@ OperandSize ConstantArrayBuilder::CreateReservedEntry() {
 }
 
 ConstantArrayBuilder::index_t ConstantArrayBuilder::AllocateReservedEntry(
-    Smi value) {
+    Tagged<Smi> value) {
   index_t index = static_cast<index_t>(AllocateIndex(Entry(value)));
   smi_map_[value] = index;
   return index;
 }
 
 size_t ConstantArrayBuilder::CommitReservedEntry(OperandSize operand_size,
-                                                 Smi value) {
+                                                 Tagged<Smi> value) {
   DiscardReservedEntry(operand_size);
   size_t index;
   auto entry = smi_map_.find(value);
diff --git a/src/interpreter/constant-array-builder.h b/src/interpreter/constant-array-builder.h
index 14eec07edcb..76d970c93a7 100644
--- a/src/interpreter/constant-array-builder.h
+++ b/src/interpreter/constant-array-builder.h
@@ -67,7 +67,7 @@ class V8_EXPORT_PRIVATE ConstantArrayBuilder final {
 
   // Insert an object into the constants array if it is not already present.
   // Returns the array index associated with the object.
-  size_t Insert(Smi smi);
+  size_t Insert(Tagged<Smi> smi);
   size_t Insert(double number);
   size_t Insert(const AstRawString* raw_string);
   size_t Insert(AstBigInt bigint);
@@ -91,7 +91,7 @@ class V8_EXPORT_PRIVATE ConstantArrayBuilder final {
 
   // Sets the jump table entry at |index| to |smi|. Note that |index| is the
   // constant pool index, not the switch case value.
-  void SetJumpTableSmi(size_t index, Smi smi);
+  void SetJumpTableSmi(size_t index, Tagged<Smi> smi);
 
   // Creates a reserved entry in the constant pool and returns
   // the size of the operand that'll be required to hold the entry
@@ -100,7 +100,7 @@ class V8_EXPORT_PRIVATE ConstantArrayBuilder final {
 
   // Commit reserved entry and returns the constant pool index for the
   // SMI value.
-  size_t CommitReservedEntry(OperandSize operand_size, Smi value);
+  size_t CommitReservedEntry(OperandSize operand_size, Tagged<Smi> value);
 
   // Discards constant pool reservation.
   void DiscardReservedEntry(OperandSize operand_size);
@@ -115,7 +115,7 @@ class V8_EXPORT_PRIVATE ConstantArrayBuilder final {
     enum class Tag : uint8_t;
 
    public:
-    explicit Entry(Smi smi) : smi_(smi), tag_(Tag::kSmi) {}
+    explicit Entry(Tagged<Smi> smi) : smi_(smi), tag_(Tag::kSmi) {}
     explicit Entry(double heap_number)
         : heap_number_(heap_number), tag_(Tag::kHeapNumber) {}
     explicit Entry(const AstRawString* raw_string)
@@ -147,7 +147,7 @@ class V8_EXPORT_PRIVATE ConstantArrayBuilder final {
       handle_ = handle;
     }
 
-    void SetJumpTableSmi(Smi smi) {
+    void SetJumpTableSmi(Tagged<Smi> smi) {
       DCHECK_EQ(tag_, Tag::kUninitializedJumpTableSmi);
       tag_ = Tag::kJumpTableSmi;
       smi_ = smi;
@@ -191,7 +191,7 @@ class V8_EXPORT_PRIVATE ConstantArrayBuilder final {
 
   index_t AllocateIndex(Entry constant_entry);
   index_t AllocateIndexArray(Entry constant_entry, size_t size);
-  index_t AllocateReservedEntry(Smi value);
+  index_t AllocateReservedEntry(Tagged<Smi> value);
 
   struct ConstantArraySlice final : public ZoneObject {
     ConstantArraySlice(Zone* zone, size_t start_index, size_t capacity,
diff --git a/src/interpreter/interpreter.cc b/src/interpreter/interpreter.cc
index a4f2d59e121..a3339f3d5a2 100644
--- a/src/interpreter/interpreter.cc
+++ b/src/interpreter/interpreter.cc
@@ -111,14 +111,15 @@ Builtin BuiltinIndexFromBytecode(Bytecode bytecode,
 
 }  // namespace
 
-Code Interpreter::GetBytecodeHandler(Bytecode bytecode,
-                                     OperandScale operand_scale) {
+Tagged<Code> Interpreter::GetBytecodeHandler(Bytecode bytecode,
+                                             OperandScale operand_scale) {
   Builtin builtin = BuiltinIndexFromBytecode(bytecode, operand_scale);
   return isolate_->builtins()->code(builtin);
 }
 
 void Interpreter::SetBytecodeHandler(Bytecode bytecode,
-                                     OperandScale operand_scale, Code handler) {
+                                     OperandScale operand_scale,
+                                     Tagged<Code> handler) {
   DCHECK(!handler->has_instruction_stream());
   DCHECK(handler->kind() == CodeKind::BYTECODE_HANDLER);
   size_t index = GetDispatchTableIndex(bytecode, operand_scale);
@@ -226,7 +227,7 @@ void InterpreterCompilationJob::CheckAndPrintBytecodeMismatch(
     } else {
       std::cerr << "anonymous";
     }
-    Object script_name = script->GetNameOrSourceURL();
+    Tagged<Object> script_name = script->GetNameOrSourceURL();
     if (IsString(script_name)) {
       std::cerr << " ";
       String::cast(script_name)->PrintUC16(std::cerr);
@@ -348,7 +349,7 @@ void Interpreter::Initialize() {
   // Initialize the dispatch table.
   ForEachBytecode([=](Bytecode bytecode, OperandScale operand_scale) {
     Builtin builtin = BuiltinIndexFromBytecode(bytecode, operand_scale);
-    Code handler = builtins->code(builtin);
+    Tagged<Code> handler = builtins->code(builtin);
     if (Bytecodes::BytecodeHasHandler(bytecode, operand_scale)) {
 #ifdef DEBUG
       std::string builtin_name(Builtins::name(builtin));
diff --git a/src/interpreter/interpreter.h b/src/interpreter/interpreter.h
index 59c88097e2c..5657e2d3e01 100644
--- a/src/interpreter/interpreter.h
+++ b/src/interpreter/interpreter.h
@@ -62,12 +62,12 @@ class Interpreter {
 
   // If the bytecode handler for |bytecode| and |operand_scale| has not yet
   // been loaded, deserialize it. Then return the handler.
-  V8_EXPORT_PRIVATE Code GetBytecodeHandler(Bytecode bytecode,
-                                            OperandScale operand_scale);
+  V8_EXPORT_PRIVATE Tagged<Code> GetBytecodeHandler(Bytecode bytecode,
+                                                    OperandScale operand_scale);
 
   // Set the bytecode handler for |bytecode| and |operand_scale|.
   void SetBytecodeHandler(Bytecode bytecode, OperandScale operand_scale,
-                          Code handler);
+                          Tagged<Code> handler);
 
   V8_EXPORT_PRIVATE Handle<JSObject> GetDispatchCountersObject();
 
diff --git a/src/json/json-parser.cc b/src/json/json-parser.cc
index e7f4a06ae77..28e18d6195d 100644
--- a/src/json/json-parser.cc
+++ b/src/json/json-parser.cc
@@ -317,9 +317,9 @@ JsonParser<Char>::JsonParser(Isolate* isolate, Handle<String> source)
   size_t length = source->length();
   PtrComprCageBase cage_base(isolate);
   if (IsSlicedString(*source, cage_base)) {
-    SlicedString string = SlicedString::cast(*source);
+    Tagged<SlicedString> string = SlicedString::cast(*source);
     start = string->offset();
-    String parent = string->parent(cage_base);
+    Tagged<String> parent = string->parent(cage_base);
     if (IsThinString(parent, cage_base))
       parent = ThinString::cast(parent)->actual(cage_base);
     source_ = handle(parent, isolate);
@@ -329,13 +329,13 @@ JsonParser<Char>::JsonParser(Isolate* isolate, Handle<String> source)
 
   if (StringShape(*source_, cage_base).IsExternal()) {
     chars_ = static_cast<const Char*>(
-        SeqExternalString::cast(*source_).GetChars(cage_base));
+        SeqExternalString::cast(*source_)->GetChars(cage_base));
     chars_may_relocate_ = false;
   } else {
     DisallowGarbageCollection no_gc;
     isolate->main_thread_local_heap()->AddGCEpilogueCallback(
         UpdatePointersCallback, this);
-    chars_ = SeqString::cast(*source_).GetChars(no_gc);
+    chars_ = SeqString::cast(*source_)->GetChars(no_gc);
     chars_may_relocate_ = true;
   }
   cursor_ = chars_ + start;
@@ -683,7 +683,7 @@ Handle<Object> JsonParser<Char>::BuildJsonObject(
       Handle<FixedArray> elms =
           factory()->NewFixedArrayWithHoles(cont.max_index + 1);
       DisallowGarbageCollection no_gc;
-      FixedArray raw_elements = *elms;
+      Tagged<FixedArray> raw_elements = *elms;
       WriteBarrierMode mode = raw_elements->GetWriteBarrierMode(no_gc);
       DCHECK_EQ(HOLEY_ELEMENTS, map->elements_kind());
 
@@ -774,7 +774,7 @@ Handle<Object> JsonParser<Char>::BuildJsonObject(
     } else if (expected_representation.IsHeapObject() &&
                !target->instance_descriptors(isolate())
                     ->GetFieldType(descriptor_index)
-                    .NowContains(value)) {
+                    ->NowContains(value)) {
       Handle<FieldType> value_type =
           Object::OptimalType(*value, isolate(), expected_representation);
       MapUpdater::GeneralizeField(isolate(), target, descriptor_index,
@@ -786,7 +786,7 @@ Handle<Object> JsonParser<Char>::BuildJsonObject(
 
     DCHECK(target->instance_descriptors(isolate())
                ->GetFieldType(descriptor_index)
-               .NowContains(value));
+               ->NowContains(value));
     map = target;
     descriptor++;
   }
@@ -839,15 +839,15 @@ Handle<Object> JsonParser<Char>::BuildJsonObject(
           raw_map->instance_descriptors(isolate())->GetDetails(
               descriptor_index);
       FieldIndex index = FieldIndex::ForDetails(raw_map, details);
-      Object value = *property.value;
+      Tagged<Object> value = *property.value;
       descriptor++;
 
       if (details.representation().IsDouble()) {
         if (IsSmi(value)) {
           if (!V8_COMPRESS_POINTERS_8GB_BOOL && kTaggedSize != kDoubleSize) {
             // Write alignment filler.
-            HeapObject filler = HeapObject::FromAddress(filler_address);
-            filler.set_map_after_allocation(roots().one_pointer_filler_map());
+            Tagged<HeapObject> filler = HeapObject::FromAddress(filler_address);
+            filler->set_map_after_allocation(roots().one_pointer_filler_map());
             filler_address += kMutableDoubleSize;
           }
 
@@ -856,8 +856,9 @@ Handle<Object> JsonParser<Char>::BuildJsonObject(
           // Allocate simple heapnumber with immortal map, with non-pointer
           // payload, so we can skip notifying object layout change.
 
-          HeapObject hn = HeapObject::FromAddress(mutable_double_address);
-          hn.set_map_after_allocation(roots().heap_number_map());
+          Tagged<HeapObject> hn =
+              HeapObject::FromAddress(mutable_double_address);
+          hn->set_map_after_allocation(roots().heap_number_map());
           HeapNumber::cast(hn)->set_value_as_bits(bits, kRelaxedStore);
           value = hn;
           mutable_double_address +=
@@ -918,7 +919,7 @@ Handle<Object> JsonParser<Char>::BuildJsonArray(
 
   ElementsKind kind = PACKED_SMI_ELEMENTS;
   for (size_t i = start; i < element_stack.size(); i++) {
-    Object value = *element_stack[i];
+    Tagged<Object> value = *element_stack[i];
     if (IsHeapObject(value)) {
       if (IsHeapNumber(HeapObject::cast(value))) {
         kind = PACKED_DOUBLE_ELEMENTS;
@@ -932,13 +933,14 @@ Handle<Object> JsonParser<Char>::BuildJsonArray(
   Handle<JSArray> array = factory()->NewJSArray(kind, length, length);
   if (kind == PACKED_DOUBLE_ELEMENTS) {
     DisallowGarbageCollection no_gc;
-    FixedDoubleArray elements = FixedDoubleArray::cast(array->elements());
+    Tagged<FixedDoubleArray> elements =
+        FixedDoubleArray::cast(array->elements());
     for (int i = 0; i < length; i++) {
       elements->set(i, Object::Number(*element_stack[start + i]));
     }
   } else {
     DisallowGarbageCollection no_gc;
-    FixedArray elements = FixedArray::cast(array->elements());
+    Tagged<FixedArray> elements = FixedArray::cast(array->elements());
     WriteBarrierMode mode = kind == PACKED_SMI_ELEMENTS
                                 ? SKIP_WRITE_BARRIER
                                 : elements->GetWriteBarrierMode(no_gc);
@@ -1224,7 +1226,8 @@ MaybeHandle<Object> JsonParser<Char>::ParseJsonValue(Handle<Object> reviver) {
               cont_stack.back().type() == JsonContinuation::kArrayElement &&
               cont_stack.back().index < element_stack.size() &&
               IsJSObject(*element_stack.back())) {
-            Map maybe_feedback = JSObject::cast(*element_stack.back())->map();
+            Tagged<Map> maybe_feedback =
+                JSObject::cast(*element_stack.back())->map();
             // Don't consume feedback from objects with a map that's detached
             // from the transition tree.
             if (!maybe_feedback->IsDetached(isolate_)) {
diff --git a/src/json/json-stringifier.cc b/src/json/json-stringifier.cc
index c696353afe0..032b49435be 100644
--- a/src/json/json-stringifier.cc
+++ b/src/json/json-stringifier.cc
@@ -1392,8 +1392,8 @@ bool JsonStringifier::TrySerializeSimplePropertyKey(String key) {
   NoExtendBuilder<DestChar> no_extend(
       reinterpret_cast<DestChar*>(part_ptr_) + current_index_, &current_index_);
   no_extend.Append('"');
-  base::Vector<const uint8_t> chars(SeqOneByteString::cast(key).GetChars(no_gc),
-                                    copy_length);
+  base::Vector<const uint8_t> chars(
+      SeqOneByteString::cast(key)->GetChars(no_gc), copy_length);
   DCHECK_LE(reinterpret_cast<Address>(chars.end()),
             key.address() + key.Size(isolate_));
 #if DEBUG
diff --git a/src/logging/code-events.h b/src/logging/code-events.h
index 735ad66ecf7..b39e59218ba 100644
--- a/src/logging/code-events.h
+++ b/src/logging/code-events.h
@@ -90,8 +90,10 @@ class LogEventListener {
   virtual void RegExpCodeCreateEvent(Handle<AbstractCode> code,
                                      Handle<String> source) = 0;
   // Not handlified as this happens during GC. No allocation allowed.
-  virtual void CodeMoveEvent(InstructionStream from, InstructionStream to) = 0;
-  virtual void BytecodeMoveEvent(BytecodeArray from, BytecodeArray to) = 0;
+  virtual void CodeMoveEvent(Tagged<InstructionStream> from,
+                             Tagged<InstructionStream> to) = 0;
+  virtual void BytecodeMoveEvent(Tagged<BytecodeArray> from,
+                                 Tagged<BytecodeArray> to) = 0;
   virtual void SharedFunctionInfoMoveEvent(Address from, Address to) = 0;
   virtual void NativeContextMoveEvent(Address from, Address to) = 0;
   virtual void CodeMovingGCEvent() = 0;
@@ -215,13 +217,14 @@ class Logger {
       listener->RegExpCodeCreateEvent(code, source);
     }
   }
-  void CodeMoveEvent(InstructionStream from, InstructionStream to) {
+  void CodeMoveEvent(Tagged<InstructionStream> from,
+                     Tagged<InstructionStream> to) {
     base::MutexGuard guard(&mutex_);
     for (auto listener : listeners_) {
       listener->CodeMoveEvent(from, to);
     }
   }
-  void BytecodeMoveEvent(BytecodeArray from, BytecodeArray to) {
+  void BytecodeMoveEvent(Tagged<BytecodeArray> from, Tagged<BytecodeArray> to) {
     base::MutexGuard guard(&mutex_);
     for (auto listener : listeners_) {
       listener->BytecodeMoveEvent(from, to);
diff --git a/src/logging/local-logger.cc b/src/logging/local-logger.cc
index 6792383c6c4..1c023cf2a88 100644
--- a/src/logging/local-logger.cc
+++ b/src/logging/local-logger.cc
@@ -17,22 +17,26 @@ LocalLogger::LocalLogger(Isolate* isolate)
       is_listening_to_code_events_(
           v8_file_logger_->is_listening_to_code_events()) {}
 
-void LocalLogger::ScriptDetails(Script script) {
+void LocalLogger::ScriptDetails(Tagged<Script> script) {
   v8_file_logger_->ScriptDetails(script);
 }
 void LocalLogger::ScriptEvent(ScriptEventType type, int script_id) {
   v8_file_logger_->ScriptEvent(type, script_id);
 }
-void LocalLogger::CodeLinePosInfoRecordEvent(Address code_start,
-                                             ByteArray source_position_table,
-                                             JitCodeEvent::CodeType code_type) {
+void LocalLogger::CodeLinePosInfoRecordEvent(
+    Address code_start, Tagged<ByteArray> source_position_table,
+    JitCodeEvent::CodeType code_type) {
   v8_file_logger_->CodeLinePosInfoRecordEvent(code_start, source_position_table,
                                               code_type);
 }
 
-void LocalLogger::MapCreate(Map map) { v8_file_logger_->MapCreate(map); }
+void LocalLogger::MapCreate(Tagged<Map> map) {
+  v8_file_logger_->MapCreate(map);
+}
 
-void LocalLogger::MapDetails(Map map) { v8_file_logger_->MapDetails(map); }
+void LocalLogger::MapDetails(Tagged<Map> map) {
+  v8_file_logger_->MapDetails(map);
+}
 
 }  // namespace internal
 }  // namespace v8
diff --git a/src/logging/local-logger.h b/src/logging/local-logger.h
index 347c4991dd4..049cfcecb68 100644
--- a/src/logging/local-logger.h
+++ b/src/logging/local-logger.h
@@ -20,14 +20,14 @@ class LocalLogger {
   bool is_listening_to_code_events() const {
     return is_listening_to_code_events_;
   }
-  void ScriptDetails(Script script);
+  void ScriptDetails(Tagged<Script> script);
   void ScriptEvent(ScriptEventType type, int script_id);
   void CodeLinePosInfoRecordEvent(Address code_start,
-                                  ByteArray source_position_table,
+                                  Tagged<ByteArray> source_position_table,
                                   JitCodeEvent::CodeType code_type);
 
-  void MapCreate(Map map);
-  void MapDetails(Map map);
+  void MapCreate(Tagged<Map> map);
+  void MapDetails(Tagged<Map> map);
 
  private:
   V8FileLogger* v8_file_logger_;
diff --git a/src/logging/log-file.cc b/src/logging/log-file.cc
index 7586031ab21..af487e7cf3e 100644
--- a/src/logging/log-file.cc
+++ b/src/logging/log-file.cc
@@ -108,7 +108,7 @@ std::string LogFile::file_name() const { return file_name_; }
 LogFile::MessageBuilder::MessageBuilder(LogFile* log)
     : log_(log), lock_guard_(&log_->mutex_) {}
 
-void LogFile::MessageBuilder::AppendString(String str,
+void LogFile::MessageBuilder::AppendString(Tagged<String> str,
                                            base::Optional<int> length_limit) {
   if (str.is_null()) return;
 
@@ -192,7 +192,7 @@ void LogFile::MessageBuilder::AppendCharacter(char c) {
   }
 }
 
-void LogFile::MessageBuilder::AppendSymbolName(Symbol symbol) {
+void LogFile::MessageBuilder::AppendSymbolName(Tagged<Symbol> symbol) {
   DCHECK(!symbol.is_null());
   OFStream& os = log_->os_;
   os << "symbol(";
@@ -204,7 +204,7 @@ void LogFile::MessageBuilder::AppendSymbolName(Symbol symbol) {
   os << "hash " << std::hex << symbol->hash() << std::dec << ")";
 }
 
-void LogFile::MessageBuilder::AppendSymbolNameDetails(String str,
+void LogFile::MessageBuilder::AppendSymbolNameDetails(Tagged<String> str,
                                                       bool show_impl_info) {
   if (str.is_null()) return;
 
@@ -275,26 +275,6 @@ LogFile::MessageBuilder& LogFile::MessageBuilder::operator<<<char>(char c) {
   return *this;
 }
 
-template <>
-LogFile::MessageBuilder& LogFile::MessageBuilder::operator<<<String>(
-    String string) {
-  static_assert(kTaggedCanConvertToRawObjects);
-  return operator<<(Tagged(string));
-}
-
-template <>
-LogFile::MessageBuilder& LogFile::MessageBuilder::operator<< <Symbol>(
-    Symbol symbol) {
-  static_assert(kTaggedCanConvertToRawObjects);
-  return operator<<(Tagged(symbol));
-}
-
-template <>
-LogFile::MessageBuilder& LogFile::MessageBuilder::operator<< <Name>(Name name) {
-  static_assert(kTaggedCanConvertToRawObjects);
-  return operator<<(Tagged(name));
-}
-
 template <>
 LogFile::MessageBuilder& LogFile::MessageBuilder::operator<< <Tagged<String>>(
     Tagged<String> string) {
diff --git a/src/logging/log-file.h b/src/logging/log-file.h
index 63cfdf333bf..e23abd4708d 100644
--- a/src/logging/log-file.h
+++ b/src/logging/log-file.h
@@ -61,7 +61,7 @@ class LogFile {
    public:
     ~MessageBuilder() = default;
 
-    void AppendString(String str,
+    void AppendString(Tagged<String> str,
                       base::Optional<int> length_limit = base::nullopt);
     void AppendString(base::Vector<const char> str);
     void AppendString(const char* str);
@@ -69,7 +69,7 @@ class LogFile {
     void PRINTF_FORMAT(2, 3) AppendFormatString(const char* format, ...);
     void AppendCharacter(char c);
     void AppendTwoByteCharacter(char c1, char c2);
-    void AppendSymbolName(Symbol symbol);
+    void AppendSymbolName(Tagged<Symbol> symbol);
 
     // Delegate insertion to the underlying {log_}.
     // All appended strings are escaped to maintain one-line log entries.
@@ -92,7 +92,7 @@ class LogFile {
     int PRINTF_FORMAT(2, 0)
         FormatStringIntoBuffer(const char* format, va_list args);
 
-    void AppendSymbolNameDetails(String str, bool show_impl_info);
+    void AppendSymbolNameDetails(Tagged<String> str, bool show_impl_info);
 
     void PRINTF_FORMAT(2, 3) AppendRawFormatString(const char* format, ...);
     void AppendRawString(const char* format);
@@ -147,14 +147,6 @@ LogFile::MessageBuilder& LogFile::MessageBuilder::operator<<<const char*>(
 template <>
 LogFile::MessageBuilder& LogFile::MessageBuilder::operator<<<char>(char c);
 template <>
-LogFile::MessageBuilder& LogFile::MessageBuilder::operator<<<String>(
-    String string);
-template <>
-LogFile::MessageBuilder& LogFile::MessageBuilder::operator<<<Symbol>(
-    Symbol symbol);
-template <>
-LogFile::MessageBuilder& LogFile::MessageBuilder::operator<<<Name>(Name name);
-template <>
 LogFile::MessageBuilder& LogFile::MessageBuilder::operator<< <Tagged<String>>(
     Tagged<String> string);
 template <>
diff --git a/src/logging/log-inl.h b/src/logging/log-inl.h
index abc80ee6c26..881f85f6769 100644
--- a/src/logging/log-inl.h
+++ b/src/logging/log-inl.h
@@ -14,7 +14,7 @@ namespace v8 {
 namespace internal {
 
 LogEventListener::CodeTag V8FileLogger::ToNativeByScript(
-    LogEventListener::CodeTag tag, Script script) {
+    LogEventListener::CodeTag tag, Tagged<Script> script) {
   if (script->type() != Script::Type::kNative) return tag;
   switch (tag) {
     case LogEventListener::CodeTag::kFunction:
diff --git a/src/logging/log.cc b/src/logging/log.cc
index 340dc965791..8bd10449687 100644
--- a/src/logging/log.cc
+++ b/src/logging/log.cc
@@ -125,7 +125,8 @@ v8::CodeEventType GetCodeEventTypeForTag(LogEventListener::CodeTag tag) {
     PROFILE(isolate_, Call);          \
   }
 
-const char* ComputeMarker(SharedFunctionInfo shared, AbstractCode code) {
+const char* ComputeMarker(Tagged<SharedFunctionInfo> shared,
+                          Tagged<AbstractCode> code) {
   PtrComprCageBase cage_base = GetPtrComprCageBase(shared);
   CodeKind kind = code->kind(cage_base);
   // We record interpreter trampoline builtin copies as having the
@@ -168,11 +169,11 @@ class CodeEventLogger::NameBuffer {
     AppendByte(':');
   }
 
-  void AppendName(Name name) {
+  void AppendName(Tagged<Name> name) {
     if (IsString(name)) {
       AppendString(String::cast(name));
     } else {
-      Symbol symbol = Symbol::cast(name);
+      Tagged<Symbol> symbol = Symbol::cast(name);
       AppendBytes("symbol(");
       if (!IsUndefined(symbol->description())) {
         AppendBytes("\"");
@@ -185,7 +186,7 @@ class CodeEventLogger::NameBuffer {
     }
   }
 
-  void AppendString(String str) {
+  void AppendString(Tagged<String> str) {
     if (str.is_null()) return;
     int length = 0;
     std::unique_ptr<char[]> c_str =
@@ -341,13 +342,15 @@ class LinuxPerfBasicLogger : public CodeEventLogger {
   explicit LinuxPerfBasicLogger(Isolate* isolate);
   ~LinuxPerfBasicLogger() override;
 
-  void CodeMoveEvent(InstructionStream from, InstructionStream to) override {}
-  void BytecodeMoveEvent(BytecodeArray from, BytecodeArray to) override {}
+  void CodeMoveEvent(Tagged<InstructionStream> from,
+                     Tagged<InstructionStream> to) override {}
+  void BytecodeMoveEvent(Tagged<BytecodeArray> from,
+                         Tagged<BytecodeArray> to) override {}
   void CodeDisableOptEvent(Handle<AbstractCode> code,
                            Handle<SharedFunctionInfo> shared) override {}
 
  private:
-  void LogRecordedBuffer(AbstractCode code,
+  void LogRecordedBuffer(Tagged<AbstractCode> code,
                          MaybeHandle<SharedFunctionInfo> maybe_shared,
                          const char* name, int length) override;
 #if V8_ENABLE_WEBASSEMBLY
@@ -437,7 +440,7 @@ void LinuxPerfBasicLogger::WriteLogRecordedBuffer(uintptr_t address, int size,
 #endif
 }
 
-void LinuxPerfBasicLogger::LogRecordedBuffer(AbstractCode code,
+void LinuxPerfBasicLogger::LogRecordedBuffer(Tagged<AbstractCode> code,
                                              MaybeHandle<SharedFunctionInfo>,
                                              const char* name, int length) {
   DisallowGarbageCollection no_gc;
@@ -631,8 +634,8 @@ void InitializeCodeEvent(Isolate* isolate, CodeEvent* event,
 
 }  // namespace
 
-void ExternalLogEventListener::CodeMoveEvent(InstructionStream from,
-                                             InstructionStream to) {
+void ExternalLogEventListener::CodeMoveEvent(Tagged<InstructionStream> from,
+                                             Tagged<InstructionStream> to) {
   CodeEvent code_event;
   InitializeCodeEvent(isolate_, &code_event, from->instruction_start(),
                       to->instruction_start(),
@@ -640,8 +643,8 @@ void ExternalLogEventListener::CodeMoveEvent(InstructionStream from,
   code_event_handler_->Handle(reinterpret_cast<v8::CodeEvent*>(&code_event));
 }
 
-void ExternalLogEventListener::BytecodeMoveEvent(BytecodeArray from,
-                                                 BytecodeArray to) {
+void ExternalLogEventListener::BytecodeMoveEvent(Tagged<BytecodeArray> from,
+                                                 Tagged<BytecodeArray> to) {
   CodeEvent code_event;
   InitializeCodeEvent(isolate_, &code_event, from->GetFirstBytecodeAddress(),
                       to->GetFirstBytecodeAddress(), to->length());
@@ -654,15 +657,17 @@ class LowLevelLogger : public CodeEventLogger {
   LowLevelLogger(Isolate* isolate, const char* file_name);
   ~LowLevelLogger() override;
 
-  void CodeMoveEvent(InstructionStream from, InstructionStream to) override;
-  void BytecodeMoveEvent(BytecodeArray from, BytecodeArray to) override;
+  void CodeMoveEvent(Tagged<InstructionStream> from,
+                     Tagged<InstructionStream> to) override;
+  void BytecodeMoveEvent(Tagged<BytecodeArray> from,
+                         Tagged<BytecodeArray> to) override;
   void CodeDisableOptEvent(Handle<AbstractCode> code,
                            Handle<SharedFunctionInfo> shared) override {}
-  void SnapshotPositionEvent(HeapObject obj, int pos);
+  void SnapshotPositionEvent(Tagged<HeapObject> obj, int pos);
   void CodeMovingGCEvent() override;
 
  private:
-  void LogRecordedBuffer(AbstractCode code,
+  void LogRecordedBuffer(Tagged<AbstractCode> code,
                          MaybeHandle<SharedFunctionInfo> maybe_shared,
                          const char* name, int length) override;
 #if V8_ENABLE_WEBASSEMBLY
@@ -752,7 +757,7 @@ void LowLevelLogger::LogCodeInfo() {
   LogWriteBytes(arch, sizeof(arch));
 }
 
-void LowLevelLogger::LogRecordedBuffer(AbstractCode code,
+void LowLevelLogger::LogRecordedBuffer(Tagged<AbstractCode> code,
                                        MaybeHandle<SharedFunctionInfo>,
                                        const char* name, int length) {
   DisallowGarbageCollection no_gc;
@@ -782,15 +787,16 @@ void LowLevelLogger::LogRecordedBuffer(const wasm::WasmCode* code,
 }
 #endif  // V8_ENABLE_WEBASSEMBLY
 
-void LowLevelLogger::CodeMoveEvent(InstructionStream from,
-                                   InstructionStream to) {
+void LowLevelLogger::CodeMoveEvent(Tagged<InstructionStream> from,
+                                   Tagged<InstructionStream> to) {
   CodeMoveStruct event;
   event.from_address = from->instruction_start();
   event.to_address = to->instruction_start();
   LogWriteStruct(event);
 }
 
-void LowLevelLogger::BytecodeMoveEvent(BytecodeArray from, BytecodeArray to) {
+void LowLevelLogger::BytecodeMoveEvent(Tagged<BytecodeArray> from,
+                                       Tagged<BytecodeArray> to) {
   CodeMoveStruct event;
   event.from_address = from->GetFirstBytecodeAddress();
   event.to_address = to->GetFirstBytecodeAddress();
@@ -813,8 +819,10 @@ class JitLogger : public CodeEventLogger {
  public:
   JitLogger(Isolate* isolate, JitCodeEventHandler code_event_handler);
 
-  void CodeMoveEvent(InstructionStream from, InstructionStream to) override;
-  void BytecodeMoveEvent(BytecodeArray from, BytecodeArray to) override;
+  void CodeMoveEvent(Tagged<InstructionStream> from,
+                     Tagged<InstructionStream> to) override;
+  void BytecodeMoveEvent(Tagged<BytecodeArray> from,
+                         Tagged<BytecodeArray> to) override;
   void CodeDisableOptEvent(Handle<AbstractCode> code,
                            Handle<SharedFunctionInfo> shared) override {}
   void AddCodeLinePosInfoEvent(void* jit_handler_data, int pc_offset,
@@ -827,7 +835,7 @@ class JitLogger : public CodeEventLogger {
                            JitCodeEvent::CodeType code_type);
 
  private:
-  void LogRecordedBuffer(AbstractCode code,
+  void LogRecordedBuffer(Tagged<AbstractCode> code,
                          MaybeHandle<SharedFunctionInfo> maybe_shared,
                          const char* name, int length) override;
 #if V8_ENABLE_WEBASSEMBLY
@@ -844,7 +852,7 @@ JitLogger::JitLogger(Isolate* isolate, JitCodeEventHandler code_event_handler)
   DCHECK_NOT_NULL(code_event_handler);
 }
 
-void JitLogger::LogRecordedBuffer(AbstractCode code,
+void JitLogger::LogRecordedBuffer(Tagged<AbstractCode> code,
                                   MaybeHandle<SharedFunctionInfo> maybe_shared,
                                   const char* name, int length) {
   DisallowGarbageCollection no_gc;
@@ -919,10 +927,11 @@ void JitLogger::LogRecordedBuffer(const wasm::WasmCode* code, const char* name,
 }
 #endif  // V8_ENABLE_WEBASSEMBLY
 
-void JitLogger::CodeMoveEvent(InstructionStream from, InstructionStream to) {
+void JitLogger::CodeMoveEvent(Tagged<InstructionStream> from,
+                              Tagged<InstructionStream> to) {
   base::MutexGuard guard(&logger_mutex_);
 
-  Code code;
+  Tagged<Code> code;
   if (!from->TryGetCodeUnchecked(&code, kAcquireLoad)) {
     // Not yet fully initialized and no CodeCreateEvent has been emitted yet.
     return;
@@ -939,7 +948,8 @@ void JitLogger::CodeMoveEvent(InstructionStream from, InstructionStream to) {
   code_event_handler_(&event);
 }
 
-void JitLogger::BytecodeMoveEvent(BytecodeArray from, BytecodeArray to) {
+void JitLogger::BytecodeMoveEvent(Tagged<BytecodeArray> from,
+                                  Tagged<BytecodeArray> to) {
   base::MutexGuard guard(&logger_mutex_);
 
   JitCodeEvent event;
@@ -1329,8 +1339,8 @@ void AppendCodeCreateHeader(LogFile::MessageBuilder& msg,
 }
 
 void AppendCodeCreateHeader(Isolate* isolate, LogFile::MessageBuilder& msg,
-                            LogEventListener::CodeTag tag, AbstractCode code,
-                            uint64_t time) {
+                            LogEventListener::CodeTag tag,
+                            Tagged<AbstractCode> code, uint64_t time) {
   PtrComprCageBase cage_base(isolate);
   AppendCodeCreateHeader(
       msg, tag, code->kind(cage_base),
@@ -1364,9 +1374,9 @@ void AppendCodeCreateHeader(Isolate* isolate, LogFile::MessageBuilder& msg,
 void V8FileLogger::LogSourceCodeInformation(Handle<AbstractCode> code,
                                             Handle<SharedFunctionInfo> shared) {
   PtrComprCageBase cage_base(isolate_);
-  Object script_object = shared->script(cage_base);
+  Tagged<Object> script_object = shared->script(cage_base);
   if (!IsScript(script_object, cage_base)) return;
-  Script script = Script::cast(script_object);
+  Tagged<Script> script = Script::cast(script_object);
   EnsureLogScriptSource(script);
 
   if (!v8_flags.log_source_position) return;
@@ -1394,7 +1404,7 @@ void V8FileLogger::LogSourceCodeInformation(Handle<AbstractCode> code,
   msg << V8FileLogger::kNext;
   int maxInlinedId = -1;
   if (hasInlined) {
-    PodArray<InliningPosition> inlining_positions =
+    Tagged<PodArray<InliningPosition>> inlining_positions =
         DeoptimizationData::cast(
             Handle<Code>::cast(code)->deoptimization_data())
             ->InliningPositions();
@@ -1416,7 +1426,7 @@ void V8FileLogger::LogSourceCodeInformation(Handle<AbstractCode> code,
   }
   msg << V8FileLogger::kNext;
   if (hasInlined) {
-    DeoptimizationData deopt_data = DeoptimizationData::cast(
+    Tagged<DeoptimizationData> deopt_data = DeoptimizationData::cast(
         Handle<Code>::cast(code)->deoptimization_data());
     msg << std::hex;
     for (int i = 0; i <= maxInlinedId; i++) {
@@ -1500,8 +1510,8 @@ void V8FileLogger::CodeCreateEvent(CodeTag tag, Handle<AbstractCode> code,
   LogCodeDisassemble(code);
 }
 
-void V8FileLogger::FeedbackVectorEvent(FeedbackVector vector,
-                                       AbstractCode code) {
+void V8FileLogger::FeedbackVectorEvent(Tagged<FeedbackVector> vector,
+                                       Tagged<AbstractCode> code) {
   DisallowGarbageCollection no_gc;
   if (!v8_flags.log_feedback_vector) return;
   PtrComprCageBase cage_base(isolate_);
@@ -1608,13 +1618,15 @@ void V8FileLogger::RegExpCodeCreateEvent(Handle<AbstractCode> code,
   msg.WriteToLogFile();
 }
 
-void V8FileLogger::CodeMoveEvent(InstructionStream from, InstructionStream to) {
+void V8FileLogger::CodeMoveEvent(Tagged<InstructionStream> from,
+                                 Tagged<InstructionStream> to) {
   if (!is_listening_to_code_events()) return;
   MoveEventInternal(Event::kCodeMove, from->instruction_start(),
                     to->instruction_start());
 }
 
-void V8FileLogger::BytecodeMoveEvent(BytecodeArray from, BytecodeArray to) {
+void V8FileLogger::BytecodeMoveEvent(Tagged<BytecodeArray> from,
+                                     Tagged<BytecodeArray> to) {
   if (!is_listening_to_code_events()) return;
   MoveEventInternal(Event::kCodeMove, from->GetFirstBytecodeAddress(),
                     to->GetFirstBytecodeAddress());
@@ -1703,7 +1715,7 @@ void CodeLinePosEvent(JitLogger& jit_logger, Address code_start,
 }  // namespace
 
 void V8FileLogger::CodeLinePosInfoRecordEvent(
-    Address code_start, ByteArray source_position_table,
+    Address code_start, Tagged<ByteArray> source_position_table,
     JitCodeEvent::CodeType code_type) {
   if (!jit_logger_) return;
   SourcePositionTableIterator iter(source_position_table);
@@ -1753,7 +1765,8 @@ void AppendFunctionMessage(LogFile::MessageBuilder& msg, const char* reason,
 
 void V8FileLogger::FunctionEvent(const char* reason, int script_id,
                                  double time_delta, int start_position,
-                                 int end_position, String function_name) {
+                                 int end_position,
+                                 Tagged<String> function_name) {
   if (!v8_flags.log_function_events) return;
   MSG_BUILDER();
   AppendFunctionMessage(msg, reason, script_id, time_delta, start_position,
@@ -1779,7 +1792,7 @@ void V8FileLogger::FunctionEvent(const char* reason, int script_id,
 
 void V8FileLogger::CompilationCacheEvent(const char* action,
                                          const char* cache_type,
-                                         SharedFunctionInfo sfi) {
+                                         Tagged<SharedFunctionInfo> sfi) {
   if (!v8_flags.log_function_events) return;
   MSG_BUILDER();
   int script_id = -1;
@@ -1821,7 +1834,7 @@ void V8FileLogger::ScriptEvent(ScriptEventType type, int script_id) {
   msg.WriteToLogFile();
 }
 
-void V8FileLogger::ScriptDetails(Script script) {
+void V8FileLogger::ScriptDetails(Tagged<Script> script) {
   if (!v8_flags.log_function_events) return;
   {
     MSG_BUILDER();
@@ -1840,7 +1853,7 @@ void V8FileLogger::ScriptDetails(Script script) {
   EnsureLogScriptSource(script);
 }
 
-bool V8FileLogger::EnsureLogScriptSource(Script script) {
+bool V8FileLogger::EnsureLogScriptSource(Tagged<Script> script) {
   if (!v8_flags.log_source_code) return true;
   // Make sure the script is written to the log file.
   int script_id = script->id();
@@ -1849,14 +1862,14 @@ bool V8FileLogger::EnsureLogScriptSource(Script script) {
   }
   // This script has not been logged yet.
   logged_source_code_.insert(script_id);
-  Object source_object = script->source();
+  Tagged<Object> source_object = script->source();
   if (!IsString(source_object)) return false;
 
   std::unique_ptr<LogFile::MessageBuilder> msg_ptr = log_->NewMessageBuilder();
   if (!msg_ptr) return false;
   LogFile::MessageBuilder& msg = *msg_ptr.get();
 
-  String source_code = String::cast(source_object);
+  Tagged<String> source_code = String::cast(source_object);
   msg << "script-source" << kNext << script_id << kNext;
 
   // Log the script name.
@@ -1958,7 +1971,7 @@ void V8FileLogger::MapEvent(const char* type, Handle<Map> from, Handle<Map> to,
     if (IsName(*name_or_sfi)) {
       msg << Name::cast(*name_or_sfi);
     } else if (IsSharedFunctionInfo(*name_or_sfi)) {
-      SharedFunctionInfo sfi = SharedFunctionInfo::cast(*name_or_sfi);
+      Tagged<SharedFunctionInfo> sfi = SharedFunctionInfo::cast(*name_or_sfi);
       msg << sfi->DebugNameCStr().get();
       msg << " " << sfi->unique_id();
     }
@@ -1966,7 +1979,7 @@ void V8FileLogger::MapEvent(const char* type, Handle<Map> from, Handle<Map> to,
   msg.WriteToLogFile();
 }
 
-void V8FileLogger::MapCreate(Map map) {
+void V8FileLogger::MapCreate(Tagged<Map> map) {
   if (!v8_flags.log_maps) return;
   DisallowGarbageCollection no_gc;
   MSG_BUILDER();
@@ -1974,7 +1987,7 @@ void V8FileLogger::MapCreate(Map map) {
   msg.WriteToLogFile();
 }
 
-void V8FileLogger::MapDetails(Map map) {
+void V8FileLogger::MapDetails(Tagged<Map> map) {
   if (!v8_flags.log_maps) return;
   DisallowGarbageCollection no_gc;
   MSG_BUILDER();
@@ -1988,7 +2001,7 @@ void V8FileLogger::MapDetails(Map map) {
   msg.WriteToLogFile();
 }
 
-void V8FileLogger::MapMoveEvent(Map from, Map to) {
+void V8FileLogger::MapMoveEvent(Tagged<Map> from, Tagged<Map> to) {
   if (!v8_flags.log_maps) return;
   DisallowGarbageCollection no_gc;
   MSG_BUILDER();
@@ -2011,23 +2024,23 @@ EnumerateCompiledFunctions(Heap* heap) {
                      decltype(hash)>
       seen(8, hash);
 
-  auto record = [&](SharedFunctionInfo sfi, AbstractCode c) {
+  auto record = [&](Tagged<SharedFunctionInfo> sfi, Tagged<AbstractCode> c) {
     if (auto [iter, inserted] = seen.emplace(sfi, c); inserted)
       compiled_funcs.emplace_back(handle(sfi, isolate), handle(c, isolate));
   };
 
   // Iterate the heap to find JSFunctions and record their optimized code.
-  for (HeapObject obj = iterator.Next(); !obj.is_null();
+  for (Tagged<HeapObject> obj = iterator.Next(); !obj.is_null();
        obj = iterator.Next()) {
     if (IsSharedFunctionInfo(obj)) {
-      SharedFunctionInfo sfi = SharedFunctionInfo::cast(obj);
+      Tagged<SharedFunctionInfo> sfi = SharedFunctionInfo::cast(obj);
       if (sfi->is_compiled() && !sfi->HasBytecodeArray()) {
         record(sfi, AbstractCode::cast(sfi->abstract_code(isolate)));
       }
     } else if (IsJSFunction(obj)) {
       // Given that we no longer iterate over all optimized JSFunctions, we need
       // to take care of this here.
-      JSFunction function = JSFunction::cast(obj);
+      Tagged<JSFunction> function = JSFunction::cast(obj);
       // TODO(jarin) This leaves out deoptimized code that might still be on the
       // stack. Also note that we will not log optimized code objects that are
       // only on a type feedback vector. We should make this more precise.
@@ -2039,12 +2052,12 @@ EnumerateCompiledFunctions(Heap* heap) {
   }
 
   Script::Iterator script_iterator(heap->isolate());
-  for (Script script = script_iterator.Next(); !script.is_null();
+  for (Tagged<Script> script = script_iterator.Next(); !script.is_null();
        script = script_iterator.Next()) {
     if (!script->HasValidSource()) continue;
 
     SharedFunctionInfo::ScriptIterator sfi_iterator(heap->isolate(), script);
-    for (SharedFunctionInfo sfi = sfi_iterator.Next(); !sfi.is_null();
+    for (Tagged<SharedFunctionInfo> sfi = sfi_iterator.Next(); !sfi.is_null();
          sfi = sfi_iterator.Next()) {
       if (sfi->is_compiled()) {
         record(sfi, AbstractCode::cast(sfi->abstract_code(isolate)));
@@ -2117,10 +2130,10 @@ void V8FileLogger::LogAccessorCallbacks() {
   Heap* heap = isolate_->heap();
   HeapObjectIterator iterator(heap);
   DisallowGarbageCollection no_gc;
-  for (HeapObject obj = iterator.Next(); !obj.is_null();
+  for (Tagged<HeapObject> obj = iterator.Next(); !obj.is_null();
        obj = iterator.Next()) {
     if (!IsAccessorInfo(obj)) continue;
-    AccessorInfo ai = AccessorInfo::cast(obj);
+    Tagged<AccessorInfo> ai = AccessorInfo::cast(obj);
     if (!IsName(ai->name())) continue;
     Address getter_entry = ai->getter(isolate_);
     HandleScope scope(isolate_);
@@ -2144,10 +2157,10 @@ void V8FileLogger::LogAccessorCallbacks() {
 void V8FileLogger::LogAllMaps() {
   Heap* heap = isolate_->heap();
   CombinedHeapObjectIterator iterator(heap);
-  for (HeapObject obj = iterator.Next(); !obj.is_null();
+  for (Tagged<HeapObject> obj = iterator.Next(); !obj.is_null();
        obj = iterator.Next()) {
     if (!IsMap(obj)) continue;
-    Map map = Map::cast(obj);
+    Tagged<Map> map = Map::cast(obj);
     MapCreate(map);
     MapDetails(map);
   }
@@ -2388,7 +2401,7 @@ void V8FileLogger::UpdateIsLogging(bool value) {
   isolate_->UpdateLogObjectRelocation();
 }
 
-void ExistingCodeLogger::LogCodeObject(AbstractCode object) {
+void ExistingCodeLogger::LogCodeObject(Tagged<AbstractCode> object) {
   HandleScope scope(isolate_);
   Handle<AbstractCode> abstract_code(object, isolate_);
   CodeTag tag = CodeTag::kStub;
@@ -2457,7 +2470,7 @@ void ExistingCodeLogger::LogCodeObjects() {
   CombinedHeapObjectIterator iterator(heap);
   DisallowGarbageCollection no_gc;
   PtrComprCageBase cage_base(isolate_);
-  for (HeapObject obj = iterator.Next(); !obj.is_null();
+  for (Tagged<HeapObject> obj = iterator.Next(); !obj.is_null();
        obj = iterator.Next()) {
     InstanceType instance_type = obj->map(cage_base)->instance_type();
     if (InstanceTypeChecker::IsCode(instance_type) ||
@@ -2490,7 +2503,7 @@ void ExistingCodeLogger::LogCompiledFunctions(
 
     // If the script is a Smi, then the SharedFunctionInfo is in
     // the process of being deserialized.
-    Object script = shared->raw_script(kAcquireLoad);
+    Tagged<Object> script = shared->raw_script(kAcquireLoad);
     if (IsSmi(script)) {
       DCHECK_EQ(script, Smi::uninitialized_deserialization_value());
       continue;
@@ -2521,7 +2534,7 @@ void ExistingCodeLogger::LogCompiledFunctions(
   HeapObjectIterator iterator(heap);
   DisallowGarbageCollection no_gc;
 
-  for (HeapObject obj = iterator.Next(); !obj.is_null();
+  for (Tagged<HeapObject> obj = iterator.Next(); !obj.is_null();
        obj = iterator.Next()) {
     if (!IsWasmModuleObject(obj)) continue;
     auto module_object = WasmModuleObject::cast(obj);
@@ -2561,9 +2574,9 @@ void ExistingCodeLogger::LogExistingFunction(Handle<SharedFunctionInfo> shared,
     // API function.
     Handle<FunctionTemplateInfo> fun_data =
         handle(shared->api_func_data(), isolate_);
-    Object raw_call_data = fun_data->call_code(kAcquireLoad);
+    Tagged<Object> raw_call_data = fun_data->call_code(kAcquireLoad);
     if (!IsUndefined(raw_call_data, isolate_)) {
-      CallHandlerInfo call_data = CallHandlerInfo::cast(raw_call_data);
+      Tagged<CallHandlerInfo> call_data = CallHandlerInfo::cast(raw_call_data);
       Address entry_point = call_data->callback(isolate_);
 #if USES_FUNCTION_DESCRIPTORS
       entry_point = *FUNCTION_ENTRYPOINT_ADDRESS(entry_point);
diff --git a/src/logging/log.h b/src/logging/log.h
index 6e9092c0011..f2f4fe07906 100644
--- a/src/logging/log.h
+++ b/src/logging/log.h
@@ -99,7 +99,7 @@ class ExistingCodeLogger {
   void LogExistingFunction(
       Handle<SharedFunctionInfo> shared, Handle<AbstractCode> code,
       LogEventListener::CodeTag tag = LogEventListener::CodeTag::kFunction);
-  void LogCodeObject(AbstractCode object);
+  void LogCodeObject(Tagged<AbstractCode> object);
 
 #if defined(V8_OS_WIN) && defined(V8_ENABLE_ETW_STACK_WALKING)
   void LogInterpretedFunctions();
@@ -157,16 +157,16 @@ class V8FileLogger : public LogEventListener {
   // ==== Events logged by --log-function-events ====
   void FunctionEvent(const char* reason, int script_id, double time_delta_ms,
                      int start_position, int end_position,
-                     String function_name);
+                     Tagged<String> function_name);
   void FunctionEvent(const char* reason, int script_id, double time_delta_ms,
                      int start_position, int end_position,
                      const char* function_name = nullptr,
                      size_t function_name_length = 0, bool is_one_byte = true);
 
   void CompilationCacheEvent(const char* action, const char* cache_type,
-                             SharedFunctionInfo sfi);
+                             Tagged<SharedFunctionInfo> sfi);
   void ScriptEvent(ScriptEventType type, int script_id);
-  void ScriptDetails(Script script);
+  void ScriptDetails(Tagged<Script> script);
 
   // ==== Events logged by --log-code. ====
   V8_EXPORT_PRIVATE void AddLogEventListener(LogEventListener* listener);
@@ -194,8 +194,10 @@ class V8FileLogger : public LogEventListener {
   void SetterCallbackEvent(Handle<Name> name, Address entry_point) override;
   void RegExpCodeCreateEvent(Handle<AbstractCode> code,
                              Handle<String> source) override;
-  void CodeMoveEvent(InstructionStream from, InstructionStream to) override;
-  void BytecodeMoveEvent(BytecodeArray from, BytecodeArray to) override;
+  void CodeMoveEvent(Tagged<InstructionStream> from,
+                     Tagged<InstructionStream> to) override;
+  void BytecodeMoveEvent(Tagged<BytecodeArray> from,
+                         Tagged<BytecodeArray> to) override;
   void SharedFunctionInfoMoveEvent(Address from, Address to) override;
   void NativeContextMoveEvent(Address from, Address to) override {}
   void CodeMovingGCEvent() override;
@@ -206,7 +208,8 @@ class V8FileLogger : public LogEventListener {
   void CodeDependencyChangeEvent(Handle<Code> code,
                                  Handle<SharedFunctionInfo> sfi,
                                  const char* reason) override;
-  void FeedbackVectorEvent(FeedbackVector vector, AbstractCode code);
+  void FeedbackVectorEvent(Tagged<FeedbackVector> vector,
+                           Tagged<AbstractCode> code);
   void WeakCodeClearEvent() override {}
 
   void ProcessDeoptEvent(Handle<Code> code, SourcePosition position,
@@ -214,7 +217,7 @@ class V8FileLogger : public LogEventListener {
 
   // Emits a code line info record event.
   void CodeLinePosInfoRecordEvent(Address code_start,
-                                  ByteArray source_position_table,
+                                  Tagged<ByteArray> source_position_table,
                                   JitCodeEvent::CodeType code_type);
 #if V8_ENABLE_WEBASSEMBLY
   void WasmCodeLinePosInfoRecordEvent(
@@ -230,9 +233,9 @@ class V8FileLogger : public LogEventListener {
   void MapEvent(const char* type, Handle<Map> from, Handle<Map> to,
                 const char* reason = nullptr,
                 Handle<HeapObject> name_or_sfi = Handle<HeapObject>());
-  void MapCreate(Map map);
-  void MapDetails(Map map);
-  void MapMoveEvent(Map from, Map to);
+  void MapCreate(Tagged<Map> map);
+  void MapDetails(Tagged<Map> map);
+  void MapMoveEvent(Tagged<Map> from, Tagged<Map> to);
 
   void SharedLibraryEvent(const std::string& library_path, uintptr_t start,
                           uintptr_t end, intptr_t aslr_slide);
@@ -294,7 +297,7 @@ class V8FileLogger : public LogEventListener {
   void LogAllMaps();
 
   // Converts tag to a corresponding NATIVE_... if the script is native.
-  V8_INLINE static CodeTag ToNativeByScript(CodeTag tag, Script script);
+  V8_INLINE static CodeTag ToNativeByScript(CodeTag tag, Tagged<Script> script);
 
 #if defined(V8_OS_WIN) && defined(V8_ENABLE_ETW_STACK_WALKING)
   void LogInterpretedFunctions();
@@ -325,18 +328,18 @@ class V8FileLogger : public LogEventListener {
 
   // Logs a scripts sources. Keeps track of all logged scripts to ensure that
   // each script is logged only once.
-  bool EnsureLogScriptSource(Script script);
+  bool EnsureLogScriptSource(Tagged<Script> script);
 
   void LogSourceCodeInformation(Handle<AbstractCode> code,
                                 Handle<SharedFunctionInfo> shared);
   void LogCodeDisassemble(Handle<AbstractCode> code);
 
   void WriteApiSecurityCheck();
-  void WriteApiNamedPropertyAccess(const char* tag, JSObject holder,
-                                   Object name);
-  void WriteApiIndexedPropertyAccess(const char* tag, JSObject holder,
+  void WriteApiNamedPropertyAccess(const char* tag, Tagged<JSObject> holder,
+                                   Tagged<Object> name);
+  void WriteApiIndexedPropertyAccess(const char* tag, Tagged<JSObject> holder,
                                      uint32_t index);
-  void WriteApiObjectAccess(const char* tag, JSReceiver obj);
+  void WriteApiObjectAccess(const char* tag, Tagged<JSReceiver> obj);
   void WriteApiEntryCall(const char* name);
 
   int64_t Time();
@@ -461,7 +464,7 @@ class V8_EXPORT_PRIVATE CodeEventLogger : public LogEventListener {
  private:
   class NameBuffer;
 
-  virtual void LogRecordedBuffer(AbstractCode code,
+  virtual void LogRecordedBuffer(Tagged<AbstractCode> code,
                                  MaybeHandle<SharedFunctionInfo> maybe_shared,
                                  const char* name, int length) = 0;
 #if V8_ENABLE_WEBASSEMBLY
@@ -513,8 +516,10 @@ class ExternalLogEventListener : public LogEventListener {
   void SetterCallbackEvent(Handle<Name> name, Address entry_point) override {}
   void SharedFunctionInfoMoveEvent(Address from, Address to) override {}
   void NativeContextMoveEvent(Address from, Address to) override {}
-  void CodeMoveEvent(InstructionStream from, InstructionStream to) override;
-  void BytecodeMoveEvent(BytecodeArray from, BytecodeArray to) override;
+  void CodeMoveEvent(Tagged<InstructionStream> from,
+                     Tagged<InstructionStream> to) override;
+  void BytecodeMoveEvent(Tagged<BytecodeArray> from,
+                         Tagged<BytecodeArray> to) override;
   void CodeDisableOptEvent(Handle<AbstractCode> code,
                            Handle<SharedFunctionInfo> shared) override {}
   void CodeMovingGCEvent() override {}
diff --git a/src/maglev/arm/maglev-assembler-arm-inl.h b/src/maglev/arm/maglev-assembler-arm-inl.h
index a83ac02104b..2cdd9035b30 100644
--- a/src/maglev/arm/maglev-assembler-arm-inl.h
+++ b/src/maglev/arm/maglev-assembler-arm-inl.h
@@ -505,7 +505,7 @@ inline void MaglevAssembler::Move(Register dst, Register src) {
     mov(dst, src);
   }
 }
-inline void MaglevAssembler::Move(Register dst, TaggedIndex i) {
+inline void MaglevAssembler::Move(Register dst, Tagged<TaggedIndex> i) {
   mov(dst, Operand(i.ptr()));
 }
 inline void MaglevAssembler::Move(Register dst, int32_t i) {
@@ -742,7 +742,7 @@ inline void MaglevAssembler::CompareInstanceTypeRange(
                                            higher_limit);
 }
 
-inline void MaglevAssembler::CompareTagged(Register reg, Smi smi) {
+inline void MaglevAssembler::CompareTagged(Register reg, Tagged<Smi> smi) {
   cmp(reg, Operand(smi));
 }
 
@@ -894,7 +894,7 @@ inline void MaglevAssembler::CompareInt32AndJumpIf(Register r1, int32_t value,
   b(cond, target);
 }
 
-inline void MaglevAssembler::CompareSmiAndJumpIf(Register r1, Smi value,
+inline void MaglevAssembler::CompareSmiAndJumpIf(Register r1, Tagged<Smi> value,
                                                  Condition cond, Label* target,
                                                  Label::Distance distance) {
   cmp(r1, Operand(value));
@@ -911,7 +911,8 @@ inline void MaglevAssembler::CompareByteAndJumpIf(MemOperand left, int8_t right,
   JumpIf(cond, target, distance);
 }
 
-inline void MaglevAssembler::CompareTaggedAndJumpIf(Register r1, Smi value,
+inline void MaglevAssembler::CompareTaggedAndJumpIf(Register r1,
+                                                    Tagged<Smi> value,
                                                     Condition cond,
                                                     Label* target,
                                                     Label::Distance distance) {
diff --git a/src/maglev/arm64/maglev-assembler-arm64-inl.h b/src/maglev/arm64/maglev-assembler-arm64-inl.h
index c7ce9cbadf9..85610e84f0c 100644
--- a/src/maglev/arm64/maglev-assembler-arm64-inl.h
+++ b/src/maglev/arm64/maglev-assembler-arm64-inl.h
@@ -665,7 +665,7 @@ inline void MaglevAssembler::Move(Register dst, ExternalReference src) {
 inline void MaglevAssembler::Move(Register dst, Register src) {
   MacroAssembler::Move(dst, src);
 }
-inline void MaglevAssembler::Move(Register dst, TaggedIndex i) {
+inline void MaglevAssembler::Move(Register dst, Tagged<TaggedIndex> i) {
   Mov(dst, i.ptr());
 }
 inline void MaglevAssembler::Move(Register dst, int32_t i) {
@@ -869,7 +869,7 @@ inline void MaglevAssembler::CompareInstanceTypeRange(
                                            higher_limit);
 }
 
-inline void MaglevAssembler::CompareTagged(Register reg, Smi smi) {
+inline void MaglevAssembler::CompareTagged(Register reg, Tagged<Smi> smi) {
   CmpTagged(reg, Immediate(smi));
 }
 
@@ -1004,7 +1004,7 @@ inline void MaglevAssembler::CompareInt32AndJumpIf(Register r1, int32_t value,
   CompareAndBranch(r1.W(), Immediate(value), cond, target);
 }
 
-inline void MaglevAssembler::CompareSmiAndJumpIf(Register r1, Smi value,
+inline void MaglevAssembler::CompareSmiAndJumpIf(Register r1, Tagged<Smi> value,
                                                  Condition cond, Label* target,
                                                  Label::Distance distance) {
   AssertSmi(r1);
@@ -1020,7 +1020,8 @@ inline void MaglevAssembler::CompareByteAndJumpIf(MemOperand left, int8_t right,
   CompareAndBranch(scratch.W(), Immediate(right), cond, target);
 }
 
-inline void MaglevAssembler::CompareTaggedAndJumpIf(Register r1, Smi value,
+inline void MaglevAssembler::CompareTaggedAndJumpIf(Register r1,
+                                                    Tagged<Smi> value,
                                                     Condition cond,
                                                     Label* target,
                                                     Label::Distance distance) {
diff --git a/src/maglev/maglev-assembler-inl.h b/src/maglev/maglev-assembler-inl.h
index 58db73083ac..1edf8bd1901 100644
--- a/src/maglev/maglev-assembler-inl.h
+++ b/src/maglev/maglev-assembler-inl.h
@@ -338,7 +338,7 @@ inline bool ClobberedBy(RegList written_registers, Handle<Object> handle) {
 inline bool ClobberedBy(RegList written_registers, Tagged<Smi> smi) {
   return false;
 }
-inline bool ClobberedBy(RegList written_registers, TaggedIndex index) {
+inline bool ClobberedBy(RegList written_registers, Tagged<TaggedIndex> index) {
   return false;
 }
 inline bool ClobberedBy(RegList written_registers, int32_t imm) {
@@ -365,7 +365,8 @@ inline bool ClobberedBy(DoubleRegList written_registers,
 inline bool ClobberedBy(DoubleRegList written_registers, Tagged<Smi> smi) {
   return false;
 }
-inline bool ClobberedBy(DoubleRegList written_registers, TaggedIndex index) {
+inline bool ClobberedBy(DoubleRegList written_registers,
+                        Tagged<TaggedIndex> index) {
   return false;
 }
 inline bool ClobberedBy(DoubleRegList written_registers, int32_t imm) {
@@ -396,7 +397,7 @@ inline bool MachineTypeMatches(MachineType type, Handle<HeapObject> handle) {
 inline bool MachineTypeMatches(MachineType type, Tagged<Smi> smi) {
   return type.IsTagged() && !type.IsTaggedPointer();
 }
-inline bool MachineTypeMatches(MachineType type, TaggedIndex index) {
+inline bool MachineTypeMatches(MachineType type, Tagged<TaggedIndex> index) {
   // TaggedIndex doesn't have a separate type, so check for the same type as for
   // Smis.
   return type.IsTagged() && !type.IsTaggedPointer();
diff --git a/src/maglev/maglev-assembler.h b/src/maglev/maglev-assembler.h
index af05f286137..7fb1c6590d7 100644
--- a/src/maglev/maglev-assembler.h
+++ b/src/maglev/maglev-assembler.h
@@ -335,7 +335,7 @@ class MaglevAssembler : public MacroAssembler {
   inline void Move(Register dst, Tagged<Smi> src);
   inline void Move(Register dst, ExternalReference src);
   inline void Move(Register dst, Register src);
-  inline void Move(Register dst, TaggedIndex i);
+  inline void Move(Register dst, Tagged<TaggedIndex> i);
   inline void Move(Register dst, int32_t i);
   inline void Move(DoubleRegister dst, double n);
   inline void Move(DoubleRegister dst, Float64 n);
@@ -396,12 +396,12 @@ class MaglevAssembler : public MacroAssembler {
                                        InstanceType lower_limit,
                                        InstanceType higher_limit);
 
-  inline void CompareTagged(Register reg, Smi smi);
+  inline void CompareTagged(Register reg, Tagged<Smi> smi);
   inline void CompareTagged(Register reg, Handle<HeapObject> obj);
   inline void CompareTagged(Register src1, Register src2);
 
-  inline void CompareTaggedAndJumpIf(Register reg, Smi smi, Condition cond,
-                                     Label* target,
+  inline void CompareTaggedAndJumpIf(Register reg, Tagged<Smi> smi,
+                                     Condition cond, Label* target,
                                      Label::Distance distance = Label::kFar);
 
   inline void CompareInt32(Register reg, int32_t imm);
@@ -450,8 +450,8 @@ class MaglevAssembler : public MacroAssembler {
   inline void CompareInt32AndJumpIf(Register r1, int32_t value, Condition cond,
                                     Label* target,
                                     Label::Distance distance = Label::kFar);
-  inline void CompareSmiAndJumpIf(Register r1, Smi value, Condition cond,
-                                  Label* target,
+  inline void CompareSmiAndJumpIf(Register r1, Tagged<Smi> value,
+                                  Condition cond, Label* target,
                                   Label::Distance distance = Label::kFar);
   inline void CompareByteAndJumpIf(MemOperand left, int8_t right,
                                    Condition cond, Register scratch,
diff --git a/src/maglev/maglev-code-generator.cc b/src/maglev/maglev-code-generator.cc
index 61c804c3b21..cb42c56148e 100644
--- a/src/maglev/maglev-code-generator.cc
+++ b/src/maglev/maglev-code-generator.cc
@@ -1386,7 +1386,7 @@ class MaglevFrameTranslationBuilder {
     }
   }
 
-  int GetDeoptLiteral(Object obj) {
+  int GetDeoptLiteral(Tagged<Object> obj) {
     IdentityMapFindResult<int> res = deopt_literals_->FindOrInsert(obj);
     if (!res.already_exists) {
       DCHECK_EQ(0, *res.entry);
diff --git a/src/maglev/maglev-graph-printer.cc b/src/maglev/maglev-graph-printer.cc
index c816d4730ca..b2b0f91ec88 100644
--- a/src/maglev/maglev-graph-printer.cc
+++ b/src/maglev/maglev-graph-printer.cc
@@ -636,7 +636,7 @@ void MaybePrintProvenance(std::ostream& os, std::vector<BasicBlock*> targets,
 
   // Print function every time the compilation unit changes.
   bool needs_function_print = provenance.unit != existing_provenance.unit;
-  Script script;
+  Tagged<Script> script;
   Script::PositionInfo position_info;
   bool has_position_info = false;
 
diff --git a/src/maglev/maglev-ir.cc b/src/maglev/maglev-ir.cc
index cafd0d925a0..e2fa798a208 100644
--- a/src/maglev/maglev-ir.cc
+++ b/src/maglev/maglev-ir.cc
@@ -2561,9 +2561,7 @@ void LoadPolymorphicTaggedField::GenerateCode(MaglevAssembler* masm,
           case PolymorphicAccessInfo::kConstant: {
             Handle<Object> constant = access_info.constant();
             if (IsSmi(*constant)) {
-              // Remove the cast to Tagged<Smi> once Smi::cast returns Tagged.
-              static_assert(kTaggedCanConvertToRawObjects);
-              __ Move(result, Tagged<Smi>(Smi::cast(*constant)));
+              __ Move(result, Smi::cast(*constant));
             } else {
               DCHECK(IsHeapObject(*access_info.constant()));
               __ Move(result, Handle<HeapObject>::cast(constant));
@@ -2629,8 +2627,7 @@ void LoadPolymorphicDoubleField::GenerateCode(MaglevAssembler* masm,
             Handle<Object> constant = access_info.constant();
             if (IsSmi(*constant)) {
               // Remove the cast to Tagged<Smi> once Smi::cast returns Tagged.
-              static_assert(kTaggedCanConvertToRawObjects);
-              __ Move(scratch, Tagged<Smi>(Smi::cast(*constant)));
+              __ Move(scratch, Smi::cast(*constant));
               __ SmiToDouble(result, scratch);
             } else {
               DCHECK(IsHeapNumber(*constant));
diff --git a/src/maglev/maglev-ir.h b/src/maglev/maglev-ir.h
index ac20db9ccec..888944b606d 100644
--- a/src/maglev/maglev-ir.h
+++ b/src/maglev/maglev-ir.h
@@ -4184,10 +4184,10 @@ class TaggedIndexConstant
  public:
   using OutputRegister = Register;
 
-  explicit TaggedIndexConstant(uint64_t bitfield, TaggedIndex value)
+  explicit TaggedIndexConstant(uint64_t bitfield, Tagged<TaggedIndex> value)
       : Base(bitfield), value_(value) {}
 
-  TaggedIndex value() const { return value_; }
+  Tagged<TaggedIndex> value() const { return value_; }
 
   bool ToBoolean(LocalIsolate* local_isolate) const { UNREACHABLE(); }
 
@@ -4199,7 +4199,7 @@ class TaggedIndexConstant
   Handle<Object> DoReify(LocalIsolate* isolate) const;
 
  private:
-  const TaggedIndex value_;
+  const Tagged<TaggedIndex> value_;
 };
 
 class ExternalConstant : public FixedInputValueNodeT<0, ExternalConstant> {
diff --git a/src/maglev/x64/maglev-assembler-x64-inl.h b/src/maglev/x64/maglev-assembler-x64-inl.h
index 430ec9a3114..8c37f8c8a62 100644
--- a/src/maglev/x64/maglev-assembler-x64-inl.h
+++ b/src/maglev/x64/maglev-assembler-x64-inl.h
@@ -499,7 +499,7 @@ inline void MaglevAssembler::Move(MemOperand dst, Register src) {
   movq(dst, src);
 }
 
-inline void MaglevAssembler::Move(Register dst, TaggedIndex i) {
+inline void MaglevAssembler::Move(Register dst, Tagged<TaggedIndex> i) {
   MacroAssembler::Move(dst, i);
 }
 
@@ -708,7 +708,7 @@ inline void MaglevAssembler::CompareInstanceTypeRange(
   CmpInstanceTypeRange(map, instance_type_out, lower_limit, higher_limit);
 }
 
-inline void MaglevAssembler::CompareTagged(Register reg, Smi obj) {
+inline void MaglevAssembler::CompareTagged(Register reg, Tagged<Smi> obj) {
   Cmp(reg, obj);
 }
 
@@ -866,7 +866,7 @@ inline void MaglevAssembler::CompareInt32AndJumpIf(Register r1, int32_t value,
   JumpIf(cond, target, distance);
 }
 
-inline void MaglevAssembler::CompareSmiAndJumpIf(Register r1, Smi value,
+inline void MaglevAssembler::CompareSmiAndJumpIf(Register r1, Tagged<Smi> value,
                                                  Condition cond, Label* target,
                                                  Label::Distance distance) {
   AssertSmi(r1);
@@ -883,7 +883,8 @@ inline void MaglevAssembler::CompareByteAndJumpIf(MemOperand left, int8_t right,
   JumpIf(cond, target, distance);
 }
 
-inline void MaglevAssembler::CompareTaggedAndJumpIf(Register r1, Smi value,
+inline void MaglevAssembler::CompareTaggedAndJumpIf(Register r1,
+                                                    Tagged<Smi> value,
                                                     Condition cond,
                                                     Label* target,
                                                     Label::Distance distance) {
diff --git a/src/numbers/conversions-inl.h b/src/numbers/conversions-inl.h
index 204fee9994c..4026501cdae 100644
--- a/src/numbers/conversions-inl.h
+++ b/src/numbers/conversions-inl.h
@@ -191,17 +191,17 @@ bool DoubleToUint32IfEqualToSelf(double value, uint32_t* uint32_value) {
   return false;
 }
 
-int32_t NumberToInt32(Object number) {
+int32_t NumberToInt32(Tagged<Object> number) {
   if (IsSmi(number)) return Smi::ToInt(number);
   return DoubleToInt32(HeapNumber::cast(number)->value());
 }
 
-uint32_t NumberToUint32(Object number) {
+uint32_t NumberToUint32(Tagged<Object> number) {
   if (IsSmi(number)) return Smi::ToInt(number);
   return DoubleToUint32(HeapNumber::cast(number)->value());
 }
 
-uint32_t PositiveNumberToUint32(Object number) {
+uint32_t PositiveNumberToUint32(Tagged<Object> number) {
   if (IsSmi(number)) {
     int value = Smi::ToInt(number);
     if (value <= 0) return 0;
@@ -215,7 +215,7 @@ uint32_t PositiveNumberToUint32(Object number) {
   return max;
 }
 
-int64_t NumberToInt64(Object number) {
+int64_t NumberToInt64(Tagged<Object> number) {
   if (IsSmi(number)) return Smi::ToInt(number);
   double d = HeapNumber::cast(number)->value();
   if (std::isnan(d)) return 0;
@@ -228,7 +228,7 @@ int64_t NumberToInt64(Object number) {
   return static_cast<int64_t>(d);
 }
 
-uint64_t PositiveNumberToUint64(Object number) {
+uint64_t PositiveNumberToUint64(Tagged<Object> number) {
   if (IsSmi(number)) {
     int value = Smi::ToInt(number);
     if (value <= 0) return 0;
@@ -242,7 +242,7 @@ uint64_t PositiveNumberToUint64(Object number) {
   return max;
 }
 
-bool TryNumberToSize(Object number, size_t* result) {
+bool TryNumberToSize(Tagged<Object> number, size_t* result) {
   // Do not create handles in this function! Don't use SealHandleScope because
   // the function can be used concurrently.
   if (IsSmi(number)) {
@@ -270,7 +270,7 @@ bool TryNumberToSize(Object number, size_t* result) {
   }
 }
 
-size_t NumberToSize(Object number) {
+size_t NumberToSize(Tagged<Object> number) {
   size_t result = 0;
   bool is_valid = TryNumberToSize(number, &result);
   CHECK(is_valid);
diff --git a/src/numbers/conversions.cc b/src/numbers/conversions.cc
index 59b1fd1bc68..03bd9ff214f 100644
--- a/src/numbers/conversions.cc
+++ b/src/numbers/conversions.cc
@@ -1448,7 +1448,8 @@ double StringToDouble(Isolate* isolate, Handle<String> string, int flags,
   return FlatStringToDouble(*flattened, flags, empty_string_val);
 }
 
-double FlatStringToDouble(String string, int flags, double empty_string_val) {
+double FlatStringToDouble(Tagged<String> string, int flags,
+                          double empty_string_val) {
   DisallowGarbageCollection no_gc;
   DCHECK(string->IsFlat());
   String::FlatContent flat = string->GetFlatContent(no_gc);
@@ -1501,7 +1502,7 @@ base::Optional<double> TryStringToInt(LocalIsolate* isolate,
   }
 }
 
-bool IsSpecialIndex(String string) {
+bool IsSpecialIndex(Tagged<String> string) {
   // Max length of canonical double: -X.XXXXXXXXXXXXXXXXX-eXXX
   const int kBufferSize = 24;
   const int length = string->length();
diff --git a/src/numbers/conversions.h b/src/numbers/conversions.h
index 2464db7350b..702e59a787a 100644
--- a/src/numbers/conversions.h
+++ b/src/numbers/conversions.h
@@ -160,15 +160,16 @@ inline bool IsUint32Double(double value);
 inline bool DoubleToUint32IfEqualToSelf(double value, uint32_t* uint32_value);
 
 // Convert from Number object to C integer.
-inline uint32_t PositiveNumberToUint32(Object number);
-inline int32_t NumberToInt32(Object number);
-inline uint32_t NumberToUint32(Object number);
-inline int64_t NumberToInt64(Object number);
-inline uint64_t PositiveNumberToUint64(Object number);
+inline uint32_t PositiveNumberToUint32(Tagged<Object> number);
+inline int32_t NumberToInt32(Tagged<Object> number);
+inline uint32_t NumberToUint32(Tagged<Object> number);
+inline int64_t NumberToInt64(Tagged<Object> number);
+inline uint64_t PositiveNumberToUint64(Tagged<Object> number);
 
 double StringToDouble(Isolate* isolate, Handle<String> string, int flags,
                       double empty_string_val = 0.0);
-double FlatStringToDouble(String string, int flags, double empty_string_val);
+double FlatStringToDouble(Tagged<String> string, int flags,
+                          double empty_string_val);
 
 // String to double helper without heap allocation.
 // Returns base::nullopt if the string is longer than
@@ -183,13 +184,13 @@ V8_EXPORT_PRIVATE base::Optional<double> TryStringToInt(LocalIsolate* isolate,
                                                         Handle<String> object,
                                                         int radix);
 
-inline bool TryNumberToSize(Object number, size_t* result);
+inline bool TryNumberToSize(Tagged<Object> number, size_t* result);
 
 // Converts a number into size_t.
-inline size_t NumberToSize(Object number);
+inline size_t NumberToSize(Tagged<Object> number);
 
 // returns DoubleToString(StringToDouble(string)) == string
-V8_EXPORT_PRIVATE bool IsSpecialIndex(String string);
+V8_EXPORT_PRIVATE bool IsSpecialIndex(Tagged<String> string);
 
 }  // namespace internal
 }  // namespace v8
diff --git a/src/numbers/math-random.cc b/src/numbers/math-random.cc
index ebdae19e863..53fb393416c 100644
--- a/src/numbers/math-random.cc
+++ b/src/numbers/math-random.cc
@@ -26,16 +26,16 @@ void MathRandom::InitializeContext(Isolate* isolate,
   ResetContext(*native_context);
 }
 
-void MathRandom::ResetContext(Context native_context) {
+void MathRandom::ResetContext(Tagged<Context> native_context) {
   native_context->set_math_random_index(Smi::zero());
   State state = {0, 0};
   PodArray<State>::cast(native_context->math_random_state())->set(0, state);
 }
 
 Address MathRandom::RefillCache(Isolate* isolate, Address raw_native_context) {
-  Context native_context = Context::cast(Object(raw_native_context));
+  Tagged<Context> native_context = Context::cast(Object(raw_native_context));
   DisallowGarbageCollection no_gc;
-  PodArray<State> pod =
+  Tagged<PodArray<State>> pod =
       PodArray<State>::cast(native_context->math_random_state());
   State state = pod->get(0);
   // Initialize state if not yet initialized. If a fixed random seed was
@@ -54,7 +54,7 @@ Address MathRandom::RefillCache(Isolate* isolate, Address raw_native_context) {
     CHECK(state.s0 != 0 || state.s1 != 0);
   }
 
-  FixedDoubleArray cache =
+  Tagged<FixedDoubleArray> cache =
       FixedDoubleArray::cast(native_context->math_random_cache());
   // Create random numbers.
   for (int i = 0; i < kCacheSize; i++) {
@@ -64,7 +64,7 @@ Address MathRandom::RefillCache(Isolate* isolate, Address raw_native_context) {
   }
   pod->set(0, state);
 
-  Smi new_index = Smi::FromInt(kCacheSize);
+  Tagged<Smi> new_index = Smi::FromInt(kCacheSize);
   native_context->set_math_random_index(new_index);
   return new_index.ptr();
 }
diff --git a/src/numbers/math-random.h b/src/numbers/math-random.h
index c321b82ba2c..fa7fba07bdc 100644
--- a/src/numbers/math-random.h
+++ b/src/numbers/math-random.h
@@ -16,7 +16,7 @@ class MathRandom : public AllStatic {
   static void InitializeContext(Isolate* isolate,
                                 Handle<Context> native_context);
 
-  static void ResetContext(Context native_context);
+  static void ResetContext(Tagged<Context> native_context);
   // Takes native context as a raw Address for ExternalReference usage.
   // Returns a tagged Smi as a raw Address.
   static Address RefillCache(Isolate* isolate, Address raw_native_context);
diff --git a/src/objects/abstract-code-inl.h b/src/objects/abstract-code-inl.h
index b0d4e61fba7..c2eafde6052 100644
--- a/src/objects/abstract-code-inl.h
+++ b/src/objects/abstract-code-inl.h
@@ -20,7 +20,7 @@ OBJECT_CONSTRUCTORS_IMPL(AbstractCode, HeapObject)
 CAST_ACCESSOR(AbstractCode)
 
 int AbstractCode::InstructionSize(PtrComprCageBase cage_base) {
-  Map map_object = map(cage_base);
+  Tagged<Map> map_object = map(cage_base);
   if (InstanceTypeChecker::IsCode(map_object)) {
     return GetCode()->instruction_size();
   } else {
@@ -29,11 +29,11 @@ int AbstractCode::InstructionSize(PtrComprCageBase cage_base) {
   }
 }
 
-ByteArray AbstractCode::SourcePositionTableInternal(
+Tagged<ByteArray> AbstractCode::SourcePositionTableInternal(
     PtrComprCageBase cage_base) {
-  Map map_object = map(cage_base);
+  Tagged<Map> map_object = map(cage_base);
   if (InstanceTypeChecker::IsCode(map_object)) {
-    Code code = GetCode();
+    Tagged<Code> code = GetCode();
     if (!code->has_instruction_stream()) {
       return GetReadOnlyRoots().empty_byte_array();
     }
@@ -44,9 +44,9 @@ ByteArray AbstractCode::SourcePositionTableInternal(
   }
 }
 
-ByteArray AbstractCode::SourcePositionTable(Isolate* isolate,
-                                            SharedFunctionInfo sfi) {
-  Map map_object = map(isolate);
+Tagged<ByteArray> AbstractCode::SourcePositionTable(
+    Isolate* isolate, Tagged<SharedFunctionInfo> sfi) {
+  Tagged<Map> map_object = map(isolate);
   if (InstanceTypeChecker::IsCode(map_object)) {
     return GetCode()->SourcePositionTable(isolate, sfi);
   } else {
@@ -56,7 +56,7 @@ ByteArray AbstractCode::SourcePositionTable(Isolate* isolate,
 }
 
 int AbstractCode::SizeIncludingMetadata(PtrComprCageBase cage_base) {
-  Map map_object = map(cage_base);
+  Tagged<Map> map_object = map(cage_base);
   if (InstanceTypeChecker::IsCode(map_object)) {
     return GetCode()->SizeIncludingMetadata();
   } else {
@@ -66,7 +66,7 @@ int AbstractCode::SizeIncludingMetadata(PtrComprCageBase cage_base) {
 }
 
 Address AbstractCode::InstructionStart(PtrComprCageBase cage_base) {
-  Map map_object = map(cage_base);
+  Tagged<Map> map_object = map(cage_base);
   if (InstanceTypeChecker::IsCode(map_object)) {
     return GetCode()->instruction_start();
   } else {
@@ -76,19 +76,19 @@ Address AbstractCode::InstructionStart(PtrComprCageBase cage_base) {
 }
 
 Address AbstractCode::InstructionEnd(PtrComprCageBase cage_base) {
-  Map map_object = map(cage_base);
+  Tagged<Map> map_object = map(cage_base);
   if (InstanceTypeChecker::IsCode(map_object)) {
     return GetCode()->instruction_end();
   } else {
     DCHECK(InstanceTypeChecker::IsBytecodeArray(map_object));
-    BytecodeArray bytecode_array = GetBytecodeArray();
+    Tagged<BytecodeArray> bytecode_array = GetBytecodeArray();
     return bytecode_array->GetFirstBytecodeAddress() + bytecode_array->length();
   }
 }
 
 bool AbstractCode::contains(Isolate* isolate, Address inner_pointer) {
   PtrComprCageBase cage_base(isolate);
-  Map map_object = map(cage_base);
+  Tagged<Map> map_object = map(cage_base);
   if (InstanceTypeChecker::IsCode(map_object)) {
     return GetCode()->contains(isolate, inner_pointer);
   } else {
@@ -99,7 +99,7 @@ bool AbstractCode::contains(Isolate* isolate, Address inner_pointer) {
 }
 
 CodeKind AbstractCode::kind(PtrComprCageBase cage_base) {
-  Map map_object = map(cage_base);
+  Tagged<Map> map_object = map(cage_base);
   if (InstanceTypeChecker::IsCode(map_object)) {
     return GetCode()->kind();
   } else {
@@ -109,7 +109,7 @@ CodeKind AbstractCode::kind(PtrComprCageBase cage_base) {
 }
 
 Builtin AbstractCode::builtin_id(PtrComprCageBase cage_base) {
-  Map map_object = map(cage_base);
+  Tagged<Map> map_object = map(cage_base);
   if (InstanceTypeChecker::IsCode(map_object)) {
     return GetCode()->builtin_id();
   } else {
@@ -123,9 +123,9 @@ bool AbstractCode::has_instruction_stream(PtrComprCageBase cage_base) {
   return GetCode()->has_instruction_stream();
 }
 
-Code AbstractCode::GetCode() { return Code::cast(*this); }
+Tagged<Code> AbstractCode::GetCode() { return Code::cast(*this); }
 
-BytecodeArray AbstractCode::GetBytecodeArray() {
+Tagged<BytecodeArray> AbstractCode::GetBytecodeArray() {
   return BytecodeArray::cast(*this);
 }
 
diff --git a/src/objects/abstract-code.cc b/src/objects/abstract-code.cc
index 22b8c4a18f8..16bf8b79d92 100644
--- a/src/objects/abstract-code.cc
+++ b/src/objects/abstract-code.cc
@@ -12,10 +12,10 @@ namespace internal {
 // TODO(cbruni): Move to BytecodeArray
 int AbstractCode::SourcePosition(PtrComprCageBase cage_base, int offset) {
   CHECK_NE(kind(cage_base), CodeKind::BASELINE);
-  Object maybe_table = SourcePositionTableInternal(cage_base);
+  Tagged<Object> maybe_table = SourcePositionTableInternal(cage_base);
   if (IsException(maybe_table)) return kNoSourcePosition;
 
-  ByteArray source_position_table = ByteArray::cast(maybe_table);
+  Tagged<ByteArray> source_position_table = ByteArray::cast(maybe_table);
   // Subtract one because the current PC is one instruction after the call site.
   if (IsCode(*this, cage_base)) offset--;
   int position = 0;
diff --git a/src/objects/abstract-code.h b/src/objects/abstract-code.h
index 70709d690d0..93ca3496af7 100644
--- a/src/objects/abstract-code.h
+++ b/src/objects/abstract-code.h
@@ -32,8 +32,8 @@ class AbstractCode : public HeapObject {
   inline int InstructionSize(PtrComprCageBase cage_base);
 
   // Return the source position table for interpreter code.
-  inline ByteArray SourcePositionTable(Isolate* isolate,
-                                       SharedFunctionInfo sfi);
+  inline Tagged<ByteArray> SourcePositionTable(Isolate* isolate,
+                                               Tagged<SharedFunctionInfo> sfi);
 
   void DropStackFrameCache(PtrComprCageBase cage_base);
 
@@ -52,11 +52,12 @@ class AbstractCode : public HeapObject {
 
   DECL_CAST(AbstractCode)
 
-  inline Code GetCode();
-  inline BytecodeArray GetBytecodeArray();
+  inline Tagged<Code> GetCode();
+  inline Tagged<BytecodeArray> GetBytecodeArray();
 
  private:
-  inline ByteArray SourcePositionTableInternal(PtrComprCageBase cage_base);
+  inline Tagged<ByteArray> SourcePositionTableInternal(
+      PtrComprCageBase cage_base);
 
   OBJECT_CONSTRUCTORS(AbstractCode, HeapObject);
 };
diff --git a/src/objects/allocation-site-inl.h b/src/objects/allocation-site-inl.h
index 2aebdc5c338..7d55e5ae788 100644
--- a/src/objects/allocation-site-inl.h
+++ b/src/objects/allocation-site-inl.h
@@ -117,7 +117,7 @@ void AllocationSite::SetDoNotInlineCall() {
 }
 
 bool AllocationSite::PointsToLiteral() const {
-  Object raw_value = transition_info_or_boilerplate(kAcquireLoad);
+  Tagged<Object> raw_value = transition_info_or_boilerplate(kAcquireLoad);
   DCHECK_EQ(!IsSmi(raw_value), IsJSArray(raw_value) || IsJSObject(raw_value));
   return !IsSmi(raw_value);
 }
@@ -202,7 +202,7 @@ bool AllocationMemento::IsValid() const {
          !AllocationSite::cast(allocation_site())->IsZombie();
 }
 
-AllocationSite AllocationMemento::GetAllocationSite() const {
+Tagged<AllocationSite> AllocationMemento::GetAllocationSite() const {
   DCHECK(IsValid());
   return AllocationSite::cast(allocation_site());
 }
diff --git a/src/objects/allocation-site-scopes-inl.h b/src/objects/allocation-site-scopes-inl.h
index 9899254c946..fd93799b3d3 100644
--- a/src/objects/allocation-site-scopes-inl.h
+++ b/src/objects/allocation-site-scopes-inl.h
@@ -24,7 +24,7 @@ Handle<AllocationSite> AllocationSiteUsageContext::EnterNewScope() {
     InitializeTraversal(top_site_);
   } else {
     // Advance current site
-    Object nested_site = current()->nested_site();
+    Tagged<Object> nested_site = current()->nested_site();
     // Something is wrong if we advance to the end of the list here.
     update_current_site(AllocationSite::cast(nested_site));
   }
diff --git a/src/objects/allocation-site-scopes.h b/src/objects/allocation-site-scopes.h
index 9617b76b5e2..87a3a36bc0b 100644
--- a/src/objects/allocation-site-scopes.h
+++ b/src/objects/allocation-site-scopes.h
@@ -27,7 +27,9 @@ class AllocationSiteContext {
   Isolate* isolate() { return isolate_; }
 
  protected:
-  void update_current_site(AllocationSite site) { current_.PatchValue(site); }
+  void update_current_site(Tagged<AllocationSite> site) {
+    current_.PatchValue(site);
+  }
 
   inline void InitializeTraversal(Handle<AllocationSite> site);
 
diff --git a/src/objects/allocation-site.h b/src/objects/allocation-site.h
index 8f73eb4accb..ee161af6f25 100644
--- a/src/objects/allocation-site.h
+++ b/src/objects/allocation-site.h
@@ -175,7 +175,7 @@ class AllocationMemento
   DECL_ACCESSORS(allocation_site, Tagged<Object>)
 
   inline bool IsValid() const;
-  inline AllocationSite GetAllocationSite() const;
+  inline Tagged<AllocationSite> GetAllocationSite() const;
   inline Address GetAllocationSiteUnchecked() const;
 
   DECL_PRINTER(AllocationMemento)
diff --git a/src/objects/api-callbacks.h b/src/objects/api-callbacks.h
index 6bb19e0f2f3..cd715a3bb13 100644
--- a/src/objects/api-callbacks.h
+++ b/src/objects/api-callbacks.h
@@ -63,7 +63,7 @@ class AccessorInfo
   // Checks whether the given receiver is compatible with this accessor.
   static bool IsCompatibleReceiverMap(Handle<AccessorInfo> info,
                                       Handle<Map> map);
-  inline bool IsCompatibleReceiver(Object receiver);
+  inline bool IsCompatibleReceiver(Tagged<Object> receiver);
 
   // Append all descriptors to the array that are not already there.
   // Return number added.
@@ -95,7 +95,8 @@ class AccessorInfo
 class AccessCheckInfo
     : public TorqueGeneratedAccessCheckInfo<AccessCheckInfo, Struct> {
  public:
-  static AccessCheckInfo Get(Isolate* isolate, Handle<JSObject> receiver);
+  static Tagged<AccessCheckInfo> Get(Isolate* isolate,
+                                     Handle<JSObject> receiver);
 
   using BodyDescriptor = StructBodyDescriptor;
 
diff --git a/src/objects/backing-store.cc b/src/objects/backing-store.cc
index 11f2be9cb5a..f02299b2f39 100644
--- a/src/objects/backing-store.cc
+++ b/src/objects/backing-store.cc
@@ -939,8 +939,8 @@ void GlobalBackingStoreRegistry::UpdateSharedWasmMemoryObjects(
       isolate->factory()->shared_wasm_memories();
 
   for (int i = 0, e = shared_wasm_memories->length(); i < e; ++i) {
-    HeapObject obj;
-    if (!shared_wasm_memories->Get(i).GetHeapObject(&obj)) continue;
+    Tagged<HeapObject> obj;
+    if (!shared_wasm_memories->Get(i)->GetHeapObject(&obj)) continue;
 
     Handle<WasmMemoryObject> memory_object(WasmMemoryObject::cast(obj),
                                            isolate);
diff --git a/src/objects/bigint.cc b/src/objects/bigint.cc
index 1ee21f2ae46..52693852162 100644
--- a/src/objects/bigint.cc
+++ b/src/objects/bigint.cc
@@ -49,7 +49,7 @@ class MutableBigInt : public FreshlyAllocatedBigInt {
   template <typename Isolate = v8::internal::Isolate>
   static Handle<BigInt> MakeImmutable(Handle<MutableBigInt> result);
 
-  static void Canonicalize(MutableBigInt result);
+  static void Canonicalize(Tagged<MutableBigInt> result);
 
   // Allocation helpers.
   template <typename IsolateT>
@@ -73,18 +73,18 @@ class MutableBigInt : public FreshlyAllocatedBigInt {
     SLOW_DCHECK(IsBigInt(*bigint));
     return Handle<MutableBigInt>::cast(bigint);
   }
-  static MutableBigInt cast(Object o) {
+  static Tagged<MutableBigInt> cast(Tagged<Object> o) {
     SLOW_DCHECK(IsBigInt(o));
     return MutableBigInt(o.ptr());
   }
-  static MutableBigInt unchecked_cast(Object o) {
+  static Tagged<MutableBigInt> unchecked_cast(Tagged<Object> o) {
     return MutableBigInt(o.ptr());
   }
 
   // Internal helpers.
   static MaybeHandle<MutableBigInt> AbsoluteAddOne(
       Isolate* isolate, Handle<BigIntBase> x, bool sign,
-      MutableBigInt result_storage = MutableBigInt());
+      Tagged<MutableBigInt> result_storage = MutableBigInt());
   static Handle<MutableBigInt> AbsoluteSubOne(Isolate* isolate,
                                               Handle<BigIntBase> x);
 
@@ -105,7 +105,7 @@ class MutableBigInt : public FreshlyAllocatedBigInt {
 
   // Returns the least significant 64 bits, simulating two's complement
   // representation.
-  static uint64_t GetRawBits(BigIntBase x, bool* lossless);
+  static uint64_t GetRawBits(Tagged<BigIntBase> x, bool* lossless);
 
   static inline bool digit_ismax(digit_t x) {
     return static_cast<digit_t>(~x) == 0;
@@ -134,7 +134,7 @@ class MutableBigInt : public FreshlyAllocatedBigInt {
 
   void set_64_bits(uint64_t bits);
 
-  static bool IsMutableBigInt(MutableBigInt o) { return IsBigInt(o); }
+  static bool IsMutableBigInt(Tagged<MutableBigInt> o) { return IsBigInt(o); }
 
   static_assert(std::is_same<bigint::digit_t, BigIntBase::digit_t>::value,
                 "We must be able to call BigInt library functions");
@@ -149,7 +149,7 @@ NEVER_READ_ONLY_SPACE_IMPL(MutableBigInt)
 
 #include "src/objects/object-macros-undef.h"
 
-bigint::Digits GetDigits(BigIntBase bigint) {
+bigint::Digits GetDigits(Tagged<BigIntBase> bigint) {
   return bigint::Digits(
       reinterpret_cast<bigint::digit_t*>(
           bigint.ptr() + BigIntBase::kDigitsOffset - kHeapObjectTag),
@@ -159,7 +159,7 @@ bigint::Digits GetDigits(Handle<BigIntBase> bigint) {
   return GetDigits(*bigint);
 }
 
-bigint::RWDigits GetRWDigits(MutableBigInt bigint) {
+bigint::RWDigits GetRWDigits(Tagged<MutableBigInt> bigint) {
   return bigint::RWDigits(
       reinterpret_cast<bigint::digit_t*>(
           bigint.ptr() + BigIntBase::kDigitsOffset - kHeapObjectTag),
@@ -313,7 +313,7 @@ Handle<BigInt> MutableBigInt::MakeImmutable(Handle<MutableBigInt> result) {
   return Handle<BigInt>::cast(result);
 }
 
-void MutableBigInt::Canonicalize(MutableBigInt result) {
+void MutableBigInt::Canonicalize(Tagged<MutableBigInt> result) {
   // Check if we need to right-trim any leading zero-digits.
   int old_length = result->length();
   int new_length = old_length;
@@ -619,7 +619,7 @@ ComparisonResult BigInt::CompareToBigInt(Handle<BigInt> x, Handle<BigInt> y) {
   return ComparisonResult::kEqual;
 }
 
-bool BigInt::EqualToBigInt(BigInt x, BigInt y) {
+bool BigInt::EqualToBigInt(Tagged<BigInt> x, Tagged<BigInt> y) {
   if (x->sign() != y->sign()) return false;
   if (x->length() != y->length()) return false;
   for (int i = 0; i < x->length(); i++) {
@@ -1236,7 +1236,7 @@ void BigInt::BigIntShortPrint(std::ostream& os) {
 // modification.
 MaybeHandle<MutableBigInt> MutableBigInt::AbsoluteAddOne(
     Isolate* isolate, Handle<BigIntBase> x, bool sign,
-    MutableBigInt result_storage) {
+    Tagged<MutableBigInt> result_storage) {
   int input_length = x->length();
   // The addition will overflow into a new digit if all existing digits are
   // at maximum.
@@ -1587,7 +1587,7 @@ void BigInt::ToWordsArray64(int* sign_bit, int* words64_count,
   }
 }
 
-uint64_t MutableBigInt::GetRawBits(BigIntBase x, bool* lossless) {
+uint64_t MutableBigInt::GetRawBits(Tagged<BigIntBase> x, bool* lossless) {
   if (lossless != nullptr) *lossless = true;
   if (x->is_zero()) return 0;
   int len = x->length();
@@ -1643,26 +1643,26 @@ void BigIntBase::BigIntBasePrint(std::ostream& os) {
 
 void MutableBigInt_AbsoluteAddAndCanonicalize(Address result_addr,
                                               Address x_addr, Address y_addr) {
-  BigInt x = BigInt::cast(Object(x_addr));
-  BigInt y = BigInt::cast(Object(y_addr));
-  MutableBigInt result = MutableBigInt::cast(Object(result_addr));
+  Tagged<BigInt> x = BigInt::cast(Object(x_addr));
+  Tagged<BigInt> y = BigInt::cast(Object(y_addr));
+  Tagged<MutableBigInt> result = MutableBigInt::cast(Object(result_addr));
 
   bigint::Add(GetRWDigits(result), GetDigits(x), GetDigits(y));
   MutableBigInt::Canonicalize(result);
 }
 
 int32_t MutableBigInt_AbsoluteCompare(Address x_addr, Address y_addr) {
-  BigInt x = BigInt::cast(Object(x_addr));
-  BigInt y = BigInt::cast(Object(y_addr));
+  Tagged<BigInt> x = BigInt::cast(Object(x_addr));
+  Tagged<BigInt> y = BigInt::cast(Object(y_addr));
 
   return bigint::Compare(GetDigits(x), GetDigits(y));
 }
 
 void MutableBigInt_AbsoluteSubAndCanonicalize(Address result_addr,
                                               Address x_addr, Address y_addr) {
-  BigInt x = BigInt::cast(Object(x_addr));
-  BigInt y = BigInt::cast(Object(y_addr));
-  MutableBigInt result = MutableBigInt::cast(Object(result_addr));
+  Tagged<BigInt> x = BigInt::cast(Object(x_addr));
+  Tagged<BigInt> y = BigInt::cast(Object(y_addr));
+  Tagged<MutableBigInt> result = MutableBigInt::cast(Object(result_addr));
 
   bigint::Subtract(GetRWDigits(result), GetDigits(x), GetDigits(y));
   MutableBigInt::Canonicalize(result);
@@ -1673,9 +1673,9 @@ void MutableBigInt_AbsoluteSubAndCanonicalize(Address result_addr,
 int32_t MutableBigInt_AbsoluteMulAndCanonicalize(Address result_addr,
                                                  Address x_addr,
                                                  Address y_addr) {
-  BigInt x = BigInt::cast(Object(x_addr));
-  BigInt y = BigInt::cast(Object(y_addr));
-  MutableBigInt result = MutableBigInt::cast(Object(result_addr));
+  Tagged<BigInt> x = BigInt::cast(Object(x_addr));
+  Tagged<BigInt> y = BigInt::cast(Object(y_addr));
+  Tagged<MutableBigInt> result = MutableBigInt::cast(Object(result_addr));
 
   Isolate* isolate;
   if (!GetIsolateFromHeapObject(x, &isolate)) {
@@ -1696,9 +1696,9 @@ int32_t MutableBigInt_AbsoluteMulAndCanonicalize(Address result_addr,
 int32_t MutableBigInt_AbsoluteDivAndCanonicalize(Address result_addr,
                                                  Address x_addr,
                                                  Address y_addr) {
-  BigInt x = BigInt::cast(Object(x_addr));
-  BigInt y = BigInt::cast(Object(y_addr));
-  MutableBigInt result = MutableBigInt::cast(Object(result_addr));
+  Tagged<BigInt> x = BigInt::cast(Object(x_addr));
+  Tagged<BigInt> y = BigInt::cast(Object(y_addr));
+  Tagged<MutableBigInt> result = MutableBigInt::cast(Object(result_addr));
   DCHECK_GE(result->length(),
             bigint::DivideResultLength(GetDigits(x), GetDigits(y)));
 
@@ -1721,9 +1721,9 @@ int32_t MutableBigInt_AbsoluteDivAndCanonicalize(Address result_addr,
 int32_t MutableBigInt_AbsoluteModAndCanonicalize(Address result_addr,
                                                  Address x_addr,
                                                  Address y_addr) {
-  BigInt x = BigInt::cast(Object(x_addr));
-  BigInt y = BigInt::cast(Object(y_addr));
-  MutableBigInt result = MutableBigInt::cast(Object(result_addr));
+  Tagged<BigInt> x = BigInt::cast(Object(x_addr));
+  Tagged<BigInt> y = BigInt::cast(Object(y_addr));
+  Tagged<MutableBigInt> result = MutableBigInt::cast(Object(result_addr));
 
   Isolate* isolate;
   if (!GetIsolateFromHeapObject(x, &isolate)) {
@@ -1744,9 +1744,9 @@ int32_t MutableBigInt_AbsoluteModAndCanonicalize(Address result_addr,
 void MutableBigInt_BitwiseAndPosPosAndCanonicalize(Address result_addr,
                                                    Address x_addr,
                                                    Address y_addr) {
-  BigInt x = BigInt::cast(Object(x_addr));
-  BigInt y = BigInt::cast(Object(y_addr));
-  MutableBigInt result = MutableBigInt::cast(Object(result_addr));
+  Tagged<BigInt> x = BigInt::cast(Object(x_addr));
+  Tagged<BigInt> y = BigInt::cast(Object(y_addr));
+  Tagged<MutableBigInt> result = MutableBigInt::cast(Object(result_addr));
 
   bigint::BitwiseAnd_PosPos(GetRWDigits(result), GetDigits(x), GetDigits(y));
   MutableBigInt::Canonicalize(result);
@@ -1755,9 +1755,9 @@ void MutableBigInt_BitwiseAndPosPosAndCanonicalize(Address result_addr,
 void MutableBigInt_BitwiseAndNegNegAndCanonicalize(Address result_addr,
                                                    Address x_addr,
                                                    Address y_addr) {
-  BigInt x = BigInt::cast(Object(x_addr));
-  BigInt y = BigInt::cast(Object(y_addr));
-  MutableBigInt result = MutableBigInt::cast(Object(result_addr));
+  Tagged<BigInt> x = BigInt::cast(Object(x_addr));
+  Tagged<BigInt> y = BigInt::cast(Object(y_addr));
+  Tagged<MutableBigInt> result = MutableBigInt::cast(Object(result_addr));
 
   bigint::BitwiseAnd_NegNeg(GetRWDigits(result), GetDigits(x), GetDigits(y));
   MutableBigInt::Canonicalize(result);
@@ -1766,9 +1766,9 @@ void MutableBigInt_BitwiseAndNegNegAndCanonicalize(Address result_addr,
 void MutableBigInt_BitwiseAndPosNegAndCanonicalize(Address result_addr,
                                                    Address x_addr,
                                                    Address y_addr) {
-  BigInt x = BigInt::cast(Object(x_addr));
-  BigInt y = BigInt::cast(Object(y_addr));
-  MutableBigInt result = MutableBigInt::cast(Object(result_addr));
+  Tagged<BigInt> x = BigInt::cast(Object(x_addr));
+  Tagged<BigInt> y = BigInt::cast(Object(y_addr));
+  Tagged<MutableBigInt> result = MutableBigInt::cast(Object(result_addr));
 
   bigint::BitwiseAnd_PosNeg(GetRWDigits(result), GetDigits(x), GetDigits(y));
   MutableBigInt::Canonicalize(result);
@@ -1777,9 +1777,9 @@ void MutableBigInt_BitwiseAndPosNegAndCanonicalize(Address result_addr,
 void MutableBigInt_BitwiseOrPosPosAndCanonicalize(Address result_addr,
                                                   Address x_addr,
                                                   Address y_addr) {
-  BigInt x = BigInt::cast(Object(x_addr));
-  BigInt y = BigInt::cast(Object(y_addr));
-  MutableBigInt result = MutableBigInt::cast(Object(result_addr));
+  Tagged<BigInt> x = BigInt::cast(Object(x_addr));
+  Tagged<BigInt> y = BigInt::cast(Object(y_addr));
+  Tagged<MutableBigInt> result = MutableBigInt::cast(Object(result_addr));
 
   bigint::BitwiseOr_PosPos(GetRWDigits(result), GetDigits(x), GetDigits(y));
   MutableBigInt::Canonicalize(result);
@@ -1788,9 +1788,9 @@ void MutableBigInt_BitwiseOrPosPosAndCanonicalize(Address result_addr,
 void MutableBigInt_BitwiseOrNegNegAndCanonicalize(Address result_addr,
                                                   Address x_addr,
                                                   Address y_addr) {
-  BigInt x = BigInt::cast(Object(x_addr));
-  BigInt y = BigInt::cast(Object(y_addr));
-  MutableBigInt result = MutableBigInt::cast(Object(result_addr));
+  Tagged<BigInt> x = BigInt::cast(Object(x_addr));
+  Tagged<BigInt> y = BigInt::cast(Object(y_addr));
+  Tagged<MutableBigInt> result = MutableBigInt::cast(Object(result_addr));
 
   bigint::BitwiseOr_NegNeg(GetRWDigits(result), GetDigits(x), GetDigits(y));
   MutableBigInt::Canonicalize(result);
@@ -1799,9 +1799,9 @@ void MutableBigInt_BitwiseOrNegNegAndCanonicalize(Address result_addr,
 void MutableBigInt_BitwiseOrPosNegAndCanonicalize(Address result_addr,
                                                   Address x_addr,
                                                   Address y_addr) {
-  BigInt x = BigInt::cast(Object(x_addr));
-  BigInt y = BigInt::cast(Object(y_addr));
-  MutableBigInt result = MutableBigInt::cast(Object(result_addr));
+  Tagged<BigInt> x = BigInt::cast(Object(x_addr));
+  Tagged<BigInt> y = BigInt::cast(Object(y_addr));
+  Tagged<MutableBigInt> result = MutableBigInt::cast(Object(result_addr));
 
   bigint::BitwiseOr_PosNeg(GetRWDigits(result), GetDigits(x), GetDigits(y));
   MutableBigInt::Canonicalize(result);
@@ -1810,9 +1810,9 @@ void MutableBigInt_BitwiseOrPosNegAndCanonicalize(Address result_addr,
 void MutableBigInt_BitwiseXorPosPosAndCanonicalize(Address result_addr,
                                                    Address x_addr,
                                                    Address y_addr) {
-  BigInt x = BigInt::cast(Object(x_addr));
-  BigInt y = BigInt::cast(Object(y_addr));
-  MutableBigInt result = MutableBigInt::cast(Object(result_addr));
+  Tagged<BigInt> x = BigInt::cast(Object(x_addr));
+  Tagged<BigInt> y = BigInt::cast(Object(y_addr));
+  Tagged<MutableBigInt> result = MutableBigInt::cast(Object(result_addr));
 
   bigint::BitwiseXor_PosPos(GetRWDigits(result), GetDigits(x), GetDigits(y));
   MutableBigInt::Canonicalize(result);
@@ -1821,9 +1821,9 @@ void MutableBigInt_BitwiseXorPosPosAndCanonicalize(Address result_addr,
 void MutableBigInt_BitwiseXorNegNegAndCanonicalize(Address result_addr,
                                                    Address x_addr,
                                                    Address y_addr) {
-  BigInt x = BigInt::cast(Object(x_addr));
-  BigInt y = BigInt::cast(Object(y_addr));
-  MutableBigInt result = MutableBigInt::cast(Object(result_addr));
+  Tagged<BigInt> x = BigInt::cast(Object(x_addr));
+  Tagged<BigInt> y = BigInt::cast(Object(y_addr));
+  Tagged<MutableBigInt> result = MutableBigInt::cast(Object(result_addr));
 
   bigint::BitwiseXor_NegNeg(GetRWDigits(result), GetDigits(x), GetDigits(y));
   MutableBigInt::Canonicalize(result);
@@ -1832,9 +1832,9 @@ void MutableBigInt_BitwiseXorNegNegAndCanonicalize(Address result_addr,
 void MutableBigInt_BitwiseXorPosNegAndCanonicalize(Address result_addr,
                                                    Address x_addr,
                                                    Address y_addr) {
-  BigInt x = BigInt::cast(Object(x_addr));
-  BigInt y = BigInt::cast(Object(y_addr));
-  MutableBigInt result = MutableBigInt::cast(Object(result_addr));
+  Tagged<BigInt> x = BigInt::cast(Object(x_addr));
+  Tagged<BigInt> y = BigInt::cast(Object(y_addr));
+  Tagged<MutableBigInt> result = MutableBigInt::cast(Object(result_addr));
 
   bigint::BitwiseXor_PosNeg(GetRWDigits(result), GetDigits(x), GetDigits(y));
   MutableBigInt::Canonicalize(result);
@@ -1842,8 +1842,8 @@ void MutableBigInt_BitwiseXorPosNegAndCanonicalize(Address result_addr,
 
 void MutableBigInt_LeftShiftAndCanonicalize(Address result_addr, Address x_addr,
                                             intptr_t shift) {
-  BigInt x = BigInt::cast(Object(x_addr));
-  MutableBigInt result = MutableBigInt::cast(Object(result_addr));
+  Tagged<BigInt> x = BigInt::cast(Object(x_addr));
+  Tagged<MutableBigInt> result = MutableBigInt::cast(Object(result_addr));
 
   bigint::LeftShift(GetRWDigits(result), GetDigits(x), shift);
   MutableBigInt::Canonicalize(result);
@@ -1851,7 +1851,7 @@ void MutableBigInt_LeftShiftAndCanonicalize(Address result_addr, Address x_addr,
 
 uint32_t RightShiftResultLength(Address x_addr, uint32_t x_sign,
                                 intptr_t shift) {
-  BigInt x = BigInt::cast(Object(x_addr));
+  Tagged<BigInt> x = BigInt::cast(Object(x_addr));
   bigint::RightShiftState state;
   int length =
       bigint::RightShift_ResultLength(GetDigits(x), x_sign, shift, &state);
@@ -1865,8 +1865,8 @@ uint32_t RightShiftResultLength(Address x_addr, uint32_t x_sign,
 void MutableBigInt_RightShiftAndCanonicalize(Address result_addr,
                                              Address x_addr, intptr_t shift,
                                              uint32_t must_round_down) {
-  BigInt x = BigInt::cast(Object(x_addr));
-  MutableBigInt result = MutableBigInt::cast(Object(result_addr));
+  Tagged<BigInt> x = BigInt::cast(Object(x_addr));
+  Tagged<MutableBigInt> result = MutableBigInt::cast(Object(result_addr));
   bigint::RightShiftState state{must_round_down == 1};
   bigint::RightShift(GetRWDigits(result), GetDigits(x), shift, state);
   MutableBigInt::Canonicalize(result);
diff --git a/src/objects/bigint.h b/src/objects/bigint.h
index 82c0bc9124c..2b517b4c538 100644
--- a/src/objects/bigint.h
+++ b/src/objects/bigint.h
@@ -168,8 +168,9 @@ class FreshlyAllocatedBigInt : public BigIntBase {
   //   (and no explicit operator is provided either).
 
  public:
-  inline static FreshlyAllocatedBigInt cast(Object object);
-  inline static FreshlyAllocatedBigInt unchecked_cast(Object o) {
+  inline static Tagged<FreshlyAllocatedBigInt> cast(Tagged<Object> object);
+  inline static Tagged<FreshlyAllocatedBigInt> unchecked_cast(
+      Tagged<Object> o) {
     return base::bit_cast<FreshlyAllocatedBigInt>(o);
   }
 
@@ -184,7 +185,9 @@ class FreshlyAllocatedBigInt : public BigIntBase {
 
  private:
   // Only serves to make macros happy; other code should use IsBigInt.
-  static bool IsFreshlyAllocatedBigInt(FreshlyAllocatedBigInt) { return true; }
+  static bool IsFreshlyAllocatedBigInt(Tagged<FreshlyAllocatedBigInt>) {
+    return true;
+  }
 
   OBJECT_CONSTRUCTORS(FreshlyAllocatedBigInt, BigIntBase);
 };
@@ -219,7 +222,7 @@ class BigInt : public BigIntBase {
                                                 Handle<BigInt> y);
   // More convenient version of "bool LessThan(x, y)".
   static ComparisonResult CompareToBigInt(Handle<BigInt> x, Handle<BigInt> y);
-  static bool EqualToBigInt(BigInt x, BigInt y);
+  static bool EqualToBigInt(Tagged<BigInt> x, Tagged<BigInt> y);
   static MaybeHandle<BigInt> BitwiseAnd(Isolate* isolate, Handle<BigInt> x,
                                         Handle<BigInt> y);
   static MaybeHandle<BigInt> BitwiseXor(Isolate* isolate, Handle<BigInt> x,
diff --git a/src/objects/bytecode-array-inl.h b/src/objects/bytecode-array-inl.h
index 33350908373..07d7814c517 100644
--- a/src/objects/bytecode-array-inl.h
+++ b/src/objects/bytecode-array-inl.h
@@ -94,7 +94,7 @@ Address BytecodeArray::GetFirstBytecodeAddress() {
 }
 
 bool BytecodeArray::HasSourcePositionTable() const {
-  Object maybe_table = source_position_table(kAcquireLoad);
+  Tagged<Object> maybe_table = source_position_table(kAcquireLoad);
   return !(IsUndefined(maybe_table) || DidSourcePositionGenerationFail());
 }
 
@@ -109,7 +109,7 @@ void BytecodeArray::SetSourcePositionsFailedToCollect() {
 DEF_GETTER(BytecodeArray, SourcePositionTable, Tagged<ByteArray>) {
   // WARNING: This function may be called from a background thread, hence
   // changes to how it accesses the heap can easily lead to bugs.
-  Object maybe_table = source_position_table(cage_base, kAcquireLoad);
+  Tagged<Object> maybe_table = source_position_table(cage_base, kAcquireLoad);
   if (IsByteArray(maybe_table, cage_base)) return ByteArray::cast(maybe_table);
   ReadOnlyRoots roots = GetReadOnlyRoots();
   DCHECK(IsUndefined(maybe_table, roots) || IsException(maybe_table, roots));
@@ -117,7 +117,7 @@ DEF_GETTER(BytecodeArray, SourcePositionTable, Tagged<ByteArray>) {
 }
 
 DEF_GETTER(BytecodeArray, raw_constant_pool, Tagged<Object>) {
-  Object value =
+  Tagged<Object> value =
       TaggedField<Object>::load(cage_base, *this, kConstantPoolOffset);
   // This field might be 0 during deserialization.
   DCHECK(value == Smi::zero() || IsFixedArray(value));
@@ -125,7 +125,7 @@ DEF_GETTER(BytecodeArray, raw_constant_pool, Tagged<Object>) {
 }
 
 DEF_GETTER(BytecodeArray, raw_handler_table, Tagged<Object>) {
-  Object value =
+  Tagged<Object> value =
       TaggedField<Object>::load(cage_base, *this, kHandlerTableOffset);
   // This field might be 0 during deserialization.
   DCHECK(value == Smi::zero() || IsByteArray(value));
@@ -133,7 +133,7 @@ DEF_GETTER(BytecodeArray, raw_handler_table, Tagged<Object>) {
 }
 
 DEF_GETTER(BytecodeArray, raw_source_position_table, Tagged<Object>) {
-  Object value =
+  Tagged<Object> value =
       TaggedField<Object>::load(cage_base, *this, kSourcePositionTableOffset);
   // This field might be 0 during deserialization.
   DCHECK(value == Smi::zero() || IsByteArray(value) || IsUndefined(value) ||
@@ -145,19 +145,19 @@ int BytecodeArray::BytecodeArraySize() const { return SizeFor(this->length()); }
 
 DEF_GETTER(BytecodeArray, SizeIncludingMetadata, int) {
   int size = BytecodeArraySize();
-  Object maybe_constant_pool = raw_constant_pool(cage_base);
+  Tagged<Object> maybe_constant_pool = raw_constant_pool(cage_base);
   if (IsFixedArray(maybe_constant_pool)) {
     size += FixedArray::cast(maybe_constant_pool)->Size(cage_base);
   } else {
     DCHECK_EQ(maybe_constant_pool, Smi::zero());
   }
-  Object maybe_handler_table = raw_handler_table(cage_base);
+  Tagged<Object> maybe_handler_table = raw_handler_table(cage_base);
   if (IsByteArray(maybe_handler_table)) {
     size += ByteArray::cast(maybe_handler_table)->Size();
   } else {
     DCHECK_EQ(maybe_handler_table, Smi::zero());
   }
-  Object maybe_table = raw_source_position_table(cage_base);
+  Tagged<Object> maybe_table = raw_source_position_table(cage_base);
   if (IsByteArray(maybe_table)) {
     size += ByteArray::cast(maybe_table)->Size();
   }
diff --git a/src/objects/bytecode-array.cc b/src/objects/bytecode-array.cc
index bac4bb05511..a05f7ac6f73 100644
--- a/src/objects/bytecode-array.cc
+++ b/src/objects/bytecode-array.cc
@@ -156,7 +156,7 @@ void BytecodeArray::Disassemble(Handle<BytecodeArray> handle,
 #endif
 }
 
-void BytecodeArray::CopyBytecodesTo(BytecodeArray to) {
+void BytecodeArray::CopyBytecodesTo(Tagged<BytecodeArray> to) {
   BytecodeArray from = *this;
   DCHECK_EQ(from->length(), to->length());
   CopyBytes(reinterpret_cast<uint8_t*>(to->GetFirstBytecodeAddress()),
diff --git a/src/objects/bytecode-array.h b/src/objects/bytecode-array.h
index ab4aa4d0243..48b7fd11c07 100644
--- a/src/objects/bytecode-array.h
+++ b/src/objects/bytecode-array.h
@@ -80,7 +80,7 @@ class BytecodeArray
   V8_EXPORT_PRIVATE static void Disassemble(Handle<BytecodeArray> handle,
                                             std::ostream& os);
 
-  void CopyBytecodesTo(BytecodeArray to);
+  void CopyBytecodesTo(Tagged<BytecodeArray> to);
 
   // Clear uninitialized padding space. This ensures that the snapshot content
   // is deterministic.
diff --git a/src/objects/call-site-info.cc b/src/objects/call-site-info.cc
index 4c37da22f58..32c9042d36a 100644
--- a/src/objects/call-site-info.cc
+++ b/src/objects/call-site-info.cc
@@ -18,19 +18,19 @@ namespace internal {
 
 bool CallSiteInfo::IsPromiseAll() const {
   if (!IsAsync()) return false;
-  JSFunction fun = JSFunction::cast(function());
+  Tagged<JSFunction> fun = JSFunction::cast(function());
   return fun == fun->native_context()->promise_all();
 }
 
 bool CallSiteInfo::IsPromiseAllSettled() const {
   if (!IsAsync()) return false;
-  JSFunction fun = JSFunction::cast(function());
+  Tagged<JSFunction> fun = JSFunction::cast(function());
   return fun == fun->native_context()->promise_all_settled();
 }
 
 bool CallSiteInfo::IsPromiseAny() const {
   if (!IsAsync()) return false;
-  JSFunction fun = JSFunction::cast(function());
+  Tagged<JSFunction> fun = JSFunction::cast(function());
   return fun == fun->native_context()->promise_any();
 }
 
@@ -173,21 +173,21 @@ int CallSiteInfo::GetScriptId() const {
   return Message::kNoScriptIdInfo;
 }
 
-Object CallSiteInfo::GetScriptName() const {
+Tagged<Object> CallSiteInfo::GetScriptName() const {
   if (auto script = GetScript()) {
     return script->name();
   }
   return ReadOnlyRoots(GetIsolate()).null_value();
 }
 
-Object CallSiteInfo::GetScriptNameOrSourceURL() const {
+Tagged<Object> CallSiteInfo::GetScriptNameOrSourceURL() const {
   if (auto script = GetScript()) {
     return script->GetNameOrSourceURL();
   }
   return ReadOnlyRoots(GetIsolate()).null_value();
 }
 
-Object CallSiteInfo::GetScriptSource() const {
+Tagged<Object> CallSiteInfo::GetScriptSource() const {
   if (auto script = GetScript()) {
     if (script->HasValidSource()) {
       return script->source();
@@ -196,7 +196,7 @@ Object CallSiteInfo::GetScriptSource() const {
   return ReadOnlyRoots(GetIsolate()).null_value();
 }
 
-Object CallSiteInfo::GetScriptSourceMappingURL() const {
+Tagged<Object> CallSiteInfo::GetScriptSourceMappingURL() const {
   if (auto script = GetScript()) {
     return script->source_mapping_url();
   }
@@ -355,7 +355,7 @@ Tagged<PrimitiveHeapObject> InferMethodNameFromFastObject(
     if (IsSymbol(key)) continue;
     auto details = descriptors->GetDetails(i);
     if (details.IsDontEnum()) continue;
-    Object value;
+    Tagged<Object> value;
     if (details.location() == PropertyLocation::kField) {
       auto field_index = FieldIndex::ForPropertyIndex(
           map, details.field_index(), details.representation());
@@ -379,13 +379,12 @@ Tagged<PrimitiveHeapObject> InferMethodNameFromFastObject(
 }
 
 template <typename Dictionary>
-PrimitiveHeapObject InferMethodNameFromDictionary(Isolate* isolate,
-                                                  Tagged<Dictionary> dictionary,
-                                                  JSFunction fun,
-                                                  PrimitiveHeapObject name) {
+Tagged<PrimitiveHeapObject> InferMethodNameFromDictionary(
+    Isolate* isolate, Tagged<Dictionary> dictionary, Tagged<JSFunction> fun,
+    Tagged<PrimitiveHeapObject> name) {
   ReadOnlyRoots roots(isolate);
   for (auto i : dictionary->IterateEntries()) {
-    Object key;
+    Tagged<Object> key;
     if (!dictionary->ToKey(roots, i, &key)) continue;
     if (IsSymbol(key)) continue;
     auto details = dictionary->DetailsAt(i);
@@ -394,7 +393,7 @@ PrimitiveHeapObject InferMethodNameFromDictionary(Isolate* isolate,
     if (value != fun) {
       if (!IsAccessorPair(value)) continue;
       auto pair = AccessorPair::cast(value);
-      if (pair.getter() != fun && pair.setter() != fun) continue;
+      if (pair->getter() != fun && pair->setter() != fun) continue;
     }
     if (name != key) {
       name = IsUndefined(name, isolate)
@@ -405,11 +404,12 @@ PrimitiveHeapObject InferMethodNameFromDictionary(Isolate* isolate,
   return name;
 }
 
-PrimitiveHeapObject InferMethodName(Isolate* isolate, JSReceiver receiver,
-                                    JSFunction fun) {
+Tagged<PrimitiveHeapObject> InferMethodName(Isolate* isolate,
+                                            Tagged<JSReceiver> receiver,
+                                            Tagged<JSFunction> fun) {
   DisallowGarbageCollection no_gc;
   ReadOnlyRoots roots(isolate);
-  PrimitiveHeapObject name = roots.undefined_value();
+  Tagged<PrimitiveHeapObject> name = roots.undefined_value();
   for (PrototypeIterator it(isolate, receiver, kStartAtReceiver); !it.IsAtEnd();
        it.Advance()) {
     auto current = it.GetCurrent();
@@ -524,7 +524,7 @@ uint32_t CallSiteInfo::GetWasmFunctionIndex() const {
   return Smi::ToInt(Smi::cast(function()));
 }
 
-WasmInstanceObject CallSiteInfo::GetWasmInstance() const {
+Tagged<WasmInstanceObject> CallSiteInfo::GetWasmInstance() const {
   DCHECK(IsWasm());
   return WasmInstanceObject::cast(receiver_or_instance());
 }
@@ -622,12 +622,12 @@ base::Optional<Script> CallSiteInfo::GetScript() const {
     return base::nullopt;
   }
 #endif  // V8_ENABLE_WEBASSEMBLY
-  Object script = GetSharedFunctionInfo()->script();
+  Tagged<Object> script = GetSharedFunctionInfo()->script();
   if (IsScript(script)) return Script::cast(script);
   return base::nullopt;
 }
 
-SharedFunctionInfo CallSiteInfo::GetSharedFunctionInfo() const {
+Tagged<SharedFunctionInfo> CallSiteInfo::GetSharedFunctionInfo() const {
 #if V8_ENABLE_WEBASSEMBLY
   DCHECK(!IsWasm());
   DCHECK(!IsBuiltin());
diff --git a/src/objects/call-site-info.h b/src/objects/call-site-info.h
index 896d2c2cd0e..cea5fc0300e 100644
--- a/src/objects/call-site-info.h
+++ b/src/objects/call-site-info.h
@@ -64,10 +64,10 @@ class CallSiteInfo : public TorqueGeneratedCallSiteInfo<CallSiteInfo, Struct> {
   static MaybeHandle<Script> GetScript(Isolate* isolate,
                                        Handle<CallSiteInfo> info);
   int GetScriptId() const;
-  Object GetScriptName() const;
-  Object GetScriptNameOrSourceURL() const;
-  Object GetScriptSource() const;
-  Object GetScriptSourceMappingURL() const;
+  Tagged<Object> GetScriptName() const;
+  Tagged<Object> GetScriptNameOrSourceURL() const;
+  Tagged<Object> GetScriptSource() const;
+  Tagged<Object> GetScriptSourceMappingURL() const;
 
   static Handle<PrimitiveHeapObject> GetEvalOrigin(Handle<CallSiteInfo> info);
   V8_EXPORT_PRIVATE static Handle<PrimitiveHeapObject> GetFunctionName(
@@ -80,7 +80,7 @@ class CallSiteInfo : public TorqueGeneratedCallSiteInfo<CallSiteInfo, Struct> {
 #if V8_ENABLE_WEBASSEMBLY
   // These methods are only valid for Wasm and asm.js Wasm frames.
   uint32_t GetWasmFunctionIndex() const;
-  WasmInstanceObject GetWasmInstance() const;
+  Tagged<WasmInstanceObject> GetWasmInstance() const;
   static Handle<Object> GetWasmModuleName(Handle<CallSiteInfo> info);
 #endif  // V8_ENABLE_WEBASSEMBLY
 
@@ -101,7 +101,7 @@ class CallSiteInfo : public TorqueGeneratedCallSiteInfo<CallSiteInfo, Struct> {
   static int ComputeSourcePosition(Handle<CallSiteInfo> info, int offset);
 
   base::Optional<Script> GetScript() const;
-  SharedFunctionInfo GetSharedFunctionInfo() const;
+  Tagged<SharedFunctionInfo> GetSharedFunctionInfo() const;
 
   TQ_OBJECT_CONSTRUCTORS(CallSiteInfo)
 };
diff --git a/src/objects/code-inl.h b/src/objects/code-inl.h
index 4d32683db6a..b5bcab609d9 100644
--- a/src/objects/code-inl.h
+++ b/src/objects/code-inl.h
@@ -26,7 +26,7 @@ OBJECT_CONSTRUCTORS_IMPL(GcSafeCode, HeapObject)
 CAST_ACCESSOR(GcSafeCode)
 CAST_ACCESSOR(Code)
 
-Code GcSafeCode::UnsafeCastToCode() const {
+Tagged<Code> GcSafeCode::UnsafeCastToCode() const {
   return Code::unchecked_cast(*this);
 }
 
@@ -45,7 +45,7 @@ GCSAFE_CODE_FWD_ACCESSOR(bool, is_maglevved)
 GCSAFE_CODE_FWD_ACCESSOR(bool, is_turbofanned)
 GCSAFE_CODE_FWD_ACCESSOR(bool, has_tagged_outgoing_params)
 GCSAFE_CODE_FWD_ACCESSOR(bool, marked_for_deoptimization)
-GCSAFE_CODE_FWD_ACCESSOR(Object, raw_instruction_stream)
+GCSAFE_CODE_FWD_ACCESSOR(Tagged<Object>, raw_instruction_stream)
 GCSAFE_CODE_FWD_ACCESSOR(int, stack_slots)
 GCSAFE_CODE_FWD_ACCESSOR(Address, constant_pool)
 GCSAFE_CODE_FWD_ACCESSOR(Address, safepoint_table_address)
@@ -64,12 +64,12 @@ Address GcSafeCode::InstructionEnd(Isolate* isolate, Address pc) const {
   return UnsafeCastToCode()->InstructionEnd(isolate, pc);
 }
 
-Address GcSafeCode::constant_pool(InstructionStream istream) const {
+Address GcSafeCode::constant_pool(Tagged<InstructionStream> istream) const {
   return UnsafeCastToCode()->constant_pool(istream);
 }
 
 bool GcSafeCode::CanDeoptAt(Isolate* isolate, Address pc) const {
-  DeoptimizationData deopt_data = DeoptimizationData::unchecked_cast(
+  Tagged<DeoptimizationData> deopt_data = DeoptimizationData::unchecked_cast(
       UnsafeCastToCode()->unchecked_deoptimization_data());
   Address code_start_address = instruction_start();
   for (int i = 0; i < deopt_data->DeoptCount(); i++) {
@@ -83,7 +83,7 @@ bool GcSafeCode::CanDeoptAt(Isolate* isolate, Address pc) const {
   return false;
 }
 
-Object GcSafeCode::raw_instruction_stream(
+Tagged<Object> GcSafeCode::raw_instruction_stream(
     PtrComprCageBase code_cage_base) const {
   return UnsafeCastToCode()->raw_instruction_stream(code_cage_base);
 }
@@ -112,8 +112,8 @@ ACCESSORS_CHECKED2(Code, bytecode_offset_table, Tagged<ByteArray>,
                    kind() == CodeKind::BASELINE &&
                        !ObjectInYoungGeneration(value))
 
-ByteArray Code::SourcePositionTable(Isolate* isolate,
-                                    SharedFunctionInfo sfi) const {
+Tagged<ByteArray> Code::SourcePositionTable(
+    Isolate* isolate, Tagged<SharedFunctionInfo> sfi) const {
   if (!has_instruction_stream()) {
     return GetReadOnlyRoots().empty_byte_array();
   }
@@ -203,7 +203,7 @@ int Code::constant_pool_size() const {
 
 bool Code::has_constant_pool() const { return constant_pool_size() > 0; }
 
-FixedArray Code::unchecked_deoptimization_data() const {
+Tagged<FixedArray> Code::unchecked_deoptimization_data() const {
   return FixedArray::unchecked_cast(
       TaggedField<HeapObject, kDeoptimizationDataOrInterpreterDataOffset>::load(
           *this));
@@ -249,7 +249,7 @@ int Code::SizeIncludingMetadata() const {
 CodeKind Code::kind() const { return KindField::decode(flags(kRelaxedLoad)); }
 
 int Code::GetBytecodeOffsetForBaselinePC(Address baseline_pc,
-                                         BytecodeArray bytecodes) {
+                                         Tagged<BytecodeArray> bytecodes) {
   DisallowGarbageCollection no_gc;
   CHECK(!is_baseline_trampoline_builtin());
   if (is_baseline_leave_frame_builtin()) return kFunctionExitBytecodeOffset;
@@ -261,9 +261,9 @@ int Code::GetBytecodeOffsetForBaselinePC(Address baseline_pc,
   return offset_iterator.current_bytecode_offset();
 }
 
-uintptr_t Code::GetBaselinePCForBytecodeOffset(int bytecode_offset,
-                                               BytecodeToPCPosition position,
-                                               BytecodeArray bytecodes) {
+uintptr_t Code::GetBaselinePCForBytecodeOffset(
+    int bytecode_offset, BytecodeToPCPosition position,
+    Tagged<BytecodeArray> bytecodes) {
   DisallowGarbageCollection no_gc;
   CHECK_EQ(kind(), CodeKind::BASELINE);
   baseline::BytecodeOffsetIterator offset_iterator(
@@ -279,20 +279,20 @@ uintptr_t Code::GetBaselinePCForBytecodeOffset(int bytecode_offset,
   return pc;
 }
 
-uintptr_t Code::GetBaselineStartPCForBytecodeOffset(int bytecode_offset,
-                                                    BytecodeArray bytecodes) {
+uintptr_t Code::GetBaselineStartPCForBytecodeOffset(
+    int bytecode_offset, Tagged<BytecodeArray> bytecodes) {
   return GetBaselinePCForBytecodeOffset(bytecode_offset, kPcAtStartOfBytecode,
                                         bytecodes);
 }
 
-uintptr_t Code::GetBaselineEndPCForBytecodeOffset(int bytecode_offset,
-                                                  BytecodeArray bytecodes) {
+uintptr_t Code::GetBaselineEndPCForBytecodeOffset(
+    int bytecode_offset, Tagged<BytecodeArray> bytecodes) {
   return GetBaselinePCForBytecodeOffset(bytecode_offset, kPcAtEndOfBytecode,
                                         bytecodes);
 }
 
-uintptr_t Code::GetBaselinePCForNextExecutedBytecode(int bytecode_offset,
-                                                     BytecodeArray bytecodes) {
+uintptr_t Code::GetBaselinePCForNextExecutedBytecode(
+    int bytecode_offset, Tagged<BytecodeArray> bytecodes) {
   DisallowGarbageCollection no_gc;
   CHECK_EQ(kind(), CodeKind::BASELINE);
   baseline::BytecodeOffsetIterator offset_iterator(
@@ -427,7 +427,8 @@ Address Code::constant_pool() const {
   return metadata_start() + constant_pool_offset();
 }
 
-Address Code::constant_pool(InstructionStream instruction_stream) const {
+Address Code::constant_pool(
+    Tagged<InstructionStream> instruction_stream) const {
   if (!has_constant_pool()) return kNullAddress;
   static_assert(InstructionStream::kOnHeapBodyIsContiguous);
   return instruction_stream->instruction_start() + instruction_size() +
@@ -457,7 +458,7 @@ int Code::unwinding_info_size() const {
 bool Code::has_unwinding_info() const { return unwinding_info_size() > 0; }
 
 // static
-Code Code::FromTargetAddress(Address address) {
+Tagged<Code> Code::FromTargetAddress(Address address) {
   return InstructionStream::FromTargetAddress(address)->code(kAcquireLoad);
 }
 
@@ -465,12 +466,12 @@ bool Code::CanContainWeakObjects() {
   return is_optimized_code() && can_have_weak_objects();
 }
 
-bool Code::IsWeakObject(HeapObject object) {
+bool Code::IsWeakObject(Tagged<HeapObject> object) {
   return (CanContainWeakObjects() && IsWeakObjectInOptimizedCode(object));
 }
 
-bool Code::IsWeakObjectInOptimizedCode(HeapObject object) {
-  Map map_object = object->map(kAcquireLoad);
+bool Code::IsWeakObjectInOptimizedCode(Tagged<HeapObject> object) {
+  Tagged<Map> map_object = object->map(kAcquireLoad);
   if (InstanceTypeChecker::IsMap(map_object)) {
     return Map::cast(object)->CanTransition();
   }
@@ -479,7 +480,7 @@ bool Code::IsWeakObjectInOptimizedCode(HeapObject object) {
          InstanceTypeChecker::IsContext(map_object);
 }
 
-bool Code::IsWeakObjectInDeoptimizationLiteralArray(Object object) {
+bool Code::IsWeakObjectInDeoptimizationLiteralArray(Tagged<Object> object) {
   // Maps must be strong because they can be used as part of the description for
   // how to materialize an object upon deoptimization, in which case it is
   // possible to reach the code that requires the Map without anything else
@@ -494,11 +495,11 @@ void Code::IterateDeoptimizationLiterals(RootVisitor* v) {
   auto deopt_data = DeoptimizationData::cast(deoptimization_data());
   if (deopt_data->length() == 0) return;
 
-  DeoptimizationLiteralArray literals = deopt_data->LiteralArray();
+  Tagged<DeoptimizationLiteralArray> literals = deopt_data->LiteralArray();
   const int literals_length = literals->length();
   for (int i = 0; i < literals_length; ++i) {
     MaybeObject maybe_literal = literals->Get(i);
-    HeapObject heap_literal;
+    Tagged<HeapObject> heap_literal;
     if (maybe_literal.GetHeapObject(&heap_literal)) {
       v->VisitRootPointer(Root::kStackRoots, "deoptimization literal",
                           FullObjectSlot(&heap_literal));
@@ -609,9 +610,9 @@ void Code::set_instruction_start(Isolate* isolate, Address value) {
   WriteCodeEntrypointField(kInstructionStartOffset, value);
 }
 
-void Code::SetInstructionStreamAndInstructionStart(Isolate* isolate_for_sandbox,
-                                                   InstructionStream code,
-                                                   WriteBarrierMode mode) {
+void Code::SetInstructionStreamAndInstructionStart(
+    Isolate* isolate_for_sandbox, Tagged<InstructionStream> code,
+    WriteBarrierMode mode) {
   set_raw_instruction_stream(code, mode);
   set_instruction_start(isolate_for_sandbox, code->instruction_start());
 }
@@ -632,7 +633,7 @@ void Code::ClearInstructionStartForSerialization(Isolate* isolate) {
 }
 
 void Code::UpdateInstructionStart(Isolate* isolate_for_sandbox,
-                                  InstructionStream istream) {
+                                  Tagged<InstructionStream> istream) {
   DCHECK_EQ(raw_instruction_stream(), istream);
   set_instruction_start(isolate_for_sandbox, istream->instruction_start());
 }
diff --git a/src/objects/code.cc b/src/objects/code.cc
index 3e16573fd97..43b19adafa1 100644
--- a/src/objects/code.cc
+++ b/src/objects/code.cc
@@ -20,19 +20,19 @@
 namespace v8 {
 namespace internal {
 
-ByteArray Code::raw_position_table() const {
+Tagged<ByteArray> Code::raw_position_table() const {
   return TaggedField<ByteArray, kPositionTableOffset>::load(*this);
 }
 
-HeapObject Code::raw_deoptimization_data_or_interpreter_data() const {
+Tagged<HeapObject> Code::raw_deoptimization_data_or_interpreter_data() const {
   return TaggedField<HeapObject,
                      kDeoptimizationDataOrInterpreterDataOffset>::load(*this);
 }
 
 void Code::ClearEmbeddedObjects(Heap* heap) {
   DisallowGarbageCollection no_gc;
-  HeapObject undefined = ReadOnlyRoots(heap).undefined_value();
-  InstructionStream istream = unchecked_instruction_stream();
+  Tagged<HeapObject> undefined = ReadOnlyRoots(heap).undefined_value();
+  Tagged<InstructionStream> istream = unchecked_instruction_stream();
   int mode_mask = RelocInfo::EmbeddedObjectModeMask();
   {
     CodePageMemoryModificationScope memory_modification_scope(istream);
@@ -97,7 +97,7 @@ bool Code::IsIsolateIndependent(Isolate* isolate) {
       if (OffHeapInstructionStream::PcIsOffHeap(isolate, target_address))
         continue;
 
-      Code target = Code::FromTargetAddress(target_address);
+      Tagged<Code> target = Code::FromTargetAddress(target_address);
       if (Builtins::IsIsolateIndependentBuiltin(target)) {
         continue;
       }
@@ -113,15 +113,15 @@ bool Code::IsIsolateIndependent(Isolate* isolate) {
 #endif
 }
 
-bool Code::Inlines(SharedFunctionInfo sfi) {
+bool Code::Inlines(Tagged<SharedFunctionInfo> sfi) {
   // We can only check for inlining for optimized code.
   DCHECK(is_optimized_code());
   DisallowGarbageCollection no_gc;
-  DeoptimizationData const data =
+  Tagged<DeoptimizationData> const data =
       DeoptimizationData::cast(deoptimization_data());
   if (data->length() == 0) return false;
   if (data->SharedFunctionInfo() == sfi) return true;
-  DeoptimizationLiteralArray const literals = data->LiteralArray();
+  Tagged<DeoptimizationLiteralArray> const literals = data->LiteralArray();
   int const inlined_count = data->InlinedFunctionCount().value();
   for (int i = 0; i < inlined_count; ++i) {
     if (SharedFunctionInfo::cast(literals->get(i)) == sfi) return true;
@@ -133,7 +133,7 @@ bool Code::Inlines(SharedFunctionInfo sfi) {
 
 namespace {
 
-void DisassembleCodeRange(Isolate* isolate, std::ostream& os, Code code,
+void DisassembleCodeRange(Isolate* isolate, std::ostream& os, Tagged<Code> code,
                           Address begin, size_t size, Address current_pc,
                           size_t range_limit = 0) {
   Address end = begin + size;
@@ -147,14 +147,15 @@ void DisassembleCodeRange(Isolate* isolate, std::ostream& os, Code code,
 }
 
 void DisassembleOnlyCode(const char* name, std::ostream& os, Isolate* isolate,
-                         Code code, Address current_pc, size_t range_limit) {
+                         Tagged<Code> code, Address current_pc,
+                         size_t range_limit) {
   int code_size = code->instruction_size();
   DisassembleCodeRange(isolate, os, code, code->instruction_start(), code_size,
                        current_pc, range_limit);
 }
 
 void Disassemble(const char* name, std::ostream& os, Isolate* isolate,
-                 Code code, Address current_pc) {
+                 Tagged<Code> code, Address current_pc) {
   CodeKind kind = code->kind();
   os << "kind = " << CodeKindToString(kind) << "\n";
   if (name == nullptr && code->is_builtin()) {
@@ -228,7 +229,7 @@ void Disassemble(const char* name, std::ostream& os, Isolate* isolate,
   }
 
   if (CodeKindCanDeoptimize(kind)) {
-    DeoptimizationData data =
+    Tagged<DeoptimizationData> data =
         DeoptimizationData::cast(code->deoptimization_data());
     data->PrintDeoptimizationData(os);
   }
diff --git a/src/objects/code.h b/src/objects/code.h
index d7c5dedc46e..89e591ae39d 100644
--- a/src/objects/code.h
+++ b/src/objects/code.h
@@ -85,13 +85,13 @@ class Code : public HeapObject {
 
   inline void init_instruction_start(Isolate* isolate, Address initial_value);
   inline void SetInstructionStreamAndInstructionStart(
-      Isolate* isolate_for_sandbox, InstructionStream code,
+      Isolate* isolate_for_sandbox, Tagged<InstructionStream> code,
       WriteBarrierMode mode = UPDATE_WRITE_BARRIER);
   inline void SetInstructionStartForOffHeapBuiltin(Isolate* isolate_for_sandbox,
                                                    Address entry);
   inline void ClearInstructionStartForSerialization(Isolate* isolate);
   inline void UpdateInstructionStart(Isolate* isolate_for_sandbox,
-                                     InstructionStream istream);
+                                     Tagged<InstructionStream> istream);
 
   inline void initialize_flags(CodeKind kind, bool is_turbofanned,
                                int stack_slots);
@@ -133,7 +133,7 @@ class Code : public HeapObject {
   DECL_PRIMITIVE_ACCESSORS(constant_pool_offset, int)
 
   // Unchecked accessors to be used during GC.
-  inline FixedArray unchecked_deoptimization_data() const;
+  inline Tagged<FixedArray> unchecked_deoptimization_data() const;
 
   DECL_RELAXED_UINT32_ACCESSORS(flags)
 
@@ -174,8 +174,8 @@ class Code : public HeapObject {
   // reserved in the code prologue; otherwise 0.
   inline int stack_slots() const;
 
-  inline ByteArray SourcePositionTable(Isolate* isolate,
-                                       SharedFunctionInfo sfi) const;
+  inline Tagged<ByteArray> SourcePositionTable(
+      Isolate* isolate, Tagged<SharedFunctionInfo> sfi) const;
 
   inline Address safepoint_table_address() const;
   inline int safepoint_table_size() const;
@@ -188,7 +188,8 @@ class Code : public HeapObject {
   inline Address constant_pool() const;
   // An accessor to be used during GC if the instruction_stream moved and the
   // field was not updated yet.
-  inline Address constant_pool(InstructionStream instruction_stream) const;
+  inline Address constant_pool(
+      Tagged<InstructionStream> instruction_stream) const;
   inline int constant_pool_size() const;
   inline bool has_constant_pool() const;
 
@@ -247,9 +248,10 @@ class Code : public HeapObject {
   void SetMarkedForDeoptimization(Isolate* isolate, const char* reason);
 
   inline bool CanContainWeakObjects();
-  inline bool IsWeakObject(HeapObject object);
-  static inline bool IsWeakObjectInOptimizedCode(HeapObject object);
-  static inline bool IsWeakObjectInDeoptimizationLiteralArray(Object object);
+  inline bool IsWeakObject(Tagged<HeapObject> object);
+  static inline bool IsWeakObjectInOptimizedCode(Tagged<HeapObject> object);
+  static inline bool IsWeakObjectInDeoptimizationLiteralArray(
+      Tagged<Object> object);
 
   // This function should be called only from GC.
   void ClearEmbeddedObjects(Heap* heap);
@@ -263,14 +265,14 @@ class Code : public HeapObject {
 
   bool IsIsolateIndependent(Isolate* isolate);
 
-  inline uintptr_t GetBaselineStartPCForBytecodeOffset(int bytecode_offset,
-                                                       BytecodeArray bytecodes);
+  inline uintptr_t GetBaselineStartPCForBytecodeOffset(
+      int bytecode_offset, Tagged<BytecodeArray> bytecodes);
 
-  inline uintptr_t GetBaselineEndPCForBytecodeOffset(int bytecode_offset,
-                                                     BytecodeArray bytecodes);
+  inline uintptr_t GetBaselineEndPCForBytecodeOffset(
+      int bytecode_offset, Tagged<BytecodeArray> bytecodes);
 
   // Returns true if the function is inlined in the code.
-  bool Inlines(SharedFunctionInfo sfi);
+  bool Inlines(Tagged<SharedFunctionInfo> sfi);
 
   // Returns the PC of the next bytecode in execution order.
   // If the bytecode at the given offset is JumpLoop, the PC of the jump target
@@ -278,14 +280,14 @@ class Code : public HeapObject {
   // For other bytecodes this is equivalent to
   // GetBaselineEndPCForBytecodeOffset.
   inline uintptr_t GetBaselinePCForNextExecutedBytecode(
-      int bytecode_offset, BytecodeArray bytecodes);
+      int bytecode_offset, Tagged<BytecodeArray> bytecodes);
 
   inline int GetBytecodeOffsetForBaselinePC(Address baseline_pc,
-                                            BytecodeArray bytecodes);
+                                            Tagged<BytecodeArray> bytecodes);
 
   inline void IterateDeoptimizationLiterals(RootVisitor* v);
 
-  static inline Code FromTargetAddress(Address address);
+  static inline Tagged<Code> FromTargetAddress(Address address);
 
 #ifdef ENABLE_DISASSEMBLER
   V8_EXPORT_PRIVATE void Disassemble(const char* name, std::ostream& os,
@@ -386,8 +388,8 @@ class Code : public HeapObject {
 
   // TODO(jgruber): These field names are incomplete, we've squashed in more
   // overloaded contents in the meantime. Update the field names.
-  HeapObject raw_deoptimization_data_or_interpreter_data() const;
-  ByteArray raw_position_table() const;
+  Tagged<HeapObject> raw_deoptimization_data_or_interpreter_data() const;
+  Tagged<ByteArray> raw_position_table() const;
 
   enum BytecodeToPCPosition {
     kPcAtStartOfBytecode,
@@ -396,9 +398,9 @@ class Code : public HeapObject {
     // of non-topmost frame).
     kPcAtEndOfBytecode
   };
-  inline uintptr_t GetBaselinePCForBytecodeOffset(int bytecode_offset,
-                                                  BytecodeToPCPosition position,
-                                                  BytecodeArray bytecodes);
+  inline uintptr_t GetBaselinePCForBytecodeOffset(
+      int bytecode_offset, BytecodeToPCPosition position,
+      Tagged<BytecodeArray> bytecodes);
 
   template <typename IsolateT>
   friend class Deserializer;
@@ -433,7 +435,7 @@ class GcSafeCode : public HeapObject {
 
   // Use with care, this casts away knowledge that we're dealing with a
   // special-semantics object.
-  inline Code UnsafeCastToCode() const;
+  inline Tagged<Code> UnsafeCastToCode() const;
 
   // Safe accessors (these just forward to Code methods).
   inline Address instruction_start() const;
@@ -449,9 +451,9 @@ class GcSafeCode : public HeapObject {
   inline bool is_turbofanned() const;
   inline bool has_tagged_outgoing_params() const;
   inline bool marked_for_deoptimization() const;
-  inline Object raw_instruction_stream() const;
+  inline Tagged<Object> raw_instruction_stream() const;
   inline Address constant_pool() const;
-  inline Address constant_pool(InstructionStream istream) const;
+  inline Address constant_pool(Tagged<InstructionStream> istream) const;
   inline Address safepoint_table_address() const;
   inline int stack_slots() const;
 
@@ -459,7 +461,8 @@ class GcSafeCode : public HeapObject {
   inline Address InstructionStart(Isolate* isolate, Address pc) const;
   inline Address InstructionEnd(Isolate* isolate, Address pc) const;
   inline bool CanDeoptAt(Isolate* isolate, Address pc) const;
-  inline Object raw_instruction_stream(PtrComprCageBase code_cage_base) const;
+  inline Tagged<Object> raw_instruction_stream(
+      PtrComprCageBase code_cage_base) const;
 
  private:
   OBJECT_CONSTRUCTORS(GcSafeCode, HeapObject);
diff --git a/src/objects/compilation-cache-table-inl.h b/src/objects/compilation-cache-table-inl.h
index 4c6149ceb9b..bcaf2399b09 100644
--- a/src/objects/compilation-cache-table-inl.h
+++ b/src/objects/compilation-cache-table-inl.h
@@ -26,22 +26,23 @@ CompilationCacheTable::CompilationCacheTable(Address ptr)
 NEVER_READ_ONLY_SPACE_IMPL(CompilationCacheTable)
 CAST_ACCESSOR(CompilationCacheTable)
 
-Object CompilationCacheTable::PrimaryValueAt(InternalIndex entry) {
+Tagged<Object> CompilationCacheTable::PrimaryValueAt(InternalIndex entry) {
   return get(EntryToIndex(entry) + 1);
 }
 
-void CompilationCacheTable::SetPrimaryValueAt(InternalIndex entry, Object value,
+void CompilationCacheTable::SetPrimaryValueAt(InternalIndex entry,
+                                              Tagged<Object> value,
                                               WriteBarrierMode mode) {
   set(EntryToIndex(entry) + 1, value, mode);
 }
 
-Object CompilationCacheTable::EvalFeedbackValueAt(InternalIndex entry) {
+Tagged<Object> CompilationCacheTable::EvalFeedbackValueAt(InternalIndex entry) {
   static_assert(CompilationCacheShape::kEntrySize == 3);
   return get(EntryToIndex(entry) + 2);
 }
 
 void CompilationCacheTable::SetEvalFeedbackValueAt(InternalIndex entry,
-                                                   Object value,
+                                                   Tagged<Object> value,
                                                    WriteBarrierMode mode) {
   set(EntryToIndex(entry) + 2, value, mode);
 }
@@ -69,20 +70,21 @@ class ScriptCacheKey : public HashTableKey {
                  v8::ScriptOriginOptions origin_options,
                  MaybeHandle<Object> host_defined_options, Isolate* isolate);
 
-  bool IsMatch(Object other) override;
-  bool MatchesOrigin(Script script);
+  bool IsMatch(Tagged<Object> other) override;
+  bool MatchesOrigin(Tagged<Script> script);
 
   Handle<Object> AsHandle(Isolate* isolate, Handle<SharedFunctionInfo> shared);
 
-  static base::Optional<String> SourceFromObject(Object obj) {
+  static base::Optional<String> SourceFromObject(Tagged<Object> obj) {
     DisallowGarbageCollection no_gc;
     DCHECK(IsWeakFixedArray(obj));
-    WeakFixedArray array = WeakFixedArray::cast(obj);
+    Tagged<WeakFixedArray> array = WeakFixedArray::cast(obj);
     DCHECK_EQ(array->length(), kEnd);
 
     MaybeObject maybe_script = array->Get(kWeakScript);
-    if (HeapObject script; maybe_script.GetHeapObjectIfWeak(&script)) {
-      PrimitiveHeapObject source_or_undefined = Script::cast(script)->source();
+    if (Tagged<HeapObject> script; maybe_script.GetHeapObjectIfWeak(&script)) {
+      Tagged<PrimitiveHeapObject> source_or_undefined =
+          Script::cast(script)->source();
       // Scripts stored in the script cache should always have a source string.
       return String::cast(source_or_undefined);
     }
@@ -101,12 +103,13 @@ class ScriptCacheKey : public HashTableKey {
   Isolate* isolate_;
 };
 
-uint32_t CompilationCacheShape::RegExpHash(String string, Smi flags) {
+uint32_t CompilationCacheShape::RegExpHash(Tagged<String> string,
+                                           Tagged<Smi> flags) {
   return string->EnsureHash() + flags.value();
 }
 
-uint32_t CompilationCacheShape::EvalHash(String source,
-                                         SharedFunctionInfo shared,
+uint32_t CompilationCacheShape::EvalHash(Tagged<String> source,
+                                         Tagged<SharedFunctionInfo> shared,
                                          LanguageMode language_mode,
                                          int position) {
   uint32_t hash = source->EnsureHash();
@@ -116,7 +119,7 @@ uint32_t CompilationCacheShape::EvalHash(String source,
     // script source code and the start position of the calling scope.
     // We do this to ensure that the cache entries can survive garbage
     // collection.
-    Script script(Script::cast(shared->script()));
+    Tagged<Script> script(Script::cast(shared->script()));
     hash ^= String::cast(script->source())->EnsureHash();
   }
   static_assert(LanguageModeSize == 2);
@@ -126,7 +129,7 @@ uint32_t CompilationCacheShape::EvalHash(String source,
 }
 
 uint32_t CompilationCacheShape::HashForObject(ReadOnlyRoots roots,
-                                              Object object) {
+                                              Tagged<Object> object) {
   // Eval: The key field contains the hash as a Number.
   if (IsNumber(object)) return static_cast<uint32_t>(Object::Number(object));
 
@@ -143,15 +146,15 @@ uint32_t CompilationCacheShape::HashForObject(ReadOnlyRoots roots,
   }
 
   // Eval: See EvalCacheKey::ToHandle for the encoding.
-  FixedArray val = FixedArray::cast(object);
+  Tagged<FixedArray> val = FixedArray::cast(object);
   if (val->map() == roots.fixed_cow_array_map()) {
     DCHECK_EQ(4, val->length());
-    String source = String::cast(val->get(1));
+    Tagged<String> source = String::cast(val->get(1));
     int language_unchecked = Smi::ToInt(val->get(2));
     DCHECK(is_valid_language_mode(language_unchecked));
     LanguageMode language_mode = static_cast<LanguageMode>(language_unchecked);
     int position = Smi::ToInt(val->get(3));
-    Object shared = val->get(0);
+    Tagged<Object> shared = val->get(0);
     return EvalHash(source, SharedFunctionInfo::cast(shared), language_mode,
                     position);
   }
@@ -163,8 +166,8 @@ uint32_t CompilationCacheShape::HashForObject(ReadOnlyRoots roots,
                     Smi::cast(val->get(JSRegExp::kFlagsIndex)));
 }
 
-InfoCellPair::InfoCellPair(Isolate* isolate, SharedFunctionInfo shared,
-                           FeedbackCell feedback_cell)
+InfoCellPair::InfoCellPair(Isolate* isolate, Tagged<SharedFunctionInfo> shared,
+                           Tagged<FeedbackCell> feedback_cell)
     : is_compiled_scope_(!shared.is_null() ? shared->is_compiled_scope(isolate)
                                            : IsCompiledScope()),
       shared_(shared),
diff --git a/src/objects/compilation-cache-table.cc b/src/objects/compilation-cache-table.cc
index 416cc10fbf3..6f37de6dc91 100644
--- a/src/objects/compilation-cache-table.cc
+++ b/src/objects/compilation-cache-table.cc
@@ -18,17 +18,18 @@ const int kLiteralInitialLength = 2;
 const int kLiteralContextOffset = 0;
 const int kLiteralLiteralsOffset = 1;
 
-int SearchLiteralsMapEntry(CompilationCacheTable cache,
-                           InternalIndex cache_entry, Context native_context) {
+int SearchLiteralsMapEntry(Tagged<CompilationCacheTable> cache,
+                           InternalIndex cache_entry,
+                           Tagged<Context> native_context) {
   DisallowGarbageCollection no_gc;
   DCHECK(IsNativeContext(native_context));
-  Object obj = cache->EvalFeedbackValueAt(cache_entry);
+  Tagged<Object> obj = cache->EvalFeedbackValueAt(cache_entry);
 
   // Check that there's no confusion between FixedArray and WeakFixedArray (the
   // object used to be a FixedArray here).
   DCHECK(!IsFixedArray(obj));
   if (IsWeakFixedArray(obj)) {
-    WeakFixedArray literals_map = WeakFixedArray::cast(obj);
+    Tagged<WeakFixedArray> literals_map = WeakFixedArray::cast(obj);
     int length = literals_map->length();
     for (int i = 0; i < length; i += kLiteralEntryLength) {
       DCHECK(literals_map->Get(i + kLiteralContextOffset)->IsWeakOrCleared());
@@ -51,7 +52,7 @@ void AddToFeedbackCellsMap(Handle<CompilationCacheTable> cache,
   Handle<WeakFixedArray> new_literals_map;
   int entry;
 
-  Object obj = cache->EvalFeedbackValueAt(cache_entry);
+  Tagged<Object> obj = cache->EvalFeedbackValueAt(cache_entry);
 
   // Check that there's no confusion between FixedArray and WeakFixedArray (the
   // object used to be a FixedArray here).
@@ -105,19 +106,19 @@ void AddToFeedbackCellsMap(Handle<CompilationCacheTable> cache,
   }
 #endif
 
-  Object old_literals_map = cache->EvalFeedbackValueAt(cache_entry);
+  Tagged<Object> old_literals_map = cache->EvalFeedbackValueAt(cache_entry);
   if (old_literals_map != *new_literals_map) {
     cache->SetEvalFeedbackValueAt(cache_entry, *new_literals_map);
   }
 }
 
-FeedbackCell SearchLiteralsMap(CompilationCacheTable cache,
-                               InternalIndex cache_entry,
-                               Context native_context) {
-  FeedbackCell result;
+Tagged<FeedbackCell> SearchLiteralsMap(Tagged<CompilationCacheTable> cache,
+                                       InternalIndex cache_entry,
+                                       Tagged<Context> native_context) {
+  Tagged<FeedbackCell> result;
   int entry = SearchLiteralsMapEntry(cache, cache_entry, native_context);
   if (entry >= 0) {
-    WeakFixedArray literals_map =
+    Tagged<WeakFixedArray> literals_map =
         WeakFixedArray::cast(cache->EvalFeedbackValueAt(cache_entry));
     DCHECK_LE(entry + kLiteralEntryLength, literals_map->length());
     MaybeObject object = literals_map->Get(entry + kLiteralLiteralsOffset);
@@ -152,14 +153,14 @@ class EvalCacheKey : public HashTableKey {
         language_mode_(language_mode),
         position_(position) {}
 
-  bool IsMatch(Object other) override {
+  bool IsMatch(Tagged<Object> other) override {
     DisallowGarbageCollection no_gc;
     if (!IsFixedArray(other)) {
       DCHECK(IsNumber(other));
       uint32_t other_hash = static_cast<uint32_t>(Object::Number(other));
       return Hash() == other_hash;
     }
-    FixedArray other_array = FixedArray::cast(other);
+    Tagged<FixedArray> other_array = FixedArray::cast(other);
     DCHECK(IsSharedFunctionInfo(other_array->get(0)));
     if (*shared_ != other_array->get(0)) return false;
     int language_unchecked = Smi::ToInt(other_array->get(2));
@@ -168,7 +169,7 @@ class EvalCacheKey : public HashTableKey {
     if (language_mode != language_mode_) return false;
     int position = Smi::ToInt(other_array->get(3));
     if (position != position_) return false;
-    String source = String::cast(other_array->get(1));
+    Tagged<String> source = String::cast(other_array->get(1));
     return source->Equals(*source_);
   }
 
@@ -202,8 +203,8 @@ class RegExpKey : public HashTableKey {
   // stored value is stored where the key should be.  IsMatch then
   // compares the search key to the found object, rather than comparing
   // a key to a key.
-  bool IsMatch(Object obj) override {
-    FixedArray val = FixedArray::cast(obj);
+  bool IsMatch(Tagged<Object> obj) override {
+    Tagged<FixedArray> val = FixedArray::cast(obj);
     return string_->Equals(String::cast(val->get(JSRegExp::kSourceIndex))) &&
            (flags_ == val->get(JSRegExp::kFlagsIndex));
   }
@@ -219,14 +220,15 @@ class CodeKey : public HashTableKey {
   explicit CodeKey(Handle<SharedFunctionInfo> key)
       : HashTableKey(key->Hash()), key_(key) {}
 
-  bool IsMatch(Object string) override { return *key_ == string; }
+  bool IsMatch(Tagged<Object> string) override { return *key_ == string; }
 
   Handle<SharedFunctionInfo> key_;
 };
 
-Smi ScriptHash(String source, MaybeHandle<Object> maybe_name, int line_offset,
-               int column_offset, v8::ScriptOriginOptions origin_options,
-               Isolate* isolate) {
+Tagged<Smi> ScriptHash(Tagged<String> source, MaybeHandle<Object> maybe_name,
+                       int line_offset, int column_offset,
+                       v8::ScriptOriginOptions origin_options,
+                       Isolate* isolate) {
   DisallowGarbageCollection no_gc;
   size_t hash = base::hash_combine(source->EnsureHash());
   if (Handle<Object> name;
@@ -244,7 +246,7 @@ Smi ScriptHash(String source, MaybeHandle<Object> maybe_name, int line_offset,
 // We only re-use a cached function for some script source code if the
 // script originates from the same place. This is to avoid issues
 // when reporting errors, etc.
-bool ScriptCacheKey::MatchesOrigin(Script script) {
+bool ScriptCacheKey::MatchesOrigin(Tagged<Script> script) {
   DisallowGarbageCollection no_gc;
 
   // If the script name isn't set, the boilerplate script should have
@@ -273,9 +275,10 @@ bool ScriptCacheKey::MatchesOrigin(Script script) {
   if (!host_defined_options_.ToHandle(&maybe_host_defined_options)) {
     maybe_host_defined_options = isolate_->factory()->empty_fixed_array();
   }
-  FixedArray host_defined_options =
+  Tagged<FixedArray> host_defined_options =
       FixedArray::cast(*maybe_host_defined_options);
-  FixedArray script_options = FixedArray::cast(script->host_defined_options());
+  Tagged<FixedArray> script_options =
+      FixedArray::cast(script->host_defined_options());
   int length = host_defined_options->length();
   if (length != script_options->length()) return false;
 
@@ -318,10 +321,10 @@ ScriptCacheKey::ScriptCacheKey(Handle<String> source, MaybeHandle<Object> name,
   DCHECK(Smi::IsValid(static_cast<int>(Hash())));
 }
 
-bool ScriptCacheKey::IsMatch(Object other) {
+bool ScriptCacheKey::IsMatch(Tagged<Object> other) {
   DisallowGarbageCollection no_gc;
   DCHECK(IsWeakFixedArray(other));
-  WeakFixedArray other_array = WeakFixedArray::cast(other);
+  Tagged<WeakFixedArray> other_array = WeakFixedArray::cast(other);
   DCHECK_EQ(other_array->length(), kEnd);
 
   // A hash check can quickly reject many non-matches, even though this step
@@ -330,13 +333,13 @@ bool ScriptCacheKey::IsMatch(Object other) {
       static_cast<uint32_t>(other_array->Get(kHash).ToSmi().value());
   if (other_hash != Hash()) return false;
 
-  HeapObject other_script_object;
+  Tagged<HeapObject> other_script_object;
   if (!other_array->Get(kWeakScript)
            .GetHeapObjectIfWeak(&other_script_object)) {
     return false;
   }
-  Script other_script = Script::cast(other_script_object);
-  String other_source = String::cast(other_script->source());
+  Tagged<Script> other_script = Script::cast(other_script_object);
+  Tagged<String> other_source = String::cast(other_script->source());
   return other_source->Equals(*source_) && MatchesOrigin(other_script);
 }
 
@@ -391,13 +394,13 @@ CompilationCacheScriptLookupResult CompilationCacheTable::LookupScript(
   if (entry.is_not_found()) return {};
 
   DisallowGarbageCollection no_gc;
-  Object key_in_table = table->KeyAt(entry);
-  Script script = Script::cast(WeakFixedArray::cast(key_in_table)
-                                   ->Get(ScriptCacheKey::kWeakScript)
-                                   .GetHeapObjectAssumeWeak());
+  Tagged<Object> key_in_table = table->KeyAt(entry);
+  Tagged<Script> script = Script::cast(WeakFixedArray::cast(key_in_table)
+                                           ->Get(ScriptCacheKey::kWeakScript)
+                                           .GetHeapObjectAssumeWeak());
 
-  Object obj = table->PrimaryValueAt(entry);
-  SharedFunctionInfo toplevel_sfi;
+  Tagged<Object> obj = table->PrimaryValueAt(entry);
+  Tagged<SharedFunctionInfo> toplevel_sfi;
   if (!IsUndefined(obj, isolate)) {
     toplevel_sfi = SharedFunctionInfo::cast(obj);
     DCHECK_EQ(toplevel_sfi->script(), script);
@@ -420,11 +423,11 @@ InfoCellPair CompilationCacheTable::LookupEval(
   if (entry.is_not_found()) return empty_result;
 
   if (!IsFixedArray(table->KeyAt(entry))) return empty_result;
-  Object obj = table->PrimaryValueAt(entry);
+  Tagged<Object> obj = table->PrimaryValueAt(entry);
   if (!IsSharedFunctionInfo(obj)) return empty_result;
 
   static_assert(CompilationCacheShape::kEntrySize == 3);
-  FeedbackCell feedback_cell =
+  Tagged<FeedbackCell> feedback_cell =
       SearchLiteralsMap(*table, entry, *native_context);
   return InfoCellPair(isolate, SharedFunctionInfo::cast(obj), feedback_cell);
 }
@@ -448,7 +451,7 @@ Handle<CompilationCacheTable> CompilationCacheTable::EnsureScriptTableCapacity(
   {
     DisallowGarbageCollection no_gc;
     for (InternalIndex entry : cache->IterateEntries()) {
-      Object key;
+      Tagged<Object> key;
       if (!cache->ToKey(isolate, entry, &key)) continue;
       if (WeakFixedArray::cast(key)
               ->Get(ScriptCacheKey::kWeakScript)
@@ -556,7 +559,7 @@ Handle<CompilationCacheTable> CompilationCacheTable::PutRegExp(
   return cache;
 }
 
-void CompilationCacheTable::Remove(Object value) {
+void CompilationCacheTable::Remove(Tagged<Object> value) {
   DisallowGarbageCollection no_gc;
   for (InternalIndex entry : IterateEntries()) {
     if (PrimaryValueAt(entry) == value) {
@@ -567,7 +570,7 @@ void CompilationCacheTable::Remove(Object value) {
 
 void CompilationCacheTable::RemoveEntry(InternalIndex entry) {
   int entry_index = EntryToIndex(entry);
-  Object the_hole_value = GetReadOnlyRoots().the_hole_value();
+  Tagged<Object> the_hole_value = GetReadOnlyRoots().the_hole_value();
   for (int i = 0; i < kEntrySize; i++) {
     NoWriteBarrierSet(*this, entry_index + i, the_hole_value);
   }
diff --git a/src/objects/compilation-cache-table.h b/src/objects/compilation-cache-table.h
index fdd7fb57f0c..9b36b54bff5 100644
--- a/src/objects/compilation-cache-table.h
+++ b/src/objects/compilation-cache-table.h
@@ -21,7 +21,7 @@ struct ScriptDetails;
 
 class CompilationCacheShape : public BaseShape<HashTableKey*> {
  public:
-  static inline bool IsMatch(HashTableKey* key, Object value) {
+  static inline bool IsMatch(HashTableKey* key, Tagged<Object> value) {
     return key->IsMatch(value);
   }
 
@@ -29,12 +29,14 @@ class CompilationCacheShape : public BaseShape<HashTableKey*> {
     return key->Hash();
   }
 
-  static inline uint32_t RegExpHash(String string, Smi flags);
+  static inline uint32_t RegExpHash(Tagged<String> string, Tagged<Smi> flags);
 
-  static inline uint32_t EvalHash(String source, SharedFunctionInfo shared,
+  static inline uint32_t EvalHash(Tagged<String> source,
+                                  Tagged<SharedFunctionInfo> shared,
                                   LanguageMode language_mode, int position);
 
-  static inline uint32_t HashForObject(ReadOnlyRoots roots, Object object);
+  static inline uint32_t HashForObject(ReadOnlyRoots roots,
+                                       Tagged<Object> object);
 
   static const int kPrefixSize = 0;
   // An 'entry' is essentially a grouped collection of slots. Entries are used
@@ -48,14 +50,14 @@ class CompilationCacheShape : public BaseShape<HashTableKey*> {
 class InfoCellPair {
  public:
   InfoCellPair() = default;
-  inline InfoCellPair(Isolate* isolate, SharedFunctionInfo shared,
-                      FeedbackCell feedback_cell);
+  inline InfoCellPair(Isolate* isolate, Tagged<SharedFunctionInfo> shared,
+                      Tagged<FeedbackCell> feedback_cell);
 
-  FeedbackCell feedback_cell() const {
+  Tagged<FeedbackCell> feedback_cell() const {
     DCHECK(is_compiled_scope_.is_compiled());
     return feedback_cell_;
   }
-  SharedFunctionInfo shared() const {
+  Tagged<SharedFunctionInfo> shared() const {
     DCHECK(is_compiled_scope_.is_compiled());
     return shared_;
   }
@@ -149,15 +151,15 @@ class CompilationCacheTable
       Isolate* isolate, Handle<CompilationCacheTable> cache, Handle<String> src,
       JSRegExp::Flags flags, Handle<FixedArray> value);
 
-  void Remove(Object value);
+  void Remove(Tagged<Object> value);
   void RemoveEntry(InternalIndex entry);
 
-  inline Object PrimaryValueAt(InternalIndex entry);
-  inline void SetPrimaryValueAt(InternalIndex entry, Object value,
+  inline Tagged<Object> PrimaryValueAt(InternalIndex entry);
+  inline void SetPrimaryValueAt(InternalIndex entry, Tagged<Object> value,
                                 WriteBarrierMode mode = UPDATE_WRITE_BARRIER);
-  inline Object EvalFeedbackValueAt(InternalIndex entry);
+  inline Tagged<Object> EvalFeedbackValueAt(InternalIndex entry);
   inline void SetEvalFeedbackValueAt(
-      InternalIndex entry, Object value,
+      InternalIndex entry, Tagged<Object> value,
       WriteBarrierMode mode = UPDATE_WRITE_BARRIER);
 
   // The initial placeholder insertion of the eval cache survives this many GCs.
diff --git a/src/objects/compressed-slots-inl.h b/src/objects/compressed-slots-inl.h
index 6d82da2eedd..aefe15d05e3 100644
--- a/src/objects/compressed-slots-inl.h
+++ b/src/objects/compressed-slots-inl.h
@@ -17,7 +17,7 @@ namespace v8::internal {
 // CompressedObjectSlot implementation.
 //
 
-CompressedObjectSlot::CompressedObjectSlot(Object* object)
+CompressedObjectSlot::CompressedObjectSlot(Tagged<Object>* object)
     : SlotBase(reinterpret_cast<Address>(&object->ptr_)) {}
 
 bool CompressedObjectSlot::contains_map_value(Address raw_value) const {
@@ -34,61 +34,62 @@ bool CompressedObjectSlot::Relaxed_ContainsMapValue(Address raw_value) const {
          static_cast<uint32_t>(static_cast<Tagged_t>(raw_value));
 }
 
-Object CompressedObjectSlot::operator*() const {
+Tagged<Object> CompressedObjectSlot::operator*() const {
   Tagged_t value = *location();
   return Object(TCompressionScheme::DecompressTagged(address(), value));
 }
 
-Object CompressedObjectSlot::load(PtrComprCageBase cage_base) const {
+Tagged<Object> CompressedObjectSlot::load(PtrComprCageBase cage_base) const {
   Tagged_t value = *location();
   return Object(TCompressionScheme::DecompressTagged(cage_base, value));
 }
 
-void CompressedObjectSlot::store(Object value) const {
+void CompressedObjectSlot::store(Tagged<Object> value) const {
   *location() = TCompressionScheme::CompressObject(value.ptr());
 }
 
-void CompressedObjectSlot::store_map(Map map) const {
+void CompressedObjectSlot::store_map(Tagged<Map> map) const {
   // Simply forward to store because map packing is not supported with pointer
   // compression.
   DCHECK(!V8_MAP_PACKING_BOOL);
   store(map);
 }
 
-Map CompressedObjectSlot::load_map() const {
+Tagged<Map> CompressedObjectSlot::load_map() const {
   // Simply forward to Relaxed_Load because map packing is not supported with
   // pointer compression.
   DCHECK(!V8_MAP_PACKING_BOOL);
   return Map::unchecked_cast(Relaxed_Load());
 }
 
-Object CompressedObjectSlot::Acquire_Load() const {
+Tagged<Object> CompressedObjectSlot::Acquire_Load() const {
   AtomicTagged_t value = AsAtomicTagged::Acquire_Load(location());
   return Object(TCompressionScheme::DecompressTagged(address(), value));
 }
 
-Object CompressedObjectSlot::Relaxed_Load() const {
+Tagged<Object> CompressedObjectSlot::Relaxed_Load() const {
   AtomicTagged_t value = AsAtomicTagged::Relaxed_Load(location());
   return Object(TCompressionScheme::DecompressTagged(address(), value));
 }
 
-Object CompressedObjectSlot::Relaxed_Load(PtrComprCageBase cage_base) const {
+Tagged<Object> CompressedObjectSlot::Relaxed_Load(
+    PtrComprCageBase cage_base) const {
   AtomicTagged_t value = AsAtomicTagged::Relaxed_Load(location());
   return Object(TCompressionScheme::DecompressTagged(cage_base, value));
 }
 
-void CompressedObjectSlot::Relaxed_Store(Object value) const {
+void CompressedObjectSlot::Relaxed_Store(Tagged<Object> value) const {
   Tagged_t ptr = TCompressionScheme::CompressObject(value.ptr());
   AsAtomicTagged::Relaxed_Store(location(), ptr);
 }
 
-void CompressedObjectSlot::Release_Store(Object value) const {
+void CompressedObjectSlot::Release_Store(Tagged<Object> value) const {
   Tagged_t ptr = TCompressionScheme::CompressObject(value.ptr());
   AsAtomicTagged::Release_Store(location(), ptr);
 }
 
-Object CompressedObjectSlot::Release_CompareAndSwap(Object old,
-                                                    Object target) const {
+Tagged<Object> CompressedObjectSlot::Release_CompareAndSwap(
+    Tagged<Object> old, Tagged<Object> target) const {
   Tagged_t old_ptr = TCompressionScheme::CompressObject(old.ptr());
   Tagged_t target_ptr = TCompressionScheme::CompressObject(target.ptr());
   Tagged_t result =
@@ -158,14 +159,14 @@ void CompressedHeapObjectSlot::store(HeapObjectReference value) const {
   *location() = TCompressionScheme::CompressObject(value.ptr());
 }
 
-HeapObject CompressedHeapObjectSlot::ToHeapObject() const {
+Tagged<HeapObject> CompressedHeapObjectSlot::ToHeapObject() const {
   Tagged_t value = *location();
   DCHECK(HAS_STRONG_HEAP_OBJECT_TAG(value));
   return HeapObject::cast(
       Object(TCompressionScheme::DecompressTagged(address(), value)));
 }
 
-void CompressedHeapObjectSlot::StoreHeapObject(HeapObject value) const {
+void CompressedHeapObjectSlot::StoreHeapObject(Tagged<HeapObject> value) const {
   *location() = TCompressionScheme::CompressObject(value.ptr());
 }
 
@@ -174,26 +175,27 @@ void CompressedHeapObjectSlot::StoreHeapObject(HeapObject value) const {
 //
 
 template <typename CompressionScheme>
-Object OffHeapCompressedObjectSlot<CompressionScheme>::load(
+Tagged<Object> OffHeapCompressedObjectSlot<CompressionScheme>::load(
     PtrComprCageBase cage_base) const {
   Tagged_t value = *TSlotBase::location();
   return Object(CompressionScheme::DecompressTagged(cage_base, value));
 }
 
 template <typename CompressionScheme>
-void OffHeapCompressedObjectSlot<CompressionScheme>::store(Object value) const {
+void OffHeapCompressedObjectSlot<CompressionScheme>::store(
+    Tagged<Object> value) const {
   *TSlotBase::location() = CompressionScheme::CompressObject(value.ptr());
 }
 
 template <typename CompressionScheme>
-Object OffHeapCompressedObjectSlot<CompressionScheme>::Relaxed_Load(
+Tagged<Object> OffHeapCompressedObjectSlot<CompressionScheme>::Relaxed_Load(
     PtrComprCageBase cage_base) const {
   AtomicTagged_t value = AsAtomicTagged::Relaxed_Load(TSlotBase::location());
   return Object(CompressionScheme::DecompressTagged(cage_base, value));
 }
 
 template <typename CompressionScheme>
-Object OffHeapCompressedObjectSlot<CompressionScheme>::Acquire_Load(
+Tagged<Object> OffHeapCompressedObjectSlot<CompressionScheme>::Acquire_Load(
     PtrComprCageBase cage_base) const {
   AtomicTagged_t value = AsAtomicTagged::Acquire_Load(TSlotBase::location());
   return Object(CompressionScheme::DecompressTagged(cage_base, value));
@@ -201,21 +203,21 @@ Object OffHeapCompressedObjectSlot<CompressionScheme>::Acquire_Load(
 
 template <typename CompressionScheme>
 void OffHeapCompressedObjectSlot<CompressionScheme>::Relaxed_Store(
-    Object value) const {
+    Tagged<Object> value) const {
   Tagged_t ptr = CompressionScheme::CompressObject(value.ptr());
   AsAtomicTagged::Relaxed_Store(TSlotBase::location(), ptr);
 }
 
 template <typename CompressionScheme>
 void OffHeapCompressedObjectSlot<CompressionScheme>::Release_Store(
-    Object value) const {
+    Tagged<Object> value) const {
   Tagged_t ptr = CompressionScheme::CompressObject(value.ptr());
   AsAtomicTagged::Release_Store(TSlotBase::location(), ptr);
 }
 
 template <typename CompressionScheme>
 void OffHeapCompressedObjectSlot<CompressionScheme>::Release_CompareAndSwap(
-    Object old, Object target) const {
+    Tagged<Object> old, Tagged<Object> target) const {
   Tagged_t old_ptr = CompressionScheme::CompressObject(old.ptr());
   Tagged_t target_ptr = CompressionScheme::CompressObject(target.ptr());
   AsAtomicTagged::Release_CompareAndSwap(TSlotBase::location(), old_ptr,
diff --git a/src/objects/compressed-slots.h b/src/objects/compressed-slots.h
index 3b923f2d579..af964d704bf 100644
--- a/src/objects/compressed-slots.h
+++ b/src/objects/compressed-slots.h
@@ -21,7 +21,7 @@ class V8HeapCompressionScheme;
 class CompressedObjectSlot : public SlotBase<CompressedObjectSlot, Tagged_t> {
  public:
   using TCompressionScheme = V8HeapCompressionScheme;
-  using TObject = Object;
+  using TObject = Tagged<Object>;
   using THeapObjectSlot = CompressedHeapObjectSlot;
 
   static constexpr bool kCanBeWeak = false;
@@ -30,8 +30,8 @@ class CompressedObjectSlot : public SlotBase<CompressedObjectSlot, Tagged_t> {
   explicit CompressedObjectSlot(Address ptr) : SlotBase(ptr) {}
   explicit CompressedObjectSlot(Address* ptr)
       : SlotBase(reinterpret_cast<Address>(ptr)) {}
-  inline explicit CompressedObjectSlot(Object* object);
-  explicit CompressedObjectSlot(Object const* const* ptr)
+  inline explicit CompressedObjectSlot(Tagged<Object>* object);
+  explicit CompressedObjectSlot(Tagged<Object> const* const* ptr)
       : SlotBase(reinterpret_cast<Address>(ptr)) {}
   template <typename T>
   explicit CompressedObjectSlot(SlotBase<T, TData, kSlotDataAlignment> slot)
@@ -44,19 +44,20 @@ class CompressedObjectSlot : public SlotBase<CompressedObjectSlot, Tagged_t> {
 
   // TODO(leszeks): Consider deprecating the operator* load, and always pass the
   // Isolate.
-  inline Object operator*() const;
-  inline Object load(PtrComprCageBase cage_base) const;
-  inline void store(Object value) const;
-  inline void store_map(Map map) const;
-
-  inline Map load_map() const;
-
-  inline Object Acquire_Load() const;
-  inline Object Relaxed_Load() const;
-  inline Object Relaxed_Load(PtrComprCageBase cage_base) const;
-  inline void Relaxed_Store(Object value) const;
-  inline void Release_Store(Object value) const;
-  inline Object Release_CompareAndSwap(Object old, Object target) const;
+  inline Tagged<Object> operator*() const;
+  inline Tagged<Object> load(PtrComprCageBase cage_base) const;
+  inline void store(Tagged<Object> value) const;
+  inline void store_map(Tagged<Map> map) const;
+
+  inline Tagged<Map> load_map() const;
+
+  inline Tagged<Object> Acquire_Load() const;
+  inline Tagged<Object> Relaxed_Load() const;
+  inline Tagged<Object> Relaxed_Load(PtrComprCageBase cage_base) const;
+  inline void Relaxed_Store(Tagged<Object> value) const;
+  inline void Release_Store(Tagged<Object> value) const;
+  inline Tagged<Object> Release_CompareAndSwap(Tagged<Object> old,
+                                               Tagged<Object> target) const;
 };
 
 // A CompressedMaybeObjectSlot instance describes a kTaggedSize-sized field
@@ -75,7 +76,7 @@ class CompressedMaybeObjectSlot
 
   CompressedMaybeObjectSlot() : SlotBase(kNullAddress) {}
   explicit CompressedMaybeObjectSlot(Address ptr) : SlotBase(ptr) {}
-  explicit CompressedMaybeObjectSlot(Object* ptr)
+  explicit CompressedMaybeObjectSlot(Tagged<Object>* ptr)
       : SlotBase(reinterpret_cast<Address>(ptr)) {}
   explicit CompressedMaybeObjectSlot(MaybeObject* ptr)
       : SlotBase(reinterpret_cast<Address>(ptr)) {}
@@ -108,7 +109,7 @@ class CompressedHeapObjectSlot
 
   CompressedHeapObjectSlot() : SlotBase(kNullAddress) {}
   explicit CompressedHeapObjectSlot(Address ptr) : SlotBase(ptr) {}
-  explicit CompressedHeapObjectSlot(Object* ptr)
+  explicit CompressedHeapObjectSlot(TaggedBase* ptr)
       : SlotBase(reinterpret_cast<Address>(ptr)) {}
   template <typename T>
   explicit CompressedHeapObjectSlot(SlotBase<T, TData, kSlotDataAlignment> slot)
@@ -118,9 +119,9 @@ class CompressedHeapObjectSlot
   inline HeapObjectReference load(PtrComprCageBase cage_base) const;
   inline void store(HeapObjectReference value) const;
 
-  inline HeapObject ToHeapObject() const;
+  inline Tagged<HeapObject> ToHeapObject() const;
 
-  inline void StoreHeapObject(HeapObject value) const;
+  inline void StoreHeapObject(Tagged<HeapObject> value) const;
 };
 
 // An OffHeapCompressedObjectSlot instance describes a kTaggedSize-sized field
@@ -137,7 +138,7 @@ class OffHeapCompressedObjectSlot
   using TSlotBase =
       SlotBase<OffHeapCompressedObjectSlot<CompressionScheme>, Tagged_t>;
   using TCompressionScheme = CompressionScheme;
-  using TObject = Object;
+  using TObject = Tagged<Object>;
   using THeapObjectSlot = OffHeapCompressedObjectSlot<CompressionScheme>;
 
   static constexpr bool kCanBeWeak = false;
@@ -147,14 +148,15 @@ class OffHeapCompressedObjectSlot
   explicit OffHeapCompressedObjectSlot(const uint32_t* ptr)
       : TSlotBase(reinterpret_cast<Address>(ptr)) {}
 
-  inline Object load(PtrComprCageBase cage_base) const;
-  inline void store(Object value) const;
+  inline Tagged<Object> load(PtrComprCageBase cage_base) const;
+  inline void store(Tagged<Object> value) const;
 
-  inline Object Relaxed_Load(PtrComprCageBase cage_base) const;
-  inline Object Acquire_Load(PtrComprCageBase cage_base) const;
-  inline void Relaxed_Store(Object value) const;
-  inline void Release_Store(Object value) const;
-  inline void Release_CompareAndSwap(Object old, Object target) const;
+  inline Tagged<Object> Relaxed_Load(PtrComprCageBase cage_base) const;
+  inline Tagged<Object> Acquire_Load(PtrComprCageBase cage_base) const;
+  inline void Relaxed_Store(Tagged<Object> value) const;
+  inline void Release_Store(Tagged<Object> value) const;
+  inline void Release_CompareAndSwap(Tagged<Object> old,
+                                     Tagged<Object> target) const;
 };
 
 #endif  // V8_COMPRESS_POINTERS
diff --git a/src/objects/contexts-inl.h b/src/objects/contexts-inl.h
index b90bc191819..0119cab1215 100644
--- a/src/objects/contexts-inl.h
+++ b/src/objects/contexts-inl.h
@@ -48,12 +48,13 @@ Handle<Context> ScriptContextTable::GetContext(Isolate* isolate,
   return handle(table->get_context(i), isolate);
 }
 
-Context ScriptContextTable::get_context(int i) const {
+Tagged<Context> ScriptContextTable::get_context(int i) const {
   DCHECK_LT(i, used(kAcquireLoad));
   return Context::cast(get(i + kFirstContextSlotIndex));
 }
 
-Context ScriptContextTable::get_context(int i, AcquireLoadTag tag) const {
+Tagged<Context> ScriptContextTable::get_context(int i,
+                                                AcquireLoadTag tag) const {
   DCHECK_LT(i, used(kAcquireLoad));
   return Context::cast(get(i + kFirstContextSlotIndex, tag));
 }
@@ -65,19 +66,19 @@ CAST_ACCESSOR(NativeContext)
 
 RELAXED_SMI_ACCESSORS(Context, length, kLengthOffset)
 
-Object Context::get(int index) const {
+Tagged<Object> Context::get(int index) const {
   PtrComprCageBase cage_base = GetPtrComprCageBase(*this);
   return get(cage_base, index);
 }
 
-Object Context::get(PtrComprCageBase cage_base, int index) const {
+Tagged<Object> Context::get(PtrComprCageBase cage_base, int index) const {
   DCHECK_LT(static_cast<unsigned int>(index),
             static_cast<unsigned int>(length(kRelaxedLoad)));
   return TaggedField<Object>::Relaxed_Load(cage_base, *this,
                                            OffsetOfElementAt(index));
 }
 
-void Context::set(int index, Object value, WriteBarrierMode mode) {
+void Context::set(int index, Tagged<Object> value, WriteBarrierMode mode) {
   DCHECK_LT(static_cast<unsigned int>(index),
             static_cast<unsigned int>(length(kRelaxedLoad)));
   const int offset = OffsetOfElementAt(index);
@@ -85,19 +86,19 @@ void Context::set(int index, Object value, WriteBarrierMode mode) {
   CONDITIONAL_WRITE_BARRIER(*this, offset, value, mode);
 }
 
-Object Context::get(int index, AcquireLoadTag tag) const {
+Tagged<Object> Context::get(int index, AcquireLoadTag tag) const {
   PtrComprCageBase cage_base = GetPtrComprCageBase(*this);
   return get(cage_base, index, tag);
 }
 
-Object Context::get(PtrComprCageBase cage_base, int index,
-                    AcquireLoadTag) const {
+Tagged<Object> Context::get(PtrComprCageBase cage_base, int index,
+                            AcquireLoadTag) const {
   DCHECK_LT(static_cast<unsigned int>(index),
             static_cast<unsigned int>(length(kRelaxedLoad)));
   return ACQUIRE_READ_FIELD(*this, OffsetOfElementAt(index));
 }
 
-void Context::set(int index, Object value, WriteBarrierMode mode,
+void Context::set(int index, Tagged<Object> value, WriteBarrierMode mode,
                   ReleaseStoreTag) {
   DCHECK_LT(static_cast<unsigned int>(index),
             static_cast<unsigned int>(length(kRelaxedLoad)));
@@ -106,25 +107,27 @@ void Context::set(int index, Object value, WriteBarrierMode mode,
   CONDITIONAL_WRITE_BARRIER(*this, offset, value, mode);
 }
 
-void NativeContext::set(int index, Object value, WriteBarrierMode mode,
+void NativeContext::set(int index, Tagged<Object> value, WriteBarrierMode mode,
                         ReleaseStoreTag tag) {
   Context::set(index, value, mode, tag);
 }
 
 ACCESSORS(Context, scope_info, Tagged<ScopeInfo>, kScopeInfoOffset)
 
-Object Context::unchecked_previous() const { return get(PREVIOUS_INDEX); }
+Tagged<Object> Context::unchecked_previous() const {
+  return get(PREVIOUS_INDEX);
+}
 
-Context Context::previous() const {
-  Object result = get(PREVIOUS_INDEX);
+Tagged<Context> Context::previous() const {
+  Tagged<Object> result = get(PREVIOUS_INDEX);
   DCHECK(IsBootstrappingOrValidParentContext(result, *this));
   return Context::unchecked_cast(result);
 }
-void Context::set_previous(Context context, WriteBarrierMode mode) {
+void Context::set_previous(Tagged<Context> context, WriteBarrierMode mode) {
   set(PREVIOUS_INDEX, context, mode);
 }
 
-Object Context::next_context_link() const {
+Tagged<Object> Context::next_context_link() const {
   return get(Context::NEXT_CONTEXT_LINK);
 }
 
@@ -132,12 +135,12 @@ bool Context::has_extension() const {
   return scope_info()->HasContextExtensionSlot() && !IsUndefined(extension());
 }
 
-HeapObject Context::extension() const {
+Tagged<HeapObject> Context::extension() const {
   DCHECK(scope_info()->HasContextExtensionSlot());
   return HeapObject::cast(get(EXTENSION_INDEX));
 }
 
-NativeContext Context::native_context() const {
+Tagged<NativeContext> Context::native_context() const {
   return this->map()->native_context();
 }
 
@@ -177,7 +180,7 @@ bool Context::IsScriptContext() const {
   return map()->instance_type() == SCRIPT_CONTEXT_TYPE;
 }
 
-bool Context::HasSameSecurityTokenAs(Context that) const {
+bool Context::HasSameSecurityTokenAs(Tagged<Context> that) const {
   return this->native_context()->security_token() ==
          that->native_context()->security_token();
 }
@@ -259,11 +262,11 @@ int Context::FunctionMapIndex(LanguageMode language_mode, FunctionKind kind,
 #undef CHECK_FOLLOWS2
 #undef CHECK_FOLLOWS4
 
-Map Context::GetInitialJSArrayMap(ElementsKind kind) const {
+Tagged<Map> Context::GetInitialJSArrayMap(ElementsKind kind) const {
   DCHECK(IsNativeContext(*this));
   if (!IsFastElementsKind(kind)) return Map();
   DisallowGarbageCollection no_gc;
-  Object const initial_js_array_map = get(Context::ArrayMapIndex(kind));
+  Tagged<Object> const initial_js_array_map = get(Context::ArrayMapIndex(kind));
   DCHECK(!IsUndefined(initial_js_array_map));
   return Map::cast(initial_js_array_map);
 }
@@ -273,32 +276,33 @@ EXTERNAL_POINTER_ACCESSORS(NativeContext, microtask_queue, MicrotaskQueue*,
                            kNativeContextMicrotaskQueueTag)
 
 void NativeContext::synchronized_set_script_context_table(
-    ScriptContextTable script_context_table) {
+    Tagged<ScriptContextTable> script_context_table) {
   set(SCRIPT_CONTEXT_TABLE_INDEX, script_context_table, UPDATE_WRITE_BARRIER,
       kReleaseStore);
 }
 
-ScriptContextTable NativeContext::synchronized_script_context_table() const {
+Tagged<ScriptContextTable> NativeContext::synchronized_script_context_table()
+    const {
   return ScriptContextTable::cast(
       get(SCRIPT_CONTEXT_TABLE_INDEX, kAcquireLoad));
 }
 
-Map NativeContext::TypedArrayElementsKindToCtorMap(
+Tagged<Map> NativeContext::TypedArrayElementsKindToCtorMap(
     ElementsKind element_kind) const {
   int ctor_index = Context::FIRST_FIXED_TYPED_ARRAY_FUN_INDEX + element_kind -
                    ElementsKind::FIRST_FIXED_TYPED_ARRAY_ELEMENTS_KIND;
-  Map map = Map::cast(JSFunction::cast(get(ctor_index))->initial_map());
+  Tagged<Map> map = Map::cast(JSFunction::cast(get(ctor_index))->initial_map());
   DCHECK_EQ(map->elements_kind(), element_kind);
   DCHECK(InstanceTypeChecker::IsJSTypedArray(map));
   return map;
 }
 
-Map NativeContext::TypedArrayElementsKindToRabGsabCtorMap(
+Tagged<Map> NativeContext::TypedArrayElementsKindToRabGsabCtorMap(
     ElementsKind element_kind) const {
   int ctor_index = Context::FIRST_RAB_GSAB_TYPED_ARRAY_MAP_INDEX +
                    element_kind -
                    ElementsKind::FIRST_FIXED_TYPED_ARRAY_ELEMENTS_KIND;
-  Map map = Map::cast(get(ctor_index));
+  Tagged<Map> map = Map::cast(get(ctor_index));
   DCHECK_EQ(map->elements_kind(),
             GetCorrespondingRabGsabElementsKind(element_kind));
   DCHECK(InstanceTypeChecker::IsJSTypedArray(map));
diff --git a/src/objects/contexts.cc b/src/objects/contexts.cc
index cdeee402548..d9c76f260b5 100644
--- a/src/objects/contexts.cc
+++ b/src/objects/contexts.cc
@@ -64,7 +64,7 @@ Handle<ScriptContextTable> ScriptContextTable::Extend(
 }
 
 void Context::Initialize(Isolate* isolate) {
-  ScopeInfo scope_info = this->scope_info();
+  Tagged<ScopeInfo> scope_info = this->scope_info();
   int header = scope_info->ContextHeaderLength();
   for (int var = 0; var < scope_info->ContextLocalCount(); var++) {
     if (scope_info->ContextLocalInitFlag(var) == kNeedsInitialization) {
@@ -80,7 +80,7 @@ bool ScriptContextTable::Lookup(Handle<String> name,
   if (index == -1) return false;
   DCHECK_LE(0, index);
   DCHECK_LT(index, used(kAcquireLoad));
-  Context context = get_context(index);
+  Tagged<Context> context = get_context(index);
   DCHECK(context->IsScriptContext());
   int slot_index = context->scope_info()->ContextSlotIndex(name, result);
   if (slot_index >= 0) {
@@ -103,16 +103,16 @@ bool Context::is_declaration_context() const {
   return scope_info()->is_declaration_scope();
 }
 
-Context Context::declaration_context() const {
-  Context current = *this;
+Tagged<Context> Context::declaration_context() const {
+  Tagged<Context> current = *this;
   while (!current->is_declaration_context()) {
     current = current->previous();
   }
   return current;
 }
 
-Context Context::closure_context() const {
-  Context current = *this;
+Tagged<Context> Context::closure_context() const {
+  Tagged<Context> current = *this;
   while (!current->IsFunctionContext() && !current->IsScriptContext() &&
          !current->IsModuleContext() && !IsNativeContext(current) &&
          !current->IsEvalContext()) {
@@ -121,43 +121,43 @@ Context Context::closure_context() const {
   return current;
 }
 
-JSObject Context::extension_object() const {
+Tagged<JSObject> Context::extension_object() const {
   DCHECK(IsNativeContext(*this) || IsFunctionContext() || IsBlockContext() ||
          IsEvalContext() || IsCatchContext());
-  HeapObject object = extension();
+  Tagged<HeapObject> object = extension();
   if (IsUndefined(object)) return JSObject();
   DCHECK(IsJSContextExtensionObject(object) ||
          (IsNativeContext(*this) && IsJSGlobalObject(object)));
   return JSObject::cast(object);
 }
 
-JSReceiver Context::extension_receiver() const {
+Tagged<JSReceiver> Context::extension_receiver() const {
   DCHECK(IsNativeContext(*this) || IsWithContext() || IsEvalContext() ||
          IsFunctionContext() || IsBlockContext());
   return IsWithContext() ? JSReceiver::cast(extension()) : extension_object();
 }
 
-SourceTextModule Context::module() const {
-  Context current = *this;
+Tagged<SourceTextModule> Context::module() const {
+  Tagged<Context> current = *this;
   while (!current->IsModuleContext()) {
     current = current->previous();
   }
   return SourceTextModule::cast(current->extension());
 }
 
-JSGlobalObject Context::global_object() const {
+Tagged<JSGlobalObject> Context::global_object() const {
   return JSGlobalObject::cast(native_context()->extension());
 }
 
-Context Context::script_context() const {
-  Context current = *this;
+Tagged<Context> Context::script_context() const {
+  Tagged<Context> current = *this;
   while (!current->IsScriptContext()) {
     current = current->previous();
   }
   return current;
 }
 
-JSGlobalProxy Context::global_proxy() const {
+Tagged<JSGlobalProxy> Context::global_proxy() const {
   return native_context()->global_proxy_object();
 }
 
@@ -242,11 +242,11 @@ Handle<Object> Context::Lookup(Handle<Context> context, Handle<String> name,
           PrintF(" - trying other script contexts\n");
         }
         // Try other script contexts.
-        ScriptContextTable script_contexts =
+        Tagged<ScriptContextTable> script_contexts =
             context->native_context()->script_context_table();
         VariableLookupResult r;
         if (script_contexts->Lookup(name, &r)) {
-          Context script_context =
+          Tagged<Context> script_context =
               script_contexts->get_context(r.context_index);
           if (v8_flags.trace_contexts) {
             PrintF("=> found property in script context %d: %p\n",
@@ -312,7 +312,7 @@ Handle<Object> Context::Lookup(Handle<Context> context, Handle<String> name,
       DisallowGarbageCollection no_gc;
       // Use serialized scope information of functions and blocks to search
       // for the context index.
-      ScopeInfo scope_info = context->scope_info();
+      Tagged<ScopeInfo> scope_info = context->scope_info();
       VariableLookupResult lookup_result;
       int slot_index = scope_info->ContextSlotIndex(name, &lookup_result);
       DCHECK(slot_index < 0 || slot_index >= MIN_CONTEXT_SLOTS);
@@ -386,7 +386,7 @@ Handle<Object> Context::Lookup(Handle<Context> context, Handle<String> name,
       has_seen_debug_evaluate_context = true;
 
       // Check materialized locals.
-      Object ext = context->get(EXTENSION_INDEX);
+      Tagged<Object> ext = context->get(EXTENSION_INDEX);
       if (IsJSReceiver(ext)) {
         Handle<JSReceiver> extension(JSReceiver::cast(ext), isolate);
         LookupIterator it(isolate, extension, name, extension);
@@ -398,7 +398,7 @@ Handle<Object> Context::Lookup(Handle<Context> context, Handle<String> name,
       }
 
       // Check the original context, but do not follow its context chain.
-      Object obj = context->get(WRAPPED_CONTEXT_INDEX);
+      Tagged<Object> obj = context->get(WRAPPED_CONTEXT_INDEX);
       if (IsContext(obj)) {
         Handle<Context> wrapped_context(Context::cast(obj), isolate);
         Handle<Object> result =
@@ -420,7 +420,7 @@ Handle<Object> Context::Lookup(Handle<Context> context, Handle<String> name,
     if (has_seen_debug_evaluate_context &&
         IsEphemeronHashTable(isolate->heap()->locals_block_list_cache())) {
       Handle<ScopeInfo> scope_info = handle(context->scope_info(), isolate);
-      Object maybe_outer_block_list =
+      Tagged<Object> maybe_outer_block_list =
           isolate->LocalsBlockListCacheGet(scope_info);
       if (IsStringSet(maybe_outer_block_list) &&
           StringSet::cast(maybe_outer_block_list)->Has(isolate, name)) {
@@ -440,7 +440,7 @@ Handle<Object> Context::Lookup(Handle<Context> context, Handle<String> name,
   return Handle<Object>::null();
 }
 
-bool NativeContext::HasTemplateLiteralObject(JSArray array) {
+bool NativeContext::HasTemplateLiteralObject(Tagged<JSArray> array) {
   return array->map() == js_array_template_literal_object_map();
 }
 
@@ -491,13 +491,13 @@ namespace {
 // TODO(v8:12298): Fix js-context-specialization cctests to set up full
 // native contexts instead of using dummy internalized strings as
 // extensions.
-bool IsContexExtensionTestObject(HeapObject extension) {
+bool IsContexExtensionTestObject(Tagged<HeapObject> extension) {
   return IsInternalizedString(extension) &&
          String::cast(extension)->length() == 1;
 }
 }  // namespace
 
-void Context::VerifyExtensionSlot(HeapObject extension) {
+void Context::VerifyExtensionSlot(Tagged<HeapObject> extension) {
   CHECK(scope_info()->HasContextExtensionSlot());
   // Early exit for potentially uninitialized contexfts.
   if (IsUndefined(extension)) return;
@@ -519,7 +519,7 @@ void Context::VerifyExtensionSlot(HeapObject extension) {
 }
 #endif  // VERIFY_HEAP
 
-void Context::set_extension(HeapObject object, WriteBarrierMode mode) {
+void Context::set_extension(Tagged<HeapObject> object, WriteBarrierMode mode) {
   DCHECK(scope_info()->HasContextExtensionSlot());
 #ifdef VERIFY_HEAP
   if (v8_flags.verify_heap) VerifyExtensionSlot(object);
@@ -529,13 +529,13 @@ void Context::set_extension(HeapObject object, WriteBarrierMode mode) {
 
 #ifdef DEBUG
 
-bool Context::IsBootstrappingOrValidParentContext(Object object,
-                                                  Context child) {
+bool Context::IsBootstrappingOrValidParentContext(Tagged<Object> object,
+                                                  Tagged<Context> child) {
   // During bootstrapping we allow all objects to pass as
   // contexts. This is necessary to fix circular dependencies.
   if (child->GetIsolate()->bootstrapper()->IsActive()) return true;
   if (!IsContext(object)) return false;
-  Context context = Context::cast(object);
+  Tagged<Context> context = Context::cast(object);
   return IsNativeContext(context) || context->IsScriptContext() ||
          context->IsModuleContext() || !child->IsModuleContext();
 }
diff --git a/src/objects/contexts.h b/src/objects/contexts.h
index 59bfe556baf..cadf5e946ae 100644
--- a/src/objects/contexts.h
+++ b/src/objects/contexts.h
@@ -404,8 +404,8 @@ class ScriptContextTable : public FixedArray {
   static inline Handle<Context> GetContext(Isolate* isolate,
                                            Handle<ScriptContextTable> table,
                                            int i);
-  inline Context get_context(int i) const;
-  inline Context get_context(int i, AcquireLoadTag tag) const;
+  inline Tagged<Context> get_context(int i) const;
+  inline Tagged<Context> get_context(int i, AcquireLoadTag tag) const;
 
   DECL_ACCESSORS(names_to_context_index, Tagged<NameToIndexHashTable>)
 
@@ -495,15 +495,15 @@ class Context : public TorqueGeneratedContext<Context, HeapObject> {
   // Setter and getter for elements.
   // Note the plain accessors use relaxed semantics.
   // TODO(jgruber): Make that explicit through tags.
-  V8_INLINE Object get(int index) const;
-  V8_INLINE Object get(PtrComprCageBase cage_base, int index) const;
-  V8_INLINE void set(int index, Object value,
+  V8_INLINE Tagged<Object> get(int index) const;
+  V8_INLINE Tagged<Object> get(PtrComprCageBase cage_base, int index) const;
+  V8_INLINE void set(int index, Tagged<Object> value,
                      WriteBarrierMode mode = UPDATE_WRITE_BARRIER);
   // Accessors with acquire-release semantics.
-  V8_INLINE Object get(int index, AcquireLoadTag) const;
-  V8_INLINE Object get(PtrComprCageBase cage_base, int index,
-                       AcquireLoadTag) const;
-  V8_INLINE void set(int index, Object value, WriteBarrierMode mode,
+  V8_INLINE Tagged<Object> get(int index, AcquireLoadTag) const;
+  V8_INLINE Tagged<Object> get(PtrComprCageBase cage_base, int index,
+                               AcquireLoadTag) const;
+  V8_INLINE void set(int index, Tagged<Object> value, WriteBarrierMode mode,
                      ReleaseStoreTag);
 
   static const int kScopeInfoOffset = kElementsOffset;
@@ -595,41 +595,41 @@ class Context : public TorqueGeneratedContext<Context, HeapObject> {
   // Direct slot access.
   DECL_ACCESSORS(scope_info, Tagged<ScopeInfo>)
 
-  inline Object unchecked_previous() const;
-  inline Context previous() const;
+  inline Tagged<Object> unchecked_previous() const;
+  inline Tagged<Context> previous() const;
 
-  inline Object next_context_link() const;
+  inline Tagged<Object> next_context_link() const;
 
   inline bool has_extension() const;
-  inline HeapObject extension() const;
+  inline Tagged<HeapObject> extension() const;
   V8_EXPORT_PRIVATE void set_extension(
-      HeapObject object, WriteBarrierMode mode = UPDATE_WRITE_BARRIER);
-  JSObject extension_object() const;
-  JSReceiver extension_receiver() const;
+      Tagged<HeapObject> object, WriteBarrierMode mode = UPDATE_WRITE_BARRIER);
+  Tagged<JSObject> extension_object() const;
+  Tagged<JSReceiver> extension_receiver() const;
 
   // Find the module context (assuming there is one) and return the associated
   // module object.
-  SourceTextModule module() const;
+  Tagged<SourceTextModule> module() const;
 
   // Get the context where var declarations will be hoisted to, which
   // may be the context itself.
-  Context declaration_context() const;
+  Tagged<Context> declaration_context() const;
   bool is_declaration_context() const;
 
   // Get the next closure's context on the context chain.
-  Context closure_context() const;
+  Tagged<Context> closure_context() const;
 
   // Returns a JSGlobalProxy object or null.
-  V8_EXPORT_PRIVATE JSGlobalProxy global_proxy() const;
+  V8_EXPORT_PRIVATE Tagged<JSGlobalProxy> global_proxy() const;
 
   // Get the JSGlobalObject object.
-  V8_EXPORT_PRIVATE JSGlobalObject global_object() const;
+  V8_EXPORT_PRIVATE Tagged<JSGlobalObject> global_object() const;
 
   // Get the script context by traversing the context chain.
-  Context script_context() const;
+  Tagged<Context> script_context() const;
 
   // Compute the native context.
-  inline NativeContext native_context() const;
+  inline Tagged<NativeContext> native_context() const;
   inline bool IsDetached() const;
 
   // Predicates for context types.  IsNativeContext is already defined on
@@ -644,7 +644,7 @@ class Context : public TorqueGeneratedContext<Context, HeapObject> {
   inline bool IsEvalContext() const;
   inline bool IsScriptContext() const;
 
-  inline bool HasSameSecurityTokenAs(Context that) const;
+  inline bool HasSameSecurityTokenAs(Tagged<Context> that) const;
 
   Handle<Object> ErrorMessageForCodeGenerationFromStrings();
   Handle<Object> ErrorMessageForWasmCodeGeneration();
@@ -696,7 +696,7 @@ class Context : public TorqueGeneratedContext<Context, HeapObject> {
     return int{elements_kind} + FIRST_JS_ARRAY_MAP_SLOT;
   }
 
-  inline Map GetInitialJSArrayMap(ElementsKind kind) const;
+  inline Tagged<Map> GetInitialJSArrayMap(ElementsKind kind) const;
 
   static const int kNotFound = -1;
 
@@ -707,17 +707,18 @@ class Context : public TorqueGeneratedContext<Context, HeapObject> {
   class BodyDescriptor;
 
 #ifdef VERIFY_HEAP
-  V8_EXPORT_PRIVATE void VerifyExtensionSlot(HeapObject extension);
+  V8_EXPORT_PRIVATE void VerifyExtensionSlot(Tagged<HeapObject> extension);
 #endif
 
  private:
 #ifdef DEBUG
   // Bootstrapping-aware type checks.
-  static bool IsBootstrappingOrValidParentContext(Object object, Context kid);
+  static bool IsBootstrappingOrValidParentContext(Tagged<Object> object,
+                                                  Tagged<Context> kid);
 #endif
 
   friend class Factory;
-  inline void set_previous(Context context,
+  inline void set_previous(Tagged<Context> context,
                            WriteBarrierMode mode = UPDATE_WRITE_BARRIER);
 
   TQ_OBJECT_CONSTRUCTORS(Context)
@@ -733,16 +734,16 @@ class NativeContext : public Context {
   // thus we hide the non-atomic setter. Note this doesn't protect fully since
   // one could still use Context::set and/or write directly using offsets (e.g.
   // from CSA/Torque).
-  void set(int index, Object value, WriteBarrierMode mode) = delete;
-  V8_INLINE void set(int index, Object value, WriteBarrierMode mode,
+  void set(int index, Tagged<Object> value, WriteBarrierMode mode) = delete;
+  V8_INLINE void set(int index, Tagged<Object> value, WriteBarrierMode mode,
                      ReleaseStoreTag);
 
   // [microtask_queue]: pointer to the MicrotaskQueue object.
   DECL_EXTERNAL_POINTER_ACCESSORS(microtask_queue, MicrotaskQueue*)
 
   inline void synchronized_set_script_context_table(
-      ScriptContextTable script_context_table);
-  inline ScriptContextTable synchronized_script_context_table() const;
+      Tagged<ScriptContextTable> script_context_table);
+  inline Tagged<ScriptContextTable> synchronized_script_context_table() const;
 
   // Caution, hack: this getter ignores the AcquireLoadTag. The global_object
   // slot is safe to read concurrently since it is immutable after
@@ -750,17 +751,18 @@ class NativeContext : public Context {
   // than heap-refs.cc.
   // TODO(jgruber): Remove this function after NativeContextRef is actually
   // never serialized and BROKER_NATIVE_CONTEXT_FIELDS is removed.
-  JSGlobalObject global_object() { return Context::global_object(); }
-  JSGlobalObject global_object(AcquireLoadTag) {
+  Tagged<JSGlobalObject> global_object() { return Context::global_object(); }
+  Tagged<JSGlobalObject> global_object(AcquireLoadTag) {
     return Context::global_object();
   }
 
-  inline Map TypedArrayElementsKindToCtorMap(ElementsKind element_kind) const;
+  inline Tagged<Map> TypedArrayElementsKindToCtorMap(
+      ElementsKind element_kind) const;
 
-  inline Map TypedArrayElementsKindToRabGsabCtorMap(
+  inline Tagged<Map> TypedArrayElementsKindToRabGsabCtorMap(
       ElementsKind element_kind) const;
 
-  bool HasTemplateLiteralObject(JSArray array);
+  bool HasTemplateLiteralObject(Tagged<JSArray> array);
 
   // Dispatched behavior.
   DECL_PRINTER(NativeContext)
diff --git a/src/objects/debug-objects-inl.h b/src/objects/debug-objects-inl.h
index 379cfb262c3..2aaa544781d 100644
--- a/src/objects/debug-objects-inl.h
+++ b/src/objects/debug-objects-inl.h
@@ -56,8 +56,8 @@ Tagged<BytecodeArray> DebugInfo::DebugBytecodeArray() {
 TQ_OBJECT_CONSTRUCTORS_IMPL(StackFrameInfo)
 NEVER_READ_ONLY_SPACE_IMPL(StackFrameInfo)
 
-Script StackFrameInfo::script() const {
-  HeapObject object = shared_or_script();
+Tagged<Script> StackFrameInfo::script() const {
+  Tagged<HeapObject> object = shared_or_script();
   if (IsSharedFunctionInfo(object)) {
     object = SharedFunctionInfo::cast(object)->script();
   }
diff --git a/src/objects/debug-objects.cc b/src/objects/debug-objects.cc
index 40e2bd8c922..af9a63f512c 100644
--- a/src/objects/debug-objects.cc
+++ b/src/objects/debug-objects.cc
@@ -81,7 +81,7 @@ bool DebugInfo::CanBreakAtEntry() const {
 bool DebugInfo::HasBreakPoint(Isolate* isolate, int source_position) {
   DCHECK(HasBreakInfo());
   // Get the break point info object for this code offset.
-  Object break_point_info = GetBreakPointInfo(isolate, source_position);
+  Tagged<Object> break_point_info = GetBreakPointInfo(isolate, source_position);
 
   // If there is no break point info object or no break points in the break
   // point info object there is no break point at this code offset.
@@ -91,11 +91,12 @@ bool DebugInfo::HasBreakPoint(Isolate* isolate, int source_position) {
 }
 
 // Get the break point info object for this source position.
-Object DebugInfo::GetBreakPointInfo(Isolate* isolate, int source_position) {
+Tagged<Object> DebugInfo::GetBreakPointInfo(Isolate* isolate,
+                                            int source_position) {
   DCHECK(HasBreakInfo());
   for (int i = 0; i < break_points()->length(); i++) {
     if (!IsUndefined(break_points()->get(i), isolate)) {
-      BreakPointInfo break_point_info =
+      Tagged<BreakPointInfo> break_point_info =
           BreakPointInfo::cast(break_points()->get(i));
       if (break_point_info->source_position() == source_position) {
         return break_point_info;
@@ -169,7 +170,7 @@ void DebugInfo::SetBreakPoint(Isolate* isolate, Handle<DebugInfo> debug_info,
 Handle<Object> DebugInfo::GetBreakPoints(Isolate* isolate,
                                          int source_position) {
   DCHECK(HasBreakInfo());
-  Object break_point_info = GetBreakPointInfo(isolate, source_position);
+  Tagged<Object> break_point_info = GetBreakPointInfo(isolate, source_position);
   if (IsUndefined(break_point_info, isolate)) {
     return isolate->factory()->undefined_value();
   }
@@ -183,7 +184,7 @@ int DebugInfo::GetBreakPointCount(Isolate* isolate) {
   int count = 0;
   for (int i = 0; i < break_points()->length(); i++) {
     if (!IsUndefined(break_points()->get(i), isolate)) {
-      BreakPointInfo break_point_info =
+      Tagged<BreakPointInfo> break_point_info =
           BreakPointInfo::cast(break_points()->get(i));
       count += break_point_info->GetBreakPointCount(isolate);
     }
@@ -232,7 +233,7 @@ DebugInfo::SideEffectState DebugInfo::GetSideEffectState(Isolate* isolate) {
 }
 
 namespace {
-bool IsEqual(BreakPoint break_point1, BreakPoint break_point2) {
+bool IsEqual(Tagged<BreakPoint> break_point1, Tagged<BreakPoint> break_point2) {
   return break_point1->id() == break_point2->id();
 }
 }  // namespace
@@ -321,7 +322,7 @@ bool BreakPointInfo::HasBreakPoint(Isolate* isolate,
                    *break_point);
   }
   // Multiple break points.
-  FixedArray array = FixedArray::cast(break_point_info->break_points());
+  Tagged<FixedArray> array = FixedArray::cast(break_point_info->break_points());
   for (int i = 0; i < array->length(); i++) {
     if (IsEqual(BreakPoint::cast(array->get(i)), *break_point)) {
           return true;
@@ -339,18 +340,20 @@ MaybeHandle<BreakPoint> BreakPointInfo::GetBreakPointById(
   }
   // Single break point.
   if (!IsFixedArray(break_point_info->break_points())) {
-    BreakPoint breakpoint = BreakPoint::cast(break_point_info->break_points());
+    Tagged<BreakPoint> breakpoint =
+        BreakPoint::cast(break_point_info->break_points());
     if (breakpoint->id() == breakpoint_id) {
           return handle(breakpoint, isolate);
     }
   } else {
     // Multiple break points.
-    FixedArray array = FixedArray::cast(break_point_info->break_points());
+    Tagged<FixedArray> array =
+        FixedArray::cast(break_point_info->break_points());
     for (int i = 0; i < array->length(); i++) {
-          BreakPoint breakpoint = BreakPoint::cast(array->get(i));
-          if (breakpoint->id() == breakpoint_id) {
+      Tagged<BreakPoint> breakpoint = BreakPoint::cast(array->get(i));
+      if (breakpoint->id() == breakpoint_id) {
         return handle(breakpoint, isolate);
-          }
+      }
     }
   }
   return MaybeHandle<BreakPoint>();
@@ -461,7 +464,7 @@ void ErrorStackData::EnsureStackFrameInfos(Isolate* isolate,
 // static
 MaybeHandle<JSObject> PromiseOnStack::GetPromise(
     Handle<PromiseOnStack> promise_on_stack) {
-  HeapObject promise;
+  Tagged<HeapObject> promise;
   Isolate* isolate = promise_on_stack->GetIsolate();
   if (promise_on_stack->promise()->GetHeapObjectIfWeak(isolate, &promise)) {
     return handle(JSObject::cast(promise), isolate);
diff --git a/src/objects/debug-objects.h b/src/objects/debug-objects.h
index 04ea99cda89..3637cf696b7 100644
--- a/src/objects/debug-objects.h
+++ b/src/objects/debug-objects.h
@@ -137,7 +137,7 @@ class DebugInfo : public TorqueGeneratedDebugInfo<DebugInfo, Struct> {
 
  private:
   // Get the break point info object for a source position.
-  Object GetBreakPointInfo(Isolate* isolate, int source_position);
+  Tagged<Object> GetBreakPointInfo(Isolate* isolate, int source_position);
 
   TQ_OBJECT_CONSTRUCTORS(DebugInfo)
 };
@@ -211,7 +211,7 @@ class StackFrameInfo
   static int GetSourcePosition(Handle<StackFrameInfo> info);
 
   // The script for the stack frame.
-  inline Script script() const;
+  inline Tagged<Script> script() const;
 
   // The bytecode offset or source position for the stack frame.
   DECL_INT_ACCESSORS(bytecode_offset_or_source_position)
diff --git a/src/objects/deoptimization-data-inl.h b/src/objects/deoptimization-data-inl.h
index 82ff585fb1f..d68006f772d 100644
--- a/src/objects/deoptimization-data-inl.h
+++ b/src/objects/deoptimization-data-inl.h
@@ -59,12 +59,12 @@ inline DeoptimizationLiteralArray::DeoptimizationLiteralArray(Address ptr)
   // No type check is possible beyond that for WeakFixedArray.
 }
 
-inline Object DeoptimizationLiteralArray::get(int index) const {
+inline Tagged<Object> DeoptimizationLiteralArray::get(int index) const {
   return get(GetPtrComprCageBase(*this), index);
 }
 
-inline Object DeoptimizationLiteralArray::get(PtrComprCageBase cage_base,
-                                              int index) const {
+inline Tagged<Object> DeoptimizationLiteralArray::get(
+    PtrComprCageBase cage_base, int index) const {
   MaybeObject maybe = Get(cage_base, index);
 
   // Slots in the DeoptimizationLiteralArray should only be cleared when there
@@ -81,7 +81,7 @@ inline Object DeoptimizationLiteralArray::get(PtrComprCageBase cage_base,
   return maybe.GetHeapObjectOrSmi();
 }
 
-inline void DeoptimizationLiteralArray::set(int index, Object value) {
+inline void DeoptimizationLiteralArray::set(int index, Tagged<Object> value) {
   MaybeObject maybe = MaybeObject::FromObject(value);
   if (Code::IsWeakObjectInDeoptimizationLiteralArray(value)) {
     maybe = MaybeObject::MakeWeak(maybe);
diff --git a/src/objects/deoptimization-data.cc b/src/objects/deoptimization-data.cc
index da59d9dbf7e..994d8643a86 100644
--- a/src/objects/deoptimization-data.cc
+++ b/src/objects/deoptimization-data.cc
@@ -43,7 +43,7 @@ Handle<DeoptimizationData> DeoptimizationData::Empty(LocalIsolate* isolate) {
       isolate->factory()->empty_fixed_array());
 }
 
-SharedFunctionInfo DeoptimizationData::GetInlinedFunction(int index) {
+Tagged<SharedFunctionInfo> DeoptimizationData::GetInlinedFunction(int index) {
   if (index == -1) {
     return SharedFunctionInfo::cast(SharedFunctionInfo());
   } else {
@@ -122,7 +122,7 @@ void DeoptimizationData::PrintDeoptimizationData(std::ostream& os) const {
   int const inlined_function_count = InlinedFunctionCount().value();
   os << "Inlined functions (count = " << inlined_function_count << ")\n";
   for (int id = 0; id < inlined_function_count; ++id) {
-    Object info = LiteralArray()->get(id);
+    Tagged<Object> info = LiteralArray()->get(id);
     os << " " << Brief(SharedFunctionInfo::cast(info)) << "\n";
   }
   os << "\n";
@@ -157,7 +157,7 @@ void DeoptimizationData::PrintDeoptimizationData(std::ostream& os) const {
 #endif  // ENABLE_DISASSEMBLER
 
 DeoptimizationFrameTranslation::Iterator::Iterator(
-    DeoptimizationFrameTranslation buffer, int index)
+    Tagged<DeoptimizationFrameTranslation> buffer, int index)
     : buffer_(buffer), index_(index) {
 #ifdef V8_USE_ZLIB
   if (V8_UNLIKELY(v8_flags.turbo_compress_frame_translations)) {
@@ -179,7 +179,7 @@ DeoptimizationFrameTranslation::Iterator::Iterator(
   }
 #endif  // V8_USE_ZLIB
   DCHECK(!v8_flags.turbo_compress_frame_translations);
-  DCHECK(index >= 0 && index < buffer.length());
+  DCHECK(index >= 0 && index < buffer->length());
   // Starting at a location other than a BEGIN would make
   // MATCH_PREVIOUS_TRANSLATION instructions not work.
   DCHECK(TranslationOpcodeIsBegin(
@@ -351,7 +351,7 @@ void DeoptimizationFrameTranslation::Iterator::
 
 void DeoptimizationFrameTranslation::PrintFrameTranslation(
     std::ostream& os, int index,
-    DeoptimizationLiteralArray literal_array) const {
+    Tagged<DeoptimizationLiteralArray> literal_array) const {
   DisallowGarbageCollection gc_oh_noes;
 
   DeoptimizationFrameTranslation::Iterator iterator(*this, index);
diff --git a/src/objects/deoptimization-data.h b/src/objects/deoptimization-data.h
index 3d5a8d86d07..2794e1eea80 100644
--- a/src/objects/deoptimization-data.h
+++ b/src/objects/deoptimization-data.h
@@ -22,12 +22,12 @@ class DeoptimizationLiteralArray : public WeakFixedArray {
  public:
   // Getters for literals. These include runtime checks that the pointer was not
   // cleared, if the literal was held weakly.
-  inline Object get(int index) const;
-  inline Object get(PtrComprCageBase cage_base, int index) const;
+  inline Tagged<Object> get(int index) const;
+  inline Tagged<Object> get(PtrComprCageBase cage_base, int index) const;
 
   // Setter for literals. This will set the object as strong or weak depending
   // on InstructionStream::IsWeakObjectInOptimizedCode.
-  inline void set(int index, Object value);
+  inline void set(int index, Tagged<Object> value);
 
   DECL_CAST(DeoptimizationLiteralArray)
 
@@ -62,8 +62,9 @@ class DeoptimizationFrameTranslation : public ByteArray {
 #endif  // V8_USE_ZLIB
 
 #ifdef ENABLE_DISASSEMBLER
-  void PrintFrameTranslation(std::ostream& os, int index,
-                             DeoptimizationLiteralArray literal_array) const;
+  void PrintFrameTranslation(
+      std::ostream& os, int index,
+      Tagged<DeoptimizationLiteralArray> literal_array) const;
 #endif
 
   OBJECT_CONSTRUCTORS(DeoptimizationFrameTranslation, ByteArray);
@@ -71,7 +72,7 @@ class DeoptimizationFrameTranslation : public ByteArray {
 
 class DeoptimizationFrameTranslation::Iterator {
  public:
-  Iterator(DeoptimizationFrameTranslation buffer, int index);
+  Iterator(Tagged<DeoptimizationFrameTranslation> buffer, int index);
 
   int32_t NextOperand();
 
@@ -194,7 +195,7 @@ class DeoptimizationData : public FixedArray {
 
   // Returns the inlined function at the given position in LiteralArray, or the
   // outer function if index == kNotInlinedIndex.
-  class SharedFunctionInfo GetInlinedFunction(int index);
+  Tagged<class SharedFunctionInfo> GetInlinedFunction(int index);
 
   // Allocates a DeoptimizationData.
   static Handle<DeoptimizationData> New(Isolate* isolate, int deopt_entry_count,
diff --git a/src/objects/dependent-code.cc b/src/objects/dependent-code.cc
index 813d2b40278..96aa756fcad 100644
--- a/src/objects/dependent-code.cc
+++ b/src/objects/dependent-code.cc
@@ -12,7 +12,8 @@
 namespace v8 {
 namespace internal {
 
-DependentCode DependentCode::GetDependentCode(HeapObject object) {
+Tagged<DependentCode> DependentCode::GetDependentCode(
+    Tagged<HeapObject> object) {
   if (IsMap(object)) {
     return Map::cast(object)->dependent_code();
   } else if (IsPropertyCell(object)) {
@@ -161,7 +162,8 @@ void DependentCode::DeoptimizeDependencyGroups(
 }
 
 // static
-DependentCode DependentCode::empty_dependent_code(const ReadOnlyRoots& roots) {
+Tagged<DependentCode> DependentCode::empty_dependent_code(
+    const ReadOnlyRoots& roots) {
   return DependentCode::cast(roots.empty_weak_array_list());
 }
 
diff --git a/src/objects/dependent-code.h b/src/objects/dependent-code.h
index 89a119138bf..ddcb1fbeeed 100644
--- a/src/objects/dependent-code.h
+++ b/src/objects/dependent-code.h
@@ -87,7 +87,7 @@ class DependentCode : public WeakArrayList {
                                         Tagged<ObjectT> object,
                                         DependencyGroups groups);
 
-  V8_EXPORT_PRIVATE static DependentCode empty_dependent_code(
+  V8_EXPORT_PRIVATE static Tagged<DependentCode> empty_dependent_code(
       const ReadOnlyRoots& roots);
   static constexpr RootIndex kEmptyDependentCode =
       RootIndex::kEmptyWeakArrayList;
@@ -100,7 +100,7 @@ class DependentCode : public WeakArrayList {
 
  private:
   // Get/Set {object}'s {DependentCode}.
-  static DependentCode GetDependentCode(HeapObject object);
+  static Tagged<DependentCode> GetDependentCode(Tagged<HeapObject> object);
   static void SetDependentCode(Handle<HeapObject> object,
                                Handle<DependentCode> dep);
 
@@ -116,7 +116,8 @@ class DependentCode : public WeakArrayList {
 
   // The callback is called for all non-cleared entries, and should return true
   // iff the current entry should be cleared.
-  using IterateAndCompactFn = std::function<bool(Code, DependencyGroups)>;
+  using IterateAndCompactFn =
+      std::function<bool(Tagged<Code>, DependencyGroups)>;
   void IterateAndCompact(const IterateAndCompactFn& fn);
 
   // Fills the given entry with the last non-cleared entry in this list, and
diff --git a/src/objects/descriptor-array-inl.h b/src/objects/descriptor-array-inl.h
index 02c669450da..a36be95706f 100644
--- a/src/objects/descriptor-array-inl.h
+++ b/src/objects/descriptor-array-inl.h
@@ -44,18 +44,18 @@ inline int DescriptorArray::number_of_entries() const {
   return number_of_descriptors();
 }
 
-void DescriptorArray::CopyEnumCacheFrom(DescriptorArray array) {
+void DescriptorArray::CopyEnumCacheFrom(Tagged<DescriptorArray> array) {
   set_enum_cache(array->enum_cache());
 }
 
-InternalIndex DescriptorArray::Search(Name name, int valid_descriptors,
+InternalIndex DescriptorArray::Search(Tagged<Name> name, int valid_descriptors,
                                       bool concurrent_search) {
   DCHECK(IsUniqueName(name));
   return InternalIndex(internal::Search<VALID_ENTRIES>(
       this, name, valid_descriptors, nullptr, concurrent_search));
 }
 
-InternalIndex DescriptorArray::Search(Name name, Map map,
+InternalIndex DescriptorArray::Search(Tagged<Name> name, Tagged<Map> map,
                                       bool concurrent_search) {
   DCHECK(IsUniqueName(name));
   int number_of_own_descriptors = map->NumberOfOwnDescriptors();
@@ -76,14 +76,15 @@ InternalIndex DescriptorArray::Search(int field_index, int valid_descriptors) {
   return InternalIndex::NotFound();
 }
 
-InternalIndex DescriptorArray::Search(int field_index, Map map) {
+InternalIndex DescriptorArray::Search(int field_index, Tagged<Map> map) {
   int number_of_own_descriptors = map->NumberOfOwnDescriptors();
   if (number_of_own_descriptors == 0) return InternalIndex::NotFound();
   return Search(field_index, number_of_own_descriptors);
 }
 
-InternalIndex DescriptorArray::SearchWithCache(Isolate* isolate, Name name,
-                                               Map map) {
+InternalIndex DescriptorArray::SearchWithCache(Isolate* isolate,
+                                               Tagged<Name> name,
+                                               Tagged<Map> map) {
   DCHECK(IsUniqueName(name));
   int number_of_own_descriptors = map->NumberOfOwnDescriptors();
   if (number_of_own_descriptors == 0) return InternalIndex::NotFound();
@@ -115,20 +116,21 @@ ObjectSlot DescriptorArray::GetDescriptorSlot(int descriptor) {
   return RawField(OffsetOfDescriptorAt(descriptor));
 }
 
-Name DescriptorArray::GetKey(InternalIndex descriptor_number) const {
+Tagged<Name> DescriptorArray::GetKey(InternalIndex descriptor_number) const {
   PtrComprCageBase cage_base = GetPtrComprCageBase(*this);
   return GetKey(cage_base, descriptor_number);
 }
 
-Name DescriptorArray::GetKey(PtrComprCageBase cage_base,
-                             InternalIndex descriptor_number) const {
+Tagged<Name> DescriptorArray::GetKey(PtrComprCageBase cage_base,
+                                     InternalIndex descriptor_number) const {
   DCHECK_LT(descriptor_number.as_int(), number_of_descriptors());
   int entry_offset = OffsetOfDescriptorAt(descriptor_number.as_int());
   return Name::cast(
       EntryKeyField::Relaxed_Load(cage_base, *this, entry_offset));
 }
 
-void DescriptorArray::SetKey(InternalIndex descriptor_number, Name key) {
+void DescriptorArray::SetKey(InternalIndex descriptor_number,
+                             Tagged<Name> key) {
   DCHECK_LT(descriptor_number.as_int(), number_of_descriptors());
   int entry_offset = OffsetOfDescriptorAt(descriptor_number.as_int());
   EntryKeyField::Relaxed_Store(*this, entry_offset, key);
@@ -139,13 +141,13 @@ int DescriptorArray::GetSortedKeyIndex(int descriptor_number) {
   return GetDetails(InternalIndex(descriptor_number)).pointer();
 }
 
-Name DescriptorArray::GetSortedKey(int descriptor_number) {
+Tagged<Name> DescriptorArray::GetSortedKey(int descriptor_number) {
   PtrComprCageBase cage_base = GetPtrComprCageBase(*this);
   return GetSortedKey(cage_base, descriptor_number);
 }
 
-Name DescriptorArray::GetSortedKey(PtrComprCageBase cage_base,
-                                   int descriptor_number) {
+Tagged<Name> DescriptorArray::GetSortedKey(PtrComprCageBase cage_base,
+                                           int descriptor_number) {
   return GetKey(cage_base, InternalIndex(GetSortedKeyIndex(descriptor_number)));
 }
 
@@ -154,13 +156,14 @@ void DescriptorArray::SetSortedKey(int descriptor_number, int pointer) {
   SetDetails(InternalIndex(descriptor_number), details.set_pointer(pointer));
 }
 
-Object DescriptorArray::GetStrongValue(InternalIndex descriptor_number) {
+Tagged<Object> DescriptorArray::GetStrongValue(
+    InternalIndex descriptor_number) {
   PtrComprCageBase cage_base = GetPtrComprCageBase(*this);
   return GetStrongValue(cage_base, descriptor_number);
 }
 
-Object DescriptorArray::GetStrongValue(PtrComprCageBase cage_base,
-                                       InternalIndex descriptor_number) {
+Tagged<Object> DescriptorArray::GetStrongValue(
+    PtrComprCageBase cage_base, InternalIndex descriptor_number) {
   return GetValue(cage_base, descriptor_number).cast<Object>();
 }
 
@@ -187,7 +190,7 @@ MaybeObject DescriptorArray::GetValue(PtrComprCageBase cage_base,
 PropertyDetails DescriptorArray::GetDetails(InternalIndex descriptor_number) {
   DCHECK_LT(descriptor_number.as_int(), number_of_descriptors());
   int entry_offset = OffsetOfDescriptorAt(descriptor_number.as_int());
-  Smi details = EntryDetailsField::Relaxed_Load(*this, entry_offset);
+  Tagged<Smi> details = EntryDetailsField::Relaxed_Load(*this, entry_offset);
   return PropertyDetails(details);
 }
 
@@ -203,19 +206,20 @@ int DescriptorArray::GetFieldIndex(InternalIndex descriptor_number) {
   return GetDetails(descriptor_number).field_index();
 }
 
-FieldType DescriptorArray::GetFieldType(InternalIndex descriptor_number) {
+Tagged<FieldType> DescriptorArray::GetFieldType(
+    InternalIndex descriptor_number) {
   PtrComprCageBase cage_base = GetPtrComprCageBase(*this);
   return GetFieldType(cage_base, descriptor_number);
 }
 
-FieldType DescriptorArray::GetFieldType(PtrComprCageBase cage_base,
-                                        InternalIndex descriptor_number) {
+Tagged<FieldType> DescriptorArray::GetFieldType(
+    PtrComprCageBase cage_base, InternalIndex descriptor_number) {
   DCHECK_EQ(GetDetails(descriptor_number).location(), PropertyLocation::kField);
   MaybeObject wrapped_type = GetValue(cage_base, descriptor_number);
   return Map::UnwrapFieldType(wrapped_type);
 }
 
-void DescriptorArray::Set(InternalIndex descriptor_number, Name key,
+void DescriptorArray::Set(InternalIndex descriptor_number, Tagged<Name> key,
                           MaybeObject value, PropertyDetails details) {
   SetKey(descriptor_number, key);
   SetDetails(descriptor_number, details);
@@ -223,7 +227,7 @@ void DescriptorArray::Set(InternalIndex descriptor_number, Name key,
 }
 
 void DescriptorArray::Set(InternalIndex descriptor_number, Descriptor* desc) {
-  Name key = *desc->GetKey();
+  Tagged<Name> key = *desc->GetKey();
   MaybeObject value = *desc->GetValue();
   Set(descriptor_number, key, value, desc->GetDetails());
 }
@@ -242,7 +246,7 @@ void DescriptorArray::Append(Descriptor* desc) {
   int insertion;
 
   for (insertion = descriptor_number; insertion > 0; --insertion) {
-    Name key = GetSortedKey(insertion - 1);
+    Tagged<Name> key = GetSortedKey(insertion - 1);
     collision_hash = key->hash();
     if (collision_hash <= desc_hash) break;
     SetSortedKey(insertion, GetSortedKeyIndex(insertion - 1));
@@ -263,7 +267,8 @@ void DescriptorArray::SwapSortedKeys(int first, int second) {
 
 // static
 bool DescriptorArrayMarkingState::TryUpdateIndicesToMark(
-    unsigned gc_epoch, DescriptorArray array, DescriptorIndex index_to_mark) {
+    unsigned gc_epoch, Tagged<DescriptorArray> array,
+    DescriptorIndex index_to_mark) {
   const auto current_epoch = gc_epoch & Epoch::kMask;
   while (true) {
     const RawGCStateType raw_gc_state = array->raw_gc_state(kRelaxedLoad);
@@ -295,7 +300,7 @@ bool DescriptorArrayMarkingState::TryUpdateIndicesToMark(
 std::pair<DescriptorArrayMarkingState::DescriptorIndex,
           DescriptorArrayMarkingState::DescriptorIndex>
 DescriptorArrayMarkingState::AcquireDescriptorRangeToMark(
-    unsigned gc_epoch, DescriptorArray array) {
+    unsigned gc_epoch, Tagged<DescriptorArray> array) {
   const auto current_epoch = gc_epoch & Epoch::kMask;
   while (true) {
     const RawGCStateType raw_gc_state = array->raw_gc_state(kRelaxedLoad);
diff --git a/src/objects/descriptor-array.h b/src/objects/descriptor-array.h
index 04bd49e1db0..4b00ecadd44 100644
--- a/src/objects/descriptor-array.h
+++ b/src/objects/descriptor-array.h
@@ -59,36 +59,37 @@ class DescriptorArray
   inline int number_of_entries() const;
 
   void ClearEnumCache();
-  inline void CopyEnumCacheFrom(DescriptorArray array);
+  inline void CopyEnumCacheFrom(Tagged<DescriptorArray> array);
   static void InitializeOrChangeEnumCache(
       Handle<DescriptorArray> descriptors, Isolate* isolate,
       Handle<FixedArray> keys, Handle<FixedArray> indices,
       AllocationType allocation_if_initialize);
 
   // Accessors for fetching instance descriptor at descriptor number.
-  inline Name GetKey(InternalIndex descriptor_number) const;
-  inline Name GetKey(PtrComprCageBase cage_base,
-                     InternalIndex descriptor_number) const;
-  inline Object GetStrongValue(InternalIndex descriptor_number);
-  inline Object GetStrongValue(PtrComprCageBase cage_base,
-                               InternalIndex descriptor_number);
+  inline Tagged<Name> GetKey(InternalIndex descriptor_number) const;
+  inline Tagged<Name> GetKey(PtrComprCageBase cage_base,
+                             InternalIndex descriptor_number) const;
+  inline Tagged<Object> GetStrongValue(InternalIndex descriptor_number);
+  inline Tagged<Object> GetStrongValue(PtrComprCageBase cage_base,
+                                       InternalIndex descriptor_number);
   inline MaybeObject GetValue(InternalIndex descriptor_number);
   inline MaybeObject GetValue(PtrComprCageBase cage_base,
                               InternalIndex descriptor_number);
   inline PropertyDetails GetDetails(InternalIndex descriptor_number);
   inline int GetFieldIndex(InternalIndex descriptor_number);
-  inline FieldType GetFieldType(InternalIndex descriptor_number);
-  inline FieldType GetFieldType(PtrComprCageBase cage_base,
-                                InternalIndex descriptor_number);
+  inline Tagged<FieldType> GetFieldType(InternalIndex descriptor_number);
+  inline Tagged<FieldType> GetFieldType(PtrComprCageBase cage_base,
+                                        InternalIndex descriptor_number);
 
-  inline Name GetSortedKey(int descriptor_number);
-  inline Name GetSortedKey(PtrComprCageBase cage_base, int descriptor_number);
+  inline Tagged<Name> GetSortedKey(int descriptor_number);
+  inline Tagged<Name> GetSortedKey(PtrComprCageBase cage_base,
+                                   int descriptor_number);
   inline int GetSortedKeyIndex(int descriptor_number);
 
   // Accessor for complete descriptor.
   inline void Set(InternalIndex descriptor_number, Descriptor* desc);
-  inline void Set(InternalIndex descriptor_number, Name key, MaybeObject value,
-                  PropertyDetails details);
+  inline void Set(InternalIndex descriptor_number, Tagged<Name> key,
+                  MaybeObject value, PropertyDetails details);
   void Replace(InternalIndex descriptor_number, Descriptor* descriptor);
 
   // Generalizes constness, representation and field type of all field
@@ -122,21 +123,23 @@ class DescriptorArray
   // Search the instance descriptors for given name. {concurrent_search} signals
   // if we are doing the search on a background thread. If so, we will sacrifice
   // speed for thread-safety.
-  V8_INLINE InternalIndex Search(Name name, int number_of_own_descriptors,
+  V8_INLINE InternalIndex Search(Tagged<Name> name,
+                                 int number_of_own_descriptors,
                                  bool concurrent_search = false);
-  V8_INLINE InternalIndex Search(Name name, Map map,
+  V8_INLINE InternalIndex Search(Tagged<Name> name, Tagged<Map> map,
                                  bool concurrent_search = false);
 
   // Search the instance descriptors for given field offset.
   V8_INLINE InternalIndex Search(int field_offset,
                                  int number_of_own_descriptors);
-  V8_INLINE InternalIndex Search(int field_offset, Map map);
+  V8_INLINE InternalIndex Search(int field_offset, Tagged<Map> map);
 
   // As the above, but uses DescriptorLookupCache and updates it when
   // necessary.
-  V8_INLINE InternalIndex SearchWithCache(Isolate* isolate, Name name, Map map);
+  V8_INLINE InternalIndex SearchWithCache(Isolate* isolate, Tagged<Name> name,
+                                          Tagged<Map> map);
 
-  bool IsEqualUpTo(DescriptorArray desc, int nof_descriptors);
+  bool IsEqualUpTo(Tagged<DescriptorArray> desc, int nof_descriptors);
 
   // Allocates a DescriptorArray, but returns the singleton
   // empty descriptor array object if number_of_descriptors is 0.
@@ -145,8 +148,9 @@ class DescriptorArray
       IsolateT* isolate, int nof_descriptors, int slack,
       AllocationType allocation = AllocationType::kYoung);
 
-  void Initialize(EnumCache enum_cache, HeapObject undefined_value,
-                  int nof_descriptors, int slack, uint32_t raw_gc_state);
+  void Initialize(Tagged<EnumCache> enum_cache,
+                  Tagged<HeapObject> undefined_value, int nof_descriptors,
+                  int slack, uint32_t raw_gc_state);
 
   // Constant for denoting key was not found.
   static const int kNotFound = -1;
@@ -200,7 +204,7 @@ class DescriptorArray
   V8_EXPORT_PRIVATE bool IsSortedNoDuplicates();
 
   // Are two DescriptorArrays equal?
-  bool IsEqualTo(DescriptorArray other);
+  bool IsEqualTo(Tagged<DescriptorArray> other);
 #endif
 
   static constexpr int ToDetailsIndex(int descriptor_number) {
@@ -221,14 +225,14 @@ class DescriptorArray
   using EntryValueField = TaggedField<MaybeObject, kEntryValueOffset>;
 
  private:
-  inline void SetKey(InternalIndex descriptor_number, Name key);
+  inline void SetKey(InternalIndex descriptor_number, Tagged<Name> key);
   inline void SetValue(InternalIndex descriptor_number, MaybeObject value);
   inline void SetDetails(InternalIndex descriptor_number,
                          PropertyDetails details);
 
   // Transfer a complete descriptor from the src descriptor array to this
   // descriptor array.
-  void CopyFrom(InternalIndex index, DescriptorArray src);
+  void CopyFrom(InternalIndex index, Tagged<DescriptorArray> src);
 
   inline void SetSortedKey(int pointer, int descriptor_number);
 
@@ -279,7 +283,7 @@ class DescriptorArrayMarkingState final {
   // The call issues and Acq/Rel barrier to allow synchronizing other state
   // (e.g. value of descriptor slots) with it.
   static inline bool TryUpdateIndicesToMark(unsigned gc_epoch,
-                                            DescriptorArray array,
+                                            Tagged<DescriptorArray> array,
                                             DescriptorIndex index_to_mark);
 
   // Used from the visitor when processing a DescriptorArray. Returns a range of
@@ -287,7 +291,8 @@ class DescriptorArrayMarkingState final {
   // end. The method signals the first invocation by returning start == 0, and
   // end != 0.
   static inline std::pair<DescriptorIndex, DescriptorIndex>
-  AcquireDescriptorRangeToMark(unsigned gc_epoch, DescriptorArray array);
+  AcquireDescriptorRangeToMark(unsigned gc_epoch,
+                               Tagged<DescriptorArray> array);
 
  private:
   static constexpr RawGCStateType NewState(unsigned masked_epoch,
@@ -297,7 +302,7 @@ class DescriptorArrayMarkingState final {
            Delta::encode(delta);
   }
 
-  static bool SwapState(DescriptorArray array, RawGCStateType old_state,
+  static bool SwapState(Tagged<DescriptorArray> array, RawGCStateType old_state,
                         RawGCStateType new_state) {
     return static_cast<RawGCStateType>(base::AcquireRelease_CompareAndSwap(
                reinterpret_cast<base::Atomic32*>(
diff --git a/src/objects/dictionary-inl.h b/src/objects/dictionary-inl.h
index 1ac46842c67..4f7a06cc6ee 100644
--- a/src/objects/dictionary-inl.h
+++ b/src/objects/dictionary-inl.h
@@ -30,42 +30,42 @@ Dictionary<Derived, Shape>::Dictionary(Address ptr)
     : HashTable<Derived, Shape>(ptr) {}
 
 template <typename Derived, typename Shape>
-Object Dictionary<Derived, Shape>::ValueAt(InternalIndex entry) {
+Tagged<Object> Dictionary<Derived, Shape>::ValueAt(InternalIndex entry) {
   PtrComprCageBase cage_base = GetPtrComprCageBase(*this);
   return ValueAt(cage_base, entry);
 }
 
 template <typename Derived, typename Shape>
-Object Dictionary<Derived, Shape>::ValueAt(PtrComprCageBase cage_base,
-                                           InternalIndex entry) {
+Tagged<Object> Dictionary<Derived, Shape>::ValueAt(PtrComprCageBase cage_base,
+                                                   InternalIndex entry) {
   return this->get(cage_base, DerivedHashTable::EntryToIndex(entry) +
                                   Derived::kEntryValueIndex);
 }
 
 template <typename Derived, typename Shape>
-Object Dictionary<Derived, Shape>::ValueAt(InternalIndex entry,
-                                           SeqCstAccessTag tag) {
+Tagged<Object> Dictionary<Derived, Shape>::ValueAt(InternalIndex entry,
+                                                   SeqCstAccessTag tag) {
   PtrComprCageBase cage_base = GetPtrComprCageBase(*this);
   return ValueAt(cage_base, entry, tag);
 }
 
 template <typename Derived, typename Shape>
-Object Dictionary<Derived, Shape>::ValueAt(PtrComprCageBase cage_base,
-                                           InternalIndex entry,
-                                           SeqCstAccessTag tag) {
+Tagged<Object> Dictionary<Derived, Shape>::ValueAt(PtrComprCageBase cage_base,
+                                                   InternalIndex entry,
+                                                   SeqCstAccessTag tag) {
   return this->get(
       cage_base,
       DerivedHashTable::EntryToIndex(entry) + Derived::kEntryValueIndex, tag);
 }
 
 template <typename Derived, typename Shape>
-base::Optional<Object> Dictionary<Derived, Shape>::TryValueAt(
+base::Optional<Tagged<Object>> Dictionary<Derived, Shape>::TryValueAt(
     InternalIndex entry) {
 #if DEBUG
   Isolate* isolate;
   GetIsolateFromHeapObject(*this, &isolate);
   DCHECK_NE(isolate, nullptr);
-  SLOW_DCHECK(!isolate->heap()->IsPendingAllocation(*this));
+  SLOW_DCHECK(!isolate->heap()->IsPendingAllocation(Tagged(*this)));
 #endif  // DEBUG
   // We can read length() in a non-atomic way since we are reading an
   // initialized object which is not pending allocation.
@@ -77,32 +77,33 @@ base::Optional<Object> Dictionary<Derived, Shape>::TryValueAt(
 }
 
 template <typename Derived, typename Shape>
-void Dictionary<Derived, Shape>::ValueAtPut(InternalIndex entry, Object value) {
+void Dictionary<Derived, Shape>::ValueAtPut(InternalIndex entry,
+                                            Tagged<Object> value) {
   this->set(DerivedHashTable::EntryToIndex(entry) + Derived::kEntryValueIndex,
             value);
 }
 
 template <typename Derived, typename Shape>
-void Dictionary<Derived, Shape>::ValueAtPut(InternalIndex entry, Object value,
+void Dictionary<Derived, Shape>::ValueAtPut(InternalIndex entry,
+                                            Tagged<Object> value,
                                             SeqCstAccessTag tag) {
   this->set(DerivedHashTable::EntryToIndex(entry) + Derived::kEntryValueIndex,
             value, tag);
 }
 
 template <typename Derived, typename Shape>
-Object Dictionary<Derived, Shape>::ValueAtSwap(InternalIndex entry,
-                                               Object value,
-                                               SeqCstAccessTag tag) {
+Tagged<Object> Dictionary<Derived, Shape>::ValueAtSwap(InternalIndex entry,
+                                                       Tagged<Object> value,
+                                                       SeqCstAccessTag tag) {
   return this->swap(
       DerivedHashTable::EntryToIndex(entry) + Derived::kEntryValueIndex, value,
       tag);
 }
 
 template <typename Derived, typename Shape>
-Object Dictionary<Derived, Shape>::ValueAtCompareAndSwap(InternalIndex entry,
-                                                         Object expected,
-                                                         Object value,
-                                                         SeqCstAccessTag tag) {
+Tagged<Object> Dictionary<Derived, Shape>::ValueAtCompareAndSwap(
+    InternalIndex entry, Tagged<Object> expected, Tagged<Object> value,
+    SeqCstAccessTag tag) {
   return this->compare_and_swap(
       DerivedHashTable::EntryToIndex(entry) + Derived::kEntryValueIndex,
       expected, value, tag);
@@ -142,7 +143,7 @@ void BaseNameDictionary<Derived, Shape>::SetHash(int hash) {
 
 template <typename Derived, typename Shape>
 int BaseNameDictionary<Derived, Shape>::Hash() const {
-  Object hash_obj = this->get(kObjectHashIndex);
+  Tagged<Object> hash_obj = this->get(kObjectHashIndex);
   int hash = Smi::ToInt(hash_obj);
   DCHECK(PropertyArray::HashField::is_valid(hash));
   return hash;
@@ -169,14 +170,14 @@ SimpleNumberDictionary::SimpleNumberDictionary(Address ptr)
 }
 
 bool NumberDictionary::requires_slow_elements() {
-  Object max_index_object = get(kMaxNumberKeyIndex);
+  Tagged<Object> max_index_object = get(kMaxNumberKeyIndex);
   if (!IsSmi(max_index_object)) return false;
   return 0 != (Smi::ToInt(max_index_object) & kRequiresSlowElementsMask);
 }
 
 uint32_t NumberDictionary::max_number_key() {
   DCHECK(!requires_slow_elements());
-  Object max_index_object = get(kMaxNumberKeyIndex);
+  Tagged<Object> max_index_object = get(kMaxNumberKeyIndex);
   if (!IsSmi(max_index_object)) return 0;
   uint32_t value = static_cast<uint32_t>(Smi::ToInt(max_index_object));
   return value >> kRequiresSlowElementsTagSize;
@@ -188,14 +189,15 @@ void NumberDictionary::set_requires_slow_elements() {
 
 template <typename Derived, typename Shape>
 void Dictionary<Derived, Shape>::ClearEntry(InternalIndex entry) {
-  Object the_hole = this->GetReadOnlyRoots().the_hole_value();
+  Tagged<Object> the_hole = this->GetReadOnlyRoots().the_hole_value();
   PropertyDetails details = PropertyDetails::Empty();
   Derived::cast(*this)->SetEntry(entry, the_hole, the_hole, details);
 }
 
 template <typename Derived, typename Shape>
-void Dictionary<Derived, Shape>::SetEntry(InternalIndex entry, Object key,
-                                          Object value,
+void Dictionary<Derived, Shape>::SetEntry(InternalIndex entry,
+                                          Tagged<Object> key,
+                                          Tagged<Object> value,
                                           PropertyDetails details) {
   DCHECK(Dictionary::kEntrySize == 2 || Dictionary::kEntrySize == 3);
   DCHECK(!IsName(key) || details.dictionary_index() > 0);
@@ -233,7 +235,7 @@ void BaseDictionaryShape<Key>::DetailsAtPut(Tagged<Dictionary> dict,
             value.AsSmi());
 }
 
-Object GlobalDictionaryShape::Unwrap(Object object) {
+Tagged<Object> GlobalDictionaryShape::Unwrap(Tagged<Object> object) {
   return PropertyCell::cast(object)->name();
 }
 
@@ -241,12 +243,13 @@ Handle<Map> GlobalDictionary::GetMap(ReadOnlyRoots roots) {
   return roots.global_dictionary_map_handle();
 }
 
-Name NameDictionary::NameAt(InternalIndex entry) {
+Tagged<Name> NameDictionary::NameAt(InternalIndex entry) {
   PtrComprCageBase cage_base = GetPtrComprCageBase(*this);
   return NameAt(cage_base, entry);
 }
 
-Name NameDictionary::NameAt(PtrComprCageBase cage_base, InternalIndex entry) {
+Tagged<Name> NameDictionary::NameAt(PtrComprCageBase cage_base,
+                                    InternalIndex entry) {
   return Name::cast(KeyAt(cage_base, entry));
 }
 
@@ -265,38 +268,39 @@ void NameDictionary::set_flags(uint32_t flags) {
 BIT_FIELD_ACCESSORS(NameDictionary, flags, may_have_interesting_properties,
                     NameDictionary::MayHaveInterestingPropertiesBit)
 
-PropertyCell GlobalDictionary::CellAt(InternalIndex entry) {
+Tagged<PropertyCell> GlobalDictionary::CellAt(InternalIndex entry) {
   PtrComprCageBase cage_base = GetPtrComprCageBase(*this);
   return CellAt(cage_base, entry);
 }
 
-PropertyCell GlobalDictionary::CellAt(PtrComprCageBase cage_base,
-                                      InternalIndex entry) {
+Tagged<PropertyCell> GlobalDictionary::CellAt(PtrComprCageBase cage_base,
+                                              InternalIndex entry) {
   DCHECK(IsPropertyCell(KeyAt(cage_base, entry), cage_base));
   return PropertyCell::cast(KeyAt(cage_base, entry));
 }
 
-Name GlobalDictionary::NameAt(InternalIndex entry) {
+Tagged<Name> GlobalDictionary::NameAt(InternalIndex entry) {
   PtrComprCageBase cage_base = GetPtrComprCageBase(*this);
   return NameAt(cage_base, entry);
 }
 
-Name GlobalDictionary::NameAt(PtrComprCageBase cage_base, InternalIndex entry) {
+Tagged<Name> GlobalDictionary::NameAt(PtrComprCageBase cage_base,
+                                      InternalIndex entry) {
   return CellAt(cage_base, entry)->name(cage_base);
 }
 
-Object GlobalDictionary::ValueAt(InternalIndex entry) {
+Tagged<Object> GlobalDictionary::ValueAt(InternalIndex entry) {
   PtrComprCageBase cage_base = GetPtrComprCageBase(*this);
   return ValueAt(cage_base, entry);
 }
 
-Object GlobalDictionary::ValueAt(PtrComprCageBase cage_base,
-                                 InternalIndex entry) {
+Tagged<Object> GlobalDictionary::ValueAt(PtrComprCageBase cage_base,
+                                         InternalIndex entry) {
   return CellAt(cage_base, entry)->value(cage_base);
 }
 
-void GlobalDictionary::SetEntry(InternalIndex entry, Object key, Object value,
-                                PropertyDetails details) {
+void GlobalDictionary::SetEntry(InternalIndex entry, Tagged<Object> key,
+                                Tagged<Object> value, PropertyDetails details) {
   DCHECK_EQ(key, PropertyCell::cast(value)->name());
   set(EntryToIndex(entry) + kEntryKeyIndex, value);
   DetailsAtPut(entry, details);
@@ -307,11 +311,11 @@ void GlobalDictionary::ClearEntry(InternalIndex entry) {
   set(EntryToIndex(entry) + kEntryKeyIndex, the_hole);
 }
 
-void GlobalDictionary::ValueAtPut(InternalIndex entry, Object value) {
+void GlobalDictionary::ValueAtPut(InternalIndex entry, Tagged<Object> value) {
   set(EntryToIndex(entry), value);
 }
 
-bool NumberDictionaryBaseShape::IsMatch(uint32_t key, Object other) {
+bool NumberDictionaryBaseShape::IsMatch(uint32_t key, Tagged<Object> other) {
   DCHECK(IsNumber(other));
   return key == static_cast<uint32_t>(Object::Number(other));
 }
@@ -321,7 +325,7 @@ uint32_t NumberDictionaryBaseShape::Hash(ReadOnlyRoots roots, uint32_t key) {
 }
 
 uint32_t NumberDictionaryBaseShape::HashForObject(ReadOnlyRoots roots,
-                                                  Object other) {
+                                                  Tagged<Object> other) {
   DCHECK(IsNumber(other));
   return ComputeSeededHash(static_cast<uint32_t>(Object::Number(other)),
                            HashSeed(roots));
@@ -347,7 +351,7 @@ Handle<Map> SimpleNumberDictionary::GetMap(ReadOnlyRoots roots) {
   return roots.simple_number_dictionary_map_handle();
 }
 
-bool BaseNameDictionaryShape::IsMatch(Handle<Name> key, Object other) {
+bool BaseNameDictionaryShape::IsMatch(Handle<Name> key, Tagged<Object> other) {
   DCHECK(IsTheHole(other) || IsUniqueName(Name::cast(other)));
   DCHECK(IsUniqueName(*key));
   return *key == other;
@@ -359,19 +363,19 @@ uint32_t BaseNameDictionaryShape::Hash(ReadOnlyRoots roots, Handle<Name> key) {
 }
 
 uint32_t BaseNameDictionaryShape::HashForObject(ReadOnlyRoots roots,
-                                                Object other) {
+                                                Tagged<Object> other) {
   DCHECK(IsUniqueName(other));
   return Name::cast(other)->hash();
 }
 
-bool GlobalDictionaryShape::IsMatch(Handle<Name> key, Object other) {
+bool GlobalDictionaryShape::IsMatch(Handle<Name> key, Tagged<Object> other) {
   DCHECK(IsUniqueName(*key));
   DCHECK(IsUniqueName(PropertyCell::cast(other)->name()));
   return *key == PropertyCell::cast(other)->name();
 }
 
 uint32_t GlobalDictionaryShape::HashForObject(ReadOnlyRoots roots,
-                                              Object other) {
+                                              Tagged<Object> other) {
   return PropertyCell::cast(other)->name()->hash();
 }
 
diff --git a/src/objects/dictionary.h b/src/objects/dictionary.h
index ba8e4354d36..887c2157b0c 100644
--- a/src/objects/dictionary.h
+++ b/src/objects/dictionary.h
@@ -33,24 +33,29 @@ class EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE) Dictionary
 
  public:
   using Key = typename Shape::Key;
-  inline Object ValueAt(InternalIndex entry);
-  inline Object ValueAt(PtrComprCageBase cage_base, InternalIndex entry);
-  inline Object ValueAt(InternalIndex entry, SeqCstAccessTag);
-  inline Object ValueAt(PtrComprCageBase cage_base, InternalIndex entry,
-                        SeqCstAccessTag);
+  inline Tagged<Object> ValueAt(InternalIndex entry);
+  inline Tagged<Object> ValueAt(PtrComprCageBase cage_base,
+                                InternalIndex entry);
+  inline Tagged<Object> ValueAt(InternalIndex entry, SeqCstAccessTag);
+  inline Tagged<Object> ValueAt(PtrComprCageBase cage_base, InternalIndex entry,
+                                SeqCstAccessTag);
   // Returns {} if we would be reading out of the bounds of the object.
-  inline base::Optional<Object> TryValueAt(InternalIndex entry);
+  inline base::Optional<Tagged<Object>> TryValueAt(InternalIndex entry);
 
   // Set the value for entry.
-  inline void ValueAtPut(InternalIndex entry, Object value);
-  inline void ValueAtPut(InternalIndex entry, Object value, SeqCstAccessTag);
+  inline void ValueAtPut(InternalIndex entry, Tagged<Object> value);
+  inline void ValueAtPut(InternalIndex entry, Tagged<Object> value,
+                         SeqCstAccessTag);
 
   // Swap the value for the entry.
-  inline Object ValueAtSwap(InternalIndex entry, Object value, SeqCstAccessTag);
+  inline Tagged<Object> ValueAtSwap(InternalIndex entry, Tagged<Object> value,
+                                    SeqCstAccessTag);
 
   // Compare and swap the value for the entry.
-  inline Object ValueAtCompareAndSwap(InternalIndex entry, Object expected,
-                                      Object value, SeqCstAccessTag);
+  inline Tagged<Object> ValueAtCompareAndSwap(InternalIndex entry,
+                                              Tagged<Object> expected,
+                                              Tagged<Object> value,
+                                              SeqCstAccessTag);
 
   // Returns the property details for the property at entry.
   inline PropertyDetails DetailsAt(InternalIndex entry);
@@ -73,13 +78,13 @@ class EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE) Dictionary
   int NumberOfEnumerableProperties();
 
   // Returns the key (slow).
-  Object SlowReverseLookup(Object value);
+  Tagged<Object> SlowReverseLookup(Tagged<Object> value);
 
   inline void ClearEntry(InternalIndex entry);
 
   // Sets the entry to (key, value) pair.
-  inline void SetEntry(InternalIndex entry, Object key, Object value,
-                       PropertyDetails details);
+  inline void SetEntry(InternalIndex entry, Tagged<Object> key,
+                       Tagged<Object> value, PropertyDetails details);
 
   // Garbage collection support.
   inline ObjectSlot RawFieldOfValueAt(InternalIndex entry);
@@ -143,9 +148,10 @@ class BaseDictionaryShape : public BaseShape<Key> {
 
 class BaseNameDictionaryShape : public BaseDictionaryShape<Handle<Name>> {
  public:
-  static inline bool IsMatch(Handle<Name> key, Object other);
+  static inline bool IsMatch(Handle<Name> key, Tagged<Object> other);
   static inline uint32_t Hash(ReadOnlyRoots roots, Handle<Name> key);
-  static inline uint32_t HashForObject(ReadOnlyRoots roots, Object object);
+  static inline uint32_t HashForObject(ReadOnlyRoots roots,
+                                       Tagged<Object> object);
   template <AllocationType allocation = AllocationType::kYoung>
   static inline Handle<Object> AsHandle(Isolate* isolate, Handle<Name> key);
   template <AllocationType allocation = AllocationType::kOld>
@@ -231,8 +237,8 @@ class V8_EXPORT_PRIVATE NameDictionary
   static const int kEntryDetailsIndex = 2;
   static const int kInitialCapacity = 2;
 
-  inline Name NameAt(InternalIndex entry);
-  inline Name NameAt(PtrComprCageBase cage_base, InternalIndex entry);
+  inline Tagged<Name> NameAt(InternalIndex entry);
+  inline Tagged<Name> NameAt(PtrComprCageBase cage_base, InternalIndex entry);
 
   inline void set_hash(int hash);
   inline int hash() const;
@@ -259,8 +265,9 @@ class V8_EXPORT_PRIVATE NameDictionary
 
 class V8_EXPORT_PRIVATE GlobalDictionaryShape : public BaseNameDictionaryShape {
  public:
-  static inline bool IsMatch(Handle<Name> key, Object other);
-  static inline uint32_t HashForObject(ReadOnlyRoots roots, Object object);
+  static inline bool IsMatch(Handle<Name> key, Tagged<Object> other);
+  static inline uint32_t HashForObject(ReadOnlyRoots roots,
+                                       Tagged<Object> object);
 
   static const bool kMatchNeedsHoleCheck = true;
   static const int kPrefixSize = 2;
@@ -274,7 +281,7 @@ class V8_EXPORT_PRIVATE GlobalDictionaryShape : public BaseNameDictionaryShape {
   static inline void DetailsAtPut(Tagged<Dictionary> dict, InternalIndex entry,
                                   PropertyDetails value);
 
-  static inline Object Unwrap(Object key);
+  static inline Tagged<Object> Unwrap(Tagged<Object> key);
 };
 
 EXTERN_DECLARE_BASE_NAME_DICTIONARY(GlobalDictionary, GlobalDictionaryShape)
@@ -287,19 +294,23 @@ class V8_EXPORT_PRIVATE GlobalDictionary
   DECL_CAST(GlobalDictionary)
   DECL_PRINTER(GlobalDictionary)
 
-  inline Object ValueAt(InternalIndex entry);
-  inline Object ValueAt(PtrComprCageBase cage_base, InternalIndex entry);
-  inline PropertyCell CellAt(InternalIndex entry);
-  inline PropertyCell CellAt(PtrComprCageBase cage_base, InternalIndex entry);
-  inline void SetEntry(InternalIndex entry, Object key, Object value,
-                       PropertyDetails details);
+  inline Tagged<Object> ValueAt(InternalIndex entry);
+  inline Tagged<Object> ValueAt(PtrComprCageBase cage_base,
+                                InternalIndex entry);
+  inline Tagged<PropertyCell> CellAt(InternalIndex entry);
+  inline Tagged<PropertyCell> CellAt(PtrComprCageBase cage_base,
+                                     InternalIndex entry);
+  inline void SetEntry(InternalIndex entry, Tagged<Object> key,
+                       Tagged<Object> value, PropertyDetails details);
   inline void ClearEntry(InternalIndex entry);
-  inline Name NameAt(InternalIndex entry);
-  inline Name NameAt(PtrComprCageBase cage_base, InternalIndex entry);
-  inline void ValueAtPut(InternalIndex entry, Object value);
+  inline Tagged<Name> NameAt(InternalIndex entry);
+  inline Tagged<Name> NameAt(PtrComprCageBase cage_base, InternalIndex entry);
+  inline void ValueAtPut(InternalIndex entry, Tagged<Object> value);
 
-  base::Optional<PropertyCell> TryFindPropertyCellForConcurrentLookupIterator(
-      Isolate* isolate, Handle<Name> name, RelaxedLoadTag tag);
+  base::Optional<Tagged<PropertyCell>>
+  TryFindPropertyCellForConcurrentLookupIterator(Isolate* isolate,
+                                                 Handle<Name> name,
+                                                 RelaxedLoadTag tag);
 
   OBJECT_CONSTRUCTORS(
       GlobalDictionary,
@@ -308,14 +319,15 @@ class V8_EXPORT_PRIVATE GlobalDictionary
 
 class NumberDictionaryBaseShape : public BaseDictionaryShape<uint32_t> {
  public:
-  static inline bool IsMatch(uint32_t key, Object other);
+  static inline bool IsMatch(uint32_t key, Tagged<Object> other);
   template <AllocationType allocation = AllocationType::kYoung>
   static inline Handle<Object> AsHandle(Isolate* isolate, uint32_t key);
   template <AllocationType allocation = AllocationType::kOld>
   static inline Handle<Object> AsHandle(LocalIsolate* isolate, uint32_t key);
 
   static inline uint32_t Hash(ReadOnlyRoots roots, uint32_t key);
-  static inline uint32_t HashForObject(ReadOnlyRoots roots, Object object);
+  static inline uint32_t HashForObject(ReadOnlyRoots roots,
+                                       Tagged<Object> object);
 
   static const bool kMatchNeedsHoleCheck = true;
 };
@@ -397,7 +409,7 @@ class NumberDictionary
   void UpdateMaxNumberKey(uint32_t key, Handle<JSObject> dictionary_holder);
 
   // Sorting support
-  void CopyValuesTo(FixedArray elements);
+  void CopyValuesTo(Tagged<FixedArray> elements);
 
   // If slow elements are required we will never go back to fast-case
   // for the elements kept in this dictionary.  We require slow
diff --git a/src/objects/elements-inl.h b/src/objects/elements-inl.h
index a3407c2c802..20951c35a80 100644
--- a/src/objects/elements-inl.h
+++ b/src/objects/elements-inl.h
@@ -29,7 +29,8 @@ inline MaybeHandle<FixedArray> ElementsAccessor::PrependElementIndices(
                                convert, filter);
 }
 
-inline bool ElementsAccessor::HasElement(JSObject holder, uint32_t index,
+inline bool ElementsAccessor::HasElement(Tagged<JSObject> holder,
+                                         uint32_t index,
                                          PropertyFilter filter) {
   return HasElement(holder, index, holder->elements(), filter);
 }
diff --git a/src/objects/elements.cc b/src/objects/elements.cc
index e3961a45177..948165de2bb 100644
--- a/src/objects/elements.cc
+++ b/src/objects/elements.cc
@@ -181,7 +181,8 @@ MaybeHandle<Object> ThrowArrayLengthRangeError(Isolate* isolate) {
                   Object);
 }
 
-WriteBarrierMode GetWriteBarrierMode(FixedArrayBase elements, ElementsKind kind,
+WriteBarrierMode GetWriteBarrierMode(Tagged<FixedArrayBase> elements,
+                                     ElementsKind kind,
                                      const DisallowGarbageCollection& promise) {
   if (IsSmiElementsKind(kind)) return SKIP_WRITE_BARRIER;
   if (IsDoubleElementsKind(kind)) return SKIP_WRITE_BARRIER;
@@ -194,10 +195,12 @@ WriteBarrierMode GetWriteBarrierMode(FixedArrayBase elements, ElementsKind kind,
 // destination array with the hole.
 constexpr int kCopyToEndAndInitializeToHole = -1;
 
-void CopyObjectToObjectElements(Isolate* isolate, FixedArrayBase from_base,
+void CopyObjectToObjectElements(Isolate* isolate,
+                                Tagged<FixedArrayBase> from_base,
                                 ElementsKind from_kind, uint32_t from_start,
-                                FixedArrayBase to_base, ElementsKind to_kind,
-                                uint32_t to_start, int raw_copy_size) {
+                                Tagged<FixedArrayBase> to_base,
+                                ElementsKind to_kind, uint32_t to_start,
+                                int raw_copy_size) {
   ReadOnlyRoots roots(isolate);
   DCHECK(to_base->map() != roots.fixed_cow_array_map());
   DisallowGarbageCollection no_gc;
@@ -229,8 +232,10 @@ void CopyObjectToObjectElements(Isolate* isolate, FixedArrayBase from_base,
                    write_barrier_mode);
 }
 
-void CopyDictionaryToObjectElements(Isolate* isolate, FixedArrayBase from_base,
-                                    uint32_t from_start, FixedArrayBase to_base,
+void CopyDictionaryToObjectElements(Isolate* isolate,
+                                    Tagged<FixedArrayBase> from_base,
+                                    uint32_t from_start,
+                                    Tagged<FixedArrayBase> to_base,
                                     ElementsKind to_kind, uint32_t to_start,
                                     int raw_copy_size) {
   DisallowGarbageCollection no_gc;
@@ -258,7 +263,7 @@ void CopyDictionaryToObjectElements(Isolate* isolate, FixedArrayBase from_base,
   for (int i = 0; i < copy_size; i++) {
     InternalIndex entry = from->FindEntry(isolate, i + from_start);
     if (entry.is_found()) {
-      Object value = from->ValueAt(entry);
+      Tagged<Object> value = from->ValueAt(entry);
       DCHECK(!IsTheHole(value, isolate));
       to->set(i + to_start, value, write_barrier_mode);
     } else {
@@ -270,8 +275,10 @@ void CopyDictionaryToObjectElements(Isolate* isolate, FixedArrayBase from_base,
 // NOTE: this method violates the handlified function signature convention:
 // raw pointer parameters in the function that allocates.
 // See ElementsAccessorBase::CopyElements() for details.
-void CopyDoubleToObjectElements(Isolate* isolate, FixedArrayBase from_base,
-                                uint32_t from_start, FixedArrayBase to_base,
+void CopyDoubleToObjectElements(Isolate* isolate,
+                                Tagged<FixedArrayBase> from_base,
+                                uint32_t from_start,
+                                Tagged<FixedArrayBase> to_base,
                                 uint32_t to_start, int raw_copy_size) {
   int copy_size = raw_copy_size;
   if (raw_copy_size < 0) {
@@ -314,9 +321,10 @@ void CopyDoubleToObjectElements(Isolate* isolate, FixedArrayBase from_base,
   }
 }
 
-void CopyDoubleToDoubleElements(FixedArrayBase from_base, uint32_t from_start,
-                                FixedArrayBase to_base, uint32_t to_start,
-                                int raw_copy_size) {
+void CopyDoubleToDoubleElements(Tagged<FixedArrayBase> from_base,
+                                uint32_t from_start,
+                                Tagged<FixedArrayBase> to_base,
+                                uint32_t to_start, int raw_copy_size) {
   DisallowGarbageCollection no_gc;
   int copy_size = raw_copy_size;
   if (raw_copy_size < 0) {
@@ -350,8 +358,9 @@ void CopyDoubleToDoubleElements(FixedArrayBase from_base, uint32_t from_start,
 #endif
 }
 
-void CopySmiToDoubleElements(FixedArrayBase from_base, uint32_t from_start,
-                             FixedArrayBase to_base, uint32_t to_start,
+void CopySmiToDoubleElements(Tagged<FixedArrayBase> from_base,
+                             uint32_t from_start,
+                             Tagged<FixedArrayBase> to_base, uint32_t to_start,
                              int raw_copy_size) {
   DisallowGarbageCollection no_gc;
   int copy_size = raw_copy_size;
@@ -367,10 +376,10 @@ void CopySmiToDoubleElements(FixedArrayBase from_base, uint32_t from_start,
   if (copy_size == 0) return;
   FixedArray from = FixedArray::cast(from_base);
   FixedDoubleArray to = FixedDoubleArray::cast(to_base);
-  Object the_hole = from->GetReadOnlyRoots().the_hole_value();
+  Tagged<Object> the_hole = from->GetReadOnlyRoots().the_hole_value();
   for (uint32_t from_end = from_start + static_cast<uint32_t>(copy_size);
        from_start < from_end; from_start++, to_start++) {
-    Object hole_or_smi = from->get(from_start);
+    Tagged<Object> hole_or_smi = from->get(from_start);
     if (hole_or_smi == the_hole) {
       to->set_the_hole(to_start);
     } else {
@@ -379,8 +388,9 @@ void CopySmiToDoubleElements(FixedArrayBase from_base, uint32_t from_start,
   }
 }
 
-void CopyPackedSmiToDoubleElements(FixedArrayBase from_base,
-                                   uint32_t from_start, FixedArrayBase to_base,
+void CopyPackedSmiToDoubleElements(Tagged<FixedArrayBase> from_base,
+                                   uint32_t from_start,
+                                   Tagged<FixedArrayBase> to_base,
                                    uint32_t to_start, int packed_size,
                                    int raw_copy_size) {
   DisallowGarbageCollection no_gc;
@@ -405,15 +415,16 @@ void CopyPackedSmiToDoubleElements(FixedArrayBase from_base,
   FixedDoubleArray to = FixedDoubleArray::cast(to_base);
   for (uint32_t from_end = from_start + static_cast<uint32_t>(packed_size);
        from_start < from_end; from_start++, to_start++) {
-    Object smi = from->get(from_start);
+    Tagged<Object> smi = from->get(from_start);
     DCHECK(!IsTheHole(smi));
     to->set(to_start, Smi::ToInt(smi));
   }
 }
 
-void CopyObjectToDoubleElements(FixedArrayBase from_base, uint32_t from_start,
-                                FixedArrayBase to_base, uint32_t to_start,
-                                int raw_copy_size) {
+void CopyObjectToDoubleElements(Tagged<FixedArrayBase> from_base,
+                                uint32_t from_start,
+                                Tagged<FixedArrayBase> to_base,
+                                uint32_t to_start, int raw_copy_size) {
   DisallowGarbageCollection no_gc;
   int copy_size = raw_copy_size;
   if (raw_copy_size < 0) {
@@ -428,10 +439,10 @@ void CopyObjectToDoubleElements(FixedArrayBase from_base, uint32_t from_start,
   if (copy_size == 0) return;
   FixedArray from = FixedArray::cast(from_base);
   FixedDoubleArray to = FixedDoubleArray::cast(to_base);
-  Object the_hole = from->GetReadOnlyRoots().the_hole_value();
+  Tagged<Object> the_hole = from->GetReadOnlyRoots().the_hole_value();
   for (uint32_t from_end = from_start + copy_size; from_start < from_end;
        from_start++, to_start++) {
-    Object hole_or_object = from->get(from_start);
+    Tagged<Object> hole_or_object = from->get(from_start);
     if (hole_or_object == the_hole) {
       to->set_the_hole(to_start);
     } else {
@@ -440,8 +451,10 @@ void CopyObjectToDoubleElements(FixedArrayBase from_base, uint32_t from_start,
   }
 }
 
-void CopyDictionaryToDoubleElements(Isolate* isolate, FixedArrayBase from_base,
-                                    uint32_t from_start, FixedArrayBase to_base,
+void CopyDictionaryToDoubleElements(Isolate* isolate,
+                                    Tagged<FixedArrayBase> from_base,
+                                    uint32_t from_start,
+                                    Tagged<FixedArrayBase> to_base,
                                     uint32_t to_start, int raw_copy_size) {
   DisallowGarbageCollection no_gc;
   NumberDictionary from = NumberDictionary::cast(from_base);
@@ -479,11 +492,13 @@ void SortIndices(Isolate* isolate, Handle<FixedArray> indices,
   AtomicSlot end(start + sort_size);
   std::sort(start, end, [isolate](Tagged_t elementA, Tagged_t elementB) {
 #ifdef V8_COMPRESS_POINTERS
-    Object a(V8HeapCompressionScheme::DecompressTagged(isolate, elementA));
-    Object b(V8HeapCompressionScheme::DecompressTagged(isolate, elementB));
+    Tagged<Object> a(
+        V8HeapCompressionScheme::DecompressTagged(isolate, elementA));
+    Tagged<Object> b(
+        V8HeapCompressionScheme::DecompressTagged(isolate, elementB));
 #else
-    Object a(elementA);
-    Object b(elementB);
+    Tagged<Object> a(elementA);
+    Tagged<Object> b(elementB);
 #endif
     if (IsSmi(a) || !IsUndefined(a, isolate)) {
       if (!IsSmi(b) && IsUndefined(b, isolate)) {
@@ -540,11 +555,12 @@ Maybe<int64_t> IndexOfValueSlowPath(Isolate* isolate, Handle<JSObject> receiver,
 // that take an entry (instead of an index) as an argument.
 class InternalElementsAccessor : public ElementsAccessor {
  public:
-  InternalIndex GetEntryForIndex(Isolate* isolate, JSObject holder,
-                                 FixedArrayBase backing_store,
+  InternalIndex GetEntryForIndex(Isolate* isolate, Tagged<JSObject> holder,
+                                 Tagged<FixedArrayBase> backing_store,
                                  size_t index) override = 0;
 
-  PropertyDetails GetDetails(JSObject holder, InternalIndex entry) override = 0;
+  PropertyDetails GetDetails(Tagged<JSObject> holder,
+                             InternalIndex entry) override = 0;
 };
 
 // Base class for element handler implementations. Contains the
@@ -576,16 +592,16 @@ class ElementsAccessorBase : public InternalElementsAccessor {
 
   static ElementsKind kind() { return ElementsTraits::Kind; }
 
-  static void ValidateContents(JSObject holder, size_t length) {}
+  static void ValidateContents(Tagged<JSObject> holder, size_t length) {}
 
-  static void ValidateImpl(JSObject holder) {
-    FixedArrayBase fixed_array_base = holder->elements();
+  static void ValidateImpl(Tagged<JSObject> holder) {
+    Tagged<FixedArrayBase> fixed_array_base = holder->elements();
     if (!IsHeapObject(fixed_array_base)) return;
     // Arrays that have been shifted in place can't be verified.
     if (IsFreeSpaceOrFiller(fixed_array_base)) return;
     size_t length = 0;
     if (IsJSArray(holder)) {
-      Object length_obj = JSArray::cast(holder)->length();
+      Tagged<Object> length_obj = JSArray::cast(holder)->length();
       if (IsSmi(length_obj)) {
         length = Smi::ToInt(length_obj);
       }
@@ -597,40 +613,43 @@ class ElementsAccessorBase : public InternalElementsAccessor {
     Subclass::ValidateContents(holder, length);
   }
 
-  void Validate(JSObject holder) final {
+  void Validate(Tagged<JSObject> holder) final {
     DisallowGarbageCollection no_gc;
     Subclass::ValidateImpl(holder);
   }
 
-  bool HasElement(JSObject holder, uint32_t index, FixedArrayBase backing_store,
+  bool HasElement(Tagged<JSObject> holder, uint32_t index,
+                  Tagged<FixedArrayBase> backing_store,
                   PropertyFilter filter) final {
     return Subclass::HasElementImpl(holder->GetIsolate(), holder, index,
                                     backing_store, filter);
   }
 
-  static bool HasElementImpl(Isolate* isolate, JSObject holder, size_t index,
-                             FixedArrayBase backing_store,
+  static bool HasElementImpl(Isolate* isolate, Tagged<JSObject> holder,
+                             size_t index, Tagged<FixedArrayBase> backing_store,
                              PropertyFilter filter = ALL_PROPERTIES) {
     return Subclass::GetEntryForIndexImpl(isolate, holder, backing_store, index,
                                           filter)
         .is_found();
   }
 
-  bool HasEntry(JSObject holder, InternalIndex entry) final {
+  bool HasEntry(Tagged<JSObject> holder, InternalIndex entry) final {
     return Subclass::HasEntryImpl(holder->GetIsolate(), holder->elements(),
                                   entry);
   }
 
-  static bool HasEntryImpl(Isolate* isolate, FixedArrayBase backing_store,
+  static bool HasEntryImpl(Isolate* isolate,
+                           Tagged<FixedArrayBase> backing_store,
                            InternalIndex entry) {
     UNIMPLEMENTED();
   }
 
-  bool HasAccessors(JSObject holder) final {
+  bool HasAccessors(Tagged<JSObject> holder) final {
     return Subclass::HasAccessorsImpl(holder, holder->elements());
   }
 
-  static bool HasAccessorsImpl(JSObject holder, FixedArrayBase backing_store) {
+  static bool HasAccessorsImpl(Tagged<JSObject> holder,
+                               Tagged<FixedArrayBase> backing_store) {
     return false;
   }
 
@@ -645,9 +664,10 @@ class ElementsAccessorBase : public InternalElementsAccessor {
     return Subclass::GetImpl(isolate, holder->elements(), entry);
   }
 
-  static Handle<Object> GetImpl(Isolate* isolate, FixedArrayBase backing_store,
+  static Handle<Object> GetImpl(Isolate* isolate,
+                                Tagged<FixedArrayBase> backing_store,
                                 InternalIndex entry) {
-    return handle(BackingStore::cast(backing_store).get(entry.as_int()),
+    return handle(BackingStore::cast(backing_store)->get(entry.as_int()),
                   isolate);
   }
 
@@ -657,45 +677,45 @@ class ElementsAccessorBase : public InternalElementsAccessor {
                                            tag);
   }
 
-  static Handle<Object> GetAtomicInternalImpl(Isolate* isolate,
-                                              FixedArrayBase backing_store,
-                                              InternalIndex entry,
-                                              SeqCstAccessTag tag) {
+  static Handle<Object> GetAtomicInternalImpl(
+      Isolate* isolate, Tagged<FixedArrayBase> backing_store,
+      InternalIndex entry, SeqCstAccessTag tag) {
     UNREACHABLE();
   }
 
-  void SetAtomic(Handle<JSObject> holder, InternalIndex entry, Object value,
-                 SeqCstAccessTag tag) final {
+  void SetAtomic(Handle<JSObject> holder, InternalIndex entry,
+                 Tagged<Object> value, SeqCstAccessTag tag) final {
     Subclass::SetAtomicInternalImpl(holder->elements(), entry, value, tag);
   }
 
-  static void SetAtomicInternalImpl(FixedArrayBase backing_store,
-                                    InternalIndex entry, Object value,
+  static void SetAtomicInternalImpl(Tagged<FixedArrayBase> backing_store,
+                                    InternalIndex entry, Tagged<Object> value,
                                     SeqCstAccessTag tag) {
     UNREACHABLE();
   }
 
   Handle<Object> SwapAtomic(Isolate* isolate, Handle<JSObject> holder,
-                            InternalIndex entry, Object value,
+                            InternalIndex entry, Tagged<Object> value,
                             SeqCstAccessTag tag) final {
     return Subclass::SwapAtomicInternalImpl(isolate, holder->elements(), entry,
                                             value, tag);
   }
 
-  static Handle<Object> SwapAtomicInternalImpl(Isolate* isolate,
-                                               FixedArrayBase backing_store,
-                                               InternalIndex entry,
-                                               Object value,
-                                               SeqCstAccessTag tag) {
+  static Handle<Object> SwapAtomicInternalImpl(
+      Isolate* isolate, Tagged<FixedArrayBase> backing_store,
+      InternalIndex entry, Tagged<Object> value, SeqCstAccessTag tag) {
     UNREACHABLE();
   }
 
   Handle<Object> CompareAndSwapAtomic(Isolate* isolate, Handle<JSObject> holder,
-                                      InternalIndex entry, Object expected,
-                                      Object value, SeqCstAccessTag tag) final {
+                                      InternalIndex entry,
+                                      Tagged<Object> expected,
+                                      Tagged<Object> value,
+                                      SeqCstAccessTag tag) final {
     return handle(HeapObject::SeqCst_CompareAndSwapField(
                       expected, value,
-                      [=](Object expected_value, Object new_value) {
+                      [=](Tagged<Object> expected_value,
+                          Tagged<Object> new_value) {
                         return Subclass::CompareAndSwapAtomicInternalImpl(
                             holder->elements(), entry, expected_value,
                             new_value, tag);
@@ -703,14 +723,14 @@ class ElementsAccessorBase : public InternalElementsAccessor {
                   isolate);
   }
 
-  static Object CompareAndSwapAtomicInternalImpl(FixedArrayBase backing_store,
-                                                 InternalIndex entry,
-                                                 Object expected, Object value,
-                                                 SeqCstAccessTag tag) {
+  static Tagged<Object> CompareAndSwapAtomicInternalImpl(
+      Tagged<FixedArrayBase> backing_store, InternalIndex entry,
+      Tagged<Object> expected, Tagged<Object> value, SeqCstAccessTag tag) {
     UNREACHABLE();
   }
 
-  void Set(Handle<JSObject> holder, InternalIndex entry, Object value) final {
+  void Set(Handle<JSObject> holder, InternalIndex entry,
+           Tagged<Object> value) final {
     Subclass::SetImpl(holder, entry, value);
   }
 
@@ -821,11 +841,11 @@ class ElementsAccessorBase : public InternalElementsAccessor {
         isolate->heap()->RightTrimFixedArray(*backing_store, elements_to_trim);
         // Fill the non-trimmed elements with holes.
         BackingStore::cast(*backing_store)
-            .FillWithHoles(length,
-                           std::min(old_length, capacity - elements_to_trim));
+            ->FillWithHoles(length,
+                            std::min(old_length, capacity - elements_to_trim));
       } else {
         // Otherwise, fill the unused tail with holes.
-        BackingStore::cast(*backing_store).FillWithHoles(length, old_length);
+        BackingStore::cast(*backing_store)->FillWithHoles(length, old_length);
       }
     } else {
       // Check whether the backing store should be expanded.
@@ -839,16 +859,17 @@ class ElementsAccessorBase : public InternalElementsAccessor {
     return Just(true);
   }
 
-  size_t NumberOfElements(JSObject receiver) final {
+  size_t NumberOfElements(Tagged<JSObject> receiver) final {
     return Subclass::NumberOfElementsImpl(receiver, receiver->elements());
   }
 
-  static uint32_t NumberOfElementsImpl(JSObject receiver,
-                                       FixedArrayBase backing_store) {
+  static uint32_t NumberOfElementsImpl(Tagged<JSObject> receiver,
+                                       Tagged<FixedArrayBase> backing_store) {
     UNREACHABLE();
   }
 
-  static size_t GetMaxIndex(JSObject receiver, FixedArrayBase elements) {
+  static size_t GetMaxIndex(Tagged<JSObject> receiver,
+                            Tagged<FixedArrayBase> elements) {
     if (IsJSArray(receiver)) {
       DCHECK(IsSmi(JSArray::cast(receiver)->length()));
       return static_cast<uint32_t>(
@@ -857,8 +878,8 @@ class ElementsAccessorBase : public InternalElementsAccessor {
     return Subclass::GetCapacityImpl(receiver, elements);
   }
 
-  static size_t GetMaxNumberOfEntries(JSObject receiver,
-                                      FixedArrayBase elements) {
+  static size_t GetMaxNumberOfEntries(Tagged<JSObject> receiver,
+                                      Tagged<FixedArrayBase> elements) {
     return Subclass::GetMaxIndex(receiver, elements);
   }
 
@@ -1041,14 +1062,14 @@ class ElementsAccessorBase : public InternalElementsAccessor {
     Subclass::DeleteImpl(obj, entry);
   }
 
-  static void CopyElementsImpl(Isolate* isolate, FixedArrayBase from,
-                               uint32_t from_start, FixedArrayBase to,
+  static void CopyElementsImpl(Isolate* isolate, Tagged<FixedArrayBase> from,
+                               uint32_t from_start, Tagged<FixedArrayBase> to,
                                ElementsKind from_kind, uint32_t to_start,
                                int packed_size, int copy_size) {
     UNREACHABLE();
   }
 
-  void CopyElements(JSObject from_holder, uint32_t from_start,
+  void CopyElements(Tagged<JSObject> from_holder, uint32_t from_start,
                     ElementsKind from_kind, Handle<FixedArrayBase> to,
                     uint32_t to_start, int copy_size) final {
     int packed_size = kPackedSizeNotKnown;
@@ -1060,7 +1081,7 @@ class ElementsAccessorBase : public InternalElementsAccessor {
         packed_size = copy_size;
       }
     }
-    FixedArrayBase from = from_holder->elements();
+    Tagged<FixedArrayBase> from = from_holder->elements();
     // NOTE: the Subclass::CopyElementsImpl() methods
     // violate the handlified function signature convention:
     // raw pointer parameters in the function that allocates. This is done
@@ -1081,27 +1102,28 @@ class ElementsAccessorBase : public InternalElementsAccessor {
                                0, kPackedSizeNotKnown, size);
   }
 
-  void CopyTypedArrayElementsSlice(JSTypedArray source,
-                                   JSTypedArray destination, size_t start,
-                                   size_t end) override {
+  void CopyTypedArrayElementsSlice(Tagged<JSTypedArray> source,
+                                   Tagged<JSTypedArray> destination,
+                                   size_t start, size_t end) override {
     Subclass::CopyTypedArrayElementsSliceImpl(source, destination, start, end);
   }
 
-  static void CopyTypedArrayElementsSliceImpl(JSTypedArray source,
-                                              JSTypedArray destination,
+  static void CopyTypedArrayElementsSliceImpl(Tagged<JSTypedArray> source,
+                                              Tagged<JSTypedArray> destination,
                                               size_t start, size_t end) {
     UNREACHABLE();
   }
 
-  Object CopyElements(Handle<Object> source, Handle<JSObject> destination,
-                      size_t length, size_t offset) final {
+  Tagged<Object> CopyElements(Handle<Object> source,
+                              Handle<JSObject> destination, size_t length,
+                              size_t offset) final {
     return Subclass::CopyElementsHandleImpl(source, destination, length,
                                             offset);
   }
 
-  static Object CopyElementsHandleImpl(Handle<Object> source,
-                                       Handle<JSObject> destination,
-                                       size_t length, size_t offset) {
+  static Tagged<Object> CopyElementsHandleImpl(Handle<Object> source,
+                                               Handle<JSObject> destination,
+                                               size_t length, size_t offset) {
     UNREACHABLE();
   }
 
@@ -1338,12 +1360,13 @@ class ElementsAccessorBase : public InternalElementsAccessor {
                                                      convert);
   }
 
-  static uint32_t GetCapacityImpl(JSObject holder,
-                                  FixedArrayBase backing_store) {
+  static uint32_t GetCapacityImpl(Tagged<JSObject> holder,
+                                  Tagged<FixedArrayBase> backing_store) {
     return backing_store->length();
   }
 
-  size_t GetCapacity(JSObject holder, FixedArrayBase backing_store) final {
+  size_t GetCapacity(Tagged<JSObject> holder,
+                     Tagged<FixedArrayBase> backing_store) final {
     return Subclass::GetCapacityImpl(holder, backing_store);
   }
 
@@ -1398,14 +1421,16 @@ class ElementsAccessorBase : public InternalElementsAccessor {
     return Subclass::LastIndexOfValueImpl(receiver, value, start_from);
   }
 
-  static void ReverseImpl(JSObject receiver) { UNREACHABLE(); }
+  static void ReverseImpl(Tagged<JSObject> receiver) { UNREACHABLE(); }
 
-  void Reverse(JSObject receiver) final { Subclass::ReverseImpl(receiver); }
+  void Reverse(Tagged<JSObject> receiver) final {
+    Subclass::ReverseImpl(receiver);
+  }
 
-  static InternalIndex GetEntryForIndexImpl(Isolate* isolate, JSObject holder,
-                                            FixedArrayBase backing_store,
-                                            size_t index,
-                                            PropertyFilter filter) {
+  static InternalIndex GetEntryForIndexImpl(
+      Isolate* isolate, Tagged<JSObject> holder,
+      Tagged<FixedArrayBase> backing_store, size_t index,
+      PropertyFilter filter) {
     DCHECK(IsFastElementsKind(kind()) ||
            IsAnyNonextensibleElementsKind(kind()));
     size_t length = Subclass::GetMaxIndex(holder, backing_store);
@@ -1415,7 +1440,7 @@ class ElementsAccessorBase : public InternalElementsAccessor {
           index <= static_cast<size_t>(std::numeric_limits<int>::max()));
       return index < length &&
                      !BackingStore::cast(backing_store)
-                          .is_the_hole(isolate, static_cast<int>(index))
+                          ->is_the_hole(isolate, static_cast<int>(index))
                  ? InternalIndex(index)
                  : InternalIndex::NotFound();
     } else {
@@ -1423,25 +1448,27 @@ class ElementsAccessorBase : public InternalElementsAccessor {
     }
   }
 
-  InternalIndex GetEntryForIndex(Isolate* isolate, JSObject holder,
-                                 FixedArrayBase backing_store,
+  InternalIndex GetEntryForIndex(Isolate* isolate, Tagged<JSObject> holder,
+                                 Tagged<FixedArrayBase> backing_store,
                                  size_t index) final {
     return Subclass::GetEntryForIndexImpl(isolate, holder, backing_store, index,
                                           ALL_PROPERTIES);
   }
 
-  static PropertyDetails GetDetailsImpl(FixedArrayBase backing_store,
+  static PropertyDetails GetDetailsImpl(Tagged<FixedArrayBase> backing_store,
                                         InternalIndex entry) {
     return PropertyDetails(PropertyKind::kData, NONE,
                            PropertyCellType::kNoCell);
   }
 
-  static PropertyDetails GetDetailsImpl(JSObject holder, InternalIndex entry) {
+  static PropertyDetails GetDetailsImpl(Tagged<JSObject> holder,
+                                        InternalIndex entry) {
     return PropertyDetails(PropertyKind::kData, NONE,
                            PropertyCellType::kNoCell);
   }
 
-  PropertyDetails GetDetails(JSObject holder, InternalIndex entry) final {
+  PropertyDetails GetDetails(Tagged<JSObject> holder,
+                             InternalIndex entry) final {
     return Subclass::GetDetailsImpl(holder, entry);
   }
 
@@ -1462,18 +1489,19 @@ class DictionaryElementsAccessor
     : public ElementsAccessorBase<DictionaryElementsAccessor,
                                   ElementsKindTraits<DICTIONARY_ELEMENTS>> {
  public:
-  static uint32_t GetMaxIndex(JSObject receiver, FixedArrayBase elements) {
+  static uint32_t GetMaxIndex(Tagged<JSObject> receiver,
+                              Tagged<FixedArrayBase> elements) {
     // We cannot properly estimate this for dictionaries.
     UNREACHABLE();
   }
 
-  static uint32_t GetMaxNumberOfEntries(JSObject receiver,
-                                        FixedArrayBase backing_store) {
+  static uint32_t GetMaxNumberOfEntries(Tagged<JSObject> receiver,
+                                        Tagged<FixedArrayBase> backing_store) {
     return NumberOfElementsImpl(receiver, backing_store);
   }
 
-  static uint32_t NumberOfElementsImpl(JSObject receiver,
-                                       FixedArrayBase backing_store) {
+  static uint32_t NumberOfElementsImpl(Tagged<JSObject> receiver,
+                                       Tagged<FixedArrayBase> backing_store) {
     NumberDictionary dict = NumberDictionary::cast(backing_store);
     return dict->NumberOfElements();
   }
@@ -1493,7 +1521,7 @@ class DictionaryElementsAccessor
           // Find last non-deletable element in range of elements to be
           // deleted and adjust range accordingly.
           for (InternalIndex entry : dict->IterateEntries()) {
-            Object index = dict->KeyAt(isolate, entry);
+            Tagged<Object> index = dict->KeyAt(isolate, entry);
             if (dict->IsKey(roots, index)) {
               uint32_t number = static_cast<uint32_t>(Object::Number(index));
               if (length <= number && number < old_length) {
@@ -1511,7 +1539,7 @@ class DictionaryElementsAccessor
           // Remove elements that should be deleted.
           int removed_entries = 0;
           for (InternalIndex entry : dict->IterateEntries()) {
-            Object index = dict->KeyAt(isolate, entry);
+            Tagged<Object> index = dict->KeyAt(isolate, entry);
             if (dict->IsKey(roots, index)) {
               uint32_t number = static_cast<uint32_t>(Object::Number(index));
               if (length <= number && number < old_length) {
@@ -1534,8 +1562,8 @@ class DictionaryElementsAccessor
     return Just(true);
   }
 
-  static void CopyElementsImpl(Isolate* isolate, FixedArrayBase from,
-                               uint32_t from_start, FixedArrayBase to,
+  static void CopyElementsImpl(Isolate* isolate, Tagged<FixedArrayBase> from,
+                               uint32_t from_start, Tagged<FixedArrayBase> to,
                                ElementsKind from_kind, uint32_t to_start,
                                int packed_size, int copy_size) {
     UNREACHABLE();
@@ -1548,14 +1576,15 @@ class DictionaryElementsAccessor
     obj->set_elements(*dict);
   }
 
-  static bool HasAccessorsImpl(JSObject holder, FixedArrayBase backing_store) {
+  static bool HasAccessorsImpl(Tagged<JSObject> holder,
+                               Tagged<FixedArrayBase> backing_store) {
     DisallowGarbageCollection no_gc;
     NumberDictionary dict = NumberDictionary::cast(backing_store);
     if (!dict->requires_slow_elements()) return false;
     PtrComprCageBase cage_base = GetPtrComprCageBase(holder);
     ReadOnlyRoots roots = holder->GetReadOnlyRoots(cage_base);
     for (InternalIndex i : dict->IterateEntries()) {
-      Object key = dict->KeyAt(cage_base, i);
+      Tagged<Object> key = dict->KeyAt(cage_base, i);
       if (!dict.IsKey(roots, key)) continue;
       PropertyDetails details = dict->DetailsAt(i);
       if (details.kind() == PropertyKind::kAccessor) return true;
@@ -1563,54 +1592,52 @@ class DictionaryElementsAccessor
     return false;
   }
 
-  static Object GetRaw(FixedArrayBase store, InternalIndex entry) {
+  static Tagged<Object> GetRaw(Tagged<FixedArrayBase> store,
+                               InternalIndex entry) {
     NumberDictionary backing_store = NumberDictionary::cast(store);
     return backing_store->ValueAt(entry);
   }
 
-  static Handle<Object> GetImpl(Isolate* isolate, FixedArrayBase backing_store,
+  static Handle<Object> GetImpl(Isolate* isolate,
+                                Tagged<FixedArrayBase> backing_store,
                                 InternalIndex entry) {
     return handle(GetRaw(backing_store, entry), isolate);
   }
 
-  static Handle<Object> GetAtomicInternalImpl(Isolate* isolate,
-                                              FixedArrayBase backing_store,
-                                              InternalIndex entry,
-                                              SeqCstAccessTag tag) {
+  static Handle<Object> GetAtomicInternalImpl(
+      Isolate* isolate, Tagged<FixedArrayBase> backing_store,
+      InternalIndex entry, SeqCstAccessTag tag) {
     return handle(NumberDictionary::cast(backing_store)->ValueAt(entry, tag),
                   isolate);
   }
 
   static inline void SetImpl(Handle<JSObject> holder, InternalIndex entry,
-                             Object value) {
+                             Tagged<Object> value) {
     SetImpl(holder->elements(), entry, value);
   }
 
-  static inline void SetImpl(FixedArrayBase backing_store, InternalIndex entry,
-                             Object value) {
+  static inline void SetImpl(Tagged<FixedArrayBase> backing_store,
+                             InternalIndex entry, Tagged<Object> value) {
     NumberDictionary::cast(backing_store)->ValueAtPut(entry, value);
   }
 
-  static void SetAtomicInternalImpl(FixedArrayBase backing_store,
-                                    InternalIndex entry, Object value,
+  static void SetAtomicInternalImpl(Tagged<FixedArrayBase> backing_store,
+                                    InternalIndex entry, Tagged<Object> value,
                                     SeqCstAccessTag tag) {
     NumberDictionary::cast(backing_store)->ValueAtPut(entry, value, tag);
   }
 
-  static Handle<Object> SwapAtomicInternalImpl(Isolate* isolate,
-                                               FixedArrayBase backing_store,
-                                               InternalIndex entry,
-                                               Object value,
-                                               SeqCstAccessTag tag) {
+  static Handle<Object> SwapAtomicInternalImpl(
+      Isolate* isolate, Tagged<FixedArrayBase> backing_store,
+      InternalIndex entry, Tagged<Object> value, SeqCstAccessTag tag) {
     return handle(
         NumberDictionary::cast(backing_store)->ValueAtSwap(entry, value, tag),
         isolate);
   }
 
-  static Object CompareAndSwapAtomicInternalImpl(FixedArrayBase backing_store,
-                                                 InternalIndex entry,
-                                                 Object expected, Object value,
-                                                 SeqCstAccessTag tag) {
+  static Tagged<Object> CompareAndSwapAtomicInternalImpl(
+      Tagged<FixedArrayBase> backing_store, InternalIndex entry,
+      Tagged<Object> expected, Tagged<Object> value, SeqCstAccessTag tag) {
     return NumberDictionary::cast(backing_store)
         ->ValueAtCompareAndSwap(entry, expected, value, tag);
   }
@@ -1650,16 +1677,18 @@ class DictionaryElementsAccessor
     return Just(true);
   }
 
-  static bool HasEntryImpl(Isolate* isolate, FixedArrayBase store,
+  static bool HasEntryImpl(Isolate* isolate, Tagged<FixedArrayBase> store,
                            InternalIndex entry) {
     DisallowGarbageCollection no_gc;
     NumberDictionary dict = NumberDictionary::cast(store);
-    Object index = dict->KeyAt(isolate, entry);
+    Tagged<Object> index = dict->KeyAt(isolate, entry);
     return !IsTheHole(index, isolate);
   }
 
-  static InternalIndex GetEntryForIndexImpl(Isolate* isolate, JSObject holder,
-                                            FixedArrayBase store, size_t index,
+  static InternalIndex GetEntryForIndexImpl(Isolate* isolate,
+                                            Tagged<JSObject> holder,
+                                            Tagged<FixedArrayBase> store,
+                                            size_t index,
                                             PropertyFilter filter) {
     DisallowGarbageCollection no_gc;
     NumberDictionary dictionary = NumberDictionary::cast(store);
@@ -1676,17 +1705,18 @@ class DictionaryElementsAccessor
     return entry;
   }
 
-  static PropertyDetails GetDetailsImpl(JSObject holder, InternalIndex entry) {
+  static PropertyDetails GetDetailsImpl(Tagged<JSObject> holder,
+                                        InternalIndex entry) {
     return GetDetailsImpl(holder->elements(), entry);
   }
 
-  static PropertyDetails GetDetailsImpl(FixedArrayBase backing_store,
+  static PropertyDetails GetDetailsImpl(Tagged<FixedArrayBase> backing_store,
                                         InternalIndex entry) {
     return NumberDictionary::cast(backing_store)->DetailsAt(entry);
   }
 
   static uint32_t FilterKey(Handle<NumberDictionary> dictionary,
-                            InternalIndex entry, Object raw_key,
+                            InternalIndex entry, Tagged<Object> raw_key,
                             PropertyFilter filter) {
     DCHECK(IsNumber(raw_key));
     DCHECK_LE(Object::Number(raw_key), kMaxUInt32);
@@ -1701,7 +1731,7 @@ class DictionaryElementsAccessor
                                      InternalIndex entry,
                                      PropertyFilter filter) {
     DisallowGarbageCollection no_gc;
-    Object raw_key = dictionary->KeyAt(isolate, entry);
+    Tagged<Object> raw_key = dictionary->KeyAt(isolate, entry);
     if (!dictionary->IsKey(ReadOnlyRoots(isolate), raw_key)) return kMaxUInt32;
     return FilterKey(dictionary, entry, raw_key, filter);
   }
@@ -1720,7 +1750,7 @@ class DictionaryElementsAccessor
     ReadOnlyRoots roots(isolate);
     for (InternalIndex i : dictionary->IterateEntries()) {
       AllowGarbageCollection allow_gc;
-      Object raw_key = dictionary->KeyAt(isolate, i);
+      Tagged<Object> raw_key = dictionary->KeyAt(isolate, i);
       if (!dictionary->IsKey(roots, raw_key)) continue;
       uint32_t key = FilterKey(dictionary, i, raw_key, filter);
       if (key == kMaxUInt32) {
@@ -1767,9 +1797,9 @@ class DictionaryElementsAccessor
         NumberDictionary::cast(receiver->elements()), isolate);
     ReadOnlyRoots roots(isolate);
     for (InternalIndex i : dictionary->IterateEntries()) {
-      Object k = dictionary->KeyAt(isolate, i);
+      Tagged<Object> k = dictionary->KeyAt(isolate, i);
       if (!dictionary->IsKey(roots, k)) continue;
-      Object value = dictionary->ValueAt(isolate, i);
+      Tagged<Object> value = dictionary->ValueAt(isolate, i);
       DCHECK(!IsTheHole(value, isolate));
       DCHECK(!IsAccessorPair(value));
       DCHECK(!IsAccessorInfo(value));
@@ -1783,14 +1813,14 @@ class DictionaryElementsAccessor
                                     size_t length, Maybe<bool>* result) {
     DisallowGarbageCollection no_gc;
     NumberDictionary dictionary = NumberDictionary::cast(receiver->elements());
-    Object the_hole = ReadOnlyRoots(isolate).the_hole_value();
-    Object undefined = ReadOnlyRoots(isolate).undefined_value();
+    Tagged<Object> the_hole = ReadOnlyRoots(isolate).the_hole_value();
+    Tagged<Object> undefined = ReadOnlyRoots(isolate).undefined_value();
 
     // Scan for accessor properties. If accessors are present, then elements
     // must be accessed in order via the slow path.
     bool found = false;
     for (InternalIndex i : dictionary->IterateEntries()) {
-      Object k = dictionary->KeyAt(isolate, i);
+      Tagged<Object> k = dictionary->KeyAt(isolate, i);
       if (k == the_hole) continue;
       if (k == undefined) continue;
 
@@ -1805,7 +1835,7 @@ class DictionaryElementsAccessor
         // access getters out of order
         return false;
       } else if (!found) {
-        Object element_k = dictionary->ValueAt(isolate, i);
+        Tagged<Object> element_k = dictionary->ValueAt(isolate, i);
         if (Object::SameValueZero(*value, element_k)) found = true;
       }
     }
@@ -1846,7 +1876,7 @@ class DictionaryElementsAccessor
       PropertyDetails details = GetDetailsImpl(*dictionary, entry);
       switch (details.kind()) {
         case PropertyKind::kData: {
-          Object element_k = dictionary->ValueAt(entry);
+          Tagged<Object> element_k = dictionary->ValueAt(entry);
           if (Object::SameValueZero(*value, element_k)) return Just(true);
           break;
         }
@@ -1918,7 +1948,7 @@ class DictionaryElementsAccessor
           GetDetailsImpl(*dictionary, InternalIndex(entry));
       switch (details.kind()) {
         case PropertyKind::kData: {
-          Object element_k = dictionary->ValueAt(entry);
+          Tagged<Object> element_k = dictionary->ValueAt(entry);
           if (Object::StrictEquals(*value, element_k)) {
             return Just<int64_t>(k);
           }
@@ -1961,7 +1991,7 @@ class DictionaryElementsAccessor
     return Just<int64_t>(-1);
   }
 
-  static void ValidateContents(JSObject holder, size_t length) {
+  static void ValidateContents(Tagged<JSObject> holder, size_t length) {
     DisallowGarbageCollection no_gc;
 #if DEBUG
     DCHECK_EQ(holder->map()->elements_kind(), DICTIONARY_ELEMENTS);
@@ -1972,7 +2002,7 @@ class DictionaryElementsAccessor
     bool requires_slow_elements = false;
     int max_key = 0;
     for (InternalIndex i : dictionary->IterateEntries()) {
-      Object k;
+      Tagged<Object> k;
       if (!dictionary->ToKey(roots, i, &k)) continue;
       DCHECK_LE(0.0, Object::Number(k));
       if (Object::Number(k) > NumberDictionary::kRequiresSlowElementsLimit) {
@@ -2017,7 +2047,7 @@ class FastElementsAccessor : public ElementsAccessorBase<Subclass, KindTraits> {
     int max_number_key = -1;
     for (int i = 0; j < capacity; i++) {
       if (IsHoleyElementsKindForRead(kind)) {
-        if (BackingStore::cast(*store).is_the_hole(isolate, i)) continue;
+        if (BackingStore::cast(*store)->is_the_hole(isolate, i)) continue;
       }
       max_number_key = i;
       Handle<Object> value =
@@ -2177,14 +2207,15 @@ class FastElementsAccessor : public ElementsAccessorBase<Subclass, KindTraits> {
                  handle(obj->elements(), obj->GetIsolate()));
   }
 
-  static bool HasEntryImpl(Isolate* isolate, FixedArrayBase backing_store,
+  static bool HasEntryImpl(Isolate* isolate,
+                           Tagged<FixedArrayBase> backing_store,
                            InternalIndex entry) {
     return !BackingStore::cast(backing_store)
-                .is_the_hole(isolate, entry.as_int());
+                ->is_the_hole(isolate, entry.as_int());
   }
 
-  static uint32_t NumberOfElementsImpl(JSObject receiver,
-                                       FixedArrayBase backing_store) {
+  static uint32_t NumberOfElementsImpl(Tagged<JSObject> receiver,
+                                       Tagged<FixedArrayBase> backing_store) {
     size_t max_index = Subclass::GetMaxIndex(receiver, backing_store);
     DCHECK_LE(max_index, std::numeric_limits<uint32_t>::max());
     if (IsFastPackedElementsKind(Subclass::kind())) {
@@ -2216,11 +2247,11 @@ class FastElementsAccessor : public ElementsAccessorBase<Subclass, KindTraits> {
     return ExceptionStatus::kSuccess;
   }
 
-  static void ValidateContents(JSObject holder, size_t length) {
+  static void ValidateContents(Tagged<JSObject> holder, size_t length) {
 #if DEBUG
     Isolate* isolate = holder->GetIsolate();
     Heap* heap = isolate->heap();
-    FixedArrayBase elements = holder->elements();
+    Tagged<FixedArrayBase> elements = holder->elements();
     Map map = elements->map();
     if (IsSmiOrObjectElementsKind(KindTraits::Kind)) {
       DCHECK_NE(map, ReadOnlyRoots(heap).fixed_double_array_map());
@@ -2233,7 +2264,7 @@ class FastElementsAccessor : public ElementsAccessorBase<Subclass, KindTraits> {
     if (length == 0u) return;  // nothing to do!
 #if ENABLE_SLOW_DCHECKS
     DisallowGarbageCollection no_gc;
-    BackingStore backing_store = BackingStore::cast(elements);
+    Tagged<BackingStore> backing_store = BackingStore::cast(elements);
     DCHECK(length <= std::numeric_limits<int>::max());
     int length_int = static_cast<int>(length);
     if (IsSmiElementsKind(KindTraits::Kind)) {
@@ -2241,12 +2272,12 @@ class FastElementsAccessor : public ElementsAccessorBase<Subclass, KindTraits> {
       for (int i = 0; i < length_int; i++) {
         DCHECK(IsSmi(*BackingStore::get(backing_store, i, isolate)) ||
                (IsHoleyElementsKind(KindTraits::Kind) &&
-                backing_store.is_the_hole(isolate, i)));
+                backing_store->is_the_hole(isolate, i)));
       }
     } else if (KindTraits::Kind == PACKED_ELEMENTS ||
                KindTraits::Kind == PACKED_DOUBLE_ELEMENTS) {
       for (int i = 0; i < length_int; i++) {
-        DCHECK(!backing_store.is_the_hole(isolate, i));
+        DCHECK(!backing_store->is_the_hole(isolate, i));
       }
     } else {
       DCHECK(IsHoleyElementsKind(KindTraits::Kind));
@@ -2285,7 +2316,7 @@ class FastElementsAccessor : public ElementsAccessorBase<Subclass, KindTraits> {
                            int src_index, int len, int hole_start,
                            int hole_end) {
     DisallowGarbageCollection no_gc;
-    BackingStore dst_elms = BackingStore::cast(*backing_store);
+    Tagged<BackingStore> dst_elms = BackingStore::cast(*backing_store);
     if (len > JSArray::kMaxCopyElements && dst_index == 0 &&
         isolate->heap()->CanMoveObjectStart(dst_elms)) {
       dst_elms = BackingStore::cast(
@@ -2300,10 +2331,10 @@ class FastElementsAccessor : public ElementsAccessorBase<Subclass, KindTraits> {
     } else if (len != 0) {
       WriteBarrierMode mode =
           GetWriteBarrierMode(dst_elms, KindTraits::Kind, no_gc);
-      dst_elms.MoveElements(isolate, dst_index, src_index, len, mode);
+      dst_elms->MoveElements(isolate, dst_index, src_index, len, mode);
     }
     if (hole_start != hole_end) {
-      dst_elms.FillWithHoles(hole_start, hole_end);
+      dst_elms->FillWithHoles(hole_start, hole_end);
     }
   }
 
@@ -2340,10 +2371,10 @@ class FastElementsAccessor : public ElementsAccessorBase<Subclass, KindTraits> {
                                        size_t start_from, size_t length) {
     DCHECK(JSObject::PrototypeHasNoElements(isolate, *receiver));
     DisallowGarbageCollection no_gc;
-    FixedArrayBase elements_base = receiver->elements();
-    Object the_hole = ReadOnlyRoots(isolate).the_hole_value();
-    Object undefined = ReadOnlyRoots(isolate).undefined_value();
-    Object value = *search_value;
+    Tagged<FixedArrayBase> elements_base = receiver->elements();
+    Tagged<Object> the_hole = ReadOnlyRoots(isolate).the_hole_value();
+    Tagged<Object> undefined = ReadOnlyRoots(isolate).undefined_value();
+    Tagged<Object> value = *search_value;
 
     if (start_from >= length) return Just(false);
 
@@ -2369,7 +2400,7 @@ class FastElementsAccessor : public ElementsAccessorBase<Subclass, KindTraits> {
           FixedArray elements = FixedArray::cast(receiver->elements());
 
           for (size_t k = start_from; k < length; ++k) {
-            Object element_k = elements->get(static_cast<int>(k));
+            Tagged<Object> element_k = elements->get(static_cast<int>(k));
 
             if (element_k == the_hole || element_k == undefined) {
               return Just(true);
@@ -2403,7 +2434,7 @@ class FastElementsAccessor : public ElementsAccessorBase<Subclass, KindTraits> {
         FixedArray elements = FixedArray::cast(receiver->elements());
 
         for (size_t k = start_from; k < length; ++k) {
-          Object element_k = elements->get(static_cast<int>(k));
+          Tagged<Object> element_k = elements->get(static_cast<int>(k));
           if (element_k == the_hole) continue;
           if (Object::SameValueZero(value, element_k)) return Just(true);
         }
@@ -2433,7 +2464,7 @@ class FastElementsAccessor : public ElementsAccessorBase<Subclass, KindTraits> {
           FixedArray elements = FixedArray::cast(receiver->elements());
 
           for (size_t k = start_from; k < length; ++k) {
-            Object element_k = elements->get(static_cast<int>(k));
+            Tagged<Object> element_k = elements->get(static_cast<int>(k));
             if (IsNumber(element_k) &&
                 Object::Number(element_k) == search_number) {
               return Just(true);
@@ -2568,10 +2599,10 @@ class FastElementsAccessor : public ElementsAccessorBase<Subclass, KindTraits> {
                             uint32_t dst_index) {
     // Add the provided values.
     DisallowGarbageCollection no_gc;
-    FixedArrayBase raw_backing_store = *dst_store;
+    Tagged<FixedArrayBase> raw_backing_store = *dst_store;
     WriteBarrierMode mode = raw_backing_store->GetWriteBarrierMode(no_gc);
     for (uint32_t i = 0; i < copy_size; i++) {
-      Object argument = (*args)[src_index + i];
+      Tagged<Object> argument = (*args)[src_index + i];
       DCHECK(!IsTheHole(argument));
       Subclass::SetImpl(raw_backing_store, InternalIndex(dst_index + i),
                         argument, mode);
@@ -2584,21 +2615,22 @@ class FastSmiOrObjectElementsAccessor
     : public FastElementsAccessor<Subclass, KindTraits> {
  public:
   static inline void SetImpl(Handle<JSObject> holder, InternalIndex entry,
-                             Object value) {
+                             Tagged<Object> value) {
     SetImpl(holder->elements(), entry, value);
   }
 
-  static inline void SetImpl(FixedArrayBase backing_store, InternalIndex entry,
-                             Object value) {
+  static inline void SetImpl(Tagged<FixedArrayBase> backing_store,
+                             InternalIndex entry, Tagged<Object> value) {
     FixedArray::cast(backing_store)->set(entry.as_int(), value);
   }
 
-  static inline void SetImpl(FixedArrayBase backing_store, InternalIndex entry,
-                             Object value, WriteBarrierMode mode) {
+  static inline void SetImpl(Tagged<FixedArrayBase> backing_store,
+                             InternalIndex entry, Tagged<Object> value,
+                             WriteBarrierMode mode) {
     FixedArray::cast(backing_store)->set(entry.as_int(), value, mode);
   }
 
-  static Object GetRaw(FixedArray backing_store, InternalIndex entry) {
+  static Tagged<Object> GetRaw(FixedArray backing_store, InternalIndex entry) {
     return backing_store->get(entry.as_int());
   }
 
@@ -2607,8 +2639,8 @@ class FastSmiOrObjectElementsAccessor
   // See ElementsAccessor::CopyElements() for details.
   // This method could actually allocate if copying from double elements to
   // object elements.
-  static void CopyElementsImpl(Isolate* isolate, FixedArrayBase from,
-                               uint32_t from_start, FixedArrayBase to,
+  static void CopyElementsImpl(Isolate* isolate, Tagged<FixedArrayBase> from,
+                               uint32_t from_start, Tagged<FixedArrayBase> to,
                                ElementsKind from_kind, uint32_t to_start,
                                int packed_size, int copy_size) {
     DisallowGarbageCollection no_gc;
@@ -2682,7 +2714,7 @@ class FastSmiOrObjectElementsAccessor
       for (uint32_t index = 0; index < length; ++index) {
         InternalIndex entry(index);
         if (!Subclass::HasEntryImpl(isolate, elements, entry)) continue;
-        Object value = GetRaw(elements, entry);
+        Tagged<Object> value = GetRaw(elements, entry);
         values_or_entries->set(count++, value);
       }
     }
@@ -2696,8 +2728,8 @@ class FastSmiOrObjectElementsAccessor
                                          size_t start_from, size_t length) {
     DCHECK(JSObject::PrototypeHasNoElements(isolate, *receiver));
     DisallowGarbageCollection no_gc;
-    FixedArrayBase elements_base = receiver->elements();
-    Object value = *search_value;
+    Tagged<FixedArrayBase> elements_base = receiver->elements();
+    Tagged<Object> value = *search_value;
 
     if (start_from >= length) return Just<int64_t>(-1);
 
@@ -2916,36 +2948,32 @@ class SharedArrayElementsAccessor
           SharedArrayElementsAccessor,
           ElementsKindTraits<SHARED_ARRAY_ELEMENTS>> {
  public:
-  static Handle<Object> GetAtomicInternalImpl(Isolate* isolate,
-                                              FixedArrayBase backing_store,
-                                              InternalIndex entry,
-                                              SeqCstAccessTag tag) {
+  static Handle<Object> GetAtomicInternalImpl(
+      Isolate* isolate, Tagged<FixedArrayBase> backing_store,
+      InternalIndex entry, SeqCstAccessTag tag) {
     return handle(BackingStore::cast(backing_store)->get(entry.as_int(), tag),
                   isolate);
   }
 
-  static void SetAtomicInternalImpl(FixedArrayBase backing_store,
-                                    InternalIndex entry, Object value,
+  static void SetAtomicInternalImpl(Tagged<FixedArrayBase> backing_store,
+                                    InternalIndex entry, Tagged<Object> value,
                                     SeqCstAccessTag tag) {
     BackingStore::cast(backing_store)->set(entry.as_int(), value, tag);
   }
 
-  static Handle<Object> SwapAtomicInternalImpl(Isolate* isolate,
-                                               FixedArrayBase backing_store,
-                                               InternalIndex entry,
-                                               Object value,
-                                               SeqCstAccessTag tag) {
+  static Handle<Object> SwapAtomicInternalImpl(
+      Isolate* isolate, Tagged<FixedArrayBase> backing_store,
+      InternalIndex entry, Tagged<Object> value, SeqCstAccessTag tag) {
     return handle(
         BackingStore::cast(backing_store)->swap(entry.as_int(), value, tag),
         isolate);
   }
 
-  static Object CompareAndSwapAtomicInternalImpl(FixedArrayBase backing_store,
-                                                 InternalIndex entry,
-                                                 Object expected, Object value,
-                                                 SeqCstAccessTag tag) {
+  static Tagged<Object> CompareAndSwapAtomicInternalImpl(
+      Tagged<FixedArrayBase> backing_store, InternalIndex entry,
+      Tagged<Object> expected, Tagged<Object> value, SeqCstAccessTag tag) {
     return BackingStore::cast(backing_store)
-        .compare_and_swap(entry.as_int(), expected, value, tag);
+        ->compare_and_swap(entry.as_int(), expected, value, tag);
   }
 };
 
@@ -2961,17 +2989,18 @@ class FastFrozenObjectElementsAccessor
   using BackingStore = typename KindTraits::BackingStore;
 
   static inline void SetImpl(Handle<JSObject> holder, InternalIndex entry,
-                             Object value) {
+                             Tagged<Object> value) {
     UNREACHABLE();
   }
 
-  static inline void SetImpl(FixedArrayBase backing_store, InternalIndex entry,
-                             Object value) {
+  static inline void SetImpl(Tagged<FixedArrayBase> backing_store,
+                             InternalIndex entry, Tagged<Object> value) {
     UNREACHABLE();
   }
 
-  static inline void SetImpl(FixedArrayBase backing_store, InternalIndex entry,
-                             Object value, WriteBarrierMode mode) {
+  static inline void SetImpl(Tagged<FixedArrayBase> backing_store,
+                             InternalIndex entry, Tagged<Object> value,
+                             WriteBarrierMode mode) {
     UNREACHABLE();
   }
 
@@ -3043,31 +3072,33 @@ template <typename Subclass, typename KindTraits>
 class FastDoubleElementsAccessor
     : public FastElementsAccessor<Subclass, KindTraits> {
  public:
-  static Handle<Object> GetImpl(Isolate* isolate, FixedArrayBase backing_store,
+  static Handle<Object> GetImpl(Isolate* isolate,
+                                Tagged<FixedArrayBase> backing_store,
                                 InternalIndex entry) {
     return FixedDoubleArray::get(FixedDoubleArray::cast(backing_store),
                                  entry.as_int(), isolate);
   }
 
   static inline void SetImpl(Handle<JSObject> holder, InternalIndex entry,
-                             Object value) {
+                             Tagged<Object> value) {
     SetImpl(holder->elements(), entry, value);
   }
 
-  static inline void SetImpl(FixedArrayBase backing_store, InternalIndex entry,
-                             Object value) {
+  static inline void SetImpl(Tagged<FixedArrayBase> backing_store,
+                             InternalIndex entry, Tagged<Object> value) {
     FixedDoubleArray::cast(backing_store)
         ->set(entry.as_int(), Object::Number(value));
   }
 
-  static inline void SetImpl(FixedArrayBase backing_store, InternalIndex entry,
-                             Object value, WriteBarrierMode mode) {
+  static inline void SetImpl(Tagged<FixedArrayBase> backing_store,
+                             InternalIndex entry, Tagged<Object> value,
+                             WriteBarrierMode mode) {
     FixedDoubleArray::cast(backing_store)
         ->set(entry.as_int(), Object::Number(value));
   }
 
-  static void CopyElementsImpl(Isolate* isolate, FixedArrayBase from,
-                               uint32_t from_start, FixedArrayBase to,
+  static void CopyElementsImpl(Isolate* isolate, Tagged<FixedArrayBase> from,
+                               uint32_t from_start, Tagged<FixedArrayBase> to,
                                ElementsKind from_kind, uint32_t to_start,
                                int packed_size, int copy_size) {
     DisallowGarbageCollection no_gc;
@@ -3141,8 +3172,8 @@ class FastDoubleElementsAccessor
                                          size_t start_from, size_t length) {
     DCHECK(JSObject::PrototypeHasNoElements(isolate, *receiver));
     DisallowGarbageCollection no_gc;
-    FixedArrayBase elements_base = receiver->elements();
-    Object value = *search_value;
+    Tagged<FixedArrayBase> elements_base = receiver->elements();
+    Tagged<Object> value = *search_value;
 
     length = std::min(static_cast<size_t>(elements_base->length()), length);
 
@@ -3207,7 +3238,8 @@ class TypedElementsAccessor
   static ElementType FromScalar(uint64_t value) { UNREACHABLE(); }
 
   // Conversions from objects / handles.
-  static ElementType FromObject(Object value, bool* lossless = nullptr) {
+  static ElementType FromObject(Tagged<Object> value,
+                                bool* lossless = nullptr) {
     if (IsSmi(value)) {
       return FromScalar(Smi::ToInt(value));
     } else if (IsHeapNumber(value)) {
@@ -3228,7 +3260,7 @@ class TypedElementsAccessor
   static Handle<Object> ToHandle(Isolate* isolate, ElementType value);
 
   static void SetImpl(Handle<JSObject> holder, InternalIndex entry,
-                      Object value) {
+                      Tagged<Object> value) {
     Handle<JSTypedArray> typed_array = Handle<JSTypedArray>::cast(holder);
     DCHECK_LE(entry.raw_value(), typed_array->GetLength());
     auto* entry_ptr =
@@ -3297,7 +3329,8 @@ class TypedElementsAccessor
     return ToHandle(isolate, elem);
   }
 
-  static Handle<Object> GetImpl(Isolate* isolate, FixedArrayBase backing_store,
+  static Handle<Object> GetImpl(Isolate* isolate,
+                                Tagged<FixedArrayBase> backing_store,
                                 InternalIndex entry) {
     UNREACHABLE();
   }
@@ -3352,24 +3385,26 @@ class TypedElementsAccessor
     return result;
   }
 
-  static PropertyDetails GetDetailsImpl(JSObject holder, InternalIndex entry) {
+  static PropertyDetails GetDetailsImpl(Tagged<JSObject> holder,
+                                        InternalIndex entry) {
     return PropertyDetails(PropertyKind::kData, NONE,
                            PropertyCellType::kNoCell);
   }
 
-  static PropertyDetails GetDetailsImpl(FixedArrayBase backing_store,
+  static PropertyDetails GetDetailsImpl(Tagged<FixedArrayBase> backing_store,
                                         InternalIndex entry) {
     return PropertyDetails(PropertyKind::kData, NONE,
                            PropertyCellType::kNoCell);
   }
 
-  static bool HasElementImpl(Isolate* isolate, JSObject holder, size_t index,
-                             FixedArrayBase backing_store,
+  static bool HasElementImpl(Isolate* isolate, Tagged<JSObject> holder,
+                             size_t index, Tagged<FixedArrayBase> backing_store,
                              PropertyFilter filter) {
     return index < AccessorClass::GetCapacityImpl(holder, backing_store);
   }
 
-  static bool HasAccessorsImpl(JSObject holder, FixedArrayBase backing_store) {
+  static bool HasAccessorsImpl(Tagged<JSObject> holder,
+                               Tagged<FixedArrayBase> backing_store) {
     return false;
   }
 
@@ -3387,22 +3422,23 @@ class TypedElementsAccessor
     // deleted otherwise.
   }
 
-  static InternalIndex GetEntryForIndexImpl(Isolate* isolate, JSObject holder,
-                                            FixedArrayBase backing_store,
-                                            size_t index,
-                                            PropertyFilter filter) {
+  static InternalIndex GetEntryForIndexImpl(
+      Isolate* isolate, Tagged<JSObject> holder,
+      Tagged<FixedArrayBase> backing_store, size_t index,
+      PropertyFilter filter) {
     return index < AccessorClass::GetCapacityImpl(holder, backing_store)
                ? InternalIndex(index)
                : InternalIndex::NotFound();
   }
 
-  static size_t GetCapacityImpl(JSObject holder, FixedArrayBase backing_store) {
+  static size_t GetCapacityImpl(Tagged<JSObject> holder,
+                                Tagged<FixedArrayBase> backing_store) {
     JSTypedArray typed_array = JSTypedArray::cast(holder);
     return typed_array->GetLength();
   }
 
-  static size_t NumberOfElementsImpl(JSObject receiver,
-                                     FixedArrayBase backing_store) {
+  static size_t NumberOfElementsImpl(Tagged<JSObject> receiver,
+                                     Tagged<FixedArrayBase> backing_store) {
     return AccessorClass::GetCapacityImpl(receiver, backing_store);
   }
 
@@ -3678,7 +3714,7 @@ class TypedElementsAccessor
     return Just<int64_t>(-1);
   }
 
-  static void ReverseImpl(JSObject receiver) {
+  static void ReverseImpl(Tagged<JSObject> receiver) {
     DisallowGarbageCollection no_gc;
     JSTypedArray typed_array = JSTypedArray::cast(receiver);
 
@@ -3877,7 +3913,7 @@ class TypedElementsAccessor
     if (isolate->force_slow_path()) return true;
 #endif
 
-    Object source_proto = source->map()->prototype();
+    Tagged<Object> source_proto = source->map()->prototype();
 
     // Null prototypes are OK - we don't need to do prototype chain lookups on
     // them.
@@ -3935,7 +3971,7 @@ class TypedElementsAccessor
       FixedArray source_store = FixedArray::cast(source->elements());
 
       for (size_t i = 0; i < length; i++) {
-        Object elem = source_store->get(static_cast<int>(i));
+        Tagged<Object> elem = source_store->get(static_cast<int>(i));
         SetImpl(dest_data + i, FromScalar(Smi::ToInt(elem)),
                 destination_shared);
       }
@@ -3946,7 +3982,7 @@ class TypedElementsAccessor
         if (source_store->is_the_hole(isolate, static_cast<int>(i))) {
           SetImpl(dest_data + i, FromObject(undefined), destination_shared);
         } else {
-          Object elem = source_store->get(static_cast<int>(i));
+          Tagged<Object> elem = source_store->get(static_cast<int>(i));
           SetImpl(dest_data + i, FromScalar(Smi::ToInt(elem)),
                   destination_shared);
         }
@@ -3982,9 +4018,9 @@ class TypedElementsAccessor
   }
 
   // ES#sec-settypedarrayfromarraylike
-  static Object CopyElementsHandleSlow(Handle<Object> source,
-                                       Handle<JSTypedArray> destination,
-                                       size_t length, size_t offset) {
+  static Tagged<Object> CopyElementsHandleSlow(Handle<Object> source,
+                                               Handle<JSTypedArray> destination,
+                                               size_t length, size_t offset) {
     Isolate* isolate = destination->GetIsolate();
     // 8. Let k be 0.
     // 9. Repeat, while k < srcLength,
@@ -4033,9 +4069,9 @@ class TypedElementsAccessor
   // This doesn't guarantee that the destination array will be completely
   // filled. The caller must do this by passing a source with equal length, if
   // that is required.
-  static Object CopyElementsHandleImpl(Handle<Object> source,
-                                       Handle<JSObject> destination,
-                                       size_t length, size_t offset) {
+  static Tagged<Object> CopyElementsHandleImpl(Handle<Object> source,
+                                               Handle<JSObject> destination,
+                                               size_t length, size_t offset) {
     Isolate* isolate = destination->GetIsolate();
     if (length == 0) return *isolate->factory()->undefined_value();
 
@@ -4229,7 +4265,7 @@ int64_t TypedElementsAccessor<BIGINT64_ELEMENTS, int64_t>::FromScalar(
 // static
 template <>
 int64_t TypedElementsAccessor<BIGINT64_ELEMENTS, int64_t>::FromObject(
-    Object value, bool* lossless) {
+    Tagged<Object> value, bool* lossless) {
   return BigInt::cast(value)->AsInt64(lossless);
 }
 
@@ -4278,7 +4314,7 @@ uint64_t TypedElementsAccessor<BIGUINT64_ELEMENTS, uint64_t>::FromScalar(
 // static
 template <>
 uint64_t TypedElementsAccessor<BIGUINT64_ELEMENTS, uint64_t>::FromObject(
-    Object value, bool* lossless) {
+    Tagged<Object> value, bool* lossless) {
   return BigInt::cast(value)->AsUint64(lossless);
 }
 
@@ -4441,7 +4477,7 @@ int64_t TypedElementsAccessor<RAB_GSAB_BIGINT64_ELEMENTS, int64_t>::FromScalar(
 // static
 template <>
 int64_t TypedElementsAccessor<RAB_GSAB_BIGINT64_ELEMENTS, int64_t>::FromObject(
-    Object value, bool* lossless) {
+    Tagged<Object> value, bool* lossless) {
   return BigInt::cast(value)->AsInt64(lossless);
 }
 
@@ -4491,7 +4527,7 @@ uint64_t TypedElementsAccessor<RAB_GSAB_BIGUINT64_ELEMENTS,
 // static
 template <>
 uint64_t TypedElementsAccessor<RAB_GSAB_BIGUINT64_ELEMENTS,
-                               uint64_t>::FromObject(Object value,
+                               uint64_t>::FromObject(Tagged<Object> value,
                                                      bool* lossless) {
   return BigInt::cast(value)->AsUint64(lossless);
 }
@@ -4519,7 +4555,8 @@ class SloppyArgumentsElementsAccessor
     UNREACHABLE();
   }
 
-  static Handle<Object> GetImpl(Isolate* isolate, FixedArrayBase parameters,
+  static Handle<Object> GetImpl(Isolate* isolate,
+                                Tagged<FixedArrayBase> parameters,
                                 InternalIndex entry) {
     Handle<SloppyArgumentsElements> elements(
         SloppyArgumentsElements::cast(parameters), isolate);
@@ -4527,7 +4564,8 @@ class SloppyArgumentsElementsAccessor
     if (entry.as_uint32() < length) {
       // Read context mapped entry.
       DisallowGarbageCollection no_gc;
-      Object probe = elements->mapped_entries(entry.as_uint32(), kRelaxedLoad);
+      Tagged<Object> probe =
+          elements->mapped_entries(entry.as_uint32(), kRelaxedLoad);
       DCHECK(!IsTheHole(probe, isolate));
       Context context = elements->context();
       int context_entry = Smi::ToInt(probe);
@@ -4552,18 +4590,20 @@ class SloppyArgumentsElementsAccessor
   }
 
   static inline void SetImpl(Handle<JSObject> holder, InternalIndex entry,
-                             Object value) {
+                             Tagged<Object> value) {
     SetImpl(holder->elements(), entry, value);
   }
 
-  static inline void SetImpl(FixedArrayBase store, InternalIndex entry,
-                             Object value) {
-    SloppyArgumentsElements elements = SloppyArgumentsElements::cast(store);
+  static inline void SetImpl(Tagged<FixedArrayBase> store, InternalIndex entry,
+                             Tagged<Object> value) {
+    Tagged<SloppyArgumentsElements> elements =
+        SloppyArgumentsElements::cast(store);
     uint32_t length = elements->length();
     if (entry.as_uint32() < length) {
       // Store context mapped entry.
       DisallowGarbageCollection no_gc;
-      Object probe = elements->mapped_entries(entry.as_uint32(), kRelaxedLoad);
+      Tagged<Object> probe =
+          elements->mapped_entries(entry.as_uint32(), kRelaxedLoad);
       DCHECK(!IsTheHole(probe));
       Context context = Context::cast(elements->context());
       int context_entry = Smi::ToInt(probe);
@@ -4572,7 +4612,7 @@ class SloppyArgumentsElementsAccessor
     } else {
       //  Entry is not context mapped defer to arguments.
       FixedArray arguments = elements->arguments();
-      Object current =
+      Tagged<Object> current =
           ArgumentsAccessor::GetRaw(arguments, entry.adjust_down(length));
       if (IsAliasedArgumentsEntry(current)) {
         AliasedArgumentsEntry alias = AliasedArgumentsEntry::cast(current);
@@ -4593,30 +4633,32 @@ class SloppyArgumentsElementsAccessor
     UNREACHABLE();
   }
 
-  static uint32_t GetCapacityImpl(JSObject holder, FixedArrayBase store) {
-    SloppyArgumentsElements elements = SloppyArgumentsElements::cast(store);
-    FixedArray arguments = elements->arguments();
+  static uint32_t GetCapacityImpl(Tagged<JSObject> holder,
+                                  Tagged<FixedArrayBase> store) {
+    Tagged<SloppyArgumentsElements> elements =
+        SloppyArgumentsElements::cast(store);
+    Tagged<FixedArray> arguments = elements->arguments();
     return elements->length() +
            ArgumentsAccessor::GetCapacityImpl(holder, arguments);
   }
 
-  static uint32_t GetMaxNumberOfEntries(JSObject holder,
-                                        FixedArrayBase backing_store) {
-    SloppyArgumentsElements elements =
+  static uint32_t GetMaxNumberOfEntries(Tagged<JSObject> holder,
+                                        Tagged<FixedArrayBase> backing_store) {
+    Tagged<SloppyArgumentsElements> elements =
         SloppyArgumentsElements::cast(backing_store);
-    FixedArrayBase arguments = elements->arguments();
+    Tagged<FixedArrayBase> arguments = elements->arguments();
     size_t max_entries =
         ArgumentsAccessor::GetMaxNumberOfEntries(holder, arguments);
     DCHECK_LE(max_entries, std::numeric_limits<uint32_t>::max());
     return elements->length() + static_cast<uint32_t>(max_entries);
   }
 
-  static uint32_t NumberOfElementsImpl(JSObject receiver,
-                                       FixedArrayBase backing_store) {
+  static uint32_t NumberOfElementsImpl(Tagged<JSObject> receiver,
+                                       Tagged<FixedArrayBase> backing_store) {
     Isolate* isolate = receiver->GetIsolate();
-    SloppyArgumentsElements elements =
+    Tagged<SloppyArgumentsElements> elements =
         SloppyArgumentsElements::cast(backing_store);
-    FixedArrayBase arguments = elements->arguments();
+    Tagged<FixedArrayBase> arguments = elements->arguments();
     uint32_t nof_elements = 0;
     uint32_t length = elements->length();
     for (uint32_t index = 0; index < length; index++) {
@@ -4641,36 +4683,38 @@ class SloppyArgumentsElementsAccessor
     return ExceptionStatus::kSuccess;
   }
 
-  static bool HasEntryImpl(Isolate* isolate, FixedArrayBase parameters,
+  static bool HasEntryImpl(Isolate* isolate, Tagged<FixedArrayBase> parameters,
                            InternalIndex entry) {
-    SloppyArgumentsElements elements =
+    Tagged<SloppyArgumentsElements> elements =
         SloppyArgumentsElements::cast(parameters);
     uint32_t length = elements->length();
     if (entry.raw_value() < length) {
       return HasParameterMapArg(isolate, elements, entry.raw_value());
     }
-    FixedArrayBase arguments = elements->arguments();
+    Tagged<FixedArrayBase> arguments = elements->arguments();
     return ArgumentsAccessor::HasEntryImpl(isolate, arguments,
                                            entry.adjust_down(length));
   }
 
-  static bool HasAccessorsImpl(JSObject holder, FixedArrayBase backing_store) {
-    SloppyArgumentsElements elements =
+  static bool HasAccessorsImpl(Tagged<JSObject> holder,
+                               Tagged<FixedArrayBase> backing_store) {
+    Tagged<SloppyArgumentsElements> elements =
         SloppyArgumentsElements::cast(backing_store);
-    FixedArray arguments = elements->arguments();
+    Tagged<FixedArray> arguments = elements->arguments();
     return ArgumentsAccessor::HasAccessorsImpl(holder, arguments);
   }
 
-  static InternalIndex GetEntryForIndexImpl(Isolate* isolate, JSObject holder,
-                                            FixedArrayBase parameters,
+  static InternalIndex GetEntryForIndexImpl(Isolate* isolate,
+                                            Tagged<JSObject> holder,
+                                            Tagged<FixedArrayBase> parameters,
                                             size_t index,
                                             PropertyFilter filter) {
-    SloppyArgumentsElements elements =
+    Tagged<SloppyArgumentsElements> elements =
         SloppyArgumentsElements::cast(parameters);
     if (HasParameterMapArg(isolate, elements, index)) {
       return InternalIndex(index);
     }
-    FixedArray arguments = elements->arguments();
+    Tagged<FixedArray> arguments = elements->arguments();
     InternalIndex entry = ArgumentsAccessor::GetEntryForIndexImpl(
         isolate, holder, arguments, index, filter);
     if (entry.is_not_found()) return entry;
@@ -4679,21 +4723,22 @@ class SloppyArgumentsElementsAccessor
     return entry.adjust_up(elements->length());
   }
 
-  static PropertyDetails GetDetailsImpl(JSObject holder, InternalIndex entry) {
-    SloppyArgumentsElements elements =
+  static PropertyDetails GetDetailsImpl(Tagged<JSObject> holder,
+                                        InternalIndex entry) {
+    Tagged<SloppyArgumentsElements> elements =
         SloppyArgumentsElements::cast(holder->elements());
     uint32_t length = elements->length();
     if (entry.as_uint32() < length) {
       return PropertyDetails(PropertyKind::kData, NONE,
                              PropertyCellType::kNoCell);
     }
-    FixedArray arguments = elements->arguments();
+    Tagged<FixedArray> arguments = elements->arguments();
     return ArgumentsAccessor::GetDetailsImpl(arguments,
                                              entry.adjust_down(length));
   }
 
   static bool HasParameterMapArg(Isolate* isolate,
-                                 SloppyArgumentsElements elements,
+                                 Tagged<SloppyArgumentsElements> elements,
                                  size_t index) {
     uint32_t length = elements->length();
     if (index >= length) return false;
@@ -4866,8 +4911,9 @@ class SlowSloppyArgumentsElementsAccessor
     // Elements of the arguments object in slow mode might be slow aliases.
     if (IsAliasedArgumentsEntry(*result)) {
       DisallowGarbageCollection no_gc;
-      AliasedArgumentsEntry alias = AliasedArgumentsEntry::cast(*result);
-      Context context = elements->context();
+      Tagged<AliasedArgumentsEntry> alias =
+          AliasedArgumentsEntry::cast(*result);
+      Tagged<Context> context = elements->context();
       int context_entry = alias->aliased_context_slot();
       DCHECK(!IsTheHole(context->get(context_entry), isolate));
       return handle(context->get(context_entry), isolate);
@@ -4920,9 +4966,10 @@ class SlowSloppyArgumentsElementsAccessor
         Handle<SloppyArgumentsElements>::cast(store);
     uint32_t length = elements->length();
     if (entry.as_uint32() < length) {
-      Object probe = elements->mapped_entries(entry.as_uint32(), kRelaxedLoad);
+      Tagged<Object> probe =
+          elements->mapped_entries(entry.as_uint32(), kRelaxedLoad);
       DCHECK(!IsTheHole(probe, isolate));
-      Context context = elements->context();
+      Tagged<Context> context = elements->context();
       int context_entry = Smi::ToInt(probe);
       DCHECK(!IsTheHole(context->get(context_entry), isolate));
       context->set(context_entry, *value);
@@ -4967,8 +5014,9 @@ class FastSloppyArgumentsElementsAccessor
   }
 
   static Handle<FixedArray> GetArguments(Isolate* isolate,
-                                         FixedArrayBase store) {
-    SloppyArgumentsElements elements = SloppyArgumentsElements::cast(store);
+                                         Tagged<FixedArrayBase> store) {
+    Tagged<SloppyArgumentsElements> elements =
+        SloppyArgumentsElements::cast(store);
     return Handle<FixedArray>(elements->arguments(), isolate);
   }
 
@@ -5019,7 +5067,7 @@ class FastSloppyArgumentsElementsAccessor
       MAYBE_RETURN(GrowCapacityAndConvertImpl(object, new_capacity),
                    Nothing<bool>());
     }
-    FixedArray arguments = elements->arguments();
+    Tagged<FixedArray> arguments = elements->arguments();
     // For fast holey objects, the entry equals the index. The code above made
     // sure that there's enough space to store the value. We cannot convert
     // index to entry explicitly since the slot still contains the hole, so the
@@ -5042,8 +5090,8 @@ class FastSloppyArgumentsElementsAccessor
                                                          value, attributes);
   }
 
-  static void CopyElementsImpl(Isolate* isolate, FixedArrayBase from,
-                               uint32_t from_start, FixedArrayBase to,
+  static void CopyElementsImpl(Isolate* isolate, Tagged<FixedArrayBase> from,
+                               uint32_t from_start, Tagged<FixedArrayBase> to,
                                ElementsKind from_kind, uint32_t to_start,
                                int packed_size, int copy_size) {
     DCHECK(!IsNumberDictionary(to));
@@ -5105,12 +5153,14 @@ class StringWrapperElementsAccessor
                                          entry.adjust_down(length));
   }
 
-  static Handle<Object> GetImpl(Isolate* isolate, FixedArrayBase elements,
+  static Handle<Object> GetImpl(Isolate* isolate,
+                                Tagged<FixedArrayBase> elements,
                                 InternalIndex entry) {
     UNREACHABLE();
   }
 
-  static PropertyDetails GetDetailsImpl(JSObject holder, InternalIndex entry) {
+  static PropertyDetails GetDetailsImpl(Tagged<JSObject> holder,
+                                        InternalIndex entry) {
     uint32_t length = static_cast<uint32_t>(GetString(holder)->length());
     if (entry.as_uint32() < length) {
       PropertyAttributes attributes =
@@ -5122,10 +5172,10 @@ class StringWrapperElementsAccessor
                                                 entry.adjust_down(length));
   }
 
-  static InternalIndex GetEntryForIndexImpl(Isolate* isolate, JSObject holder,
-                                            FixedArrayBase backing_store,
-                                            size_t index,
-                                            PropertyFilter filter) {
+  static InternalIndex GetEntryForIndexImpl(
+      Isolate* isolate, Tagged<JSObject> holder,
+      Tagged<FixedArrayBase> backing_store, size_t index,
+      PropertyFilter filter) {
     uint32_t length = static_cast<uint32_t>(GetString(holder)->length());
     if (index < length) return InternalIndex(index);
     InternalIndex backing_store_entry =
@@ -5144,7 +5194,7 @@ class StringWrapperElementsAccessor
   }
 
   static void SetImpl(Handle<JSObject> holder, InternalIndex entry,
-                      Object value) {
+                      Tagged<Object> value) {
     uint32_t length = static_cast<uint32_t>(GetString(*holder)->length());
     if (entry.as_uint32() < length) {
       return;  // String contents are read-only.
@@ -5235,8 +5285,8 @@ class StringWrapperElementsAccessor
         capacity);
   }
 
-  static void CopyElementsImpl(Isolate* isolate, FixedArrayBase from,
-                               uint32_t from_start, FixedArrayBase to,
+  static void CopyElementsImpl(Isolate* isolate, Tagged<FixedArrayBase> from,
+                               uint32_t from_start, Tagged<FixedArrayBase> to,
                                ElementsKind from_kind, uint32_t to_start,
                                int packed_size, int copy_size) {
     DCHECK(!IsNumberDictionary(to));
@@ -5250,17 +5300,17 @@ class StringWrapperElementsAccessor
     }
   }
 
-  static uint32_t NumberOfElementsImpl(JSObject object,
-                                       FixedArrayBase backing_store) {
+  static uint32_t NumberOfElementsImpl(Tagged<JSObject> object,
+                                       Tagged<FixedArrayBase> backing_store) {
     uint32_t length = GetString(object)->length();
     return length +
            BackingStoreAccessor::NumberOfElementsImpl(object, backing_store);
   }
 
  private:
-  static String GetString(JSObject holder) {
+  static Tagged<String> GetString(Tagged<JSObject> holder) {
     DCHECK(IsJSPrimitiveWrapper(holder));
-    JSPrimitiveWrapper js_value = JSPrimitiveWrapper::cast(holder);
+    Tagged<JSPrimitiveWrapper> js_value = JSPrimitiveWrapper::cast(holder);
     DCHECK(IsString(js_value->value()));
     return String::cast(js_value->value());
   }
@@ -5282,7 +5332,8 @@ class SlowStringWrapperElementsAccessor
           SlowStringWrapperElementsAccessor, DictionaryElementsAccessor,
           ElementsKindTraits<SLOW_STRING_WRAPPER_ELEMENTS>> {
  public:
-  static bool HasAccessorsImpl(JSObject holder, FixedArrayBase backing_store) {
+  static bool HasAccessorsImpl(Tagged<JSObject> holder,
+                               Tagged<FixedArrayBase> backing_store) {
     return DictionaryElementsAccessor::HasAccessorsImpl(holder, backing_store);
   }
 };
@@ -5383,9 +5434,10 @@ void CopyFastNumberJSArrayElementsToTypedArray(Address raw_context,
                                                Address raw_destination,
                                                uintptr_t length,
                                                uintptr_t offset) {
-  Context context = Context::cast(Object(raw_context));
-  JSArray source = JSArray::cast(Object(raw_source));
-  JSTypedArray destination = JSTypedArray::cast(Object(raw_destination));
+  Tagged<Context> context = Context::cast(Object(raw_context));
+  Tagged<JSArray> source = JSArray::cast(Object(raw_source));
+  Tagged<JSTypedArray> destination =
+      JSTypedArray::cast(Object(raw_destination));
 
   switch (destination->GetElementsKind()) {
 #define TYPED_ARRAYS_CASE(Type, type, TYPE, ctype)           \
@@ -5404,8 +5456,9 @@ void CopyFastNumberJSArrayElementsToTypedArray(Address raw_context,
 void CopyTypedArrayElementsToTypedArray(Address raw_source,
                                         Address raw_destination,
                                         uintptr_t length, uintptr_t offset) {
-  JSTypedArray source = JSTypedArray::cast(Object(raw_source));
-  JSTypedArray destination = JSTypedArray::cast(Object(raw_destination));
+  Tagged<JSTypedArray> source = JSTypedArray::cast(Object(raw_source));
+  Tagged<JSTypedArray> destination =
+      JSTypedArray::cast(Object(raw_destination));
 
   switch (destination->GetElementsKind()) {
 #define TYPED_ARRAYS_CASE(Type, type, TYPE, ctype)                          \
@@ -5423,8 +5476,9 @@ void CopyTypedArrayElementsToTypedArray(Address raw_source,
 
 void CopyTypedArrayElementsSlice(Address raw_source, Address raw_destination,
                                  uintptr_t start, uintptr_t end) {
-  JSTypedArray source = JSTypedArray::cast(Object(raw_source));
-  JSTypedArray destination = JSTypedArray::cast(Object(raw_destination));
+  Tagged<JSTypedArray> source = JSTypedArray::cast(Object(raw_source));
+  Tagged<JSTypedArray> destination =
+      JSTypedArray::cast(Object(raw_destination));
 
   destination->GetElementsAccessor()->CopyTypedArrayElementsSlice(
       source, destination, start, end);
@@ -5461,7 +5515,7 @@ Handle<JSArray> ElementsAccessor::Concat(Isolate* isolate,
     DisallowGarbageCollection no_gc;
     bool is_holey = false;
     for (uint32_t i = 0; i < concat_size; i++) {
-      Object arg = (*args)[i];
+      Tagged<Object> arg = (*args)[i];
       ElementsKind arg_kind = JSArray::cast(arg)->GetElementsKind();
       has_raw_doubles = has_raw_doubles || IsDoubleElementsKind(arg_kind);
       is_holey = is_holey || IsHoleyElementsKind(arg_kind);
@@ -5492,7 +5546,7 @@ Handle<JSArray> ElementsAccessor::Concat(Isolate* isolate,
   for (uint32_t i = 0; i < concat_size; i++) {
     // It is crucial to keep |array| in a raw pointer form to avoid
     // performance degradation.
-    JSArray array = JSArray::cast((*args)[i]);
+    Tagged<JSArray> array = JSArray::cast((*args)[i]);
     uint32_t len = 0;
     Object::ToArrayLength(array->length(), &len);
     if (len == 0) continue;
diff --git a/src/objects/elements.h b/src/objects/elements.h
index 28ea57d46ac..7cfa42f4db7 100644
--- a/src/objects/elements.h
+++ b/src/objects/elements.h
@@ -33,7 +33,7 @@ class ElementsAccessor {
 
   // Checks the elements of an object for consistency, asserting when a problem
   // is found.
-  virtual void Validate(JSObject obj) = 0;
+  virtual void Validate(Tagged<JSObject> obj) = 0;
 
   // Returns true if a holder contains an element with the specified index
   // without iterating up the prototype chain. The first version takes the
@@ -45,16 +45,16 @@ class ElementsAccessor {
   // Note that only Dictionary elements have custom
   // PropertyAttributes associated, hence the |filter| argument is ignored for
   // all but DICTIONARY_ELEMENTS and SLOW_SLOPPY_ARGUMENTS_ELEMENTS.
-  virtual bool HasElement(JSObject holder, uint32_t index,
-                          FixedArrayBase backing_store,
+  virtual bool HasElement(Tagged<JSObject> holder, uint32_t index,
+                          Tagged<FixedArrayBase> backing_store,
                           PropertyFilter filter = ALL_PROPERTIES) = 0;
 
-  inline bool HasElement(JSObject holder, uint32_t index,
+  inline bool HasElement(Tagged<JSObject> holder, uint32_t index,
                          PropertyFilter filter = ALL_PROPERTIES);
 
   // Note: this is currently not implemented for string wrapper and
   // typed array elements.
-  virtual bool HasEntry(JSObject holder, InternalIndex entry) = 0;
+  virtual bool HasEntry(Tagged<JSObject> holder, InternalIndex entry) = 0;
 
   virtual Handle<Object> Get(Isolate* isolate, Handle<JSObject> holder,
                              InternalIndex entry) = 0;
@@ -65,8 +65,8 @@ class ElementsAccessor {
                                    InternalIndex entry,
                                    SeqCstAccessTag tag) = 0;
 
-  virtual bool HasAccessors(JSObject holder) = 0;
-  virtual size_t NumberOfElements(JSObject holder) = 0;
+  virtual bool HasAccessors(Tagged<JSObject> holder) = 0;
+  virtual size_t NumberOfElements(Tagged<JSObject> holder) = 0;
 
   // Modifies the length data property as specified for JSArrays and resizes the
   // underlying backing store accordingly. The method honors the semantics of
@@ -117,24 +117,22 @@ class ElementsAccessor {
   static void TearDown();
 
   virtual void Set(Handle<JSObject> holder, InternalIndex entry,
-                   Object value) = 0;
+                   Tagged<Object> value) = 0;
 
   // Currently only shared array elements support sequentially consistent
   // access.
   virtual void SetAtomic(Handle<JSObject> holder, InternalIndex entry,
-                         Object value, SeqCstAccessTag tag) = 0;
+                         Tagged<Object> value, SeqCstAccessTag tag) = 0;
 
   // Currently only shared array elements support sequentially consistent
   // access.
   virtual Handle<Object> SwapAtomic(Isolate* isolate, Handle<JSObject> holder,
-                                    InternalIndex entry, Object value,
+                                    InternalIndex entry, Tagged<Object> value,
                                     SeqCstAccessTag tag) = 0;
 
-  virtual Handle<Object> CompareAndSwapAtomic(Isolate* isolate,
-                                              Handle<JSObject> holder,
-                                              InternalIndex entry,
-                                              Object expected, Object value,
-                                              SeqCstAccessTag tag) = 0;
+  virtual Handle<Object> CompareAndSwapAtomic(
+      Isolate* isolate, Handle<JSObject> holder, InternalIndex entry,
+      Tagged<Object> expected, Tagged<Object> value, SeqCstAccessTag tag) = 0;
 
   V8_WARN_UNUSED_RESULT virtual Maybe<bool> Add(Handle<JSObject> object,
                                                 uint32_t index,
@@ -161,7 +159,8 @@ class ElementsAccessor {
 
   virtual Handle<NumberDictionary> Normalize(Handle<JSObject> object) = 0;
 
-  virtual size_t GetCapacity(JSObject holder, FixedArrayBase backing_store) = 0;
+  virtual size_t GetCapacity(Tagged<JSObject> holder,
+                             Tagged<FixedArrayBase> backing_store) = 0;
 
   V8_WARN_UNUSED_RESULT virtual MaybeHandle<Object> Fill(
       Handle<JSObject> receiver, Handle<Object> obj_value, size_t start,
@@ -184,22 +183,22 @@ class ElementsAccessor {
                                           Handle<Object> value,
                                           size_t start) = 0;
 
-  virtual void Reverse(JSObject receiver) = 0;
+  virtual void Reverse(Tagged<JSObject> receiver) = 0;
 
   virtual void CopyElements(Isolate* isolate, Handle<FixedArrayBase> source,
                             ElementsKind source_kind,
                             Handle<FixedArrayBase> destination, int size) = 0;
 
-  virtual Object CopyElements(Handle<Object> source,
-                              Handle<JSObject> destination, size_t length,
-                              size_t offset) = 0;
+  virtual Tagged<Object> CopyElements(Handle<Object> source,
+                                      Handle<JSObject> destination,
+                                      size_t length, size_t offset) = 0;
 
   virtual Handle<FixedArray> CreateListFromArrayLike(Isolate* isolate,
                                                      Handle<JSObject> object,
                                                      uint32_t length) = 0;
 
-  virtual void CopyTypedArrayElementsSlice(JSTypedArray source,
-                                           JSTypedArray destination,
+  virtual void CopyTypedArrayElementsSlice(Tagged<JSTypedArray> source,
+                                           Tagged<JSTypedArray> destination,
                                            size_t start, size_t end) = 0;
 
  protected:
@@ -213,11 +212,13 @@ class ElementsAccessor {
   // indices are equivalent to entries. In the NumberDictionary
   // ElementsAccessor, entries are mapped to an index using the KeyAt method on
   // the NumberDictionary.
-  virtual InternalIndex GetEntryForIndex(Isolate* isolate, JSObject holder,
-                                         FixedArrayBase backing_store,
+  virtual InternalIndex GetEntryForIndex(Isolate* isolate,
+                                         Tagged<JSObject> holder,
+                                         Tagged<FixedArrayBase> backing_store,
                                          size_t index) = 0;
 
-  virtual PropertyDetails GetDetails(JSObject holder, InternalIndex entry) = 0;
+  virtual PropertyDetails GetDetails(Tagged<JSObject> holder,
+                                     InternalIndex entry) = 0;
   virtual void Reconfigure(Handle<JSObject> object,
                            Handle<FixedArrayBase> backing_store,
                            InternalIndex entry, Handle<Object> value,
@@ -230,8 +231,8 @@ class ElementsAccessor {
   // raw pointer parameter |source_holder| in the function that allocates.
   // This is done intentionally to avoid ArrayConcat() builtin performance
   // degradation.
-  virtual void CopyElements(JSObject source_holder, uint32_t source_start,
-                            ElementsKind source_kind,
+  virtual void CopyElements(Tagged<JSObject> source_holder,
+                            uint32_t source_start, ElementsKind source_kind,
                             Handle<FixedArrayBase> destination,
                             uint32_t destination_start, int copy_size) = 0;
 
diff --git a/src/objects/embedder-data-slot-inl.h b/src/objects/embedder-data-slot-inl.h
index 83a39eb760b..d4e5a9b932b 100644
--- a/src/objects/embedder-data-slot-inl.h
+++ b/src/objects/embedder-data-slot-inl.h
@@ -20,18 +20,20 @@
 namespace v8 {
 namespace internal {
 
-EmbedderDataSlot::EmbedderDataSlot(EmbedderDataArray array, int entry_index)
+EmbedderDataSlot::EmbedderDataSlot(Tagged<EmbedderDataArray> array,
+                                   int entry_index)
     : SlotBase(FIELD_ADDR(array,
                           EmbedderDataArray::OffsetOfElementAt(entry_index))) {}
 
-EmbedderDataSlot::EmbedderDataSlot(JSObject object, int embedder_field_index)
+EmbedderDataSlot::EmbedderDataSlot(Tagged<JSObject> object,
+                                   int embedder_field_index)
     : SlotBase(FIELD_ADDR(
           object, object->GetEmbedderFieldOffset(embedder_field_index))) {}
 
 EmbedderDataSlot::EmbedderDataSlot(const EmbedderDataSlotSnapshot& snapshot)
     : SlotBase(reinterpret_cast<Address>(&snapshot)) {}
 
-void EmbedderDataSlot::Initialize(Object initial_value) {
+void EmbedderDataSlot::Initialize(Tagged<Object> initial_value) {
   // TODO(v8) initialize the slot with Smi::zero() instead. This'll also
   // guarantee that we don't need a write barrier.
   DCHECK(IsSmi(initial_value) ||
@@ -42,11 +44,11 @@ void EmbedderDataSlot::Initialize(Object initial_value) {
 #endif
 }
 
-Object EmbedderDataSlot::load_tagged() const {
+Tagged<Object> EmbedderDataSlot::load_tagged() const {
   return ObjectSlot(address() + kTaggedPayloadOffset).Relaxed_Load();
 }
 
-void EmbedderDataSlot::store_smi(Smi value) {
+void EmbedderDataSlot::store_smi(Tagged<Smi> value) {
   ObjectSlot(address() + kTaggedPayloadOffset).Relaxed_Store(value);
 #ifdef V8_COMPRESS_POINTERS
   // See gc_safe_store() for the reasons behind two stores.
@@ -55,8 +57,8 @@ void EmbedderDataSlot::store_smi(Smi value) {
 }
 
 // static
-void EmbedderDataSlot::store_tagged(EmbedderDataArray array, int entry_index,
-                                    Object value) {
+void EmbedderDataSlot::store_tagged(Tagged<EmbedderDataArray> array,
+                                    int entry_index, Tagged<Object> value) {
 #ifdef V8_COMPRESS_POINTERS
   CHECK(IsSmi(value) ||
         V8HeapCompressionScheme::GetPtrComprCageBaseAddress(value.ptr()) ==
@@ -74,8 +76,9 @@ void EmbedderDataSlot::store_tagged(EmbedderDataArray array, int entry_index,
 }
 
 // static
-void EmbedderDataSlot::store_tagged(JSObject object, int embedder_field_index,
-                                    Object value) {
+void EmbedderDataSlot::store_tagged(Tagged<JSObject> object,
+                                    int embedder_field_index,
+                                    Tagged<Object> value) {
 #ifdef V8_COMPRESS_POINTERS
   CHECK(IsSmi(value) ||
         V8HeapCompressionScheme::GetPtrComprCageBaseAddress(value.ptr()) ==
@@ -189,7 +192,7 @@ void EmbedderDataSlot::gc_safe_store(Isolate* isolate, Address value) {
 
 // static
 void EmbedderDataSlot::PopulateEmbedderDataSnapshot(
-    Map map, JSObject js_object, int entry_index,
+    Tagged<Map> map, Tagged<JSObject> js_object, int entry_index,
     EmbedderDataSlotSnapshot& snapshot) {
 #ifdef V8_COMPRESS_POINTERS
   static_assert(sizeof(EmbedderDataSlotSnapshot) == sizeof(AtomicTagged_t) * 2);
diff --git a/src/objects/embedder-data-slot.h b/src/objects/embedder-data-slot.h
index 726c84e458a..76cbb5128cb 100644
--- a/src/objects/embedder-data-slot.h
+++ b/src/objects/embedder-data-slot.h
@@ -90,31 +90,32 @@ class EmbedderDataSlot
   static constexpr int kRequiredPtrAlignment = kSmiTagSize;
 
   using EmbedderDataSlotSnapshot = Address;
-  V8_INLINE static void PopulateEmbedderDataSnapshot(Map map,
-                                                     JSObject js_object,
+  V8_INLINE static void PopulateEmbedderDataSnapshot(Tagged<Map> map,
+                                                     Tagged<JSObject> js_object,
                                                      int entry_index,
                                                      EmbedderDataSlotSnapshot&);
 
   EmbedderDataSlot() : SlotBase(kNullAddress) {}
-  V8_INLINE EmbedderDataSlot(EmbedderDataArray array, int entry_index);
-  V8_INLINE EmbedderDataSlot(JSObject object, int embedder_field_index);
+  V8_INLINE EmbedderDataSlot(Tagged<EmbedderDataArray> array, int entry_index);
+  V8_INLINE EmbedderDataSlot(Tagged<JSObject> object, int embedder_field_index);
   V8_INLINE explicit EmbedderDataSlot(const EmbedderDataSlotSnapshot& snapshot);
 
   // Opaque type used for storing raw embedder data.
   using RawData = Address;
 
-  V8_INLINE void Initialize(Object initial_value);
+  V8_INLINE void Initialize(Tagged<Object> initial_value);
 
-  V8_INLINE Object load_tagged() const;
-  V8_INLINE void store_smi(Smi value);
+  V8_INLINE Tagged<Object> load_tagged() const;
+  V8_INLINE void store_smi(Tagged<Smi> value);
 
   // Setting an arbitrary tagged value requires triggering a write barrier
   // which requires separate object and offset values, therefore these static
   // functions also has the target object parameter.
-  static V8_INLINE void store_tagged(EmbedderDataArray array, int entry_index,
-                                     Object value);
-  static V8_INLINE void store_tagged(JSObject object, int embedder_field_index,
-                                     Object value);
+  static V8_INLINE void store_tagged(Tagged<EmbedderDataArray> array,
+                                     int entry_index, Tagged<Object> value);
+  static V8_INLINE void store_tagged(Tagged<JSObject> object,
+                                     int embedder_field_index,
+                                     Tagged<Object> value);
 
   // Tries reinterpret the value as an aligned pointer and sets *out_result to
   // the pointer-like value. Note, that some Smis could still look like an
diff --git a/src/objects/feedback-cell-inl.h b/src/objects/feedback-cell-inl.h
index 845083e91c1..0e0a59d2a9d 100644
--- a/src/objects/feedback-cell-inl.h
+++ b/src/objects/feedback-cell-inl.h
@@ -32,14 +32,14 @@ void FeedbackCell::clear_padding() {
 }
 
 void FeedbackCell::reset_feedback_vector(
-    base::Optional<std::function<void(HeapObject object, ObjectSlot slot,
-                                      HeapObject target)>>
+    base::Optional<std::function<void(
+        Tagged<HeapObject> object, ObjectSlot slot, Tagged<HeapObject> target)>>
         gc_notify_updated_slot) {
   clear_interrupt_budget();
   if (IsUndefined(value()) || IsClosureFeedbackCellArray(value())) return;
 
   CHECK(IsFeedbackVector(value()));
-  ClosureFeedbackCellArray closure_feedback_cell_array =
+  Tagged<ClosureFeedbackCellArray> closure_feedback_cell_array =
       FeedbackVector::cast(value())->closure_feedback_cell_array();
   set_value(closure_feedback_cell_array, kReleaseStore);
   if (gc_notify_updated_slot) {
diff --git a/src/objects/feedback-cell.h b/src/objects/feedback-cell.h
index 70c3612f66d..25cebb5bd59 100644
--- a/src/objects/feedback-cell.h
+++ b/src/objects/feedback-cell.h
@@ -37,8 +37,9 @@ class FeedbackCell : public TorqueGeneratedFeedbackCell<FeedbackCell, Struct> {
 
   inline void clear_padding();
   inline void reset_feedback_vector(
-      base::Optional<std::function<void(HeapObject object, ObjectSlot slot,
-                                        HeapObject target)>>
+      base::Optional<
+          std::function<void(Tagged<HeapObject> object, ObjectSlot slot,
+                             Tagged<HeapObject> target)>>
           gc_notify_updated_slot = base::nullopt);
 
   // The closure count is encoded in the cell's map, which distinguishes
diff --git a/src/objects/feedback-vector-inl.h b/src/objects/feedback-vector-inl.h
index 951524ca10e..da1de042316 100644
--- a/src/objects/feedback-vector-inl.h
+++ b/src/objects/feedback-vector-inl.h
@@ -99,7 +99,7 @@ Handle<FeedbackCell> ClosureFeedbackCellArray::GetFeedbackCell(int index) {
   return handle(FeedbackCell::cast(get(index)), GetIsolate());
 }
 
-FeedbackCell ClosureFeedbackCellArray::cell(int index) {
+Tagged<FeedbackCell> ClosureFeedbackCellArray::cell(int index) {
   return FeedbackCell::cast(get(index));
 }
 
@@ -162,11 +162,11 @@ void FeedbackVector::set_maybe_has_optimized_osr_code(bool value,
   }
 }
 
-Code FeedbackVector::optimized_code() const {
+Tagged<Code> FeedbackVector::optimized_code() const {
   MaybeObject slot = maybe_optimized_code();
   DCHECK(slot->IsWeakOrCleared());
-  HeapObject heap_object;
-  Code code;
+  Tagged<HeapObject> heap_object;
+  Tagged<Code> code;
   if (slot->GetHeapObject(&heap_object)) {
     code = Code::cast(heap_object);
   }
@@ -221,7 +221,7 @@ base::Optional<Code> FeedbackVector::GetOptimizedOsrCode(Isolate* isolate,
   MaybeObject maybe_code = Get(isolate, slot);
   if (maybe_code->IsCleared()) return {};
 
-  Code code = Code::cast(maybe_code->GetHeapObject());
+  Tagged<Code> code = Code::cast(maybe_code->GetHeapObject());
   if (code->marked_for_deoptimization()) {
     // Clear the cached Code object if deoptimized.
     // TODO(jgruber): Add tracing.
@@ -244,7 +244,7 @@ FeedbackSlot FeedbackVector::ToSlot(intptr_t index) {
 // Instead of FixedArray, the Feedback and the Extra should contain
 // WeakFixedArrays. The only allowed FixedArray subtype is HashTable.
 bool FeedbackVector::IsOfLegacyType(MaybeObject value) {
-  HeapObject heap_object;
+  Tagged<HeapObject> heap_object;
   if (value->GetHeapObject(&heap_object)) {
     return IsFixedArray(heap_object) && !IsHashTable(heap_object);
   }
@@ -271,7 +271,7 @@ Handle<FeedbackCell> FeedbackVector::GetClosureFeedbackCell(int index) const {
   return closure_feedback_cell_array()->GetFeedbackCell(index);
 }
 
-FeedbackCell FeedbackVector::closure_feedback_cell(int index) const {
+Tagged<FeedbackCell> FeedbackVector::closure_feedback_cell(int index) const {
   DCHECK_GE(index, 0);
   return closure_feedback_cell_array()->cell(index);
 }
@@ -295,7 +295,7 @@ void FeedbackVector::SynchronizedSet(FeedbackSlot slot, MaybeObject value,
   CONDITIONAL_WEAK_WRITE_BARRIER(*this, offset, value, mode);
 }
 
-void FeedbackVector::SynchronizedSet(FeedbackSlot slot, Object value,
+void FeedbackVector::SynchronizedSet(FeedbackSlot slot, Tagged<Object> value,
                                      WriteBarrierMode mode) {
   SynchronizedSet(slot, MaybeObject::FromObject(value), mode);
 }
@@ -306,7 +306,7 @@ void FeedbackVector::Set(FeedbackSlot slot, MaybeObject value,
   set_raw_feedback_slots(GetIndex(slot), value, mode);
 }
 
-void FeedbackVector::Set(FeedbackSlot slot, Object value,
+void FeedbackVector::Set(FeedbackSlot slot, Tagged<Object> value,
                          WriteBarrierMode mode) {
   MaybeObject maybe_value = MaybeObject::FromObject(value);
   DCHECK(!IsOfLegacyType(maybe_value));
@@ -415,7 +415,7 @@ Handle<Symbol> FeedbackVector::MegaDOMSentinel(Isolate* isolate) {
   return ReadOnlyRoots(isolate).mega_dom_symbol_handle();
 }
 
-Symbol FeedbackVector::RawUninitializedSentinel(Isolate* isolate) {
+Tagged<Symbol> FeedbackVector::RawUninitializedSentinel(Isolate* isolate) {
   return ReadOnlyRoots(isolate).uninitialized_symbol();
 }
 
@@ -435,12 +435,12 @@ int FeedbackMetadataIterator::entry_size() const {
   return FeedbackMetadata::GetSlotSize(kind());
 }
 
-MaybeObject NexusConfig::GetFeedback(FeedbackVector vector,
+MaybeObject NexusConfig::GetFeedback(Tagged<FeedbackVector> vector,
                                      FeedbackSlot slot) const {
   return vector->SynchronizedGet(slot);
 }
 
-void NexusConfig::SetFeedback(FeedbackVector vector, FeedbackSlot slot,
+void NexusConfig::SetFeedback(Tagged<FeedbackVector> vector, FeedbackSlot slot,
                               MaybeObject feedback,
                               WriteBarrierMode mode) const {
   DCHECK(can_write());
diff --git a/src/objects/feedback-vector.cc b/src/objects/feedback-vector.cc
index 50b74f58f44..3be897c7d56 100644
--- a/src/objects/feedback-vector.cc
+++ b/src/objects/feedback-vector.cc
@@ -33,14 +33,14 @@ FeedbackSlot FeedbackVectorSpec::AddSlot(FeedbackSlotKind kind) {
 }
 
 static bool IsPropertyNameFeedback(MaybeObject feedback) {
-  HeapObject heap_object;
+  Tagged<HeapObject> heap_object;
   if (!feedback->GetHeapObjectIfStrong(&heap_object)) return false;
   if (IsString(heap_object)) {
     DCHECK(IsInternalizedString(heap_object));
     return true;
   }
   if (!IsSymbol(heap_object)) return false;
-  Symbol symbol = Symbol::cast(heap_object);
+  Tagged<Symbol> symbol = Symbol::cast(heap_object);
   ReadOnlyRoots roots = symbol->GetReadOnlyRoots();
   return symbol != roots.uninitialized_symbol() &&
          symbol != roots.mega_dom_symbol() &&
@@ -347,7 +347,7 @@ void FeedbackVector::AddToVectorsForProfilingTools(
   isolate->SetFeedbackVectorsForProfilingTools(*list);
 }
 
-void FeedbackVector::SetOptimizedCode(Code code) {
+void FeedbackVector::SetOptimizedCode(Tagged<Code> code) {
   DCHECK(CodeKindIsOptimizedJSFunction(code->kind()));
   int32_t state = flags();
   // Skip setting optimized code if it would cause us to tier down.
@@ -393,7 +393,7 @@ void FeedbackVector::ClearOptimizedCode() {
 }
 
 void FeedbackVector::SetOptimizedOsrCode(Isolate* isolate, FeedbackSlot slot,
-                                         Code code) {
+                                         Tagged<Code> code) {
   DCHECK(CodeKindIsOptimizedJSFunction(code->kind()));
   DCHECK(!slot.IsInvalid());
   auto current = GetOptimizedOsrCode(isolate, slot);
@@ -438,7 +438,7 @@ void FeedbackVector::set_osr_tiering_state(TieringState marker) {
 }
 
 void FeedbackVector::EvictOptimizedCodeMarkedForDeoptimization(
-    Isolate* isolate, SharedFunctionInfo shared, const char* reason) {
+    Isolate* isolate, Tagged<SharedFunctionInfo> shared, const char* reason) {
   MaybeObject slot = maybe_optimized_code();
   if (slot->IsCleared()) {
     set_maybe_has_maglev_code(false);
@@ -446,7 +446,7 @@ void FeedbackVector::EvictOptimizedCodeMarkedForDeoptimization(
     return;
   }
 
-  Code code = Code::cast(slot->GetHeapObject());
+  Tagged<Code> code = Code::cast(slot->GetHeapObject());
   if (code->marked_for_deoptimization()) {
     Deoptimizer::TraceEvictFromOptimizedCodeCache(isolate, shared, reason);
     ClearOptimizedCode();
@@ -481,7 +481,7 @@ MaybeObjectHandle NexusConfig::NewHandle(MaybeObject object) const {
 }
 
 template <typename T>
-Handle<T> NexusConfig::NewHandle(T object) const {
+Handle<T> NexusConfig::NewHandle(Tagged<T> object) const {
   if (mode() == Mode::MainThread) {
     return handle(object, isolate_);
   }
@@ -489,7 +489,7 @@ Handle<T> NexusConfig::NewHandle(T object) const {
   return handle(object, local_heap_);
 }
 
-void NexusConfig::SetFeedbackPair(FeedbackVector vector,
+void NexusConfig::SetFeedbackPair(Tagged<FeedbackVector> vector,
                                   FeedbackSlot start_slot, MaybeObject feedback,
                                   WriteBarrierMode mode,
                                   MaybeObject feedback_extra,
@@ -503,7 +503,7 @@ void NexusConfig::SetFeedbackPair(FeedbackVector vector,
 }
 
 std::pair<MaybeObject, MaybeObject> NexusConfig::GetFeedbackPair(
-    FeedbackVector vector, FeedbackSlot slot) const {
+    Tagged<FeedbackVector> vector, FeedbackSlot slot) const {
   base::SharedMutexGuardIf<base::kShared> scope(
       isolate()->feedback_vector_access(), mode() == BackgroundThread);
   MaybeObject feedback = vector->Get(slot);
@@ -519,7 +519,7 @@ FeedbackNexus::FeedbackNexus(Handle<FeedbackVector> vector, FeedbackSlot slot)
   kind_ = vector.is_null() ? FeedbackSlotKind::kInvalid : vector->GetKind(slot);
 }
 
-FeedbackNexus::FeedbackNexus(FeedbackVector vector, FeedbackSlot slot)
+FeedbackNexus::FeedbackNexus(Tagged<FeedbackVector> vector, FeedbackSlot slot)
     : vector_(vector),
       slot_(slot),
       config_(NexusConfig::FromMainThread(
@@ -672,7 +672,7 @@ bool FeedbackNexus::ConfigureMegamorphic(IcCheckType property_type) {
   return update_required;
 }
 
-Map FeedbackNexus::GetFirstMap() const {
+Tagged<Map> FeedbackNexus::GetFirstMap() const {
   FeedbackIterator it(this);
   if (!it.done()) {
     return it.map();
@@ -728,7 +728,7 @@ InlineCacheState FeedbackNexus::ic_state() const {
         // Don't check if the map is cleared.
         return InlineCacheState::MONOMORPHIC;
       }
-      HeapObject heap_object;
+      Tagged<HeapObject> heap_object;
       if (feedback->GetHeapObjectIfStrong(&heap_object)) {
         if (IsWeakFixedArray(heap_object)) {
           // Determine state purely by our structure, don't check if the maps
@@ -738,8 +738,9 @@ InlineCacheState FeedbackNexus::ic_state() const {
         if (IsName(heap_object)) {
           DCHECK(IsKeyedLoadICKind(kind()) || IsKeyedStoreICKind(kind()) ||
                  IsKeyedHasICKind(kind()) || IsDefineKeyedOwnICKind(kind()));
-          Object extra_object = extra->GetHeapObjectAssumeStrong();
-          WeakFixedArray extra_array = WeakFixedArray::cast(extra_object);
+          Tagged<Object> extra_object = extra->GetHeapObjectAssumeStrong();
+          Tagged<WeakFixedArray> extra_array =
+              WeakFixedArray::cast(extra_object);
           return extra_array->length() > 2 ? InlineCacheState::POLYMORPHIC
                                            : InlineCacheState::MONOMORPHIC;
         }
@@ -758,7 +759,7 @@ InlineCacheState FeedbackNexus::ic_state() const {
       UNREACHABLE();
     }
     case FeedbackSlotKind::kCall: {
-      HeapObject heap_object;
+      Tagged<HeapObject> heap_object;
       if (feedback == MegamorphicSentinel()) {
         return InlineCacheState::GENERIC;
       } else if (feedback->IsWeakOrCleared()) {
@@ -973,7 +974,7 @@ void FeedbackNexus::ConfigureCloneObject(
 int FeedbackNexus::GetCallCount() {
   DCHECK(IsCallICKind(kind()));
 
-  Object call_count = GetFeedbackExtra()->cast<Object>();
+  Tagged<Object> call_count = GetFeedbackExtra()->cast<Object>();
   CHECK(IsSmi(call_count));
   uint32_t value = static_cast<uint32_t>(Smi::ToInt(call_count));
   return CallCountField::decode(value);
@@ -982,7 +983,7 @@ int FeedbackNexus::GetCallCount() {
 void FeedbackNexus::SetSpeculationMode(SpeculationMode mode) {
   DCHECK(IsCallICKind(kind()));
 
-  Object call_count = GetFeedbackExtra()->cast<Object>();
+  Tagged<Object> call_count = GetFeedbackExtra()->cast<Object>();
   CHECK(IsSmi(call_count));
   uint32_t count = static_cast<uint32_t>(Smi::ToInt(call_count));
   count = SpeculationModeField::update(count, mode);
@@ -996,7 +997,7 @@ void FeedbackNexus::SetSpeculationMode(SpeculationMode mode) {
 SpeculationMode FeedbackNexus::GetSpeculationMode() {
   DCHECK(IsCallICKind(kind()));
 
-  Object call_count = GetFeedbackExtra()->cast<Object>();
+  Tagged<Object> call_count = GetFeedbackExtra()->cast<Object>();
   CHECK(IsSmi(call_count));
   uint32_t value = static_cast<uint32_t>(Smi::ToInt(call_count));
   return SpeculationModeField::decode(value);
@@ -1005,7 +1006,7 @@ SpeculationMode FeedbackNexus::GetSpeculationMode() {
 CallFeedbackContent FeedbackNexus::GetCallFeedbackContent() {
   DCHECK(IsCallICKind(kind()));
 
-  Object call_count = GetFeedbackExtra()->cast<Object>();
+  Tagged<Object> call_count = GetFeedbackExtra()->cast<Object>();
   CHECK(IsSmi(call_count));
   uint32_t value = static_cast<uint32_t>(Smi::ToInt(call_count));
   return CallFeedbackContentField::decode(value);
@@ -1144,7 +1145,7 @@ MaybeObjectHandle FeedbackNexus::FindHandlerForMap(Handle<Map> map) const {
   return MaybeObjectHandle();
 }
 
-Name FeedbackNexus::GetName() const {
+Tagged<Name> FeedbackNexus::GetName() const {
   if (IsKeyedStoreICKind(kind()) || IsKeyedLoadICKind(kind()) ||
       IsKeyedHasICKind(kind()) || IsDefineKeyedOwnICKind(kind())) {
     MaybeObject feedback = GetFeedback();
@@ -1252,7 +1253,7 @@ KeyedAccessStoreMode FeedbackNexus::GetKeyedAccessStoreMode() const {
         if (mode != STANDARD_STORE) return mode;
         continue;
       } else {
-        Code code = Code::cast(data_handler->smi_handler());
+        Tagged<Code> code = Code::cast(data_handler->smi_handler());
         builtin_handler = code->builtin_id();
       }
 
@@ -1271,7 +1272,7 @@ KeyedAccessStoreMode FeedbackNexus::GetKeyedAccessStoreMode() const {
       continue;
     } else {
       // Element store without prototype chain check.
-      Code code = Code::cast(*maybe_code_handler.object());
+      Tagged<Code> code = Code::cast(*maybe_code_handler.object());
       builtin_handler = code->builtin_id();
     }
 
@@ -1326,7 +1327,7 @@ ForInHint FeedbackNexus::GetForInFeedback() const {
 MaybeHandle<JSObject> FeedbackNexus::GetConstructorFeedback() const {
   DCHECK_EQ(kind(), FeedbackSlotKind::kInstanceOf);
   MaybeObject feedback = GetFeedback();
-  HeapObject heap_object;
+  Tagged<HeapObject> heap_object;
   if (feedback->GetHeapObjectIfWeak(&heap_object)) {
     return config()->NewHandle(JSObject::cast(heap_object));
   }
@@ -1347,7 +1348,7 @@ FeedbackIterator::FeedbackIterator(const FeedbackNexus* nexus)
   auto pair = nexus->GetFeedbackPair();
   MaybeObject feedback = pair.first;
   bool is_named_feedback = IsPropertyNameFeedback(feedback);
-  HeapObject heap_object;
+  Tagged<HeapObject> heap_object;
 
   if ((feedback->GetHeapObjectIfStrong(&heap_object) &&
        IsWeakFixedArray(heap_object)) ||
@@ -1389,7 +1390,7 @@ void FeedbackIterator::AdvancePolymorphic() {
   CHECK(!done_);
   CHECK_EQ(state_, kPolymorphic);
   int length = polymorphic_feedback_->length();
-  HeapObject heap_object;
+  Tagged<HeapObject> heap_object;
 
   while (index_ < length) {
     if (polymorphic_feedback_->Get(index_)->GetHeapObjectIfWeak(&heap_object)) {
diff --git a/src/objects/feedback-vector.h b/src/objects/feedback-vector.h
index 8cfa042727d..4e446d809ed 100644
--- a/src/objects/feedback-vector.h
+++ b/src/objects/feedback-vector.h
@@ -186,7 +186,7 @@ class ClosureFeedbackCellArray : public FixedArray {
       Isolate* isolate, Handle<SharedFunctionInfo> shared);
 
   inline Handle<FeedbackCell> GetFeedbackCell(int index);
-  inline FeedbackCell cell(int index);
+  inline Tagged<FeedbackCell> cell(int index);
 
   DECL_VERIFIER(ClosureFeedbackCellArray)
   DECL_PRINTER(ClosureFeedbackCellArray)
@@ -252,7 +252,7 @@ class FeedbackVector
   // The `osr_state` contains the osr_urgency and maybe_has_optimized_osr_code.
   inline void reset_osr_state();
 
-  inline Code optimized_code() const;
+  inline Tagged<Code> optimized_code() const;
   // Whether maybe_optimized_code contains a cached Code object.
   inline bool has_optimized_code() const;
 
@@ -266,17 +266,17 @@ class FeedbackVector
   inline bool maybe_has_turbofan_code() const;
   inline void set_maybe_has_turbofan_code(bool value);
 
-  void SetOptimizedCode(Code code);
-  void EvictOptimizedCodeMarkedForDeoptimization(Isolate* isolate,
-                                                 SharedFunctionInfo shared,
-                                                 const char* reason);
+  void SetOptimizedCode(Tagged<Code> code);
+  void EvictOptimizedCodeMarkedForDeoptimization(
+      Isolate* isolate, Tagged<SharedFunctionInfo> shared, const char* reason);
   void ClearOptimizedCode();
 
   // Optimized OSR'd code is cached in JumpLoop feedback vector slots. The
   // slots either contain a Code object or the ClearedValue.
   inline base::Optional<Code> GetOptimizedOsrCode(Isolate* isolate,
                                                   FeedbackSlot slot);
-  void SetOptimizedOsrCode(Isolate* isolate, FeedbackSlot slot, Code code);
+  void SetOptimizedOsrCode(Isolate* isolate, FeedbackSlot slot,
+                           Tagged<Code> code);
 
   inline TieringState tiering_state() const;
   void set_tiering_state(TieringState state);
@@ -296,7 +296,7 @@ class FeedbackVector
   inline MaybeObject SynchronizedGet(FeedbackSlot slot) const;
   inline void SynchronizedSet(FeedbackSlot slot, MaybeObject value,
                               WriteBarrierMode mode = UPDATE_WRITE_BARRIER);
-  inline void SynchronizedSet(FeedbackSlot slot, Object value,
+  inline void SynchronizedSet(FeedbackSlot slot, Tagged<Object> value,
                               WriteBarrierMode mode = UPDATE_WRITE_BARRIER);
 
   inline MaybeObject Get(FeedbackSlot slot) const;
@@ -305,7 +305,7 @@ class FeedbackVector
   // Returns the feedback cell at |index| that is used to create the
   // closure.
   inline Handle<FeedbackCell> GetClosureFeedbackCell(int index) const;
-  inline FeedbackCell closure_feedback_cell(int index) const;
+  inline Tagged<FeedbackCell> closure_feedback_cell(int index) const;
 
   // Gives access to raw memory which stores the array's data.
   inline MaybeObjectSlot slots_start();
@@ -377,7 +377,7 @@ class FeedbackVector
 
   // A raw version of the uninitialized sentinel that's safe to read during
   // garbage collection (e.g., for patching the cache).
-  static inline Symbol RawUninitializedSentinel(Isolate* isolate);
+  static inline Tagged<Symbol> RawUninitializedSentinel(Isolate* isolate);
 
   static_assert(kHeaderSize % kObjectAlignment == 0,
                 "Header must be padded for alignment");
@@ -399,7 +399,7 @@ class FeedbackVector
   // Private for initializing stores in FeedbackVector::New().
   inline void Set(FeedbackSlot slot, MaybeObject value,
                   WriteBarrierMode mode = UPDATE_WRITE_BARRIER);
-  inline void Set(FeedbackSlot slot, Object value,
+  inline void Set(FeedbackSlot slot, Tagged<Object> value,
                   WriteBarrierMode mode = UPDATE_WRITE_BARRIER);
 
 #ifdef DEBUG
@@ -720,19 +720,19 @@ class V8_EXPORT_PRIVATE NexusConfig {
 
   MaybeObjectHandle NewHandle(MaybeObject object) const;
   template <typename T>
-  Handle<T> NewHandle(T object) const;
+  Handle<T> NewHandle(Tagged<T> object) const;
 
   bool can_write() const { return mode() == MainThread; }
 
-  inline MaybeObject GetFeedback(FeedbackVector vector,
+  inline MaybeObject GetFeedback(Tagged<FeedbackVector> vector,
                                  FeedbackSlot slot) const;
-  inline void SetFeedback(FeedbackVector vector, FeedbackSlot slot,
+  inline void SetFeedback(Tagged<FeedbackVector> vector, FeedbackSlot slot,
                           MaybeObject object,
                           WriteBarrierMode mode = UPDATE_WRITE_BARRIER) const;
 
-  std::pair<MaybeObject, MaybeObject> GetFeedbackPair(FeedbackVector vector,
-                                                      FeedbackSlot slot) const;
-  void SetFeedbackPair(FeedbackVector vector, FeedbackSlot start_slot,
+  std::pair<MaybeObject, MaybeObject> GetFeedbackPair(
+      Tagged<FeedbackVector> vector, FeedbackSlot slot) const;
+  void SetFeedbackPair(Tagged<FeedbackVector> vector, FeedbackSlot start_slot,
                        MaybeObject feedback, WriteBarrierMode mode,
                        MaybeObject feedback_extra,
                        WriteBarrierMode mode_extra) const;
@@ -752,7 +752,7 @@ class V8_EXPORT_PRIVATE FeedbackNexus final {
  public:
   // For use on the main thread. A null {vector} is accepted as well.
   FeedbackNexus(Handle<FeedbackVector> vector, FeedbackSlot slot);
-  FeedbackNexus(FeedbackVector vector, FeedbackSlot slot);
+  FeedbackNexus(Tagged<FeedbackVector> vector, FeedbackSlot slot);
 
   // For use on the main or background thread as configured by {config}.
   // {vector} must be valid.
@@ -787,7 +787,7 @@ class V8_EXPORT_PRIVATE FeedbackNexus final {
   void Print(std::ostream& os);
 
   // For map-based ICs (load, keyed-load, store, keyed-store).
-  Map GetFirstMap() const;
+  Tagged<Map> GetFirstMap() const;
   int ExtractMaps(MapHandles* maps) const;
   // Used to obtain maps and the associated handlers stored in the feedback
   // vector. This should be called when we expect only a handler to be stored in
@@ -844,7 +844,7 @@ class V8_EXPORT_PRIVATE FeedbackNexus final {
 
   // For KeyedLoad and KeyedStore ICs.
   IcCheckType GetKeyType() const;
-  Name GetName() const;
+  Tagged<Name> GetName() const;
 
   // For Call ICs.
   int GetCallCount();
@@ -927,7 +927,7 @@ class V8_EXPORT_PRIVATE FeedbackIterator final {
   explicit FeedbackIterator(const FeedbackNexus* nexus);
   void Advance();
   bool done() { return done_; }
-  Map map() { return map_; }
+  Tagged<Map> map() { return map_; }
   MaybeObject handler() { return handler_; }
 
   static int SizeFor(int number_of_entries) {
diff --git a/src/objects/field-index-inl.h b/src/objects/field-index-inl.h
index 80219c8f050..056809a9fb2 100644
--- a/src/objects/field-index-inl.h
+++ b/src/objects/field-index-inl.h
@@ -21,7 +21,7 @@ FieldIndex FieldIndex::ForInObjectOffset(int offset, Encoding encoding) {
   return FieldIndex(true, offset, encoding, 0, 0);
 }
 
-FieldIndex FieldIndex::ForSmiLoadHandler(Map map, int32_t handler) {
+FieldIndex FieldIndex::ForSmiLoadHandler(Tagged<Map> map, int32_t handler) {
   DCHECK_EQ(LoadHandler::KindBits::decode(handler), LoadHandler::Kind::kField);
 
   bool is_inobject = LoadHandler::IsInobjectBits::decode(handler);
@@ -38,7 +38,7 @@ FieldIndex FieldIndex::ForSmiLoadHandler(Map map, int32_t handler) {
       inobject_properties, first_inobject_offset);
 }
 
-FieldIndex FieldIndex::ForPropertyIndex(Map map, int property_index,
+FieldIndex FieldIndex::ForPropertyIndex(Tagged<Map> map, int property_index,
                                         Representation representation) {
   DCHECK(map->instance_type() >= FIRST_NONSTRING_TYPE);
   int inobject_properties = map->GetInObjectProperties();
@@ -79,19 +79,21 @@ int FieldIndex::GetLoadByFieldIndex() const {
   return is_double() ? (result | 1) : result;
 }
 
-FieldIndex FieldIndex::ForDescriptor(Map map, InternalIndex descriptor_index) {
+FieldIndex FieldIndex::ForDescriptor(Tagged<Map> map,
+                                     InternalIndex descriptor_index) {
   PtrComprCageBase cage_base = GetPtrComprCageBase(map);
   return ForDescriptor(cage_base, map, descriptor_index);
 }
 
-FieldIndex FieldIndex::ForDescriptor(PtrComprCageBase cage_base, Map map,
+FieldIndex FieldIndex::ForDescriptor(PtrComprCageBase cage_base,
+                                     Tagged<Map> map,
                                      InternalIndex descriptor_index) {
   PropertyDetails details = map->instance_descriptors(cage_base, kRelaxedLoad)
                                 ->GetDetails(descriptor_index);
   return ForDetails(map, details);
 }
 
-FieldIndex FieldIndex::ForDetails(Map map, PropertyDetails details) {
+FieldIndex FieldIndex::ForDetails(Tagged<Map> map, PropertyDetails details) {
   int field_index = details.field_index();
   return ForPropertyIndex(map, field_index, details.representation());
 }
diff --git a/src/objects/field-index.h b/src/objects/field-index.h
index aa4d859c28d..6d943f86c06 100644
--- a/src/objects/field-index.h
+++ b/src/objects/field-index.h
@@ -26,15 +26,16 @@ class FieldIndex final {
   FieldIndex() : bit_field_(0) {}
 
   static inline FieldIndex ForPropertyIndex(
-      Map map, int index,
+      Tagged<Map> map, int index,
       Representation representation = Representation::Tagged());
   static inline FieldIndex ForInObjectOffset(int offset, Encoding encoding);
-  static inline FieldIndex ForSmiLoadHandler(Map map, int32_t handler);
-  static inline FieldIndex ForDescriptor(Map map,
+  static inline FieldIndex ForSmiLoadHandler(Tagged<Map> map, int32_t handler);
+  static inline FieldIndex ForDescriptor(Tagged<Map> map,
                                          InternalIndex descriptor_index);
-  static inline FieldIndex ForDescriptor(PtrComprCageBase cage_base, Map map,
+  static inline FieldIndex ForDescriptor(PtrComprCageBase cage_base,
+                                         Tagged<Map> map,
                                          InternalIndex descriptor_index);
-  static inline FieldIndex ForDetails(Map map, PropertyDetails details);
+  static inline FieldIndex ForDetails(Tagged<Map> map, PropertyDetails details);
 
   inline int GetLoadByFieldIndex() const;
 
diff --git a/src/objects/field-type.cc b/src/objects/field-type.cc
index bcebeb08679..8b2224d5d56 100644
--- a/src/objects/field-type.cc
+++ b/src/objects/field-type.cc
@@ -13,10 +13,14 @@ namespace v8 {
 namespace internal {
 
 // static
-FieldType FieldType::None() { return FieldType(Smi::FromInt(2).ptr()); }
+Tagged<FieldType> FieldType::None() {
+  return Tagged<FieldType>(Smi::FromInt(2).ptr());
+}
 
 // static
-FieldType FieldType::Any() { return FieldType(Smi::FromInt(1).ptr()); }
+Tagged<FieldType> FieldType::Any() {
+  return Tagged<FieldType>(Smi::FromInt(1).ptr());
+}
 
 // static
 Handle<FieldType> FieldType::None(Isolate* isolate) {
@@ -29,7 +33,9 @@ Handle<FieldType> FieldType::Any(Isolate* isolate) {
 }
 
 // static
-FieldType FieldType::Class(Map map) { return FieldType::cast(map); }
+Tagged<FieldType> FieldType::Class(Tagged<Map> map) {
+  return FieldType::cast(Tagged<Object>(map));
+}
 
 // static
 Handle<FieldType> FieldType::Class(Handle<Map> map, Isolate* isolate) {
@@ -44,7 +50,7 @@ Tagged<FieldType> FieldType::cast(Tagged<Object> object) {
 
 bool IsClass(Tagged<FieldType> obj) { return IsMap(obj); }
 
-Map FieldType::AsClass() const {
+Tagged<Map> FieldType::AsClass() const {
   DCHECK(IsClass(*this));
   return Map::cast(*this);
 }
@@ -53,7 +59,7 @@ bool FieldType::NowStable() const {
   return !IsClass(*this) || AsClass()->is_stable();
 }
 
-bool FieldType::NowIs(FieldType other) const {
+bool FieldType::NowIs(Tagged<FieldType> other) const {
   if (IsAny(other)) return true;
   if (IsNone(*this)) return true;
   if (IsNone(other)) return false;
@@ -63,7 +69,7 @@ bool FieldType::NowIs(FieldType other) const {
   return *this == other;
 }
 
-bool FieldType::Equals(FieldType other) const {
+bool FieldType::Equals(Tagged<FieldType> other) const {
   if (IsAny(*this) && IsAny(other)) return true;
   if (IsNone(*this) && IsNone(other)) return true;
   if (IsClass(*this) && IsClass(other)) {
@@ -85,7 +91,7 @@ void FieldType::PrintTo(std::ostream& os) const {
   }
 }
 
-bool FieldType::NowContains(Object value) const {
+bool FieldType::NowContains(Tagged<Object> value) const {
   if (*this == Any()) return true;
   if (*this == None()) return false;
   if (!IsHeapObject(value)) return false;
diff --git a/src/objects/field-type.h b/src/objects/field-type.h
index 3374da6a1bf..7017cf22f21 100644
--- a/src/objects/field-type.h
+++ b/src/objects/field-type.h
@@ -14,11 +14,11 @@ namespace internal {
 
 class FieldType : public Object {
  public:
-  static FieldType None();
-  static FieldType Any();
+  static Tagged<FieldType> None();
+  static Tagged<FieldType> Any();
   V8_EXPORT_PRIVATE static Handle<FieldType> None(Isolate* isolate);
   V8_EXPORT_PRIVATE static Handle<FieldType> Any(Isolate* isolate);
-  V8_EXPORT_PRIVATE static FieldType Class(Map map);
+  V8_EXPORT_PRIVATE static Tagged<FieldType> Class(Tagged<Map> map);
   V8_EXPORT_PRIVATE static Handle<FieldType> Class(Handle<Map> map,
                                                    Isolate* isolate);
   V8_EXPORT_PRIVATE static Tagged<FieldType> cast(Tagged<Object> object);
@@ -26,16 +26,16 @@ class FieldType : public Object {
     return Tagged<FieldType>(object.ptr());
   }
 
-  bool NowContains(Object value) const;
+  bool NowContains(Tagged<Object> value) const;
 
   bool NowContains(Handle<Object> value) const { return NowContains(*value); }
 
-  Map AsClass() const;
+  Tagged<Map> AsClass() const;
   bool NowStable() const;
-  bool NowIs(FieldType other) const;
+  bool NowIs(Tagged<FieldType> other) const;
   bool NowIs(Handle<FieldType> other) const;
 
-  V8_EXPORT_PRIVATE bool Equals(FieldType other) const;
+  V8_EXPORT_PRIVATE bool Equals(Tagged<FieldType> other) const;
   V8_EXPORT_PRIVATE void PrintTo(std::ostream& os) const;
 
  private:
diff --git a/src/objects/fixed-array-inl.h b/src/objects/fixed-array-inl.h
index 926c271bc95..9e44f7513b1 100644
--- a/src/objects/fixed-array-inl.h
+++ b/src/objects/fixed-array-inl.h
@@ -240,7 +240,7 @@ Tagged<Object> FixedArray::compare_and_swap(int index, Tagged<Object> expected,
                                             WriteBarrierMode mode) {
   DCHECK_NE(map(), GetReadOnlyRoots().fixed_cow_array_map());
   DCHECK_LT(static_cast<unsigned>(index), static_cast<unsigned>(length()));
-  Object previous_value = SEQ_CST_COMPARE_AND_SWAP_FIELD(
+  Tagged<Object> previous_value = SEQ_CST_COMPARE_AND_SWAP_FIELD(
       *this, OffsetOfElementAt(index), expected, value);
   if (previous_value == expected) {
     CONDITIONAL_WRITE_BARRIER(*this, OffsetOfElementAt(index), value, mode);
@@ -301,7 +301,7 @@ inline int WeakArrayList::AllocatedSize() { return SizeFor(capacity()); }
 
 // Perform a binary search in a fixed array.
 template <SearchMode search_mode, typename T>
-int BinarySearch(T* array, Name name, int valid_entries,
+int BinarySearch(T* array, Tagged<Name> name, int valid_entries,
                  int* out_insertion_index) {
   DCHECK_IMPLIES(search_mode == VALID_ENTRIES, out_insertion_index == nullptr);
   int low = 0;
@@ -318,7 +318,7 @@ int BinarySearch(T* array, Name name, int valid_entries,
 
   while (low != high) {
     int mid = low + (high - low) / 2;
-    Name mid_name = array->GetSortedKey(mid);
+    Tagged<Name> mid_name = array->GetSortedKey(mid);
     uint32_t mid_hash = mid_name->hash();
 
     if (mid_hash >= hash) {
@@ -330,7 +330,7 @@ int BinarySearch(T* array, Name name, int valid_entries,
 
   for (; low <= limit; ++low) {
     int sort_index = array->GetSortedKeyIndex(low);
-    Name entry = array->GetKey(InternalIndex(sort_index));
+    Tagged<Name> entry = array->GetKey(InternalIndex(sort_index));
     uint32_t current_hash = entry->hash();
     if (current_hash != hash) {
       // 'search_mode == ALL_ENTRIES' here and below is not needed since
@@ -360,14 +360,14 @@ int BinarySearch(T* array, Name name, int valid_entries,
 // Perform a linear search in this fixed array. len is the number of entry
 // indices that are valid.
 template <SearchMode search_mode, typename T>
-int LinearSearch(T* array, Name name, int valid_entries,
+int LinearSearch(T* array, Tagged<Name> name, int valid_entries,
                  int* out_insertion_index) {
   if (search_mode == ALL_ENTRIES && out_insertion_index != nullptr) {
     uint32_t hash = name->hash();
     int len = array->number_of_entries();
     for (int number = 0; number < len; number++) {
       int sorted_index = array->GetSortedKeyIndex(number);
-      Name entry = array->GetKey(InternalIndex(sorted_index));
+      Tagged<Name> entry = array->GetKey(InternalIndex(sorted_index));
       uint32_t current_hash = entry->hash();
       if (current_hash > hash) {
         *out_insertion_index = sorted_index;
@@ -388,8 +388,8 @@ int LinearSearch(T* array, Name name, int valid_entries,
 }
 
 template <SearchMode search_mode, typename T>
-int Search(T* array, Name name, int valid_entries, int* out_insertion_index,
-           bool concurrent_search) {
+int Search(T* array, Tagged<Name> name, int valid_entries,
+           int* out_insertion_index, bool concurrent_search) {
   SLOW_DCHECK_IMPLIES(!concurrent_search, array->IsSortedNoDuplicates());
 
   if (valid_entries == 0) {
@@ -428,7 +428,7 @@ uint64_t FixedDoubleArray::get_representation(int index) {
   return base::ReadUnalignedValue<uint64_t>(field_address(offset));
 }
 
-Handle<Object> FixedDoubleArray::get(FixedDoubleArray array, int index,
+Handle<Object> FixedDoubleArray::get(Tagged<FixedDoubleArray> array, int index,
                                      Isolate* isolate) {
   if (array->is_the_hole(index)) {
     return ReadOnlyRoots(isolate).the_hole_value_handle();
@@ -517,8 +517,8 @@ MaybeObjectSlot WeakFixedArray::RawFieldOfElementAt(int index) {
 }
 
 void WeakFixedArray::CopyElements(Isolate* isolate, int dst_index,
-                                  WeakFixedArray src, int src_index, int len,
-                                  WriteBarrierMode mode) {
+                                  Tagged<WeakFixedArray> src, int src_index,
+                                  int len, WriteBarrierMode mode) {
   if (len == 0) return;
   DCHECK_LE(dst_index + len, length());
   DCHECK_LE(src_index + len, src->length());
@@ -552,8 +552,8 @@ MaybeObjectSlot WeakArrayList::data_start() {
 }
 
 void WeakArrayList::CopyElements(Isolate* isolate, int dst_index,
-                                 WeakArrayList src, int src_index, int len,
-                                 WriteBarrierMode mode) {
+                                 Tagged<WeakArrayList> src, int src_index,
+                                 int len, WriteBarrierMode mode) {
   if (len == 0) return;
   DCHECK_LE(dst_index + len, capacity());
   DCHECK_LE(src_index + len, src->capacity());
@@ -688,7 +688,7 @@ void ByteArray::clear_padding() {
   memset(reinterpret_cast<void*>(address() + data_size), 0, Size() - data_size);
 }
 
-ByteArray ByteArray::FromDataStartAddress(Address address) {
+Tagged<ByteArray> ByteArray::FromDataStartAddress(Address address) {
   DCHECK_TAG_ALIGNED(address);
   return ByteArray::cast(Object(address - kHeaderSize + kHeapObjectTag));
 }
diff --git a/src/objects/fixed-array.cc b/src/objects/fixed-array.cc
index eb4842d2fb8..7a6d0e6661f 100644
--- a/src/objects/fixed-array.cc
+++ b/src/objects/fixed-array.cc
@@ -214,7 +214,7 @@ Handle<WeakArrayList> WeakArrayList::AddToEnd(Isolate* isolate,
   array = EnsureSpace(isolate, array, length + 1);
   {
     DisallowGarbageCollection no_gc;
-    WeakArrayList raw = *array;
+    Tagged<WeakArrayList> raw = *array;
     // Reload length; GC might have removed elements from the array.
     length = raw->length();
     raw->Set(length, *value);
@@ -231,7 +231,7 @@ Handle<WeakArrayList> WeakArrayList::AddToEnd(Isolate* isolate,
   array = EnsureSpace(isolate, array, length + 2);
   {
     DisallowGarbageCollection no_gc;
-    WeakArrayList raw = *array;
+    Tagged<WeakArrayList> raw = *array;
     // Reload length; GC might have removed elements from the array.
     length = array->length();
     raw->Set(length, *value1);
@@ -250,7 +250,7 @@ Handle<WeakArrayList> WeakArrayList::Append(Isolate* isolate,
   int new_length = 0;
   {
     DisallowGarbageCollection no_gc;
-    WeakArrayList raw = *array;
+    Tagged<WeakArrayList> raw = *array;
     length = raw->length();
 
     if (length < raw->capacity()) {
@@ -283,7 +283,7 @@ Handle<WeakArrayList> WeakArrayList::Append(Isolate* isolate,
 
   {
     DisallowGarbageCollection no_gc;
-    WeakArrayList raw = *array;
+    Tagged<WeakArrayList> raw = *array;
     // Reload length, allocation might have killed some weak refs.
     int index = raw->length();
     raw->Set(index, *value);
diff --git a/src/objects/fixed-array.h b/src/objects/fixed-array.h
index bf9db0e3efd..1c186c4135c 100644
--- a/src/objects/fixed-array.h
+++ b/src/objects/fixed-array.h
@@ -203,7 +203,7 @@ class FixedDoubleArray
   // Setter and getter for elements.
   inline double get_scalar(int index);
   inline uint64_t get_representation(int index);
-  static inline Handle<Object> get(FixedDoubleArray array, int index,
+  static inline Handle<Object> get(Tagged<FixedDoubleArray> array, int index,
                                    Isolate* isolate);
   inline void set(int index, double value);
   inline void set_the_hole(Isolate* isolate, int index);
@@ -269,8 +269,9 @@ class WeakFixedArray
 
   inline MaybeObjectSlot RawFieldOfElementAt(int index);
 
-  inline void CopyElements(Isolate* isolate, int dst_index, WeakFixedArray src,
-                           int src_index, int len, WriteBarrierMode mode);
+  inline void CopyElements(Isolate* isolate, int dst_index,
+                           Tagged<WeakFixedArray> src, int src_index, int len,
+                           WriteBarrierMode mode);
 
   DECL_PRINTER(WeakFixedArray)
   DECL_VERIFIER(WeakFixedArray)
@@ -348,8 +349,9 @@ class WeakArrayList
   // Gives access to raw memory which stores the array's data.
   inline MaybeObjectSlot data_start();
 
-  inline void CopyElements(Isolate* isolate, int dst_index, WeakArrayList src,
-                           int src_index, int len, WriteBarrierMode mode);
+  inline void CopyElements(Isolate* isolate, int dst_index,
+                           Tagged<WeakArrayList> src, int src_index, int len,
+                           WriteBarrierMode mode);
 
   V8_EXPORT_PRIVATE bool IsFull() const;
 
@@ -391,7 +393,7 @@ class WeakArrayList
 
 class WeakArrayList::Iterator {
  public:
-  explicit Iterator(WeakArrayList array) : index_(0), array_(array) {}
+  explicit Iterator(Tagged<WeakArrayList> array) : index_(0), array_(array) {}
   Iterator(const Iterator&) = delete;
   Iterator& operator=(const Iterator&) = delete;
 
@@ -473,7 +475,7 @@ class ArrayList : public TorqueGeneratedArrayList<ArrayList, FixedArray> {
 enum SearchMode { ALL_ENTRIES, VALID_ENTRIES };
 
 template <SearchMode search_mode, typename T>
-inline int Search(T* array, Name name, int valid_entries = 0,
+inline int Search(T* array, Tagged<Name> name, int valid_entries = 0,
                   int* out_insertion_index = nullptr,
                   bool concurrent_search = false);
 
@@ -519,7 +521,7 @@ class ByteArray : public TorqueGeneratedByteArray<ByteArray, FixedArrayBase> {
   inline int DataSize() const;
 
   // Returns a pointer to the ByteArray object for a given data start address.
-  static inline ByteArray FromDataStartAddress(Address address);
+  static inline Tagged<ByteArray> FromDataStartAddress(Address address);
 
   // Code Generation support.
   static int OffsetOfElementAt(int index) { return kHeaderSize + index; }
diff --git a/src/objects/free-space-inl.h b/src/objects/free-space-inl.h
index f0c92da8a01..d250ffb7717 100644
--- a/src/objects/free-space-inl.h
+++ b/src/objects/free-space-inl.h
@@ -25,7 +25,7 @@ RELAXED_SMI_ACCESSORS(FreeSpace, size, kSizeOffset)
 
 int FreeSpace::Size() { return size(kRelaxedLoad); }
 
-FreeSpace FreeSpace::next() const {
+Tagged<FreeSpace> FreeSpace::next() const {
   DCHECK(IsValid());
 #ifdef V8_EXTERNAL_CODE_SPACE
   intptr_t diff_to_next =
@@ -41,7 +41,7 @@ FreeSpace FreeSpace::next() const {
 #endif  // V8_EXTERNAL_CODE_SPACE
 }
 
-void FreeSpace::set_next(FreeSpace next) {
+void FreeSpace::set_next(Tagged<FreeSpace> next) {
   DCHECK(IsValid());
 #ifdef V8_EXTERNAL_CODE_SPACE
   if (next.is_null()) {
@@ -57,19 +57,19 @@ void FreeSpace::set_next(FreeSpace next) {
 #endif  // V8_EXTERNAL_CODE_SPACE
 }
 
-FreeSpace FreeSpace::cast(HeapObject o) {
+Tagged<FreeSpace> FreeSpace::cast(Tagged<HeapObject> o) {
   SLOW_DCHECK((!GetHeapFromWritableObject(o)->deserialization_complete()) ||
               IsFreeSpace(o));
   return base::bit_cast<FreeSpace>(o);
 }
 
-FreeSpace FreeSpace::unchecked_cast(const Object o) {
+Tagged<FreeSpace> FreeSpace::unchecked_cast(const Tagged<Object> o) {
   return base::bit_cast<FreeSpace>(o);
 }
 
 bool FreeSpace::IsValid() const {
   Heap* heap = GetHeapFromWritableObject(*this);
-  Object free_space_map =
+  Tagged<Object> free_space_map =
       Isolate::FromHeap(heap)->root(RootIndex::kFreeSpaceMap);
   CHECK(!heap->deserialization_complete() ||
         map_slot().contains_map_value(free_space_map.ptr()));
diff --git a/src/objects/free-space.h b/src/objects/free-space.h
index c9ee47cc8fa..cb54915600d 100644
--- a/src/objects/free-space.h
+++ b/src/objects/free-space.h
@@ -38,11 +38,11 @@ class FreeSpace : public TorqueGeneratedFreeSpace<FreeSpace, HeapObject> {
   inline int Size();
 
   // Accessors for the next field.
-  inline FreeSpace next() const;
-  inline void set_next(FreeSpace next);
+  inline Tagged<FreeSpace> next() const;
+  inline void set_next(Tagged<FreeSpace> next);
 
-  inline static FreeSpace cast(HeapObject obj);
-  inline static FreeSpace unchecked_cast(const Object obj);
+  inline static Tagged<FreeSpace> cast(Tagged<HeapObject> obj);
+  inline static Tagged<FreeSpace> unchecked_cast(const Tagged<Object> obj);
 
   // Dispatched behavior.
   DECL_PRINTER(FreeSpace)
diff --git a/src/objects/hash-table-inl.h b/src/objects/hash-table-inl.h
index 1be5481fe17..82f0814c7b4 100644
--- a/src/objects/hash-table-inl.h
+++ b/src/objects/hash-table-inl.h
@@ -71,7 +71,7 @@ CAST_ACCESSOR(ObjectHashSet)
 CAST_ACCESSOR(NameToIndexHashTable)
 CAST_ACCESSOR(ObjectTwoHashTable)
 
-void EphemeronHashTable::set_key(int index, Object value) {
+void EphemeronHashTable::set_key(int index, Tagged<Object> value) {
   DCHECK_NE(GetReadOnlyRoots().fixed_cow_array_map(), map());
   DCHECK(IsEphemeronHashTable(*this));
   DCHECK_GE(index, 0);
@@ -81,7 +81,7 @@ void EphemeronHashTable::set_key(int index, Object value) {
   EPHEMERON_KEY_WRITE_BARRIER(*this, offset, value);
 }
 
-void EphemeronHashTable::set_key(int index, Object value,
+void EphemeronHashTable::set_key(int index, Tagged<Object> value,
                                  WriteBarrierMode mode) {
   DCHECK_NE(GetReadOnlyRoots().fixed_cow_array_map(), map());
   DCHECK(IsEphemeronHashTable(*this));
@@ -187,7 +187,7 @@ InternalIndex HashTable<Derived, Shape>::FindEntry(PtrComprCageBase cage_base,
   // EnsureCapacity will guarantee the hash table is never full.
   for (InternalIndex entry = FirstProbe(hash, capacity);;
        entry = NextProbe(entry, count++, capacity)) {
-    Object element = KeyAt(cage_base, entry);
+    Tagged<Object> element = KeyAt(cage_base, entry);
     // Empty entry. Uses raw unchecked accessors because it is called by the
     // string table during bootstrapping.
     if (element == undefined) return InternalIndex::NotFound();
@@ -205,7 +205,7 @@ InternalIndex HashTable<Derived, Shape>::FindInsertionEntry(IsolateT* isolate,
 
 // static
 template <typename Derived, typename Shape>
-bool HashTable<Derived, Shape>::IsKey(ReadOnlyRoots roots, Object k) {
+bool HashTable<Derived, Shape>::IsKey(ReadOnlyRoots roots, Tagged<Object> k) {
   // TODO(leszeks): Dictionaries that don't delete could skip the hole check.
   return k != roots.unchecked_undefined_value() &&
          k != roots.unchecked_the_hole_value();
@@ -213,8 +213,8 @@ bool HashTable<Derived, Shape>::IsKey(ReadOnlyRoots roots, Object k) {
 
 template <typename Derived, typename Shape>
 bool HashTable<Derived, Shape>::ToKey(ReadOnlyRoots roots, InternalIndex entry,
-                                      Object* out_k) {
-  Object k = KeyAt(entry);
+                                      Tagged<Object>* out_k) {
+  Tagged<Object> k = KeyAt(entry);
   if (!IsKey(roots, k)) return false;
   *out_k = Shape::Unwrap(k);
   return true;
@@ -222,53 +222,55 @@ bool HashTable<Derived, Shape>::ToKey(ReadOnlyRoots roots, InternalIndex entry,
 
 template <typename Derived, typename Shape>
 bool HashTable<Derived, Shape>::ToKey(PtrComprCageBase cage_base,
-                                      InternalIndex entry, Object* out_k) {
-  Object k = KeyAt(cage_base, entry);
+                                      InternalIndex entry,
+                                      Tagged<Object>* out_k) {
+  Tagged<Object> k = KeyAt(cage_base, entry);
   if (!IsKey(GetReadOnlyRoots(cage_base), k)) return false;
   *out_k = Shape::Unwrap(k);
   return true;
 }
 
 template <typename Derived, typename Shape>
-Object HashTable<Derived, Shape>::KeyAt(InternalIndex entry) {
+Tagged<Object> HashTable<Derived, Shape>::KeyAt(InternalIndex entry) {
   PtrComprCageBase cage_base = GetPtrComprCageBase(*this);
   return KeyAt(cage_base, entry);
 }
 
 template <typename Derived, typename Shape>
-Object HashTable<Derived, Shape>::KeyAt(PtrComprCageBase cage_base,
-                                        InternalIndex entry) {
+Tagged<Object> HashTable<Derived, Shape>::KeyAt(PtrComprCageBase cage_base,
+                                                InternalIndex entry) {
   return get(cage_base, EntryToIndex(entry) + kEntryKeyIndex);
 }
 
 template <typename Derived, typename Shape>
-Object HashTable<Derived, Shape>::KeyAt(InternalIndex entry,
-                                        RelaxedLoadTag tag) {
+Tagged<Object> HashTable<Derived, Shape>::KeyAt(InternalIndex entry,
+                                                RelaxedLoadTag tag) {
   PtrComprCageBase cage_base = GetPtrComprCageBase(*this);
   return KeyAt(cage_base, entry, tag);
 }
 
 template <typename Derived, typename Shape>
-Object HashTable<Derived, Shape>::KeyAt(PtrComprCageBase cage_base,
-                                        InternalIndex entry,
-                                        RelaxedLoadTag tag) {
+Tagged<Object> HashTable<Derived, Shape>::KeyAt(PtrComprCageBase cage_base,
+                                                InternalIndex entry,
+                                                RelaxedLoadTag tag) {
   return get(cage_base, EntryToIndex(entry) + kEntryKeyIndex, tag);
 }
 
 template <typename Derived, typename Shape>
-void HashTable<Derived, Shape>::SetKeyAt(InternalIndex entry, Object value,
+void HashTable<Derived, Shape>::SetKeyAt(InternalIndex entry,
+                                         Tagged<Object> value,
                                          WriteBarrierMode mode) {
   set_key(EntryToIndex(entry), value, mode);
 }
 
 template <typename Derived, typename Shape>
-void HashTable<Derived, Shape>::set_key(int index, Object value) {
+void HashTable<Derived, Shape>::set_key(int index, Tagged<Object> value) {
   DCHECK(!IsEphemeronHashTable(*this));
   FixedArray::set(index, value);
 }
 
 template <typename Derived, typename Shape>
-void HashTable<Derived, Shape>::set_key(int index, Object value,
+void HashTable<Derived, Shape>::set_key(int index, Tagged<Object> value,
                                         WriteBarrierMode mode) {
   DCHECK(!IsEphemeronHashTable(*this));
   FixedArray::set(index, value, mode);
@@ -289,17 +291,18 @@ bool ObjectHashSet::Has(Isolate* isolate, Handle<Object> key, int32_t hash) {
 }
 
 bool ObjectHashSet::Has(Isolate* isolate, Handle<Object> key) {
-  Object hash = Object::GetHash(*key);
+  Tagged<Object> hash = Object::GetHash(*key);
   if (!IsSmi(hash)) return false;
   return FindEntry(isolate, ReadOnlyRoots(isolate), key, Smi::ToInt(hash))
       .is_found();
 }
 
-bool ObjectHashTableShape::IsMatch(Handle<Object> key, Object other) {
+bool ObjectHashTableShape::IsMatch(Handle<Object> key, Tagged<Object> other) {
   return Object::SameValue(*key, other);
 }
 
-bool RegisteredSymbolTableShape::IsMatch(Handle<String> key, Object value) {
+bool RegisteredSymbolTableShape::IsMatch(Handle<String> key,
+                                         Tagged<Object> value) {
   DCHECK(IsString(value));
   return key->Equals(String::cast(value));
 }
@@ -310,15 +313,16 @@ uint32_t RegisteredSymbolTableShape::Hash(ReadOnlyRoots roots,
 }
 
 uint32_t RegisteredSymbolTableShape::HashForObject(ReadOnlyRoots roots,
-                                                   Object object) {
+                                                   Tagged<Object> object) {
   return String::cast(object)->EnsureHash();
 }
 
-bool NameToIndexShape::IsMatch(Handle<Name> key, Object other) {
+bool NameToIndexShape::IsMatch(Handle<Name> key, Tagged<Object> other) {
   return *key == other;
 }
 
-uint32_t NameToIndexShape::HashForObject(ReadOnlyRoots roots, Object other) {
+uint32_t NameToIndexShape::HashForObject(ReadOnlyRoots roots,
+                                         Tagged<Object> other) {
   return Name::cast(other)->hash();
 }
 
@@ -331,7 +335,7 @@ uint32_t ObjectHashTableShape::Hash(ReadOnlyRoots roots, Handle<Object> key) {
 }
 
 uint32_t ObjectHashTableShape::HashForObject(ReadOnlyRoots roots,
-                                             Object other) {
+                                             Tagged<Object> other) {
   return Smi::ToInt(Object::GetHash(other));
 }
 
diff --git a/src/objects/hash-table.h b/src/objects/hash-table.h
index 43cf82dc1e4..bf52429a715 100644
--- a/src/objects/hash-table.h
+++ b/src/objects/hash-table.h
@@ -64,7 +64,7 @@ template <typename KeyT>
 class V8_EXPORT_PRIVATE BaseShape {
  public:
   using Key = KeyT;
-  static Object Unwrap(Object key) { return key; }
+  static Tagged<Object> Unwrap(Tagged<Object> key) { return key; }
 };
 
 class V8_EXPORT_PRIVATE HashTableBase : public NON_EXPORTED_BASE(FixedArray) {
@@ -156,20 +156,21 @@ class EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE) HashTable
 
   // Returns whether k is a real key.  The hole and undefined are not allowed as
   // keys and can be used to indicate missing or deleted elements.
-  static inline bool IsKey(ReadOnlyRoots roots, Object k);
+  static inline bool IsKey(ReadOnlyRoots roots, Tagged<Object> k);
 
-  inline bool ToKey(ReadOnlyRoots roots, InternalIndex entry, Object* out_k);
+  inline bool ToKey(ReadOnlyRoots roots, InternalIndex entry,
+                    Tagged<Object>* out_k);
   inline bool ToKey(PtrComprCageBase cage_base, InternalIndex entry,
-                    Object* out_k);
+                    Tagged<Object>* out_k);
 
   // Returns the key at entry.
-  inline Object KeyAt(InternalIndex entry);
-  inline Object KeyAt(PtrComprCageBase cage_base, InternalIndex entry);
-  inline Object KeyAt(InternalIndex entry, RelaxedLoadTag tag);
-  inline Object KeyAt(PtrComprCageBase cage_base, InternalIndex entry,
-                      RelaxedLoadTag tag);
+  inline Tagged<Object> KeyAt(InternalIndex entry);
+  inline Tagged<Object> KeyAt(PtrComprCageBase cage_base, InternalIndex entry);
+  inline Tagged<Object> KeyAt(InternalIndex entry, RelaxedLoadTag tag);
+  inline Tagged<Object> KeyAt(PtrComprCageBase cage_base, InternalIndex entry,
+                              RelaxedLoadTag tag);
 
-  inline void SetKeyAt(InternalIndex entry, Object value,
+  inline void SetKeyAt(InternalIndex entry, Tagged<Object> value,
                        WriteBarrierMode mode = UPDATE_WRITE_BARRIER);
 
   static const int kElementsStartIndex = kPrefixStartIndex + Shape::kPrefixSize;
@@ -249,8 +250,8 @@ class EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE) HashTable
   // Rehashes this hash-table into the new table.
   void Rehash(PtrComprCageBase cage_base, Derived new_table);
 
-  inline void set_key(int index, Object value);
-  inline void set_key(int index, Object value, WriteBarrierMode mode);
+  inline void set_key(int index, Tagged<Object> value);
+  inline void set_key(int index, Tagged<Object> value, WriteBarrierMode mode);
 
  private:
   // Ensure that kMaxRegularCapacity yields a non-large object dictionary.
@@ -269,7 +270,7 @@ class EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE) HashTable
   // Returns _expected_ if one of entries given by the first _probe_ probes is
   // equal to  _expected_. Otherwise, returns the entry given by the probe
   // number _probe_.
-  InternalIndex EntryForProbe(ReadOnlyRoots roots, Object k, int probe,
+  InternalIndex EntryForProbe(ReadOnlyRoots roots, Tagged<Object> k, int probe,
                               InternalIndex expected);
 
   void Swap(InternalIndex entry1, InternalIndex entry2, WriteBarrierMode mode);
@@ -301,7 +302,7 @@ class HashTableKey {
   explicit HashTableKey(uint32_t hash) : hash_(hash) {}
 
   // Returns whether the other object matches this key.
-  virtual bool IsMatch(Object other) = 0;
+  virtual bool IsMatch(Tagged<Object> other) = 0;
   // Returns the hash value for this key.
   // Required.
   virtual ~HashTableKey() = default;
@@ -320,9 +321,10 @@ class HashTableKey {
 
 class ObjectHashTableShape : public BaseShape<Handle<Object>> {
  public:
-  static inline bool IsMatch(Handle<Object> key, Object other);
+  static inline bool IsMatch(Handle<Object> key, Tagged<Object> other);
   static inline uint32_t Hash(ReadOnlyRoots roots, Handle<Object> key);
-  static inline uint32_t HashForObject(ReadOnlyRoots roots, Object object);
+  static inline uint32_t HashForObject(ReadOnlyRoots roots,
+                                       Tagged<Object> object);
   static inline Handle<Object> AsHandle(Handle<Object> key);
   static const int kPrefixSize = 0;
   static const int kEntryValueIndex = 1;
@@ -336,12 +338,13 @@ class EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE) ObjectHashTableBase
  public:
   // Looks up the value associated with the given key. The hole value is
   // returned in case the key is not present.
-  Object Lookup(Handle<Object> key);
-  Object Lookup(Handle<Object> key, int32_t hash);
-  Object Lookup(PtrComprCageBase cage_base, Handle<Object> key, int32_t hash);
+  Tagged<Object> Lookup(Handle<Object> key);
+  Tagged<Object> Lookup(Handle<Object> key, int32_t hash);
+  Tagged<Object> Lookup(PtrComprCageBase cage_base, Handle<Object> key,
+                        int32_t hash);
 
   // Returns the value at entry.
-  Object ValueAt(InternalIndex entry);
+  Tagged<Object> ValueAt(InternalIndex entry);
 
   // Overwrite all keys and values with the hole value.
   static void FillEntriesWithHoles(Handle<Derived>);
@@ -367,7 +370,7 @@ class EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE) ObjectHashTableBase
   }
 
  protected:
-  void AddEntry(InternalIndex entry, Object key, Object value);
+  void AddEntry(InternalIndex entry, Tagged<Object> key, Tagged<Object> value);
   void RemoveEntry(InternalIndex entry);
 
   OBJECT_CONSTRUCTORS(ObjectHashTableBase, HashTable<Derived, Shape>);
@@ -415,8 +418,8 @@ class V8_EXPORT_PRIVATE EphemeronHashTable
   friend class third_party_heap::Impl;
   friend class HashTable<EphemeronHashTable, ObjectHashTableShape>;
   friend class ObjectHashTableBase<EphemeronHashTable, ObjectHashTableShape>;
-  inline void set_key(int index, Object value);
-  inline void set_key(int index, Object value, WriteBarrierMode mode);
+  inline void set_key(int index, Tagged<Object> value);
+  inline void set_key(int index, Tagged<Object> value, WriteBarrierMode mode);
 
   OBJECT_CONSTRUCTORS(
       EphemeronHashTable,
@@ -498,9 +501,10 @@ class V8_EXPORT_PRIVATE ObjectHashSet
 
 class NameToIndexShape : public BaseShape<Handle<Name>> {
  public:
-  static inline bool IsMatch(Handle<Name> key, Object other);
+  static inline bool IsMatch(Handle<Name> key, Tagged<Object> other);
   static inline uint32_t Hash(ReadOnlyRoots roots, Handle<Name> key);
-  static inline uint32_t HashForObject(ReadOnlyRoots roots, Object object);
+  static inline uint32_t HashForObject(ReadOnlyRoots roots,
+                                       Tagged<Object> object);
   static inline Handle<Object> AsHandle(Handle<Name> key);
   static const int kPrefixSize = 0;
   static const int kEntryValueIndex = 1;
@@ -517,7 +521,7 @@ class V8_EXPORT_PRIVATE NameToIndexHashTable
   int Lookup(Handle<Name> key);
 
   // Returns the value at entry.
-  Object ValueAt(InternalIndex entry);
+  Tagged<Object> ValueAt(InternalIndex entry);
   int IndexAt(InternalIndex entry);
 
   template <typename IsolateT>
@@ -543,9 +547,10 @@ class V8_EXPORT_PRIVATE NameToIndexHashTable
 
 class RegisteredSymbolTableShape : public BaseShape<Handle<String>> {
  public:
-  static inline bool IsMatch(Handle<String> key, Object other);
+  static inline bool IsMatch(Handle<String> key, Tagged<Object> other);
   static inline uint32_t Hash(ReadOnlyRoots roots, Handle<String> key);
-  static inline uint32_t HashForObject(ReadOnlyRoots roots, Object object);
+  static inline uint32_t HashForObject(ReadOnlyRoots roots,
+                                       Tagged<Object> object);
   static const int kPrefixSize = 0;
   static const int kEntryValueIndex = 1;
   static const int kEntrySize = 2;
@@ -555,10 +560,10 @@ class RegisteredSymbolTableShape : public BaseShape<Handle<String>> {
 class RegisteredSymbolTable
     : public HashTable<RegisteredSymbolTable, RegisteredSymbolTableShape> {
  public:
-  Object SlowReverseLookup(Object value);
+  Tagged<Object> SlowReverseLookup(Tagged<Object> value);
 
   // Returns the value at entry.
-  Object ValueAt(InternalIndex entry);
+  Tagged<Object> ValueAt(InternalIndex entry);
 
   inline static Handle<Map> GetMap(ReadOnlyRoots roots);
 
diff --git a/src/objects/heap-object.h b/src/objects/heap-object.h
index 99353226ecd..ba996bd8307 100644
--- a/src/objects/heap-object.h
+++ b/src/objects/heap-object.h
@@ -34,51 +34,53 @@ class HeapObject : public Object {
   // [map]: Contains a map which contains the object's reflective
   // information.
   DECL_GETTER(map, Tagged<Map>)
-  inline void set_map(Map value);
+  inline void set_map(Tagged<Map> value);
 
   // This method behaves the same as `set_map` but marks the map transition as
   // safe for the concurrent marker (object layout doesn't change) during
   // verification.
-  inline void set_map_safe_transition(Map value);
+  inline void set_map_safe_transition(Tagged<Map> value);
 
   inline ObjectSlot map_slot() const;
 
   // The no-write-barrier version.  This is OK if the object is white and in
   // new space, or if the value is an immortal immutable object, like the maps
   // of primitive (non-JS) objects like strings, heap numbers etc.
-  inline void set_map_no_write_barrier(Map value,
+  inline void set_map_no_write_barrier(Tagged<Map> value,
                                        RelaxedStoreTag = kRelaxedStore);
-  inline void set_map_no_write_barrier(Map value, ReleaseStoreTag);
+  inline void set_map_no_write_barrier(Tagged<Map> value, ReleaseStoreTag);
   inline void set_map_safe_transition_no_write_barrier(
-      Map value, RelaxedStoreTag = kRelaxedStore);
-  inline void set_map_safe_transition_no_write_barrier(Map value,
+      Tagged<Map> value, RelaxedStoreTag = kRelaxedStore);
+  inline void set_map_safe_transition_no_write_barrier(Tagged<Map> value,
                                                        ReleaseStoreTag);
 
   // Access the map using acquire load and release store.
   DECL_ACQUIRE_GETTER(map, Tagged<Map>)
-  inline void set_map(Map value, ReleaseStoreTag);
-  inline void set_map_safe_transition(Map value, ReleaseStoreTag);
+  inline void set_map(Tagged<Map> value, ReleaseStoreTag);
+  inline void set_map_safe_transition(Tagged<Map> value, ReleaseStoreTag);
 
   // Compare-and-swaps map word using release store, returns true if the map
   // word was actually swapped.
   inline bool release_compare_and_swap_map_word_forwarded(
-      MapWord old_map_word, HeapObject new_target_object);
+      MapWord old_map_word, Tagged<HeapObject> new_target_object);
 
   // Initialize the map immediately after the object is allocated.
   // Do not use this outside Heap.
   inline void set_map_after_allocation(
-      Map value, WriteBarrierMode mode = UPDATE_WRITE_BARRIER);
+      Tagged<Map> value, WriteBarrierMode mode = UPDATE_WRITE_BARRIER);
 
   // During garbage collection, the map word of a heap object does not
   // necessarily contain a map pointer.
   DECL_RELAXED_GETTER(map_word, MapWord)
-  inline void set_map_word(Map map, RelaxedStoreTag);
-  inline void set_map_word_forwarded(HeapObject target_object, RelaxedStoreTag);
+  inline void set_map_word(Tagged<Map> map, RelaxedStoreTag);
+  inline void set_map_word_forwarded(Tagged<HeapObject> target_object,
+                                     RelaxedStoreTag);
 
   // Access the map word using acquire load and release store.
   DECL_ACQUIRE_GETTER(map_word, MapWord)
-  inline void set_map_word(Map map, ReleaseStoreTag);
-  inline void set_map_word_forwarded(HeapObject target_object, ReleaseStoreTag);
+  inline void set_map_word(Tagged<Map> map, ReleaseStoreTag);
+  inline void set_map_word_forwarded(Tagged<HeapObject> target_object,
+                                     ReleaseStoreTag);
 
   // This method exists to help remove GetIsolate/GetHeap from HeapObject, in a
   // way that doesn't require passing Isolate/Heap down huge call chains or to
@@ -117,10 +119,10 @@ class HeapObject : public Object {
   inline void IterateFast(PtrComprCageBase cage_base, ObjectVisitor* v);
 
   template <typename ObjectVisitor>
-  inline void IterateFast(Map map, ObjectVisitor* v);
+  inline void IterateFast(Tagged<Map> map, ObjectVisitor* v);
 
   template <typename ObjectVisitor>
-  inline void IterateFast(Map map, int object_size, ObjectVisitor* v);
+  inline void IterateFast(Tagged<Map> map, int object_size, ObjectVisitor* v);
 
   // Iterates over all pointers contained in the object except the
   // first map pointer.  The object type is given in the first
@@ -129,13 +131,14 @@ class HeapObject : public Object {
   // If it's not performance critical iteration use the non-templatized
   // version.
   void IterateBody(PtrComprCageBase cage_base, ObjectVisitor* v);
-  void IterateBody(Map map, int object_size, ObjectVisitor* v);
+  void IterateBody(Tagged<Map> map, int object_size, ObjectVisitor* v);
 
   template <typename ObjectVisitor>
   inline void IterateBodyFast(PtrComprCageBase cage_base, ObjectVisitor* v);
 
   template <typename ObjectVisitor>
-  inline void IterateBodyFast(Map map, int object_size, ObjectVisitor* v);
+  inline void IterateBodyFast(Tagged<Map> map, int object_size,
+                              ObjectVisitor* v);
 
   // Returns the heap object's size in bytes
   DECL_GETTER(Size, int)
@@ -143,7 +146,7 @@ class HeapObject : public Object {
   // Given a heap object's map pointer, returns the heap size in bytes
   // Useful when the map pointer field is used for other purposes.
   // GC internal.
-  V8_EXPORT_PRIVATE int SizeFromMap(Map map) const;
+  V8_EXPORT_PRIVATE int SizeFromMap(Tagged<Map> map) const;
 
   template <class T, typename std::enable_if<std::is_arithmetic<T>::value ||
                                                  std::is_enum<T>::value,
@@ -180,8 +183,8 @@ class HeapObject : public Object {
   // Atomically compares and swaps a field using seq cst memory ordering.
   // Contains the required logic to properly handle number comparison.
   template <typename CompareAndSwapImpl>
-  static Object SeqCst_CompareAndSwapField(
-      Object expected_value, Object new_value,
+  static Tagged<Object> SeqCst_CompareAndSwapField(
+      Tagged<Object> expected_value, Tagged<Object> new_value,
       CompareAndSwapImpl compare_and_swap_impl);
 
   //
@@ -224,13 +227,13 @@ class HeapObject : public Object {
   //
   // IndirectPointer field accessors.
   //
-  inline Object ReadIndirectPointerField(size_t offset) const;
+  inline Tagged<Object> ReadIndirectPointerField(size_t offset) const;
 
   //
   // CodePointer field accessors.
   //
   inline void InitCodePointerTableEntryField(size_t offset, Isolate* isolate,
-                                             Code owning_code,
+                                             Tagged<Code> owning_code,
                                              Address entrypoint);
   inline Address ReadCodeEntrypointField(size_t offset) const;
   inline void WriteCodeEntrypointField(size_t offset, Address value);
@@ -269,11 +272,11 @@ class HeapObject : public Object {
 
   // Verify a pointer is a valid HeapObject pointer that points to object
   // areas in the heap.
-  static void VerifyHeapPointer(Isolate* isolate, Object p);
-  static void VerifyCodePointer(Isolate* isolate, Object p);
+  static void VerifyHeapPointer(Isolate* isolate, Tagged<Object> p);
+  static void VerifyCodePointer(Isolate* isolate, Tagged<Object> p);
 #endif
 
-  static inline AllocationAlignment RequiredAlignment(Map map);
+  static inline AllocationAlignment RequiredAlignment(Tagged<Map> map);
   bool inline CheckRequiredAlignment(PtrComprCageBase cage_base) const;
 
   // Whether the object needs rehashing. That is the case if the object's
@@ -325,7 +328,8 @@ class HeapObject : public Object {
   };
 
   template <EmitWriteBarrier emit_write_barrier, typename MemoryOrder>
-  V8_INLINE void set_map(Map value, MemoryOrder order, VerificationMode mode);
+  V8_INLINE void set_map(Tagged<Map> value, MemoryOrder order,
+                         VerificationMode mode);
 };
 
 OBJECT_CONSTRUCTORS_IMPL(HeapObject, Object)
diff --git a/src/objects/instance-type-checker.h b/src/objects/instance-type-checker.h
index 40b7717a161..529bcbc19a6 100644
--- a/src/objects/instance-type-checker.h
+++ b/src/objects/instance-type-checker.h
@@ -43,7 +43,7 @@ class Map;
 namespace InstanceTypeChecker {
 #define IS_TYPE_FUNCTION_DECL(Type, ...)                         \
   V8_INLINE constexpr bool Is##Type(InstanceType instance_type); \
-  V8_INLINE bool Is##Type(Map map);
+  V8_INLINE bool Is##Type(Tagged<Map> map);
 
 INSTANCE_TYPE_CHECKERS(IS_TYPE_FUNCTION_DECL)
 
diff --git a/src/objects/instance-type-inl.h b/src/objects/instance-type-inl.h
index 7768eb579f7..272d30770d0 100644
--- a/src/objects/instance-type-inl.h
+++ b/src/objects/instance-type-inl.h
@@ -134,12 +134,12 @@ inline bool MayHaveMapCheckFastCase(InstanceType type) {
   return false;
 }
 
-inline bool CheckInstanceMap(RootIndex expected, Map map) {
+inline bool CheckInstanceMap(RootIndex expected, Tagged<Map> map) {
   return V8HeapCompressionScheme::CompressObject(map.ptr()) ==
          StaticReadOnlyRootsPointerTable[static_cast<size_t>(expected)];
 }
 
-inline bool CheckInstanceMapRange(RootIndexRange expected, Map map) {
+inline bool CheckInstanceMapRange(RootIndexRange expected, Tagged<Map> map) {
   Tagged_t ptr = V8HeapCompressionScheme::CompressObject(map.ptr());
   Tagged_t first =
       StaticReadOnlyRootsPointerTable[static_cast<size_t>(expected.first)];
@@ -168,7 +168,7 @@ inline bool MayHaveMapCheckFastCase(InstanceType type) { return false; }
 #if V8_STATIC_ROOTS_BOOL
 
 #define INSTANCE_TYPE_CHECKER2(type, forinstancetype_)       \
-  V8_INLINE bool Is##type(Map map_object) {                  \
+  V8_INLINE bool Is##type(Tagged<Map> map_object) {          \
     InstanceType forinstancetype =                           \
         static_cast<InstanceType>(forinstancetype_);         \
     if (base::Optional<RootIndex> expected =                 \
@@ -185,7 +185,7 @@ inline bool MayHaveMapCheckFastCase(InstanceType type) { return false; }
 #else
 
 #define INSTANCE_TYPE_CHECKER2(type, forinstancetype) \
-  V8_INLINE bool Is##type(Map map_object) {           \
+  V8_INLINE bool Is##type(Tagged<Map> map_object) {   \
     return Is##type(map_object->instance_type());     \
   }
 
@@ -236,7 +236,7 @@ struct InstanceRangeChecker<lower_limit, LAST_TYPE> {
 
 #define INSTANCE_TYPE_CHECKER_RANGE2(type, first_instance_type,      \
                                      last_instance_type)             \
-  V8_INLINE bool Is##type(Map map_object) {                          \
+  V8_INLINE bool Is##type(Tagged<Map> map_object) {                  \
     if (base::Optional<RootIndexRange> range =                       \
             UniqueMapRangeOfInstanceTypeRange(first_instance_type,   \
                                               last_instance_type)) { \
@@ -250,7 +250,7 @@ struct InstanceRangeChecker<lower_limit, LAST_TYPE> {
 
 #define INSTANCE_TYPE_CHECKER_RANGE2(type, first_instance_type, \
                                      last_instance_type)        \
-  V8_INLINE bool Is##type(Map map_object) {                     \
+  V8_INLINE bool Is##type(Tagged<Map> map_object) {             \
     return Is##type(map_object->instance_type());               \
   }
 
@@ -271,7 +271,7 @@ V8_INLINE constexpr bool IsInternalizedString(InstanceType instance_type) {
          (kStringTag | kInternalizedTag);
 }
 
-V8_INLINE bool IsInternalizedString(Map map_object) {
+V8_INLINE bool IsInternalizedString(Tagged<Map> map_object) {
 #if V8_STATIC_ROOTS_BOOL
   return CheckInstanceMapRange(kUniqueMapRangeOfStringType::kInternalizedString,
                                map_object);
@@ -285,7 +285,7 @@ V8_INLINE constexpr bool IsExternalString(InstanceType instance_type) {
          kExternalStringTag;
 }
 
-V8_INLINE bool IsExternalString(Map map_object) {
+V8_INLINE bool IsExternalString(Tagged<Map> map_object) {
 #if V8_STATIC_ROOTS_BOOL
   return CheckInstanceMapRange(kUniqueMapRangeOfStringType::kExternalString,
                                map_object);
@@ -298,7 +298,7 @@ V8_INLINE constexpr bool IsThinString(InstanceType instance_type) {
   return (instance_type & kStringRepresentationMask) == kThinStringTag;
 }
 
-V8_INLINE bool IsThinString(Map map_object) {
+V8_INLINE bool IsThinString(Tagged<Map> map_object) {
 #if V8_STATIC_ROOTS_BOOL
   return CheckInstanceMapRange(kUniqueMapRangeOfStringType::kThinString,
                                map_object);
@@ -316,13 +316,15 @@ V8_INLINE constexpr bool IsGcSafeCode(InstanceType instance_type) {
   return IsCode(instance_type);
 }
 
-V8_INLINE bool IsGcSafeCode(Map map_object) { return IsCode(map_object); }
+V8_INLINE bool IsGcSafeCode(Tagged<Map> map_object) {
+  return IsCode(map_object);
+}
 
 V8_INLINE constexpr bool IsAbstractCode(InstanceType instance_type) {
   return IsBytecodeArray(instance_type) || IsCode(instance_type);
 }
 
-V8_INLINE bool IsAbstractCode(Map map_object) {
+V8_INLINE bool IsAbstractCode(Tagged<Map> map_object) {
   return IsAbstractCode(map_object->instance_type());
 }
 
@@ -330,7 +332,7 @@ V8_INLINE constexpr bool IsFreeSpaceOrFiller(InstanceType instance_type) {
   return instance_type == FREE_SPACE_TYPE || instance_type == FILLER_TYPE;
 }
 
-V8_INLINE bool IsFreeSpaceOrFiller(Map map_object) {
+V8_INLINE bool IsFreeSpaceOrFiller(Tagged<Map> map_object) {
   return IsFreeSpaceOrFiller(map_object->instance_type());
 }
 
diff --git a/src/objects/instruction-stream-inl.h b/src/objects/instruction-stream-inl.h
index 2437329836c..84f2a9f3192 100644
--- a/src/objects/instruction-stream-inl.h
+++ b/src/objects/instruction-stream-inl.h
@@ -26,10 +26,9 @@ DEF_PRIMITIVE_ACCESSORS(InstructionStream, body_size, kBodySizeOffset, uint32_t)
 // through the WritableJitAllocation, e.g. the body_size setter above.
 
 // static
-InstructionStream InstructionStream::Initialize(Tagged<HeapObject> self,
-                                                Tagged<Map> map,
-                                                uint32_t body_size,
-                                                ByteArray reloc_info) {
+Tagged<InstructionStream> InstructionStream::Initialize(
+    Tagged<HeapObject> self, Tagged<Map> map, uint32_t body_size,
+    Tagged<ByteArray> reloc_info) {
   {
     ThreadIsolation::WritableJitAllocation writable_allocation =
         ThreadIsolation::RegisterInstructionStreamAllocation(
@@ -98,7 +97,8 @@ InstructionStream InstructionStream::Initialize(Tagged<HeapObject> self,
 // This is transformed into the on-heap representation, where
 // InstructionStream contains all instructions and inline metadata, and a
 // pointer to the relocation info byte array.
-void InstructionStream::Finalize(Code code, ByteArray reloc_info, CodeDesc desc,
+void InstructionStream::Finalize(Tagged<Code> code,
+                                 Tagged<ByteArray> reloc_info, CodeDesc desc,
                                  Heap* heap) {
   DisallowGarbageCollection no_gc;
   base::Optional<WriteBarrierPromise> promise;
@@ -123,7 +123,8 @@ void InstructionStream::Finalize(Code code, ByteArray reloc_info, CodeDesc desc,
                                  desc.unwinding_info,
                                  static_cast<size_t>(desc.unwinding_info_size));
     DCHECK_EQ(desc.body_size(), desc.instr_size + desc.unwinding_info_size);
-    DCHECK_EQ(code.body_size(), code.instruction_size() + code.metadata_size());
+    DCHECK_EQ(code->body_size(),
+              code->instruction_size() + code->metadata_size());
 
     promise.emplace(RelocateFromDesc(heap, desc, code->constant_pool(), no_gc));
 
@@ -144,42 +145,43 @@ Address InstructionStream::body_end() const {
   return instruction_start() + body_size();
 }
 
-Object InstructionStream::raw_code(AcquireLoadTag tag) const {
+Tagged<Object> InstructionStream::raw_code(AcquireLoadTag tag) const {
   PtrComprCageBase cage_base = main_cage_base();
-  Object value =
+  Tagged<Object> value =
       TaggedField<Object, kCodeOffset>::Acquire_Load(cage_base, *this);
   DCHECK(!ObjectInYoungGeneration(value));
   return value;
 }
 
-Code InstructionStream::code(AcquireLoadTag tag) const {
+Tagged<Code> InstructionStream::code(AcquireLoadTag tag) const {
   return Code::cast(raw_code(tag));
 }
 
-void InstructionStream::set_code(Code value, ReleaseStoreTag) {
+void InstructionStream::set_code(Tagged<Code> value, ReleaseStoreTag) {
   DCHECK(!ObjectInYoungGeneration(value));
   TaggedField<Code, kCodeOffset>::Release_Store(*this, value);
   CONDITIONAL_WRITE_BARRIER(*this, kCodeOffset, value, UPDATE_WRITE_BARRIER);
 }
 
-bool InstructionStream::TryGetCode(Code* code_out, AcquireLoadTag tag) const {
-  Object maybe_code = raw_code(tag);
+bool InstructionStream::TryGetCode(Tagged<Code>* code_out,
+                                   AcquireLoadTag tag) const {
+  Tagged<Object> maybe_code = raw_code(tag);
   if (maybe_code == Smi::zero()) return false;
   *code_out = Code::cast(maybe_code);
   return true;
 }
 
-bool InstructionStream::TryGetCodeUnchecked(Code* code_out,
+bool InstructionStream::TryGetCodeUnchecked(Tagged<Code>* code_out,
                                             AcquireLoadTag tag) const {
-  Object maybe_code = raw_code(tag);
+  Tagged<Object> maybe_code = raw_code(tag);
   if (maybe_code == Smi::zero()) return false;
   *code_out = Code::unchecked_cast(maybe_code);
   return true;
 }
 
-ByteArray InstructionStream::relocation_info() const {
+Tagged<ByteArray> InstructionStream::relocation_info() const {
   PtrComprCageBase cage_base = main_cage_base();
-  ByteArray value =
+  Tagged<ByteArray> value =
       TaggedField<ByteArray, kRelocationInfoOffset>::load(cage_base, *this);
   DCHECK(!ObjectInYoungGeneration(value));
   return value;
@@ -189,7 +191,7 @@ Address InstructionStream::instruction_start() const {
   return field_address(kHeaderSize);
 }
 
-ByteArray InstructionStream::unchecked_relocation_info() const {
+Tagged<ByteArray> InstructionStream::unchecked_relocation_info() const {
   PtrComprCageBase cage_base = main_cage_base();
   return ByteArray::unchecked_cast(
       TaggedField<HeapObject, kRelocationInfoOffset>::Acquire_Load(cage_base,
@@ -211,7 +213,8 @@ int InstructionStream::relocation_size() const {
 int InstructionStream::Size() const { return SizeFor(body_size()); }
 
 // static
-InstructionStream InstructionStream::FromTargetAddress(Address address) {
+Tagged<InstructionStream> InstructionStream::FromTargetAddress(
+    Address address) {
   {
     // TODO(jgruber,v8:6666): Support embedded builtins here. We'd need to pass
     // in the current isolate.
@@ -221,7 +224,7 @@ InstructionStream InstructionStream::FromTargetAddress(Address address) {
     CHECK(address < start || address >= end);
   }
 
-  HeapObject code =
+  Tagged<HeapObject> code =
       HeapObject::FromAddress(address - InstructionStream::kHeaderSize);
   // Unchecked cast because we can't rely on the map currently not being a
   // forwarding pointer.
@@ -229,10 +232,10 @@ InstructionStream InstructionStream::FromTargetAddress(Address address) {
 }
 
 // static
-InstructionStream InstructionStream::FromEntryAddress(
+Tagged<InstructionStream> InstructionStream::FromEntryAddress(
     Address location_of_address) {
   Address code_entry = base::Memory<Address>(location_of_address);
-  HeapObject code =
+  Tagged<HeapObject> code =
       HeapObject::FromAddress(code_entry - InstructionStream::kHeaderSize);
   // Unchecked cast because we can't rely on the map currently not being a
   // forwarding pointer.
diff --git a/src/objects/instruction-stream.cc b/src/objects/instruction-stream.cc
index d0547d2273d..babdede0bec 100644
--- a/src/objects/instruction-stream.cc
+++ b/src/objects/instruction-stream.cc
@@ -14,7 +14,7 @@ namespace v8 {
 namespace internal {
 
 void InstructionStream::Relocate(intptr_t delta) {
-  Code code;
+  Tagged<Code> code;
   if (!TryGetCodeUnchecked(&code, kAcquireLoad)) return;
   // This is called during evacuation and code.instruction_stream() will point
   // to the old object. So pass *this directly to the RelocIterator.
@@ -51,7 +51,8 @@ InstructionStream::WriteBarrierPromise InstructionStream::RelocateFromDesc(
       // code object.
       Handle<HeapObject> p = it.rinfo()->target_object_handle(origin);
       DCHECK(IsCode(*p));
-      InstructionStream target_istream = Code::cast(*p)->instruction_stream();
+      Tagged<InstructionStream> target_istream =
+          Code::cast(*p)->instruction_stream();
       it.rinfo()->set_target_address(*this, target_istream->instruction_start(),
                                      UNSAFE_SKIP_WRITE_BARRIER,
                                      SKIP_ICACHE_FLUSH);
@@ -102,11 +103,11 @@ void InstructionStream::RelocateFromDescWriteBarriers(
 
     RelocInfo::Mode mode = it.rinfo()->rmode();
     if (RelocInfo::IsEmbeddedObjectMode(mode)) {
-      HeapObject p = it.rinfo()->target_object(heap->isolate());
+      Tagged<HeapObject> p = it.rinfo()->target_object(heap->isolate());
       WriteBarrierForCode(*this, it.rinfo(), p, UPDATE_WRITE_BARRIER);
       write_barrier_promise.ResolveAddress(it.rinfo()->pc());
     } else if (RelocInfo::IsCodeTargetMode(mode)) {
-      InstructionStream target_istream =
+      Tagged<InstructionStream> target_istream =
           InstructionStream::FromTargetAddress(it.rinfo()->target_address());
       WriteBarrierForCode(*this, it.rinfo(), target_istream,
                           UPDATE_WRITE_BARRIER);
diff --git a/src/objects/instruction-stream.h b/src/objects/instruction-stream.h
index b428f3ebc90..cb353dad2e9 100644
--- a/src/objects/instruction-stream.h
+++ b/src/objects/instruction-stream.h
@@ -69,17 +69,18 @@ class InstructionStream : public HeapObject {
   //
   // Set to Smi::zero() during initialization. Heap iterators may see
   // InstructionStream objects in this state.
-  inline Code code(AcquireLoadTag tag) const;
-  inline void set_code(Code value, ReleaseStoreTag tag);
-  inline Object raw_code(AcquireLoadTag tag) const;
+  inline Tagged<Code> code(AcquireLoadTag tag) const;
+  inline void set_code(Tagged<Code> value, ReleaseStoreTag tag);
+  inline Tagged<Object> raw_code(AcquireLoadTag tag) const;
   // Use when the InstructionStream may be uninitialized:
-  inline bool TryGetCode(Code* code_out, AcquireLoadTag tag) const;
-  inline bool TryGetCodeUnchecked(Code* code_out, AcquireLoadTag tag) const;
+  inline bool TryGetCode(Tagged<Code>* code_out, AcquireLoadTag tag) const;
+  inline bool TryGetCodeUnchecked(Tagged<Code>* code_out,
+                                  AcquireLoadTag tag) const;
 
   // [relocation_info]: InstructionStream relocation information.
-  inline ByteArray relocation_info() const;
+  inline Tagged<ByteArray> relocation_info() const;
   // Unchecked accessor to be used during GC.
-  inline ByteArray unchecked_relocation_info() const;
+  inline Tagged<ByteArray> unchecked_relocation_info() const;
 
   inline uint8_t* relocation_start() const;
   inline uint8_t* relocation_end() const;
@@ -99,18 +100,18 @@ class InstructionStream : public HeapObject {
   }
   inline int Size() const;
 
-  static inline InstructionStream FromTargetAddress(Address address);
-  static inline InstructionStream FromEntryAddress(Address location_of_address);
+  static inline Tagged<InstructionStream> FromTargetAddress(Address address);
+  static inline Tagged<InstructionStream> FromEntryAddress(
+      Address location_of_address);
 
   // Relocate the code by delta bytes.
   void Relocate(intptr_t delta);
 
-  static V8_INLINE InstructionStream Initialize(Tagged<HeapObject> self,
-                                                Tagged<Map> map,
-                                                uint32_t body_size,
-                                                ByteArray reloc_info);
-  V8_INLINE void Finalize(Code code, ByteArray reloc_info, CodeDesc desc,
-                          Heap* heap);
+  static V8_INLINE Tagged<InstructionStream> Initialize(
+      Tagged<HeapObject> self, Tagged<Map> map, uint32_t body_size,
+      Tagged<ByteArray> reloc_info);
+  V8_INLINE void Finalize(Tagged<Code> code, Tagged<ByteArray> reloc_info,
+                          CodeDesc desc, Heap* heap);
 
   DECL_CAST(InstructionStream)
   DECL_PRINTER(InstructionStream)
diff --git a/src/objects/intl-objects.cc b/src/objects/intl-objects.cc
index e14d0a7fc2b..d6e63aac760 100644
--- a/src/objects/intl-objects.cc
+++ b/src/objects/intl-objects.cc
@@ -158,7 +158,7 @@ void ToUpperWithSharpS(base::Vector<const Char> src,
   }
 }
 
-inline int FindFirstUpperOrNonAscii(String s, int length) {
+inline int FindFirstUpperOrNonAscii(Tagged<String> s, int length) {
   for (int index = 0; index < length; ++index) {
     uint16_t ch = s->Get(index);
     if (V8_UNLIKELY(IsAsciiUpper(ch) || ch & ~0x7F)) {
@@ -288,7 +288,8 @@ MaybeHandle<String> LocaleConvertCase(Isolate* isolate, Handle<String> s,
 // strings and does not allocate. Note that {src} could still be, e.g., a
 // one-byte sliced string with a two-byte parent string.
 // Called from TF builtins.
-String Intl::ConvertOneByteToLower(String src, String dst) {
+Tagged<String> Intl::ConvertOneByteToLower(Tagged<String> src,
+                                           Tagged<String> dst) {
   DCHECK_EQ(src->length(), dst->length());
   DCHECK(src->IsOneByteRepresentation());
   DCHECK(src->IsFlat());
diff --git a/src/objects/intl-objects.h b/src/objects/intl-objects.h
index 9b52002626c..02b3b2c4256 100644
--- a/src/objects/intl-objects.h
+++ b/src/objects/intl-objects.h
@@ -358,7 +358,8 @@ class Intl {
   static const uint8_t* AsciiCollationWeightsL3();
   static const int kAsciiCollationWeightsLength;
 
-  static String ConvertOneByteToLower(String src, String dst);
+  static Tagged<String> ConvertOneByteToLower(Tagged<String> src,
+                                              Tagged<String> dst);
 
   static const std::set<std::string>& GetAvailableLocales();
 
diff --git a/src/objects/js-array-buffer.cc b/src/objects/js-array-buffer.cc
index 70d56aaa6dd..8644f3a28db 100644
--- a/src/objects/js-array-buffer.cc
+++ b/src/objects/js-array-buffer.cc
@@ -177,7 +177,7 @@ size_t JSArrayBuffer::GsabByteLength(Isolate* isolate,
   DCHECK(v8_flags.harmony_rab_gsab);
   DisallowGarbageCollection no_gc;
   DisallowJavascriptExecution no_js(isolate);
-  JSArrayBuffer buffer = JSArrayBuffer::cast(Object(raw_array_buffer));
+  Tagged<JSArrayBuffer> buffer = JSArrayBuffer::cast(Object(raw_array_buffer));
   CHECK(buffer->is_resizable_by_js());
   CHECK(buffer->is_shared());
   return buffer->GetBackingStore()->byte_length(std::memory_order_seq_cst);
@@ -404,9 +404,9 @@ size_t JSTypedArray::LengthTrackingGsabBackedTypedArrayLength(
   DCHECK(v8_flags.harmony_rab_gsab);
   DisallowGarbageCollection no_gc;
   DisallowJavascriptExecution no_js(isolate);
-  JSTypedArray array = JSTypedArray::cast(Object(raw_array));
+  Tagged<JSTypedArray> array = JSTypedArray::cast(Object(raw_array));
   CHECK(array->is_length_tracking());
-  JSArrayBuffer buffer = array->buffer();
+  Tagged<JSArrayBuffer> buffer = array->buffer();
   CHECK(buffer->is_resizable_by_js());
   CHECK(buffer->is_shared());
   size_t backing_byte_length =
diff --git a/src/objects/js-atomics-synchronization.cc b/src/objects/js-atomics-synchronization.cc
index e435e5cccb8..c4e4f22606f 100644
--- a/src/objects/js-atomics-synchronization.cc
+++ b/src/objects/js-atomics-synchronization.cc
@@ -573,7 +573,7 @@ uint32_t JSAtomicsCondition::Notify(Isolate* requester, uint32_t count) {
   return old_head->NotifyAllInList();
 }
 
-Object JSAtomicsCondition::NumWaitersForTesting(Isolate* isolate) {
+Tagged<Object> JSAtomicsCondition::NumWaitersForTesting(Isolate* isolate) {
   DisallowGarbageCollection no_gc;
   std::atomic<StateT>* state = AtomicStatePtr();
   StateT current_state = state->load(std::memory_order_relaxed);
diff --git a/src/objects/js-atomics-synchronization.h b/src/objects/js-atomics-synchronization.h
index d1e7037ddc0..a083b3bbfd3 100644
--- a/src/objects/js-atomics-synchronization.h
+++ b/src/objects/js-atomics-synchronization.h
@@ -191,7 +191,7 @@ class JSAtomicsCondition
   // Notify {count} waiters. Returns the number of waiters woken up.
   V8_EXPORT_PRIVATE uint32_t Notify(Isolate* requester, uint32_t count);
 
-  Object NumWaitersForTesting(Isolate* isolate);
+  Tagged<Object> NumWaitersForTesting(Isolate* isolate);
 
   TQ_OBJECT_CONSTRUCTORS(JSAtomicsCondition)
 
diff --git a/src/objects/js-break-iterator-inl.h b/src/objects/js-break-iterator-inl.h
index dc85efe3fe6..96d2a6f97fb 100644
--- a/src/objects/js-break-iterator-inl.h
+++ b/src/objects/js-break-iterator-inl.h
@@ -22,10 +22,10 @@ namespace internal {
 
 TQ_OBJECT_CONSTRUCTORS_IMPL(JSV8BreakIterator)
 
-ACCESSORS(JSV8BreakIterator, break_iterator, Managed<icu::BreakIterator>,
-          kBreakIteratorOffset)
-ACCESSORS(JSV8BreakIterator, unicode_string, Managed<icu::UnicodeString>,
-          kUnicodeStringOffset)
+ACCESSORS(JSV8BreakIterator, break_iterator,
+          Tagged<Managed<icu::BreakIterator>>, kBreakIteratorOffset)
+ACCESSORS(JSV8BreakIterator, unicode_string,
+          Tagged<Managed<icu::UnicodeString>>, kUnicodeStringOffset)
 
 }  // namespace internal
 }  // namespace v8
diff --git a/src/objects/js-break-iterator.cc b/src/objects/js-break-iterator.cc
index ae84189655b..20a2c8d8527 100644
--- a/src/objects/js-break-iterator.cc
+++ b/src/objects/js-break-iterator.cc
@@ -218,8 +218,8 @@ Handle<Object> JSV8BreakIterator::Next(
       break_iterator->break_iterator()->raw()->next());
 }
 
-String JSV8BreakIterator::BreakType(Isolate* isolate,
-                                    Handle<JSV8BreakIterator> break_iterator) {
+Tagged<String> JSV8BreakIterator::BreakType(
+    Isolate* isolate, Handle<JSV8BreakIterator> break_iterator) {
   int32_t status = break_iterator->break_iterator()->raw()->getRuleStatus();
   // Keep return values in sync with JavaScript BreakType enum.
   if (status >= UBRK_WORD_NONE && status < UBRK_WORD_NONE_LIMIT) {
diff --git a/src/objects/js-break-iterator.h b/src/objects/js-break-iterator.h
index 6fc02c856b7..56a8cb561bd 100644
--- a/src/objects/js-break-iterator.h
+++ b/src/objects/js-break-iterator.h
@@ -50,13 +50,13 @@ class JSV8BreakIterator
                               Handle<JSV8BreakIterator> break_iterator);
   static Handle<Object> Next(Isolate* isolate,
                              Handle<JSV8BreakIterator> break_iterator);
-  static String BreakType(Isolate* isolate,
-                          Handle<JSV8BreakIterator> break_iterator);
+  static Tagged<String> BreakType(Isolate* isolate,
+                                  Handle<JSV8BreakIterator> break_iterator);
 
   DECL_PRINTER(JSV8BreakIterator)
 
-  DECL_ACCESSORS(break_iterator, Managed<icu::BreakIterator>)
-  DECL_ACCESSORS(unicode_string, Managed<icu::UnicodeString>)
+  DECL_ACCESSORS(break_iterator, Tagged<Managed<icu::BreakIterator>>)
+  DECL_ACCESSORS(unicode_string, Tagged<Managed<icu::UnicodeString>>)
 
   TQ_OBJECT_CONSTRUCTORS(JSV8BreakIterator)
 };
diff --git a/src/objects/js-collator-inl.h b/src/objects/js-collator-inl.h
index 81ee95326ae..c4d803ad7fa 100644
--- a/src/objects/js-collator-inl.h
+++ b/src/objects/js-collator-inl.h
@@ -22,7 +22,8 @@ namespace internal {
 
 TQ_OBJECT_CONSTRUCTORS_IMPL(JSCollator)
 
-ACCESSORS(JSCollator, icu_collator, Managed<icu::Collator>, kIcuCollatorOffset)
+ACCESSORS(JSCollator, icu_collator, Tagged<Managed<icu::Collator>>,
+          kIcuCollatorOffset)
 
 }  // namespace internal
 }  // namespace v8
diff --git a/src/objects/js-collator.h b/src/objects/js-collator.h
index 9dc19ac4ed8..a3dea6290ac 100644
--- a/src/objects/js-collator.h
+++ b/src/objects/js-collator.h
@@ -46,7 +46,7 @@ class JSCollator : public TorqueGeneratedJSCollator<JSCollator, JSObject> {
 
   DECL_PRINTER(JSCollator)
 
-  DECL_ACCESSORS(icu_collator, Managed<icu::Collator>)
+  DECL_ACCESSORS(icu_collator, Tagged<Managed<icu::Collator>>)
 
   TQ_OBJECT_CONSTRUCTORS(JSCollator)
 };
diff --git a/src/objects/js-collection-inl.h b/src/objects/js-collection-inl.h
index dbfb4e575ea..109de049a5c 100644
--- a/src/objects/js-collection-inl.h
+++ b/src/objects/js-collection-inl.h
@@ -46,12 +46,12 @@ JSSetIterator::JSSetIterator(Address ptr)
 CAST_ACCESSOR(JSSetIterator)
 CAST_ACCESSOR(JSMapIterator)
 
-Object JSMapIterator::CurrentValue() {
-  OrderedHashMap table = OrderedHashMap::cast(this->table());
+Tagged<Object> JSMapIterator::CurrentValue() {
+  Tagged<OrderedHashMap> table = OrderedHashMap::cast(this->table());
   int index = Smi::ToInt(this->index());
   DCHECK_GE(index, 0);
   InternalIndex entry(index);
-  Object value = table->ValueAt(entry);
+  Tagged<Object> value = table->ValueAt(entry);
   DCHECK(!IsTheHole(value));
   return value;
 }
diff --git a/src/objects/js-collection-iterator.h b/src/objects/js-collection-iterator.h
index feb3da37fa4..00a674abceb 100644
--- a/src/objects/js-collection-iterator.h
+++ b/src/objects/js-collection-iterator.h
@@ -51,7 +51,7 @@ class OrderedHashTableIterator : public JSCollectionIterator {
 
   // Returns the current key of the iterator. This should only be called when
   // |HasMore| returns true.
-  inline Object CurrentKey();
+  inline Tagged<Object> CurrentKey();
 
  private:
   // Transitions the iterator to the non obsolete backing store. This is a NOP
diff --git a/src/objects/js-collection.h b/src/objects/js-collection.h
index 8de079e69cf..6adc47cb464 100644
--- a/src/objects/js-collection.h
+++ b/src/objects/js-collection.h
@@ -79,7 +79,7 @@ class JSMapIterator
 
   // Returns the current value of the iterator. This should only be called when
   // |HasMore| returns true.
-  inline Object CurrentValue();
+  inline Tagged<Object> CurrentValue();
 
   OBJECT_CONSTRUCTORS(JSMapIterator,
                       OrderedHashTableIterator<JSMapIterator, OrderedHashMap>);
diff --git a/src/objects/js-date-time-format-inl.h b/src/objects/js-date-time-format-inl.h
index fefe081f8f5..e55d50d1ab8 100644
--- a/src/objects/js-date-time-format-inl.h
+++ b/src/objects/js-date-time-format-inl.h
@@ -22,11 +22,13 @@ namespace internal {
 
 TQ_OBJECT_CONSTRUCTORS_IMPL(JSDateTimeFormat)
 
-ACCESSORS(JSDateTimeFormat, icu_locale, Managed<icu::Locale>, kIcuLocaleOffset)
+ACCESSORS(JSDateTimeFormat, icu_locale, Tagged<Managed<icu::Locale>>,
+          kIcuLocaleOffset)
 ACCESSORS(JSDateTimeFormat, icu_simple_date_format,
-          Managed<icu::SimpleDateFormat>, kIcuSimpleDateFormatOffset)
+          Tagged<Managed<icu::SimpleDateFormat>>, kIcuSimpleDateFormatOffset)
 ACCESSORS(JSDateTimeFormat, icu_date_interval_format,
-          Managed<icu::DateIntervalFormat>, kIcuDateIntervalFormatOffset)
+          Tagged<Managed<icu::DateIntervalFormat>>,
+          kIcuDateIntervalFormatOffset)
 
 inline void JSDateTimeFormat::set_hour_cycle(HourCycle hour_cycle) {
   int hints = flags();
diff --git a/src/objects/js-date-time-format.cc b/src/objects/js-date-time-format.cc
index 540659bdd3d..825a66315a8 100644
--- a/src/objects/js-date-time-format.cc
+++ b/src/objects/js-date-time-format.cc
@@ -1817,7 +1817,7 @@ std::unique_ptr<icu::SimpleDateFormat> CreateICUDateFormatFromCache(
 std::unique_ptr<icu::DateIntervalFormat> LazyCreateDateIntervalFormat(
     Isolate* isolate, Handle<JSDateTimeFormat> date_time_format,
     PatternKind kind) {
-  Managed<icu::DateIntervalFormat> managed_format =
+  Tagged<Managed<icu::DateIntervalFormat>> managed_format =
       date_time_format->icu_date_interval_format();
   if (kind == PatternKind::kDate && managed_format->get()) {
     return std::unique_ptr<icu::DateIntervalFormat>(
diff --git a/src/objects/js-date-time-format.h b/src/objects/js-date-time-format.h
index 27d074a42c7..ad9be342e0e 100644
--- a/src/objects/js-date-time-format.h
+++ b/src/objects/js-date-time-format.h
@@ -149,9 +149,10 @@ class JSDateTimeFormat
   static_assert(DateTimeStyle::kMedium <= TimeStyleBits::kMax);
   static_assert(DateTimeStyle::kShort <= TimeStyleBits::kMax);
 
-  DECL_ACCESSORS(icu_locale, Managed<icu::Locale>)
-  DECL_ACCESSORS(icu_simple_date_format, Managed<icu::SimpleDateFormat>)
-  DECL_ACCESSORS(icu_date_interval_format, Managed<icu::DateIntervalFormat>)
+  DECL_ACCESSORS(icu_locale, Tagged<Managed<icu::Locale>>)
+  DECL_ACCESSORS(icu_simple_date_format, Tagged<Managed<icu::SimpleDateFormat>>)
+  DECL_ACCESSORS(icu_date_interval_format,
+                 Tagged<Managed<icu::DateIntervalFormat>>)
 
   DECL_PRINTER(JSDateTimeFormat)
 
diff --git a/src/objects/js-display-names-inl.h b/src/objects/js-display-names-inl.h
index d5c5553cf67..edca4571a30 100644
--- a/src/objects/js-display-names-inl.h
+++ b/src/objects/js-display-names-inl.h
@@ -20,7 +20,7 @@ namespace internal {
 
 #include "torque-generated/src/objects/js-display-names-tq-inl.inc"
 
-ACCESSORS(JSDisplayNames, internal, Managed<DisplayNamesInternal>,
+ACCESSORS(JSDisplayNames, internal, Tagged<Managed<DisplayNamesInternal>>,
           kInternalOffset)
 TQ_OBJECT_CONSTRUCTORS_IMPL(JSDisplayNames)
 
diff --git a/src/objects/js-display-names.h b/src/objects/js-display-names.h
index f637d91292d..6a352915c49 100644
--- a/src/objects/js-display-names.h
+++ b/src/objects/js-display-names.h
@@ -87,7 +87,7 @@ class JSDisplayNames
   static_assert(LanguageDisplay::kDialect <= LanguageDisplayBit::kMax);
   static_assert(LanguageDisplay::kStandard <= LanguageDisplayBit::kMax);
 
-  DECL_ACCESSORS(internal, Managed<DisplayNamesInternal>)
+  DECL_ACCESSORS(internal, Tagged<Managed<DisplayNamesInternal>>)
 
   DECL_PRINTER(JSDisplayNames)
 
diff --git a/src/objects/js-duration-format-inl.h b/src/objects/js-duration-format-inl.h
index 1d32a600a8f..3c9c78ff8c5 100644
--- a/src/objects/js-duration-format-inl.h
+++ b/src/objects/js-duration-format-inl.h
@@ -22,7 +22,8 @@ namespace internal {
 
 TQ_OBJECT_CONSTRUCTORS_IMPL(JSDurationFormat)
 
-ACCESSORS(JSDurationFormat, icu_locale, Managed<icu::Locale>, kIcuLocaleOffset)
+ACCESSORS(JSDurationFormat, icu_locale, Tagged<Managed<icu::Locale>>,
+          kIcuLocaleOffset)
 
 #define IMPL_INLINE_SETTER_GETTER(T, n, B, f, M)           \
   inline void JSDurationFormat::set_##n(T value) {         \
@@ -96,7 +97,7 @@ inline int32_t JSDurationFormat::fractional_digits() const {
 }
 
 ACCESSORS(JSDurationFormat, icu_number_formatter,
-          Managed<icu::number::LocalizedNumberFormatter>,
+          Tagged<Managed<icu::number::LocalizedNumberFormatter>>,
           kIcuNumberFormatterOffset)
 }  // namespace internal
 }  // namespace v8
diff --git a/src/objects/js-duration-format.cc b/src/objects/js-duration-format.cc
index 359b258a5c2..b07868a3bc4 100644
--- a/src/objects/js-duration-format.cc
+++ b/src/objects/js-duration-format.cc
@@ -756,8 +756,8 @@ MaybeHandle<T> PartitionDurationFormatPattern(Isolate* isolate,
   std::vector<std::vector<Part>>* parts = Details ? &list : nullptr;
   std::vector<icu::UnicodeString> string_list;
 
-  DurationRecordToListOfFormattedNumber(df, *(df->icu_number_formatter().raw()),
-                                        record, parts, &string_list);
+  DurationRecordToListOfFormattedNumber(
+      df, *(df->icu_number_formatter()->raw()), record, parts, &string_list);
 
   icu::FormattedList formatted = formatter->formatStringsToValue(
       string_list.data(), static_cast<int32_t>(string_list.size()), status);
diff --git a/src/objects/js-duration-format.h b/src/objects/js-duration-format.h
index 9c47333cc11..df13e767f9b 100644
--- a/src/objects/js-duration-format.h
+++ b/src/objects/js-duration-format.h
@@ -162,9 +162,9 @@ class JSDurationFormat
   static_assert(MicrosecondsStyleBits::is_valid(FieldStyle::kStyle4Max));
   static_assert(NanosecondsStyleBits::is_valid(FieldStyle::kStyle4Max));
 
-  DECL_ACCESSORS(icu_locale, Managed<icu::Locale>)
+  DECL_ACCESSORS(icu_locale, Tagged<Managed<icu::Locale>>)
   DECL_ACCESSORS(icu_number_formatter,
-                 Managed<icu::number::LocalizedNumberFormatter>)
+                 Tagged<Managed<icu::number::LocalizedNumberFormatter>>)
 
   DECL_PRINTER(JSDurationFormat)
 
diff --git a/src/objects/js-function-inl.h b/src/objects/js-function-inl.h
index 93ddb056b98..1d796729725 100644
--- a/src/objects/js-function-inl.h
+++ b/src/objects/js-function-inl.h
@@ -42,7 +42,8 @@ DEF_GETTER(JSFunction, feedback_vector, Tagged<FeedbackVector>) {
   return FeedbackVector::cast(raw_feedback_cell(cage_base)->value(cage_base));
 }
 
-ClosureFeedbackCellArray JSFunction::closure_feedback_cell_array() const {
+Tagged<ClosureFeedbackCellArray> JSFunction::closure_feedback_cell_array()
+    const {
   DCHECK(has_closure_feedback_cell_array());
   return ClosureFeedbackCellArray::cast(raw_feedback_cell()->value());
 }
@@ -62,7 +63,7 @@ void JSFunction::CompleteInobjectSlackTrackingIfActive() {
 }
 
 template <typename IsolateT>
-AbstractCode JSFunction::abstract_code(IsolateT* isolate) {
+Tagged<AbstractCode> JSFunction::abstract_code(IsolateT* isolate) {
   if (ActiveTierIsIgnition()) {
     return AbstractCode::cast(shared()->GetBytecodeArray(isolate));
   } else {
@@ -179,7 +180,7 @@ bool JSFunction::has_closure_feedback_cell_array() const {
          IsClosureFeedbackCellArray(raw_feedback_cell()->value());
 }
 
-Context JSFunction::context() {
+Tagged<Context> JSFunction::context() {
   return TaggedField<Context, kContextOffset>::load(*this);
 }
 
@@ -191,9 +192,11 @@ bool JSFunction::has_context() const {
   return IsContext(TaggedField<HeapObject, kContextOffset>::load(*this));
 }
 
-JSGlobalProxy JSFunction::global_proxy() { return context()->global_proxy(); }
+Tagged<JSGlobalProxy> JSFunction::global_proxy() {
+  return context()->global_proxy();
+}
 
-NativeContext JSFunction::native_context() {
+Tagged<NativeContext> JSFunction::native_context() {
   return context()->native_context();
 }
 
@@ -252,7 +255,7 @@ DEF_GETTER(JSFunction, prototype, Tagged<Object>) {
   DCHECK(has_prototype(cage_base));
   // If the function's prototype property has been set to a non-JSReceiver
   // value, that value is stored in the constructor field of the map.
-  Map map = this->map(cage_base);
+  Tagged<Map> map = this->map(cage_base);
   if (map->has_non_instance_prototype()) {
     return map->GetNonInstancePrototype(cage_base);
   }
@@ -272,14 +275,15 @@ bool JSFunction::NeedsResetDueToFlushedBytecode() {
   // TODO(v8) the branches for !IsSharedFunctionInfo() and !IsCode() are
   // probably dead code by now. Investigate removing them or replacing them
   // with CHECKs.
-  Object maybe_shared = ACQUIRE_READ_FIELD(*this, kSharedFunctionInfoOffset);
+  Tagged<Object> maybe_shared =
+      ACQUIRE_READ_FIELD(*this, kSharedFunctionInfoOffset);
   if (!IsSharedFunctionInfo(maybe_shared)) return false;
 
-  Object maybe_code = raw_code(kAcquireLoad);
+  Tagged<Object> maybe_code = raw_code(kAcquireLoad);
   if (!IsCode(maybe_code)) return false;
-  Code code = Code::cast(maybe_code);
+  Tagged<Code> code = Code::cast(maybe_code);
 
-  SharedFunctionInfo shared = SharedFunctionInfo::cast(maybe_shared);
+  Tagged<SharedFunctionInfo> shared = SharedFunctionInfo::cast(maybe_shared);
   return !shared->is_compiled() && code->builtin_id() != Builtin::kCompileLazy;
 }
 
@@ -288,8 +292,8 @@ bool JSFunction::NeedsResetDueToFlushedBaselineCode() {
 }
 
 void JSFunction::ResetIfCodeFlushed(
-    base::Optional<std::function<void(HeapObject object, ObjectSlot slot,
-                                      HeapObject target)>>
+    base::Optional<std::function<void(
+        Tagged<HeapObject> object, ObjectSlot slot, Tagged<HeapObject> target)>>
         gc_notify_updated_slot) {
   const bool kBytecodeCanFlush =
       v8_flags.flush_bytecode || v8_flags.stress_snapshot;
diff --git a/src/objects/js-function.cc b/src/objects/js-function.cc
index e24de3315f9..0c6779bd311 100644
--- a/src/objects/js-function.cc
+++ b/src/objects/js-function.cc
@@ -53,7 +53,7 @@ CodeKinds JSFunction::GetAvailableCodeKinds() const {
   // Check the optimized code cache.
   if (has_feedback_vector() && feedback_vector()->has_optimized_code() &&
       !feedback_vector()->optimized_code()->marked_for_deoptimization()) {
-    Code code = feedback_vector()->optimized_code();
+    Tagged<Code> code = feedback_vector()->optimized_code();
     DCHECK(CodeKindIsOptimizedJSFunction(code->kind()));
     result |= CodeKindToCodeKindFlag(code->kind());
   }
@@ -1132,7 +1132,7 @@ MaybeHandle<Map> JSFunction::GetDerivedRabGsabTypedArrayMap(
   }
   {
     DisallowHeapAllocation no_alloc;
-    NativeContext context = isolate->context()->native_context();
+    Tagged<NativeContext> context = isolate->context()->native_context();
     int ctor_index =
         TypedArrayElementsKindToConstructorIndex(map->elements_kind());
     if (*new_target == context->get(ctor_index)) {
@@ -1199,13 +1199,13 @@ bool UseFastFunctionNameLookup(Isolate* isolate, Map map) {
     return false;
   }
   DCHECK(!map->is_dictionary_map());
-  HeapObject value;
+  Tagged<HeapObject> value;
   ReadOnlyRoots roots(isolate);
   auto descriptors = map->instance_descriptors(isolate);
   InternalIndex kNameIndex{JSFunction::kNameDescriptorIndex};
   if (descriptors->GetKey(kNameIndex) != roots.name_string() ||
       !descriptors->GetValue(kNameIndex)
-           .GetHeapObjectIfStrong(isolate, &value)) {
+           ->GetHeapObjectIfStrong(isolate, &value)) {
     return false;
   }
   return IsAccessorInfo(value);
diff --git a/src/objects/js-function.h b/src/objects/js-function.h
index 3dbc4c1ab9e..1238c78f0c2 100644
--- a/src/objects/js-function.h
+++ b/src/objects/js-function.h
@@ -99,14 +99,14 @@ class JSFunction : public TorqueGeneratedJSFunction<
   static const int kMinDescriptorsForFastBindAndWrap = 2;
 
   // [context]: The context for this function.
-  inline Context context();
+  inline Tagged<Context> context();
   DECL_RELAXED_GETTER(context, Tagged<Context>)
   inline bool has_context() const;
   using TorqueGeneratedClass::context;
   using TorqueGeneratedClass::set_context;
   DECL_RELEASE_ACQUIRE_ACCESSORS(context, Tagged<Context>)
-  inline JSGlobalProxy global_proxy();
-  inline NativeContext native_context();
+  inline Tagged<JSGlobalProxy> global_proxy();
+  inline Tagged<NativeContext> native_context();
   inline int length();
 
   static Handle<String> GetName(Isolate* isolate, Handle<JSFunction> function);
@@ -134,7 +134,7 @@ class JSFunction : public TorqueGeneratedJSFunction<
   // Get the abstract code associated with the function, which will either be
   // a InstructionStream object or a BytecodeArray.
   template <typename IsolateT>
-  inline AbstractCode abstract_code(IsolateT* isolate);
+  inline Tagged<AbstractCode> abstract_code(IsolateT* isolate);
 
   // The predicates for querying code kinds related to this function have
   // specific terminology:
@@ -233,7 +233,7 @@ class JSFunction : public TorqueGeneratedJSFunction<
   // cell arrays after compile, when we want to allocate feedback vectors
   // lazily.
   inline bool has_closure_feedback_cell_array() const;
-  inline ClosureFeedbackCellArray closure_feedback_cell_array() const;
+  inline Tagged<ClosureFeedbackCellArray> closure_feedback_cell_array() const;
   static void EnsureClosureFeedbackCellArray(
       Handle<JSFunction> function, bool reset_budget_for_feedback_allocation);
 
@@ -252,8 +252,9 @@ class JSFunction : public TorqueGeneratedJSFunction<
   // Resets function to clear compiled data after bytecode has been flushed.
   inline bool NeedsResetDueToFlushedBytecode();
   inline void ResetIfCodeFlushed(
-      base::Optional<std::function<void(HeapObject object, ObjectSlot slot,
-                                        HeapObject target)>>
+      base::Optional<
+          std::function<void(Tagged<HeapObject> object, ObjectSlot slot,
+                             Tagged<HeapObject> target)>>
           gc_notify_updated_slot = base::nullopt);
 
   // Returns if the closure's code field has to be updated because it has
diff --git a/src/objects/js-list-format-inl.h b/src/objects/js-list-format-inl.h
index e7e0384c995..d494ebcf21f 100644
--- a/src/objects/js-list-format-inl.h
+++ b/src/objects/js-list-format-inl.h
@@ -23,7 +23,7 @@ namespace internal {
 TQ_OBJECT_CONSTRUCTORS_IMPL(JSListFormat)
 
 // Base list format accessors.
-ACCESSORS(JSListFormat, icu_formatter, Managed<icu::ListFormatter>,
+ACCESSORS(JSListFormat, icu_formatter, Tagged<Managed<icu::ListFormatter>>,
           kIcuFormatterOffset)
 
 inline void JSListFormat::set_style(Style style) {
diff --git a/src/objects/js-list-format.h b/src/objects/js-list-format.h
index ea68a313e24..8f3651e3304 100644
--- a/src/objects/js-list-format.h
+++ b/src/objects/js-list-format.h
@@ -59,7 +59,7 @@ class JSListFormat
   Handle<String> TypeAsString() const;
 
   // ListFormat accessors.
-  DECL_ACCESSORS(icu_formatter, Managed<icu::ListFormatter>)
+  DECL_ACCESSORS(icu_formatter, Tagged<Managed<icu::ListFormatter>>)
 
   // Style: identifying the relative time format style used.
   //
diff --git a/src/objects/js-locale-inl.h b/src/objects/js-locale-inl.h
index 49c4dc7b4f4..8ed7f3afe47 100644
--- a/src/objects/js-locale-inl.h
+++ b/src/objects/js-locale-inl.h
@@ -23,7 +23,7 @@ namespace internal {
 
 TQ_OBJECT_CONSTRUCTORS_IMPL(JSLocale)
 
-ACCESSORS(JSLocale, icu_locale, Managed<icu::Locale>, kIcuLocaleOffset)
+ACCESSORS(JSLocale, icu_locale, Tagged<Managed<icu::Locale>>, kIcuLocaleOffset)
 
 }  // namespace internal
 }  // namespace v8
diff --git a/src/objects/js-locale.h b/src/objects/js-locale.h
index 7be93dba8b3..0aa94a3509c 100644
--- a/src/objects/js-locale.h
+++ b/src/objects/js-locale.h
@@ -79,7 +79,7 @@ class JSLocale : public TorqueGeneratedJSLocale<JSLocale, JSObject> {
   // Help function to check well-formed "3alpha"
   static bool Is3Alpha(const std::string& value);
 
-  DECL_ACCESSORS(icu_locale, Managed<icu::Locale>)
+  DECL_ACCESSORS(icu_locale, Tagged<Managed<icu::Locale>>)
 
   DECL_PRINTER(JSLocale)
 
diff --git a/src/objects/js-number-format-inl.h b/src/objects/js-number-format-inl.h
index cddc93afd27..204d6f331f0 100644
--- a/src/objects/js-number-format-inl.h
+++ b/src/objects/js-number-format-inl.h
@@ -23,7 +23,7 @@ namespace internal {
 TQ_OBJECT_CONSTRUCTORS_IMPL(JSNumberFormat)
 
 ACCESSORS(JSNumberFormat, icu_number_formatter,
-          Managed<icu::number::LocalizedNumberFormatter>,
+          Tagged<Managed<icu::number::LocalizedNumberFormatter>>,
           kIcuNumberFormatterOffset)
 
 }  // namespace internal
diff --git a/src/objects/js-number-format.cc b/src/objects/js-number-format.cc
index 3b10fb703b0..4f0ef318063 100644
--- a/src/objects/js-number-format.cc
+++ b/src/objects/js-number-format.cc
@@ -2104,7 +2104,7 @@ MaybeHandle<JSArray> FormatRangeToJSArray(
 
 Maybe<icu::number::LocalizedNumberRangeFormatter>
 JSNumberFormat::GetRangeFormatter(
-    Isolate* isolate, String locale,
+    Isolate* isolate, Tagged<String> locale,
     const icu::number::LocalizedNumberFormatter& number_formatter) {
   UErrorCode status = U_ZERO_ERROR;
   UParseError perror;
diff --git a/src/objects/js-number-format.h b/src/objects/js-number-format.h
index 40c0e557f69..085e241571d 100644
--- a/src/objects/js-number-format.h
+++ b/src/objects/js-number-format.h
@@ -100,13 +100,13 @@ class JSNumberFormat
 
   V8_WARN_UNUSED_RESULT static Maybe<icu::number::LocalizedNumberRangeFormatter>
   GetRangeFormatter(
-      Isolate* isolate, String locale,
+      Isolate* isolate, Tagged<String> locale,
       const icu::number::LocalizedNumberFormatter& number_formatter);
 
   DECL_PRINTER(JSNumberFormat)
 
   DECL_ACCESSORS(icu_number_formatter,
-                 Managed<icu::number::LocalizedNumberFormatter>)
+                 Tagged<Managed<icu::number::LocalizedNumberFormatter>>)
 
   TQ_OBJECT_CONSTRUCTORS(JSNumberFormat)
 };
diff --git a/src/objects/js-objects-inl.h b/src/objects/js-objects-inl.h
index aa8f8197aa0..4a8c6484803 100644
--- a/src/objects/js-objects-inl.h
+++ b/src/objects/js-objects-inl.h
@@ -130,16 +130,17 @@ V8_WARN_UNUSED_RESULT MaybeHandle<FixedArray> JSReceiver::OwnPropertyKeys(
                                  GetKeysConversion::kConvertToString);
 }
 
-bool JSObject::PrototypeHasNoElements(Isolate* isolate, JSObject object) {
+bool JSObject::PrototypeHasNoElements(Isolate* isolate,
+                                      Tagged<JSObject> object) {
   DisallowGarbageCollection no_gc;
-  HeapObject prototype = HeapObject::cast(object->map()->prototype());
+  Tagged<HeapObject> prototype = HeapObject::cast(object->map()->prototype());
   ReadOnlyRoots roots(isolate);
-  HeapObject null = roots.null_value();
+  Tagged<HeapObject> null = roots.null_value();
   Tagged<FixedArrayBase> empty_fixed_array = roots.empty_fixed_array();
   Tagged<FixedArrayBase> empty_slow_element_dictionary =
       roots.empty_slow_element_dictionary();
   while (prototype != null) {
-    Map map = prototype->map();
+    Tagged<Map> map = prototype->map();
     if (IsCustomElementsReceiverMap(map)) return false;
     Tagged<FixedArrayBase> elements = JSObject::cast(prototype)->elements();
     if (elements != empty_fixed_array &&
@@ -182,9 +183,9 @@ void JSObject::EnsureCanContainElements(Handle<JSObject> object, TSlot objects,
     DCHECK(mode != ALLOW_COPIED_DOUBLE_ELEMENTS);
     bool is_holey = IsHoleyElementsKind(current_kind);
     if (current_kind == HOLEY_ELEMENTS) return;
-    Object the_hole = object->GetReadOnlyRoots().the_hole_value();
+    Tagged<Object> the_hole = object->GetReadOnlyRoots().the_hole_value();
     for (uint32_t i = 0; i < count; ++i, ++objects) {
-      Object current = *objects;
+      Tagged<Object> current = *objects;
       if (current == the_hole) {
         is_holey = true;
         target_kind = GetHoleyElementsKind(target_kind);
@@ -273,7 +274,7 @@ DEF_GETTER(JSObject, GetNamedInterceptor, Tagged<InterceptorInfo>) {
 }
 
 // static
-int JSObject::GetHeaderSize(Map map) {
+int JSObject::GetHeaderSize(Tagged<Map> map) {
   // Check for the most common kind of JavaScript object before
   // falling into the generic switch. This speeds up the internal
   // field operations considerably on average.
@@ -284,7 +285,7 @@ int JSObject::GetHeaderSize(Map map) {
 }
 
 // static
-int JSObject::GetEmbedderFieldsStartOffset(Map map) {
+int JSObject::GetEmbedderFieldsStartOffset(Tagged<Map> map) {
   // Embedder fields are located after the object header.
   return GetHeaderSize(map);
 }
@@ -294,7 +295,7 @@ int JSObject::GetEmbedderFieldsStartOffset() {
 }
 
 // static
-bool JSObject::MayHaveEmbedderFields(Map map) {
+bool JSObject::MayHaveEmbedderFields(Tagged<Map> map) {
   InstanceType instance_type = map->instance_type();
   // TODO(v8) It'd be nice if all objects with embedder data slots inherited
   // from JSObjectWithEmbedderSlots, but this is currently not possible due to
@@ -308,7 +309,7 @@ bool JSObject::MayHaveEmbedderFields() const {
 }
 
 // static
-int JSObject::GetEmbedderFieldCount(Map map) {
+int JSObject::GetEmbedderFieldCount(Tagged<Map> map) {
   int instance_size = map->instance_size();
   if (instance_size == kVariableSizeSentinel) return 0;
   // Embedder fields are located after the object header, whereas in-object
@@ -332,16 +333,16 @@ int JSObject::GetEmbedderFieldOffset(int index) {
   return GetEmbedderFieldsStartOffset() + (kEmbedderDataSlotSize * index);
 }
 
-Object JSObject::GetEmbedderField(int index) {
-  return EmbedderDataSlot(*this, index).load_tagged();
+Tagged<Object> JSObject::GetEmbedderField(int index) {
+  return EmbedderDataSlot(Tagged(*this), index).load_tagged();
 }
 
-void JSObject::SetEmbedderField(int index, Object value) {
-  EmbedderDataSlot::store_tagged(*this, index, value);
+void JSObject::SetEmbedderField(int index, Tagged<Object> value) {
+  EmbedderDataSlot::store_tagged(Tagged(*this), index, value);
 }
 
-void JSObject::SetEmbedderField(int index, Smi value) {
-  EmbedderDataSlot(*this, index).store_smi(value);
+void JSObject::SetEmbedderField(int index, Tagged<Smi> value) {
+  EmbedderDataSlot(Tagged(*this), index).store_smi(value);
 }
 
 bool JSObject::IsDroppableApiObject() const {
@@ -353,13 +354,13 @@ bool JSObject::IsDroppableApiObject() const {
 // Access fast-case object properties at index. The use of these routines
 // is needed to correctly distinguish between properties stored in-object and
 // properties stored in the properties array.
-Object JSObject::RawFastPropertyAt(FieldIndex index) const {
+Tagged<Object> JSObject::RawFastPropertyAt(FieldIndex index) const {
   PtrComprCageBase cage_base = GetPtrComprCageBase(*this);
   return RawFastPropertyAt(cage_base, index);
 }
 
-Object JSObject::RawFastPropertyAt(PtrComprCageBase cage_base,
-                                   FieldIndex index) const {
+Tagged<Object> JSObject::RawFastPropertyAt(PtrComprCageBase cage_base,
+                                           FieldIndex index) const {
   if (index.is_inobject()) {
     return TaggedField<Object>::Relaxed_Load(cage_base, *this, index.offset());
   } else {
@@ -370,14 +371,15 @@ Object JSObject::RawFastPropertyAt(PtrComprCageBase cage_base,
 
 // The SeqCst versions of RawFastPropertyAt are used for atomically accessing
 // shared struct fields.
-Object JSObject::RawFastPropertyAt(FieldIndex index,
-                                   SeqCstAccessTag tag) const {
+Tagged<Object> JSObject::RawFastPropertyAt(FieldIndex index,
+                                           SeqCstAccessTag tag) const {
   PtrComprCageBase cage_base = GetPtrComprCageBase(*this);
   return RawFastPropertyAt(cage_base, index, tag);
 }
 
-Object JSObject::RawFastPropertyAt(PtrComprCageBase cage_base, FieldIndex index,
-                                   SeqCstAccessTag tag) const {
+Tagged<Object> JSObject::RawFastPropertyAt(PtrComprCageBase cage_base,
+                                           FieldIndex index,
+                                           SeqCstAccessTag tag) const {
   if (index.is_inobject()) {
     return TaggedField<Object>::SeqCst_Load(cage_base, *this, index.offset());
   } else {
@@ -386,8 +388,9 @@ Object JSObject::RawFastPropertyAt(PtrComprCageBase cage_base, FieldIndex index,
   }
 }
 
-base::Optional<Object> JSObject::RawInobjectPropertyAt(
-    PtrComprCageBase cage_base, Map original_map, FieldIndex index) const {
+base::Optional<Tagged<Object>> JSObject::RawInobjectPropertyAt(
+    PtrComprCageBase cage_base, Tagged<Map> original_map,
+    FieldIndex index) const {
   CHECK(index.is_inobject());
 
   // This method implements a "snapshot" protocol to protect against reading out
@@ -415,13 +418,14 @@ base::Optional<Object> JSObject::RawInobjectPropertyAt(
   // Only if the maps match can the property be inspected. It may have a "wrong"
   // value, but it will be within the bounds of the objects instance size as
   // given by the map and it will be a valid Smi or object pointer.
-  Object maybe_tagged_object =
+  Tagged<Object> maybe_tagged_object =
       TaggedField<Object>::Acquire_Load(cage_base, *this, index.offset());
   if (original_map != map(cage_base, kAcquireLoad)) return {};
   return maybe_tagged_object;
 }
 
-void JSObject::RawFastInobjectPropertyAtPut(FieldIndex index, Object value,
+void JSObject::RawFastInobjectPropertyAtPut(FieldIndex index,
+                                            Tagged<Object> value,
                                             WriteBarrierMode mode) {
   DCHECK(index.is_inobject());
   int offset = index.offset();
@@ -429,7 +433,8 @@ void JSObject::RawFastInobjectPropertyAtPut(FieldIndex index, Object value,
   CONDITIONAL_WRITE_BARRIER(*this, offset, value, mode);
 }
 
-void JSObject::RawFastInobjectPropertyAtPut(FieldIndex index, Object value,
+void JSObject::RawFastInobjectPropertyAtPut(FieldIndex index,
+                                            Tagged<Object> value,
                                             SeqCstAccessTag tag) {
   DCHECK(index.is_inobject());
   DCHECK(IsShared(value));
@@ -437,7 +442,7 @@ void JSObject::RawFastInobjectPropertyAtPut(FieldIndex index, Object value,
   CONDITIONAL_WRITE_BARRIER(*this, index.offset(), value, UPDATE_WRITE_BARRIER);
 }
 
-void JSObject::FastPropertyAtPut(FieldIndex index, Object value,
+void JSObject::FastPropertyAtPut(FieldIndex index, Tagged<Object> value,
                                  WriteBarrierMode mode) {
   if (index.is_inobject()) {
     RawFastInobjectPropertyAtPut(index, value, mode);
@@ -447,7 +452,7 @@ void JSObject::FastPropertyAtPut(FieldIndex index, Object value,
   }
 }
 
-void JSObject::FastPropertyAtPut(FieldIndex index, Object value,
+void JSObject::FastPropertyAtPut(FieldIndex index, Tagged<Object> value,
                                  SeqCstAccessTag tag) {
   if (index.is_inobject()) {
     RawFastInobjectPropertyAtPut(index, value, tag);
@@ -457,7 +462,7 @@ void JSObject::FastPropertyAtPut(FieldIndex index, Object value,
 }
 
 void JSObject::WriteToField(InternalIndex descriptor, PropertyDetails details,
-                            Object value) {
+                            Tagged<Object> value) {
   DCHECK_EQ(PropertyLocation::kField, details.location());
   DCHECK_EQ(PropertyKind::kData, details.kind());
   DisallowGarbageCollection no_gc;
@@ -484,31 +489,32 @@ void JSObject::WriteToField(InternalIndex descriptor, PropertyDetails details,
   }
 }
 
-Object JSObject::RawFastInobjectPropertyAtSwap(FieldIndex index, Object value,
-                                               SeqCstAccessTag tag) {
+Tagged<Object> JSObject::RawFastInobjectPropertyAtSwap(FieldIndex index,
+                                                       Tagged<Object> value,
+                                                       SeqCstAccessTag tag) {
   DCHECK(index.is_inobject());
   DCHECK(IsShared(value));
   int offset = index.offset();
-  Object old_value = SEQ_CST_SWAP_FIELD(*this, offset, value);
+  Tagged<Object> old_value = SEQ_CST_SWAP_FIELD(*this, offset, value);
   CONDITIONAL_WRITE_BARRIER(*this, offset, value, UPDATE_WRITE_BARRIER);
   return old_value;
 }
 
-Object JSObject::RawFastPropertyAtSwap(FieldIndex index, Object value,
-                                       SeqCstAccessTag tag) {
+Tagged<Object> JSObject::RawFastPropertyAtSwap(FieldIndex index,
+                                               Tagged<Object> value,
+                                               SeqCstAccessTag tag) {
   if (index.is_inobject()) {
     return RawFastInobjectPropertyAtSwap(index, value, tag);
   }
   return property_array()->Swap(index.outobject_array_index(), value, tag);
 }
 
-Object JSObject::RawFastInobjectPropertyAtCompareAndSwap(FieldIndex index,
-                                                         Object expected,
-                                                         Object value,
-                                                         SeqCstAccessTag tag) {
+Tagged<Object> JSObject::RawFastInobjectPropertyAtCompareAndSwap(
+    FieldIndex index, Tagged<Object> expected, Tagged<Object> value,
+    SeqCstAccessTag tag) {
   DCHECK(index.is_inobject());
   DCHECK(IsShared(value));
-  Object previous_value =
+  Tagged<Object> previous_value =
       SEQ_CST_COMPARE_AND_SWAP_FIELD(*this, index.offset(), expected, value);
   if (previous_value == expected) {
     CONDITIONAL_WRITE_BARRIER(*this, index.offset(), value,
@@ -517,10 +523,9 @@ Object JSObject::RawFastInobjectPropertyAtCompareAndSwap(FieldIndex index,
   return previous_value;
 }
 
-Object JSObject::RawFastPropertyAtCompareAndSwapInternal(FieldIndex index,
-                                                         Object expected,
-                                                         Object value,
-                                                         SeqCstAccessTag tag) {
+Tagged<Object> JSObject::RawFastPropertyAtCompareAndSwapInternal(
+    FieldIndex index, Tagged<Object> expected, Tagged<Object> value,
+    SeqCstAccessTag tag) {
   if (index.is_inobject()) {
     return RawFastInobjectPropertyAtCompareAndSwap(index, expected, value, tag);
   }
@@ -532,13 +537,13 @@ int JSObject::GetInObjectPropertyOffset(int index) {
   return map()->GetInObjectPropertyOffset(index);
 }
 
-Object JSObject::InObjectPropertyAt(int index) {
+Tagged<Object> JSObject::InObjectPropertyAt(int index) {
   int offset = GetInObjectPropertyOffset(index);
   return TaggedField<Object>::load(*this, offset);
 }
 
-Object JSObject::InObjectPropertyAtPut(int index, Object value,
-                                       WriteBarrierMode mode) {
+Tagged<Object> JSObject::InObjectPropertyAtPut(int index, Tagged<Object> value,
+                                               WriteBarrierMode mode) {
   // Adjust for the number of properties stored in the object.
   int offset = GetInObjectPropertyOffset(index);
   WRITE_FIELD(*this, offset, value);
@@ -546,9 +551,10 @@ Object JSObject::InObjectPropertyAtPut(int index, Object value,
   return value;
 }
 
-void JSObject::InitializeBody(Map map, int start_offset,
+void JSObject::InitializeBody(Tagged<Map> map, int start_offset,
                               bool is_slack_tracking_in_progress,
-                              MapWord filler_map, Object undefined_filler) {
+                              MapWord filler_map,
+                              Tagged<Object> undefined_filler) {
   int size = map->instance_size();
   int offset = start_offset;
 
@@ -568,7 +574,7 @@ void JSObject::InitializeBody(Map map, int start_offset,
     DCHECK_EQ(offset, embedder_field_start);
     for (int i = 0; i < embedder_field_count; i++) {
       // TODO(v8): consider initializing embedded data slots with Smi::zero().
-      EmbedderDataSlot(*this, i).Initialize(undefined_filler);
+      EmbedderDataSlot(Tagged<JSObject>(*this), i).Initialize(undefined_filler);
       offset += kEmbedderDataSlotSize;
     }
   } else {
@@ -588,7 +594,7 @@ void JSObject::InitializeBody(Map map, int start_offset,
     }
     // fill the remainder with one word filler objects (ie just a map word)
     while (offset < size) {
-      Object fm = Object(filler_map.ptr());
+      Tagged<Object> fm = Object(filler_map.ptr());
       WRITE_FIELD(*this, offset, fm);
       offset += kTaggedSize;
     }
@@ -814,7 +820,7 @@ void JSReceiver::initialize_properties(Isolate* isolate) {
 }
 
 DEF_GETTER(JSReceiver, HasFastProperties, bool) {
-  Object raw_properties_or_hash_obj =
+  Tagged<Object> raw_properties_or_hash_obj =
       raw_properties_or_hash(cage_base, kRelaxedLoad);
   DCHECK(IsSmi(raw_properties_or_hash_obj) ||
          ((IsGlobalDictionary(raw_properties_or_hash_obj, cage_base) ||
@@ -830,7 +836,7 @@ DEF_GETTER(JSReceiver, property_dictionary, Tagged<NameDictionary>) {
   DCHECK(!HasFastProperties(cage_base));
   DCHECK(!V8_ENABLE_SWISS_NAME_DICTIONARY_BOOL);
 
-  Object prop = raw_properties_or_hash(cage_base);
+  Tagged<Object> prop = raw_properties_or_hash(cage_base);
   if (IsSmi(prop)) {
     return GetReadOnlyRoots(cage_base).empty_property_dictionary();
   }
@@ -842,7 +848,7 @@ DEF_GETTER(JSReceiver, property_dictionary_swiss, Tagged<SwissNameDictionary>) {
   DCHECK(!HasFastProperties(cage_base));
   DCHECK(V8_ENABLE_SWISS_NAME_DICTIONARY_BOOL);
 
-  Object prop = raw_properties_or_hash(cage_base);
+  Tagged<Object> prop = raw_properties_or_hash(cage_base);
   if (IsSmi(prop)) {
     return GetReadOnlyRoots(cage_base).empty_swiss_property_dictionary();
   }
@@ -853,7 +859,7 @@ DEF_GETTER(JSReceiver, property_dictionary_swiss, Tagged<SwissNameDictionary>) {
 // the heap from this.
 DEF_GETTER(JSReceiver, property_array, Tagged<PropertyArray>) {
   DCHECK(HasFastProperties(cage_base));
-  Object prop = raw_properties_or_hash(cage_base);
+  Tagged<Object> prop = raw_properties_or_hash(cage_base);
   if (IsSmi(prop) || prop == GetReadOnlyRoots(cage_base).empty_fixed_array()) {
     return GetReadOnlyRoots(cage_base).empty_property_array();
   }
@@ -928,8 +934,8 @@ bool JSGlobalObject::IsDetached() {
   return global_proxy()->IsDetachedFrom(*this);
 }
 
-bool JSGlobalProxy::IsDetachedFrom(JSGlobalObject global) const {
-  const PrototypeIterator iter(this->GetIsolate(), *this);
+bool JSGlobalProxy::IsDetachedFrom(Tagged<JSGlobalObject> global) const {
+  const PrototypeIterator iter(this->GetIsolate(), Tagged<JSReceiver>(*this));
   return iter.GetCurrent() != global;
 }
 
@@ -952,7 +958,7 @@ static inline bool ShouldConvertToSlowElements(uint32_t used_elements,
   return size_threshold <= new_capacity;
 }
 
-static inline bool ShouldConvertToSlowElements(JSObject object,
+static inline bool ShouldConvertToSlowElements(Tagged<JSObject> object,
                                                uint32_t capacity,
                                                uint32_t index,
                                                uint32_t* new_capacity) {
diff --git a/src/objects/js-objects.cc b/src/objects/js-objects.cc
index e8b6841c643..6cdfb2d8814 100644
--- a/src/objects/js-objects.cc
+++ b/src/objects/js-objects.cc
@@ -494,7 +494,7 @@ Maybe<bool> JSReceiver::SetOrCopyDataProperties(
   return Just(true);
 }
 
-String JSReceiver::class_name() {
+Tagged<String> JSReceiver::class_name() {
   ReadOnlyRoots roots = GetReadOnlyRoots();
   if (IsFunction(*this)) return roots.Function_string();
   if (IsJSArgumentsObject(*this)) return roots.Arguments_string();
@@ -527,7 +527,7 @@ String JSReceiver::class_name() {
 #undef SWITCH_KIND
   }
   if (IsJSPrimitiveWrapper(*this)) {
-    Object value = JSPrimitiveWrapper::cast(*this)->value();
+    Tagged<Object> value = JSPrimitiveWrapper::cast(*this)->value();
     if (IsBoolean(value)) return roots.Boolean_string();
     if (IsString(value)) return roots.String_string();
     if (IsNumber(value)) return roots.Number_string();
@@ -636,10 +636,10 @@ Handle<String> JSReceiver::GetConstructorName(Isolate* isolate,
 
 base::Optional<NativeContext> JSReceiver::GetCreationContextRaw() {
   DisallowGarbageCollection no_gc;
-  JSFunction function;
+  Tagged<JSFunction> function;
   {
-    JSReceiver receiver = *this;
-    Map receiver_map = receiver->map();
+    Tagged<JSReceiver> receiver = *this;
+    Tagged<Map> receiver_map = receiver->map();
     InstanceType receiver_instance_type = receiver_map->instance_type();
     if (V8_LIKELY(InstanceTypeChecker::IsJSFunction(receiver_instance_type))) {
       function = JSFunction::cast(receiver);
@@ -649,7 +649,7 @@ base::Optional<NativeContext> JSReceiver::GetCreationContextRaw() {
     } else {
       // Externals are JSObjects with null as a constructor.
       DCHECK(!IsJSExternalObject(receiver));
-      Object constructor = receiver_map->GetConstructor();
+      Tagged<Object> constructor = receiver_map->GetConstructor();
       if (IsJSFunction(constructor)) {
         function = JSFunction::cast(constructor);
       } else {
@@ -678,11 +678,11 @@ MaybeHandle<NativeContext> JSReceiver::GetFunctionRealm(
   // long chains of bound functions or proxies where a recursive implementation
   // would run out of stack space.
   DisallowGarbageCollection no_gc;
-  JSReceiver current = *receiver;
+  Tagged<JSReceiver> current = *receiver;
   do {
     DCHECK(current->map()->is_constructor());
     if (IsJSProxy(current)) {
-      JSProxy proxy = JSProxy::cast(current);
+      Tagged<JSProxy> proxy = JSProxy::cast(current);
       if (proxy->IsRevoked()) {
         AllowGarbageCollection allow_allocating_errors;
         THROW_NEW_ERROR(isolate, NewTypeError(MessageTemplate::kProxyRevoked),
@@ -692,20 +692,20 @@ MaybeHandle<NativeContext> JSReceiver::GetFunctionRealm(
       continue;
     }
     if (IsJSFunction(current)) {
-      JSFunction function = JSFunction::cast(current);
+      Tagged<JSFunction> function = JSFunction::cast(current);
       return handle(function->native_context(), isolate);
     }
     if (IsJSBoundFunction(current)) {
-      JSBoundFunction function = JSBoundFunction::cast(current);
+      Tagged<JSBoundFunction> function = JSBoundFunction::cast(current);
       current = function->bound_target_function();
       continue;
     }
     if (IsJSWrappedFunction(current)) {
-      JSWrappedFunction function = JSWrappedFunction::cast(current);
+      Tagged<JSWrappedFunction> function = JSWrappedFunction::cast(current);
       current = function->wrapped_target_function();
       continue;
     }
-    JSObject object = JSObject::cast(current);
+    Tagged<JSObject> object = JSObject::cast(current);
     DCHECK(!IsJSFunction(object));
     return object->GetCreationContext();
   } while (true);
@@ -770,7 +770,8 @@ Maybe<PropertyAttributes> JSReceiver::GetPropertyAttributes(
 
 namespace {
 
-Object SetHashAndUpdateProperties(HeapObject properties, int hash) {
+Tagged<Object> SetHashAndUpdateProperties(Tagged<HeapObject> properties,
+                                          int hash) {
   DCHECK_NE(PropertyArray::kNoHashSentinel, hash);
   DCHECK(PropertyArray::HashField::is_valid(hash));
 
@@ -803,9 +804,9 @@ Object SetHashAndUpdateProperties(HeapObject properties, int hash) {
   return properties;
 }
 
-int GetIdentityHashHelper(JSReceiver object) {
+int GetIdentityHashHelper(Tagged<JSReceiver> object) {
   DisallowGarbageCollection no_gc;
-  Object properties = object->raw_properties_or_hash();
+  Tagged<Object> properties = object->raw_properties_or_hash();
   if (IsSmi(properties)) {
     return Smi::ToInt(properties);
   }
@@ -844,18 +845,20 @@ void JSReceiver::SetIdentityHash(int hash) {
   DCHECK_NE(PropertyArray::kNoHashSentinel, hash);
   DCHECK(PropertyArray::HashField::is_valid(hash));
 
-  HeapObject existing_properties = HeapObject::cast(raw_properties_or_hash());
-  Object new_properties = SetHashAndUpdateProperties(existing_properties, hash);
+  Tagged<HeapObject> existing_properties =
+      HeapObject::cast(raw_properties_or_hash());
+  Tagged<Object> new_properties =
+      SetHashAndUpdateProperties(existing_properties, hash);
   set_raw_properties_or_hash(new_properties, kRelaxedStore);
 }
 
-void JSReceiver::SetProperties(HeapObject properties) {
+void JSReceiver::SetProperties(Tagged<HeapObject> properties) {
   DCHECK_IMPLIES(IsPropertyArray(properties) &&
                      PropertyArray::cast(properties)->length() == 0,
                  properties == GetReadOnlyRoots().empty_property_array());
   DisallowGarbageCollection no_gc;
   int hash = GetIdentityHashHelper(*this);
-  Object new_properties = properties;
+  Tagged<Object> new_properties = properties;
 
   // TODO(cbruni): Make GetIdentityHashHelper return a bool so that we
   // don't have to manually compare against kNoHashSentinel.
@@ -866,7 +869,7 @@ void JSReceiver::SetProperties(HeapObject properties) {
   set_raw_properties_or_hash(new_properties, kRelaxedStore);
 }
 
-Object JSReceiver::GetIdentityHash() {
+Tagged<Object> JSReceiver::GetIdentityHash() {
   DisallowGarbageCollection no_gc;
 
   int hash = GetIdentityHashHelper(*this);
@@ -878,7 +881,8 @@ Object JSReceiver::GetIdentityHash() {
 }
 
 // static
-Smi JSReceiver::CreateIdentityHash(Isolate* isolate, JSReceiver key) {
+Tagged<Smi> JSReceiver::CreateIdentityHash(Isolate* isolate,
+                                           Tagged<JSReceiver> key) {
   DisallowGarbageCollection no_gc;
   int hash = isolate->GenerateIdentityHash(PropertyArray::HashField::kMax);
   DCHECK_NE(PropertyArray::kNoHashSentinel, hash);
@@ -887,7 +891,7 @@ Smi JSReceiver::CreateIdentityHash(Isolate* isolate, JSReceiver key) {
   return Smi::FromInt(hash);
 }
 
-Smi JSReceiver::GetOrCreateIdentityHash(Isolate* isolate) {
+Tagged<Smi> JSReceiver::GetOrCreateIdentityHash(Isolate* isolate) {
   DisallowGarbageCollection no_gc;
 
   int hash = GetIdentityHashHelper(*this);
@@ -1041,9 +1045,10 @@ Maybe<bool> JSReceiver::DeletePropertyOrElement(Handle<JSReceiver> object,
 
 // ES6 19.1.2.4
 // static
-Object JSReceiver::DefineProperty(Isolate* isolate, Handle<Object> object,
-                                  Handle<Object> key,
-                                  Handle<Object> attributes) {
+Tagged<Object> JSReceiver::DefineProperty(Isolate* isolate,
+                                          Handle<Object> object,
+                                          Handle<Object> key,
+                                          Handle<Object> attributes) {
   // 1. If Type(O) is not Object, throw a TypeError exception.
   if (!IsJSReceiver(*object)) {
     Handle<String> fun_name =
@@ -2364,15 +2369,15 @@ bool JSReceiver::HasProxyInPrototype(Isolate* isolate) {
 
 bool JSReceiver::IsCodeLike(Isolate* isolate) const {
   DisallowGarbageCollection no_gc;
-  Object maybe_constructor = map()->GetConstructor();
+  Tagged<Object> maybe_constructor = map()->GetConstructor();
   if (!IsJSFunction(maybe_constructor)) return false;
   if (!JSFunction::cast(maybe_constructor)->shared()->IsApiFunction()) {
     return false;
   }
-  Object instance_template = JSFunction::cast(maybe_constructor)
-                                 ->shared()
-                                 ->api_func_data()
-                                 ->GetInstanceTemplate();
+  Tagged<Object> instance_template = JSFunction::cast(maybe_constructor)
+                                         ->shared()
+                                         ->api_func_data()
+                                         ->GetInstanceTemplate();
   if (IsUndefined(instance_template, isolate)) return false;
   return ObjectTemplateInfo::cast(instance_template)->code_like();
 }
@@ -2436,7 +2441,7 @@ void JSObject::EnsureWritableFastElements(Handle<JSObject> object) {
   DCHECK(object->HasSmiOrObjectElements() ||
          object->HasFastStringWrapperElements() ||
          object->HasAnyNonextensibleElements());
-  FixedArray raw_elems = FixedArray::cast(object->elements());
+  Tagged<FixedArray> raw_elems = FixedArray::cast(object->elements());
   Isolate* isolate = object->GetIsolate();
   if (raw_elems->map() != ReadOnlyRoots(isolate).fixed_cow_array_map()) return;
   Handle<FixedArray> elems(raw_elems, isolate);
@@ -2879,7 +2884,7 @@ void JSObject::JSObjectShortPrint(StringStream* accumulator) {
       break;
     }
     case JS_BOUND_FUNCTION_TYPE: {
-      JSBoundFunction bound_function = JSBoundFunction::cast(*this);
+      Tagged<JSBoundFunction> bound_function = JSBoundFunction::cast(*this);
       accumulator->Add("<JSBoundFunction");
       accumulator->Add(" (BoundTargetFunction %p)>",
                        reinterpret_cast<void*>(
@@ -2896,7 +2901,7 @@ void JSObject::JSObjectShortPrint(StringStream* accumulator) {
     }
     case JS_REG_EXP_TYPE: {
       accumulator->Add("<JSRegExp");
-      JSRegExp regexp = JSRegExp::cast(*this);
+      Tagged<JSRegExp> regexp = JSRegExp::cast(*this);
       if (IsString(regexp->source())) {
         accumulator->Add(" ");
         String::cast(regexp->source())->StringShortPrint(accumulator);
@@ -2914,7 +2919,7 @@ void JSObject::JSObjectShortPrint(StringStream* accumulator) {
 #undef TYPED_ARRAY_CONSTRUCTORS_SWITCH
     case JS_CLASS_CONSTRUCTOR_TYPE:
     case JS_FUNCTION_TYPE: {
-      JSFunction function = JSFunction::cast(*this);
+      Tagged<JSFunction> function = JSFunction::cast(*this);
       std::unique_ptr<char[]> fun_name = function->shared()->DebugNameCStr();
       if (fun_name[0] != '\0') {
         accumulator->Add("<JSFunction ");
@@ -2923,9 +2928,10 @@ void JSObject::JSObjectShortPrint(StringStream* accumulator) {
         accumulator->Add("<JSFunction");
       }
       if (v8_flags.trace_file_names) {
-        Object source_name = Script::cast(function->shared()->script())->name();
+        Tagged<Object> source_name =
+            Script::cast(function->shared()->script())->name();
         if (IsString(source_name)) {
-          String str = String::cast(source_name);
+          Tagged<String> str = String::cast(source_name);
           if (str->length() > 0) {
             accumulator->Add(" <");
             accumulator->Put(str);
@@ -2964,9 +2970,9 @@ void JSObject::JSObjectShortPrint(StringStream* accumulator) {
       break;
 
     default: {
-      Map map_of_this = map();
+      Tagged<Map> map_of_this = map();
       Heap* heap = GetHeap();
-      Object constructor = map_of_this->GetConstructor();
+      Tagged<Object> constructor = map_of_this->GetConstructor();
       bool printed = false;
       if (IsHeapObject(constructor) &&
           !heap->Contains(HeapObject::cast(constructor))) {
@@ -2974,11 +2980,12 @@ void JSObject::JSObjectShortPrint(StringStream* accumulator) {
       } else {
         bool is_global_proxy = IsJSGlobalProxy(*this);
         if (IsJSFunction(constructor)) {
-          SharedFunctionInfo sfi = JSFunction::cast(constructor)->shared();
+          Tagged<SharedFunctionInfo> sfi =
+              JSFunction::cast(constructor)->shared();
           if (!sfi.InReadOnlySpace() && !heap->Contains(sfi)) {
             accumulator->Add("!!!INVALID SHARED ON CONSTRUCTOR!!!");
           } else {
-            String constructor_name = sfi->Name();
+            Tagged<String> constructor_name = sfi->Name();
             if (constructor_name->length() > 0) {
               accumulator->Add(is_global_proxy ? "<GlobalObject " : "<");
               accumulator->Put(constructor_name);
@@ -3034,16 +3041,16 @@ void JSObject::PrintElementsTransition(FILE* file, Handle<JSObject> object,
   }
 }
 
-void JSObject::PrintInstanceMigration(FILE* file, Map original_map,
-                                      Map new_map) {
+void JSObject::PrintInstanceMigration(FILE* file, Tagged<Map> original_map,
+                                      Tagged<Map> new_map) {
   if (new_map->is_dictionary_map()) {
     PrintF(file, "[migrating to slow]\n");
     return;
   }
   PrintF(file, "[migrating]");
   Isolate* isolate = GetIsolate();
-  DescriptorArray o = original_map->instance_descriptors(isolate);
-  DescriptorArray n = new_map->instance_descriptors(isolate);
+  Tagged<DescriptorArray> o = original_map->instance_descriptors(isolate);
+  Tagged<DescriptorArray> n = new_map->instance_descriptors(isolate);
   for (InternalIndex i : original_map->IterateOwnDescriptors()) {
     Representation o_r = o->GetDetails(i).representation();
     Representation n_r = n->GetDetails(i).representation();
@@ -3052,7 +3059,7 @@ void JSObject::PrintInstanceMigration(FILE* file, Map original_map,
       PrintF(file, ":%s->%s ", o_r.Mnemonic(), n_r.Mnemonic());
     } else if (o->GetDetails(i).location() == PropertyLocation::kDescriptor &&
                n->GetDetails(i).location() == PropertyLocation::kField) {
-      Name name = o->GetKey(i);
+      Tagged<Name> name = o->GetKey(i);
       if (IsString(name)) {
         String::cast(name)->PrintOn(file);
       } else {
@@ -3069,15 +3076,15 @@ void JSObject::PrintInstanceMigration(FILE* file, Map original_map,
 }
 
 bool JSObject::IsUnmodifiedApiObject(FullObjectSlot o) {
-  Object object = *o;
+  Tagged<Object> object = *o;
   if (IsSmi(object)) return false;
-  HeapObject heap_object = HeapObject::cast(object);
+  Tagged<HeapObject> heap_object = HeapObject::cast(object);
   if (!IsJSObject(object)) return false;
-  JSObject js_object = JSObject::cast(object);
+  Tagged<JSObject> js_object = JSObject::cast(object);
   if (!js_object->IsDroppableApiObject()) return false;
-  Object maybe_constructor = js_object->map()->GetConstructor();
+  Tagged<Object> maybe_constructor = js_object->map()->GetConstructor();
   if (!IsJSFunction(maybe_constructor)) return false;
-  JSFunction constructor = JSFunction::cast(maybe_constructor);
+  Tagged<JSFunction> constructor = JSFunction::cast(maybe_constructor);
   if (js_object->elements()->length() != 0) return false;
   // Check that the object is not a key in a WeakMap (over-approximation).
   if (!IsUndefined(js_object->GetIdentityHash())) return false;
@@ -3310,7 +3317,7 @@ void MigrateFastToFast(Isolate* isolate, Handle<JSObject> object,
   int limit = std::min(inobject, number_of_fields);
   for (int i = 0; i < limit; i++) {
     FieldIndex index = FieldIndex::ForPropertyIndex(*new_map, i);
-    Object value = inobject_props->get(isolate, i);
+    Tagged<Object> value = inobject_props->get(isolate, i);
     object->FastPropertyAtPut(index, value);
   }
 
@@ -3574,7 +3581,7 @@ void JSObject::AllocateStorageForMap(Handle<JSObject> object, Handle<Map> map) {
   object->SetProperties(*array);
   for (int i = 0; i < inobject; i++) {
     FieldIndex index = FieldIndex::ForPropertyIndex(*map, i);
-    Object value = storage->get(i);
+    Tagged<Object> value = storage->get(i);
     object->FastPropertyAtPut(index, value);
   }
   object->set_map(*map, kReleaseStore);
@@ -3862,7 +3869,7 @@ void JSObject::MigrateSlowToFast(Handle<JSObject> object,
     PropertyKind kind;
     if constexpr (V8_ENABLE_SWISS_NAME_DICTIONARY_BOOL) {
       InternalIndex index(swiss_dictionary->EntryForEnumerationIndex(i));
-      Object key = swiss_dictionary->KeyAt(index);
+      Tagged<Object> key = swiss_dictionary->KeyAt(index);
       if (!SwissNameDictionary::IsKey(roots, key)) {
         // Ignore deleted entries.
         continue;
@@ -3931,13 +3938,13 @@ void JSObject::MigrateSlowToFast(Handle<JSObject> object,
   int current_offset = 0;
   int descriptor_index = 0;
   for (int i = 0; i < iteration_length; i++) {
-    Name k;
-    Object value;
+    Tagged<Name> k;
+    Tagged<Object> value;
     PropertyDetails details = PropertyDetails::Empty();
 
     if constexpr (V8_ENABLE_SWISS_NAME_DICTIONARY_BOOL) {
       InternalIndex index(swiss_dictionary->EntryForEnumerationIndex(i));
-      Object key_obj = swiss_dictionary->KeyAt(index);
+      Tagged<Object> key_obj = swiss_dictionary->KeyAt(index);
       if (!SwissNameDictionary::IsKey(roots, key_obj)) {
         continue;
       }
@@ -4026,7 +4033,7 @@ void JSObject::MigrateSlowToFast(Handle<JSObject> object,
   DCHECK(object->HasFastProperties());
 }
 
-void JSObject::RequireSlowElements(NumberDictionary dictionary) {
+void JSObject::RequireSlowElements(Tagged<NumberDictionary> dictionary) {
   DCHECK_NE(dictionary,
             ReadOnlyRoots(GetIsolate()).empty_slow_element_dictionary());
   if (dictionary->requires_slow_elements()) return;
@@ -4044,7 +4051,7 @@ Handle<NumberDictionary> JSObject::NormalizeElements(Handle<JSObject> object) {
   bool is_sloppy_arguments = object->HasSloppyArgumentsElements();
   {
     DisallowGarbageCollection no_gc;
-    FixedArrayBase elements = object->elements();
+    Tagged<FixedArrayBase> elements = object->elements();
 
     if (is_sloppy_arguments) {
       elements = SloppyArgumentsElements::cast(elements)->arguments();
@@ -4158,7 +4165,7 @@ bool TestDictionaryPropertiesIntegrityLevel(Tagged<Dictionary> dict,
   DCHECK(level == SEALED || level == FROZEN);
 
   for (InternalIndex i : dict->IterateEntries()) {
-    Object key;
+    Tagged<Object> key;
     if (!dict->ToKey(roots, i, &key)) continue;
     if (Object::FilterKey(key, ALL_PROPERTIES)) continue;
     PropertyDetails details = dict->DetailsAt(i);
@@ -4171,21 +4178,13 @@ bool TestDictionaryPropertiesIntegrityLevel(Tagged<Dictionary> dict,
   return true;
 }
 
-template <typename Dictionary>
-bool TestDictionaryPropertiesIntegrityLevel(Dictionary dict,
-                                            ReadOnlyRoots roots,
-                                            PropertyAttributes level) {
-  static_assert(kTaggedCanConvertToRawObjects);
-  return TestDictionaryPropertiesIntegrityLevel(Tagged<Dictionary>(dict), roots,
-                                                level);
-}
-
-bool TestFastPropertiesIntegrityLevel(Map map, PropertyAttributes level) {
+bool TestFastPropertiesIntegrityLevel(Tagged<Map> map,
+                                      PropertyAttributes level) {
   DCHECK(level == SEALED || level == FROZEN);
   DCHECK(!IsCustomElementsReceiverMap(map));
   DCHECK(!map->is_dictionary_map());
 
-  DescriptorArray descriptors = map->instance_descriptors();
+  Tagged<DescriptorArray> descriptors = map->instance_descriptors();
   for (InternalIndex i : map->IterateOwnDescriptors()) {
     if (descriptors->GetKey(i)->IsPrivate()) continue;
     PropertyDetails details = descriptors->GetDetails(i);
@@ -4198,7 +4197,8 @@ bool TestFastPropertiesIntegrityLevel(Map map, PropertyAttributes level) {
   return true;
 }
 
-bool TestPropertiesIntegrityLevel(JSObject object, PropertyAttributes level) {
+bool TestPropertiesIntegrityLevel(Tagged<JSObject> object,
+                                  PropertyAttributes level) {
   DCHECK(!IsCustomElementsReceiverMap(object->map()));
 
   if (object->HasFastProperties()) {
@@ -4214,7 +4214,8 @@ bool TestPropertiesIntegrityLevel(JSObject object, PropertyAttributes level) {
   }
 }
 
-bool TestElementsIntegrityLevel(JSObject object, PropertyAttributes level) {
+bool TestElementsIntegrityLevel(Tagged<JSObject> object,
+                                PropertyAttributes level) {
   DCHECK(!object->HasSloppyArgumentsElements());
 
   ElementsKind kind = object->GetElementsKind();
@@ -4240,7 +4241,7 @@ bool TestElementsIntegrityLevel(JSObject object, PropertyAttributes level) {
   return accessor->NumberOfElements(object) == 0;
 }
 
-bool FastTestIntegrityLevel(JSObject object, PropertyAttributes level) {
+bool FastTestIntegrityLevel(Tagged<JSObject> object, PropertyAttributes level) {
   DCHECK(!IsCustomElementsReceiverMap(object->map()));
 
   return !object->map()->is_extensible() &&
@@ -4346,14 +4347,14 @@ void JSObject::ApplyAttributesToDictionary(
     Isolate* isolate, ReadOnlyRoots roots, Handle<Dictionary> dictionary,
     const PropertyAttributes attributes) {
   for (InternalIndex i : dictionary->IterateEntries()) {
-    Object k;
+    Tagged<Object> k;
     if (!dictionary->ToKey(roots, i, &k)) continue;
     if (Object::FilterKey(k, ALL_PROPERTIES)) continue;
     PropertyDetails details = dictionary->DetailsAt(i);
     int attrs = attributes;
     // READ_ONLY is an invalid attribute for JS setters/getters.
     if ((attributes & READ_ONLY) && details.kind() == PropertyKind::kAccessor) {
-      Object v = dictionary->ValueAt(i);
+      Tagged<Object> v = dictionary->ValueAt(i);
       if (IsAccessorPair(v)) attrs &= ~READ_ONLY;
     }
     details = details.CopyAddAttributes(PropertyAttributesFromInt(attrs));
@@ -4608,23 +4609,22 @@ Handle<Object> JSObject::DictionaryPropertyAt(Isolate* isolate,
                                               InternalIndex dict_index) {
   DCHECK_EQ(ThreadId::Current(), isolate->thread_id());
   if constexpr (V8_ENABLE_SWISS_NAME_DICTIONARY_BOOL) {
-    SwissNameDictionary dict = object->property_dictionary_swiss();
+    Tagged<SwissNameDictionary> dict = object->property_dictionary_swiss();
     return handle(dict->ValueAt(dict_index), isolate);
   } else {
-    NameDictionary dict = object->property_dictionary();
+    Tagged<NameDictionary> dict = object->property_dictionary();
     return handle(dict->ValueAt(dict_index), isolate);
   }
 }
 
 // static
-base::Optional<Object> JSObject::DictionaryPropertyAt(Handle<JSObject> object,
-                                                      InternalIndex dict_index,
-                                                      Heap* heap) {
-  Object backing_store = object->raw_properties_or_hash(kRelaxedLoad);
+base::Optional<Tagged<Object>> JSObject::DictionaryPropertyAt(
+    Handle<JSObject> object, InternalIndex dict_index, Heap* heap) {
+  Tagged<Object> backing_store = object->raw_properties_or_hash(kRelaxedLoad);
   if (!IsHeapObject(backing_store)) return {};
   if (heap->IsPendingAllocation(HeapObject::cast(backing_store))) return {};
 
-  base::Optional<Object> maybe_obj;
+  base::Optional<Tagged<Object>> maybe_obj;
   if constexpr (V8_ENABLE_SWISS_NAME_DICTIONARY_BOOL) {
     if (!IsSwissNameDictionary(backing_store)) return {};
     maybe_obj =
@@ -4641,7 +4641,7 @@ base::Optional<Object> JSObject::DictionaryPropertyAt(Handle<JSObject> object,
 // TODO(cbruni/jkummerow): Consider moving this into elements.cc.
 bool JSObject::HasEnumerableElements() {
   // TODO(cbruni): cleanup
-  JSObject object = *this;
+  Tagged<JSObject> object = *this;
   switch (object->GetElementsKind()) {
     case PACKED_SMI_ELEMENTS:
     case PACKED_ELEMENTS:
@@ -4660,7 +4660,7 @@ bool JSObject::HasEnumerableElements() {
     case HOLEY_SEALED_ELEMENTS:
     case HOLEY_NONEXTENSIBLE_ELEMENTS:
     case HOLEY_ELEMENTS: {
-      FixedArray elements = FixedArray::cast(object->elements());
+      Tagged<FixedArray> elements = FixedArray::cast(object->elements());
       int length = IsJSArray(object)
                        ? Smi::ToInt(JSArray::cast(object)->length())
                        : elements->length();
@@ -4677,7 +4677,8 @@ bool JSObject::HasEnumerableElements() {
       // Zero-length arrays would use the empty FixedArray...
       if (length == 0) return false;
       // ...so only cast to FixedDoubleArray otherwise.
-      FixedDoubleArray elements = FixedDoubleArray::cast(object->elements());
+      Tagged<FixedDoubleArray> elements =
+          FixedDoubleArray::cast(object->elements());
       for (int i = 0; i < length; i++) {
         if (!elements->is_the_hole(i)) return true;
       }
@@ -4697,7 +4698,8 @@ bool JSObject::HasEnumerableElements() {
         return length > 0;
       }
     case DICTIONARY_ELEMENTS: {
-      NumberDictionary elements = NumberDictionary::cast(object->elements());
+      Tagged<NumberDictionary> elements =
+          NumberDictionary::cast(object->elements());
       return elements->NumberOfEnumerableProperties() > 0;
     }
     case FAST_SLOPPY_ARGUMENTS_ELEMENTS:
@@ -4826,16 +4828,16 @@ Maybe<bool> JSObject::CheckIfCanDefineAsConfigurable(
   return Just(true);
 }
 
-Object JSObject::SlowReverseLookup(Object value) {
+Tagged<Object> JSObject::SlowReverseLookup(Tagged<Object> value) {
   if (HasFastProperties()) {
-    DescriptorArray descs = map()->instance_descriptors();
+    Tagged<DescriptorArray> descs = map()->instance_descriptors();
     bool value_is_number = IsNumber(value);
     for (InternalIndex i : map()->IterateOwnDescriptors()) {
       PropertyDetails details = descs->GetDetails(i);
       if (details.location() == PropertyLocation::kField) {
         DCHECK_EQ(PropertyKind::kData, details.kind());
         FieldIndex field_index = FieldIndex::ForDetails(map(), details);
-        Object property = RawFastPropertyAt(field_index);
+        Tagged<Object> property = RawFastPropertyAt(field_index);
         if (field_index.is_double()) {
           DCHECK(IsHeapNumber(property));
           if (value_is_number &&
@@ -4866,13 +4868,13 @@ Object JSObject::SlowReverseLookup(Object value) {
   }
 }
 
-void JSObject::PrototypeRegistryCompactionCallback(HeapObject value,
+void JSObject::PrototypeRegistryCompactionCallback(Tagged<HeapObject> value,
                                                    int old_index,
                                                    int new_index) {
   DCHECK(IsMap(value) && Map::cast(value)->is_prototype_map());
-  Map map = Map::cast(value);
+  Tagged<Map> map = Map::cast(value);
   DCHECK(IsPrototypeInfo(map->prototype_info()));
-  PrototypeInfo proto_info = PrototypeInfo::cast(map->prototype_info());
+  Tagged<PrototypeInfo> proto_info = PrototypeInfo::cast(map->prototype_info());
   DCHECK_EQ(old_index, proto_info->registry_slot());
   proto_info->set_registry_slot(new_index);
 }
@@ -4888,7 +4890,7 @@ void JSObject::MakePrototypesFast(Handle<Object> receiver,
     Handle<Object> current = PrototypeIterator::GetCurrent(iter);
     if (!IsJSObjectThatCanBeTrackedAsPrototype(*current)) return;
     Handle<JSObject> current_obj = Handle<JSObject>::cast(current);
-    Map current_map = current_obj->map();
+    Tagged<Map> current_map = current_obj->map();
     if (current_map->is_prototype_map()) {
       // If the map is already marked as should be fast, we're done. Its
       // prototypes will have been marked already as well.
@@ -4900,7 +4902,7 @@ void JSObject::MakePrototypesFast(Handle<Object> receiver,
   }
 }
 
-static bool PrototypeBenefitsFromNormalization(JSObject object) {
+static bool PrototypeBenefitsFromNormalization(Tagged<JSObject> object) {
   DisallowGarbageCollection no_gc;
   if (!object->HasFastProperties()) return false;
   if (IsJSGlobalProxy(object)) return false;
@@ -4957,8 +4959,8 @@ void JSObject::OptimizeAsPrototype(Handle<JSObject> object,
     // Replace the pointer to the exact constructor with the Object function
     // from the same context if undetectable from JS. This is to avoid keeping
     // memory alive unnecessarily.
-    Object maybe_constructor = new_map->GetConstructorRaw();
-    Tuple2 tuple;
+    Tagged<Object> maybe_constructor = new_map->GetConstructorRaw();
+    Tagged<Tuple2> tuple;
     if (IsTuple2(maybe_constructor)) {
       // Handle the {constructor, non-instance_prototype} tuple case if the map
       // has non-instance prototype.
@@ -4966,10 +4968,10 @@ void JSObject::OptimizeAsPrototype(Handle<JSObject> object,
       maybe_constructor = tuple->value1();
     }
     if (IsJSFunction(maybe_constructor)) {
-      JSFunction constructor = JSFunction::cast(maybe_constructor);
+      Tagged<JSFunction> constructor = JSFunction::cast(maybe_constructor);
       if (!constructor->shared()->IsApiFunction()) {
-        NativeContext context = constructor->native_context();
-        JSFunction object_function = context->object_function();
+        Tagged<NativeContext> context = constructor->native_context();
+        Tagged<JSFunction> object_function = context->object_function();
         if (!tuple.is_null()) {
           tuple->set_value1(object_function);
         } else {
@@ -4985,7 +4987,7 @@ void JSObject::OptimizeAsPrototype(Handle<JSObject> object,
 
       auto make_constant = [&](auto dict) {
         for (InternalIndex index : dict->IterateEntries()) {
-          Object k;
+          Tagged<Object> k;
           if (!dict->ToKey(roots, index, &k)) continue;
 
           PropertyDetails details = dict->DetailsAt(index);
@@ -5012,7 +5014,7 @@ void JSObject::OptimizeAsPrototype(Handle<JSObject> object,
 // static
 void JSObject::ReoptimizeIfPrototype(Handle<JSObject> object) {
   {
-    Map map = object->map();
+    Tagged<Map> map = object->map();
     if (!map->is_prototype_map()) return;
     if (!map->should_be_fast_prototype_map()) return;
   }
@@ -5083,7 +5085,7 @@ bool JSObject::UnregisterPrototypeUser(Handle<Map> user, Isolate* isolate) {
   // If it had no prototype before, see if it had users that might expect
   // registration.
   if (!IsJSObject(user->prototype())) {
-    Object users =
+    Tagged<Object> users =
         PrototypeInfo::cast(user->prototype_info())->prototype_users();
     return IsWeakArrayList(users);
   }
@@ -5093,7 +5095,7 @@ bool JSObject::UnregisterPrototypeUser(Handle<Map> user, Isolate* isolate) {
   int slot = user_info->registry_slot();
   if (slot == PrototypeInfo::UNREGISTERED) return false;
   DCHECK(prototype->map()->is_prototype_map());
-  Object maybe_proto_info = prototype->map()->prototype_info();
+  Tagged<Object> maybe_proto_info = prototype->map()->prototype_info();
   // User knows its registry slot, prototype info and user registry must exist.
   DCHECK(IsPrototypeInfo(maybe_proto_info));
   Handle<PrototypeInfo> proto_info(PrototypeInfo::cast(maybe_proto_info),
@@ -5115,22 +5117,22 @@ namespace {
 // This function must be kept in sync with
 // AccessorAssembler::InvalidateValidityCellIfPrototype() which does pre-checks
 // before jumping here.
-void InvalidateOnePrototypeValidityCellInternal(Map map) {
+void InvalidateOnePrototypeValidityCellInternal(Tagged<Map> map) {
   DCHECK(map->is_prototype_map());
   if (v8_flags.trace_prototype_users) {
     PrintF("Invalidating prototype map %p 's cell\n",
            reinterpret_cast<void*>(map.ptr()));
   }
-  Object maybe_cell = map->prototype_validity_cell(kRelaxedLoad);
+  Tagged<Object> maybe_cell = map->prototype_validity_cell(kRelaxedLoad);
   if (IsCell(maybe_cell)) {
     // Just set the value; the cell will be replaced lazily.
-    Cell cell = Cell::cast(maybe_cell);
-    Smi invalid_value = Smi::FromInt(Map::kPrototypeChainInvalid);
+    Tagged<Cell> cell = Cell::cast(maybe_cell);
+    Tagged<Smi> invalid_value = Smi::FromInt(Map::kPrototypeChainInvalid);
     if (cell->value() != invalid_value) {
       cell->set_value(invalid_value);
     }
   }
-  PrototypeInfo prototype_info;
+  Tagged<PrototypeInfo> prototype_info;
   if (map->TryGetPrototypeInfo(&prototype_info)) {
     prototype_info->set_prototype_chain_enum_cache(Object());
   }
@@ -5154,26 +5156,26 @@ void InvalidateOnePrototypeValidityCellInternal(Map map) {
   }
 }
 
-void InvalidatePrototypeChainsInternal(Map map) {
+void InvalidatePrototypeChainsInternal(Tagged<Map> map) {
   // We handle linear prototype chains by looping, and multiple children
   // by recursion, in order to reduce the likelihood of running into stack
   // overflows. So, conceptually, the outer loop iterates the depth of the
   // prototype tree, and the inner loop iterates the breadth of a node.
-  Map next_map;
+  Tagged<Map> next_map;
   for (; !map.is_null(); map = next_map, next_map = Map()) {
     InvalidateOnePrototypeValidityCellInternal(map);
 
-    PrototypeInfo proto_info;
+    Tagged<PrototypeInfo> proto_info;
     if (!map->TryGetPrototypeInfo(&proto_info)) return;
     if (!IsWeakArrayList(proto_info->prototype_users())) {
       return;
     }
-    WeakArrayList prototype_users =
+    Tagged<WeakArrayList> prototype_users =
         WeakArrayList::cast(proto_info->prototype_users());
     // For now, only maps register themselves as users.
     for (int i = PrototypeUsers::kFirstIndex; i < prototype_users->length();
          ++i) {
-      HeapObject heap_object;
+      Tagged<HeapObject> heap_object;
       if (prototype_users->Get(i)->GetHeapObjectIfWeak(&heap_object) &&
           IsMap(heap_object)) {
         // Walk the prototype chain (backwards, towards leaf objects) if
@@ -5191,7 +5193,7 @@ void InvalidatePrototypeChainsInternal(Map map) {
 }  // namespace
 
 // static
-Map JSObject::InvalidatePrototypeChains(Map map) {
+Tagged<Map> JSObject::InvalidatePrototypeChains(Tagged<Map> map) {
   DisallowGarbageCollection no_gc;
   InvalidatePrototypeChainsInternal(map);
   return map;
@@ -5205,7 +5207,7 @@ Map JSObject::InvalidatePrototypeChains(Map map) {
 // in the prototype chain are not affected by appearance of a new lexical
 // variable and therefore we don't propagate invalidation down.
 // static
-void JSObject::InvalidatePrototypeValidityCell(JSGlobalObject global) {
+void JSObject::InvalidatePrototypeValidityCell(Tagged<JSGlobalObject> global) {
   DisallowGarbageCollection no_gc;
   InvalidateOnePrototypeValidityCellInternal(global->map());
 }
@@ -5329,10 +5331,10 @@ void JSObject::EnsureCanContainElements(Handle<JSObject> object,
       object, FullObjectSlot(args->address_of_arg_at(0)), arg_count, mode);
 }
 
-void JSObject::ValidateElements(JSObject object) {
+void JSObject::ValidateElements(Tagged<JSObject> object) {
 #ifdef ENABLE_SLOW_DCHECKS
   if (v8_flags.enable_slow_asserts) {
-    object.GetElementsAccessor()->Validate(object);
+    object->GetElementsAccessor()->Validate(object);
   }
 #endif
 }
@@ -5344,8 +5346,8 @@ bool JSObject::WouldConvertToSlowElements(uint32_t index) {
   return ShouldConvertToSlowElements(*this, capacity, index, &new_capacity);
 }
 
-static bool ShouldConvertToFastElements(JSObject object,
-                                        NumberDictionary dictionary,
+static bool ShouldConvertToFastElements(Tagged<JSObject> object,
+                                        Tagged<NumberDictionary> dictionary,
                                         uint32_t index,
                                         uint32_t* new_capacity) {
   // If properties with non-standard attributes or accessors were added, we
@@ -5356,7 +5358,7 @@ static bool ShouldConvertToFastElements(JSObject object,
   if (index >= static_cast<uint32_t>(Smi::kMaxValue)) return false;
 
   if (IsJSArray(object)) {
-    Object length = JSArray::cast(object)->length();
+    Tagged<Object> length = JSArray::cast(object)->length();
     if (!IsSmi(length)) return false;
     *new_capacity = static_cast<uint32_t>(Smi::ToInt(length));
   } else if (IsJSArgumentsObject(object)) {
@@ -5373,7 +5375,7 @@ static bool ShouldConvertToFastElements(JSObject object,
   return 2 * dictionary_size >= *new_capacity;
 }
 
-static ElementsKind BestFittingFastElementsKind(JSObject object) {
+static ElementsKind BestFittingFastElementsKind(Tagged<JSObject> object) {
   if (!object->map()->CanHaveFastTransitionableElementsKind()) {
     return HOLEY_ELEMENTS;
   }
@@ -5384,12 +5386,12 @@ static ElementsKind BestFittingFastElementsKind(JSObject object) {
     return FAST_STRING_WRAPPER_ELEMENTS;
   }
   DCHECK(object->HasDictionaryElements());
-  NumberDictionary dictionary = object->element_dictionary();
+  Tagged<NumberDictionary> dictionary = object->element_dictionary();
   ElementsKind kind = HOLEY_SMI_ELEMENTS;
   for (InternalIndex i : dictionary->IterateEntries()) {
-    Object key = dictionary->KeyAt(i);
+    Tagged<Object> key = dictionary->KeyAt(i);
     if (IsNumber(key)) {
-      Object value = dictionary->ValueAt(i);
+      Tagged<Object> value = dictionary->ValueAt(i);
       if (!IsNumber(value)) return HOLEY_ELEMENTS;
       if (!IsSmi(value)) {
         if (!v8_flags.unbox_double_arrays) return HOLEY_ELEMENTS;
@@ -5416,7 +5418,7 @@ Maybe<bool> JSObject::AddDataElement(Handle<JSObject> object, uint32_t index,
   }
 
   ElementsKind kind = object->GetElementsKind(isolate);
-  FixedArrayBase elements = object->elements(isolate);
+  Tagged<FixedArrayBase> elements = object->elements(isolate);
   ElementsKind dictionary_kind = DICTIONARY_ELEMENTS;
   if (IsSloppyArgumentsElementsKind(kind)) {
     elements = SloppyArgumentsElements::cast(elements)->arguments(isolate);
@@ -5472,7 +5474,7 @@ bool JSObject::UpdateAllocationSite(Handle<JSObject> object,
 
     Heap* heap = object->GetHeap();
     PretenuringHandler* pretunring_handler = heap->pretenuring_handler();
-    AllocationMemento memento =
+    Tagged<AllocationMemento> memento =
         pretunring_handler
             ->FindAllocationMemento<PretenuringHandler::kForRuntime>(
                 object->map(), *object);
@@ -5539,19 +5541,20 @@ void JSObject::TransitionElementsKind(Handle<JSObject> object,
 }
 
 template <typename BackingStore>
-static int HoleyElementsUsage(JSObject object, BackingStore store) {
+static int HoleyElementsUsage(Tagged<JSObject> object,
+                              Tagged<BackingStore> store) {
   Isolate* isolate = object->GetIsolate();
   int limit = IsJSArray(object) ? Smi::ToInt(JSArray::cast(object)->length())
-                                : store.length();
+                                : store->length();
   int used = 0;
   for (int i = 0; i < limit; ++i) {
-    if (!store.is_the_hole(isolate, i)) ++used;
+    if (!store->is_the_hole(isolate, i)) ++used;
   }
   return used;
 }
 
 int JSObject::GetFastElementsUsage() {
-  FixedArrayBase store = elements();
+  Tagged<FixedArrayBase> store = elements();
   switch (GetElementsKind()) {
     case PACKED_SMI_ELEMENTS:
     case PACKED_DOUBLE_ELEMENTS:
@@ -5623,11 +5626,12 @@ Maybe<bool> JSObject::HasRealNamedCallbackProperty(Isolate* isolate,
                                : Nothing<bool>();
 }
 
-Object JSObject::RawFastPropertyAtCompareAndSwap(FieldIndex index,
-                                                 Object expected, Object value,
-                                                 SeqCstAccessTag tag) {
+Tagged<Object> JSObject::RawFastPropertyAtCompareAndSwap(
+    FieldIndex index, Tagged<Object> expected, Tagged<Object> value,
+    SeqCstAccessTag tag) {
   return HeapObject::SeqCst_CompareAndSwapField(
-      expected, value, [=](Object expected_value, Object new_value) {
+      expected, value,
+      [=](Tagged<Object> expected_value, Tagged<Object> new_value) {
         return RawFastPropertyAtCompareAndSwapInternal(index, expected_value,
                                                        new_value, tag);
       });
@@ -5695,20 +5699,20 @@ Address JSDate::GetField(Isolate* isolate, Address raw_object,
   DisallowHandleAllocation no_handles;
   DisallowJavascriptExecution no_js(isolate);
 
-  Object object(raw_object);
-  Smi index(smi_index);
+  Tagged<Object> object(raw_object);
+  Tagged<Smi> index(smi_index);
   return JSDate::cast(object)
       ->DoGetField(isolate, static_cast<FieldIndex>(index.value()))
       .ptr();
 }
 
-Object JSDate::DoGetField(Isolate* isolate, FieldIndex index) {
+Tagged<Object> JSDate::DoGetField(Isolate* isolate, FieldIndex index) {
   DCHECK_NE(index, kDateValue);
 
   DateCache* date_cache = isolate->date_cache();
 
   if (index < kFirstUncachedField) {
-    Object stamp = cache_stamp();
+    Tagged<Object> stamp = cache_stamp();
     if (stamp != date_cache->stamp() && IsSmi(stamp)) {
       // Since the stamp is not NaN, the value is also not NaN.
       int64_t local_time_ms =
@@ -5753,8 +5757,8 @@ Object JSDate::DoGetField(Isolate* isolate, FieldIndex index) {
   return Smi::FromInt(time_in_day_ms);
 }
 
-Object JSDate::GetUTCField(FieldIndex index, double value,
-                           DateCache* date_cache) {
+Tagged<Object> JSDate::GetUTCField(FieldIndex index, double value,
+                                   DateCache* date_cache) {
   DCHECK_GE(index, kFirstUTCField);
 
   if (std::isnan(value)) return GetReadOnlyRoots().nan_value();
@@ -5808,10 +5812,10 @@ Handle<Object> JSDate::SetValue(Handle<JSDate> date, double v) {
   return value;
 }
 
-void JSDate::SetValue(Object value, bool is_value_nan) {
+void JSDate::SetValue(Tagged<Object> value, bool is_value_nan) {
   set_value(value);
   if (is_value_nan) {
-    HeapNumber nan = GetReadOnlyRoots().nan_value();
+    Tagged<HeapNumber> nan = GetReadOnlyRoots().nan_value();
     set_cache_stamp(nan, SKIP_WRITE_BARRIER);
     set_year(nan, SKIP_WRITE_BARRIER);
     set_month(nan, SKIP_WRITE_BARRIER);
@@ -5898,11 +5902,11 @@ int JSMessageObject::GetColumnNumber() const {
   return info.column;  // Note: No '+1' in contrast to GetLineNumber.
 }
 
-String JSMessageObject::GetSource() const {
+Tagged<String> JSMessageObject::GetSource() const {
   DisallowGarbageCollection no_gc;
-  Script script_object = script();
+  Tagged<Script> script_object = script();
   if (script_object->HasValidSource()) {
-    Object source = script_object->source();
+    Tagged<Object> source = script_object->source();
     if (IsString(source)) return String::cast(source);
   }
   return ReadOnlyRoots(GetIsolate()).empty_string();
diff --git a/src/objects/js-objects.h b/src/objects/js-objects.h
index b88569ce218..82d367affe3 100644
--- a/src/objects/js-objects.h
+++ b/src/objects/js-objects.h
@@ -59,7 +59,7 @@ class JSReceiver : public TorqueGeneratedJSReceiver<JSReceiver, HeapObject> {
   // Sets the properties backing store and makes sure any existing hash is moved
   // to the new properties store. To clear out the properties store, pass in the
   // empty_fixed_array(), the hash will be maintained in this case as well.
-  void SetProperties(HeapObject properties);
+  void SetProperties(Tagged<HeapObject> properties);
 
   // There are five possible values for the properties offset.
   // 1) EmptyFixedArray/EmptyPropertyDictionary - This is the standard
@@ -152,10 +152,9 @@ class JSReceiver : public TorqueGeneratedJSReceiver<JSReceiver, HeapObject> {
       Handle<JSReceiver> object, uint32_t index,
       LanguageMode language_mode = LanguageMode::kSloppy);
 
-  V8_WARN_UNUSED_RESULT static Object DefineProperty(Isolate* isolate,
-                                                     Handle<Object> object,
-                                                     Handle<Object> name,
-                                                     Handle<Object> attributes);
+  V8_WARN_UNUSED_RESULT static Tagged<Object> DefineProperty(
+      Isolate* isolate, Handle<Object> object, Handle<Object> name,
+      Handle<Object> attributes);
   V8_WARN_UNUSED_RESULT static MaybeHandle<Object> DefineProperties(
       Isolate* isolate, Handle<Object> object, Handle<Object> properties);
 
@@ -235,7 +234,7 @@ class JSReceiver : public TorqueGeneratedJSReceiver<JSReceiver, HeapObject> {
       Isolate* isolate, Handle<JSReceiver> object);
 
   // Returns the class name.
-  V8_EXPORT_PRIVATE String class_name();
+  V8_EXPORT_PRIVATE Tagged<String> class_name();
 
   // Returns the constructor (the function that was used to instantiate the
   // object).
@@ -281,12 +280,13 @@ class JSReceiver : public TorqueGeneratedJSReceiver<JSReceiver, HeapObject> {
 
   // Retrieves a permanent object identity hash code. The undefined value might
   // be returned in case no hash was created yet.
-  V8_EXPORT_PRIVATE Object GetIdentityHash();
+  V8_EXPORT_PRIVATE Tagged<Object> GetIdentityHash();
 
   // Retrieves a permanent object identity hash code. May create and store a
   // hash code if needed and none exists.
-  static Smi CreateIdentityHash(Isolate* isolate, JSReceiver key);
-  V8_EXPORT_PRIVATE Smi GetOrCreateIdentityHash(Isolate* isolate);
+  static Tagged<Smi> CreateIdentityHash(Isolate* isolate,
+                                        Tagged<JSReceiver> key);
+  V8_EXPORT_PRIVATE Tagged<Smi> GetOrCreateIdentityHash(Isolate* isolate);
 
   // Stores the hash code. The hash passed in must be masked with
   // JSReceiver::kHashMask.
@@ -348,9 +348,9 @@ class JSObject : public TorqueGeneratedJSObject<JSObject, JSReceiver> {
   // acquire/release semantics ever become necessary, the default setter should
   // be reverted to non-atomic behavior, and setters with explicit tags
   // introduced and used when required.
-  FixedArrayBase elements(PtrComprCageBase cage_base,
-                          AcquireLoadTag tag) const = delete;
-  void set_elements(FixedArrayBase value, ReleaseStoreTag tag,
+  Tagged<FixedArrayBase> elements(PtrComprCageBase cage_base,
+                                  AcquireLoadTag tag) const = delete;
+  void set_elements(Tagged<FixedArrayBase> value, ReleaseStoreTag tag,
                     WriteBarrierMode mode = UPDATE_WRITE_BARRIER) = delete;
 
   inline void initialize_elements();
@@ -508,8 +508,8 @@ class JSObject : public TorqueGeneratedJSObject<JSObject, JSReceiver> {
                                               Handle<Map> new_map,
                                               Isolate* isolate);
   static bool UnregisterPrototypeUser(Handle<Map> user, Isolate* isolate);
-  static Map InvalidatePrototypeChains(Map map);
-  static void InvalidatePrototypeValidityCell(JSGlobalObject global);
+  static Tagged<Map> InvalidatePrototypeChains(Tagged<Map> map);
+  static void InvalidatePrototypeValidityCell(Tagged<JSGlobalObject> global);
 
   // Updates prototype chain tracking information when an object changes its
   // map from |old_map| to |new_map|.
@@ -517,10 +517,11 @@ class JSObject : public TorqueGeneratedJSObject<JSObject, JSReceiver> {
                               Isolate* isolate);
 
   // Utility used by many Array builtins and runtime functions
-  static inline bool PrototypeHasNoElements(Isolate* isolate, JSObject object);
+  static inline bool PrototypeHasNoElements(Isolate* isolate,
+                                            Tagged<JSObject> object);
 
   // To be passed to PrototypeUsers::Compact.
-  static void PrototypeRegistryCompactionCallback(HeapObject value,
+  static void PrototypeRegistryCompactionCallback(Tagged<HeapObject> value,
                                                   int old_index, int new_index);
 
   // Retrieve interceptors.
@@ -560,7 +561,7 @@ class JSObject : public TorqueGeneratedJSObject<JSObject, JSReceiver> {
   V8_WARN_UNUSED_RESULT static MaybeHandle<Object> GetPropertyWithInterceptor(
       LookupIterator* it, bool* done);
 
-  static void ValidateElements(JSObject object);
+  static void ValidateElements(Tagged<JSObject> object);
 
   // Makes sure that this object can contain HeapObject as elements.
   static inline void EnsureCanContainHeapObjectElements(Handle<JSObject> obj);
@@ -617,20 +618,20 @@ class JSObject : public TorqueGeneratedJSObject<JSObject, JSReceiver> {
   // JSFunction objects.
   static V8_EXPORT_PRIVATE int GetHeaderSize(
       InstanceType instance_type, bool function_has_prototype_slot = false);
-  static inline int GetHeaderSize(Map map);
+  static inline int GetHeaderSize(Tagged<Map> map);
 
-  static inline bool MayHaveEmbedderFields(Map map);
+  static inline bool MayHaveEmbedderFields(Tagged<Map> map);
   inline bool MayHaveEmbedderFields() const;
 
-  static inline int GetEmbedderFieldsStartOffset(Map map);
+  static inline int GetEmbedderFieldsStartOffset(Tagged<Map> map);
   inline int GetEmbedderFieldsStartOffset();
 
-  static inline int GetEmbedderFieldCount(Map map);
+  static inline int GetEmbedderFieldCount(Tagged<Map> map);
   inline int GetEmbedderFieldCount() const;
   inline int GetEmbedderFieldOffset(int index);
-  inline Object GetEmbedderField(int index);
-  inline void SetEmbedderField(int index, Object value);
-  inline void SetEmbedderField(int index, Smi value);
+  inline Tagged<Object> GetEmbedderField(int index);
+  inline void SetEmbedderField(int index, Tagged<Object> value);
+  inline void SetEmbedderField(int index, Tagged<Smi> value);
 
   // Returns true if this object is an Api object which can, if unmodified, be
   // dropped during minor GC because the embedder can recreate it again later.
@@ -676,7 +677,7 @@ class JSObject : public TorqueGeneratedJSObject<JSObject, JSReceiver> {
   V8_EXPORT_PRIVATE static Handle<NumberDictionary> NormalizeElements(
       Handle<JSObject> object);
 
-  void RequireSlowElements(NumberDictionary dictionary);
+  void RequireSlowElements(Tagged<NumberDictionary> dictionary);
 
   // Transform slow named properties to fast variants.
   V8_EXPORT_PRIVATE static void MigrateSlowToFast(Handle<JSObject> object,
@@ -690,9 +691,8 @@ class JSObject : public TorqueGeneratedJSObject<JSObject, JSReceiver> {
   // Same as above, but it will return {} if we would be reading out of the
   // bounds of the object or if the dictionary is pending allocation. Use this
   // version for concurrent access.
-  static base::Optional<Object> DictionaryPropertyAt(Handle<JSObject> object,
-                                                     InternalIndex dict_index,
-                                                     Heap* heap);
+  static base::Optional<Tagged<Object>> DictionaryPropertyAt(
+      Handle<JSObject> object, InternalIndex dict_index, Heap* heap);
 
   // Access fast-case object properties at index.
   static Handle<Object> FastPropertyAt(Isolate* isolate,
@@ -703,48 +703,56 @@ class JSObject : public TorqueGeneratedJSObject<JSObject, JSReceiver> {
                                        Handle<JSObject> object,
                                        Representation representation,
                                        FieldIndex index, SeqCstAccessTag tag);
-  inline Object RawFastPropertyAt(FieldIndex index) const;
-  inline Object RawFastPropertyAt(PtrComprCageBase cage_base,
-                                  FieldIndex index) const;
-  inline Object RawFastPropertyAt(FieldIndex index, SeqCstAccessTag tag) const;
-  inline Object RawFastPropertyAt(PtrComprCageBase cage_base, FieldIndex index,
-                                  SeqCstAccessTag tag) const;
+  inline Tagged<Object> RawFastPropertyAt(FieldIndex index) const;
+  inline Tagged<Object> RawFastPropertyAt(PtrComprCageBase cage_base,
+                                          FieldIndex index) const;
+  inline Tagged<Object> RawFastPropertyAt(FieldIndex index,
+                                          SeqCstAccessTag tag) const;
+  inline Tagged<Object> RawFastPropertyAt(PtrComprCageBase cage_base,
+                                          FieldIndex index,
+                                          SeqCstAccessTag tag) const;
 
   // See comment in the body of the method to understand the conditions
   // in which this method is meant to be used, and what guarantees it
   // provides against invalid reads from another thread during object
   // mutation.
-  inline base::Optional<Object> RawInobjectPropertyAt(
-      PtrComprCageBase cage_base, Map original_map, FieldIndex index) const;
+  inline base::Optional<Tagged<Object>> RawInobjectPropertyAt(
+      PtrComprCageBase cage_base, Tagged<Map> original_map,
+      FieldIndex index) const;
 
-  inline void FastPropertyAtPut(FieldIndex index, Object value,
+  inline void FastPropertyAtPut(FieldIndex index, Tagged<Object> value,
                                 WriteBarrierMode mode = UPDATE_WRITE_BARRIER);
-  inline void FastPropertyAtPut(FieldIndex index, Object value,
+  inline void FastPropertyAtPut(FieldIndex index, Tagged<Object> value,
                                 SeqCstAccessTag tag);
   inline void RawFastInobjectPropertyAtPut(
-      FieldIndex index, Object value,
+      FieldIndex index, Tagged<Object> value,
       WriteBarrierMode mode = UPDATE_WRITE_BARRIER);
-  inline void RawFastInobjectPropertyAtPut(FieldIndex index, Object value,
+  inline void RawFastInobjectPropertyAtPut(FieldIndex index,
+                                           Tagged<Object> value,
                                            SeqCstAccessTag tag);
   inline void WriteToField(InternalIndex descriptor, PropertyDetails details,
-                           Object value);
+                           Tagged<Object> value);
 
-  inline Object RawFastInobjectPropertyAtSwap(FieldIndex index, Object value,
+  inline Tagged<Object> RawFastInobjectPropertyAtSwap(FieldIndex index,
+                                                      Tagged<Object> value,
+                                                      SeqCstAccessTag tag);
+  inline Tagged<Object> RawFastPropertyAtSwap(FieldIndex index,
+                                              Tagged<Object> value,
                                               SeqCstAccessTag tag);
-  inline Object RawFastPropertyAtSwap(FieldIndex index, Object value,
-                                      SeqCstAccessTag tag);
-  Object RawFastPropertyAtCompareAndSwap(FieldIndex index, Object expected,
-                                         Object value, SeqCstAccessTag tag);
-  inline Object RawFastInobjectPropertyAtCompareAndSwap(FieldIndex index,
-                                                        Object expected,
-                                                        Object value,
-                                                        SeqCstAccessTag tag);
+  Tagged<Object> RawFastPropertyAtCompareAndSwap(FieldIndex index,
+                                                 Tagged<Object> expected,
+                                                 Tagged<Object> value,
+                                                 SeqCstAccessTag tag);
+  inline Tagged<Object> RawFastInobjectPropertyAtCompareAndSwap(
+      FieldIndex index, Tagged<Object> expected, Tagged<Object> value,
+      SeqCstAccessTag tag);
 
   // Access to in object properties.
   inline int GetInObjectPropertyOffset(int index);
-  inline Object InObjectPropertyAt(int index);
-  inline Object InObjectPropertyAtPut(
-      int index, Object value, WriteBarrierMode mode = UPDATE_WRITE_BARRIER);
+  inline Tagged<Object> InObjectPropertyAt(int index);
+  inline Tagged<Object> InObjectPropertyAtPut(
+      int index, Tagged<Object> value,
+      WriteBarrierMode mode = UPDATE_WRITE_BARRIER);
 
   // Set the object's prototype (only JSReceiver and null are allowed values).
   V8_WARN_UNUSED_RESULT static Maybe<bool> SetPrototype(
@@ -760,12 +768,13 @@ class JSObject : public TorqueGeneratedJSObject<JSObject, JSReceiver> {
   // undefined_value and the rest with filler_map.
   // Note: this call does not update write barrier, the caller is responsible
   // to ensure that |filler_map| can be collected without WB here.
-  inline void InitializeBody(Map map, int start_offset,
+  inline void InitializeBody(Tagged<Map> map, int start_offset,
                              bool is_slack_tracking_in_progress,
-                             MapWord filler_map, Object undefined_value);
+                             MapWord filler_map,
+                             Tagged<Object> undefined_value);
 
   // Check whether this object references another object
-  bool ReferencesObject(Object obj);
+  bool ReferencesObject(Tagged<Object> obj);
 
   V8_WARN_UNUSED_RESULT static Maybe<bool> TestIntegrityLevel(
       Isolate* isolate, Handle<JSObject> object, IntegrityLevel lvl);
@@ -797,7 +806,8 @@ class JSObject : public TorqueGeneratedJSObject<JSObject, JSReceiver> {
                                       ElementsKind to_kind,
                                       Handle<FixedArrayBase> to_elements);
 
-  void PrintInstanceMigration(FILE* file, Map original_map, Map new_map);
+  void PrintInstanceMigration(FILE* file, Tagged<Map> original_map,
+                              Tagged<Map> new_map);
 
 #ifdef DEBUG
   // Structure for collecting spill information about JSObjects.
@@ -829,7 +839,7 @@ class JSObject : public TorqueGeneratedJSObject<JSObject, JSReceiver> {
       PtrComprCageBase cage_base) const;
 #endif
 
-  Object SlowReverseLookup(Object value);
+  Tagged<Object> SlowReverseLookup(Tagged<Object> value);
 
   // Maximal number of elements (numbered 0 .. kMaxElementCount - 1).
   // Also maximal value of JSArray's length property.
@@ -909,8 +919,8 @@ class JSObject : public TorqueGeneratedJSObject<JSObject, JSReceiver> {
   V8_WARN_UNUSED_RESULT static Maybe<bool> DeletePropertyWithInterceptor(
       LookupIterator* it, ShouldThrow should_throw);
 
-  bool ReferencesObjectFromElements(FixedArray elements, ElementsKind kind,
-                                    Object object);
+  bool ReferencesObjectFromElements(Tagged<FixedArray> elements,
+                                    ElementsKind kind, Tagged<Object> object);
 
   // Helper for fast versions of preventExtensions, seal, and freeze.
   // attrs is one of NONE, SEALED, or FROZEN (depending on the operation).
@@ -918,10 +928,9 @@ class JSObject : public TorqueGeneratedJSObject<JSObject, JSReceiver> {
   V8_WARN_UNUSED_RESULT static Maybe<bool> PreventExtensionsWithTransition(
       Isolate* isolate, Handle<JSObject> object, ShouldThrow should_throw);
 
-  inline Object RawFastPropertyAtCompareAndSwapInternal(FieldIndex index,
-                                                        Object expected,
-                                                        Object value,
-                                                        SeqCstAccessTag tag);
+  inline Tagged<Object> RawFastPropertyAtCompareAndSwapInternal(
+      FieldIndex index, Tagged<Object> expected, Tagged<Object> value,
+      SeqCstAccessTag tag);
 
   TQ_OBJECT_CONSTRUCTORS(JSObject)
 };
@@ -1076,7 +1085,7 @@ class JSIteratorResult : public JSObject {
 class JSGlobalProxy
     : public TorqueGeneratedJSGlobalProxy<JSGlobalProxy, JSSpecialObject> {
  public:
-  inline bool IsDetachedFrom(JSGlobalObject global) const;
+  inline bool IsDetachedFrom(Tagged<JSGlobalObject> global) const;
   V8_EXPORT_PRIVATE bool IsDetached() const;
 
   static int SizeWithEmbedderFields(int embedder_field_count);
@@ -1144,7 +1153,7 @@ class JSDate : public TorqueGeneratedJSDate<JSDate, JSObject> {
 
   static Handle<Object> SetValue(Handle<JSDate> date, double v);
 
-  void SetValue(Object value, bool is_value_nan);
+  void SetValue(Tagged<Object> value, bool is_value_nan);
 
   // Dispatched behavior.
   DECL_PRINTER(JSDate)
@@ -1180,8 +1189,9 @@ class JSDate : public TorqueGeneratedJSDate<JSDate, JSObject> {
   };
 
  private:
-  Object DoGetField(Isolate* isolate, FieldIndex index);
-  Object GetUTCField(FieldIndex index, double value, DateCache* date_cache);
+  Tagged<Object> DoGetField(Isolate* isolate, FieldIndex index);
+  Tagged<Object> GetUTCField(FieldIndex index, double value,
+                             DateCache* date_cache);
 
   // Computes and caches the cacheable fields of the date.
   inline void SetCachedFields(int64_t local_time_ms, DateCache* date_cache);
@@ -1224,7 +1234,7 @@ class JSMessageObject
   V8_EXPORT_PRIVATE int GetColumnNumber() const;
 
   // Returns the source code
-  V8_EXPORT_PRIVATE String GetSource() const;
+  V8_EXPORT_PRIVATE Tagged<String> GetSource() const;
 
   // Returns the source code line containing the given source
   // position, or the empty string if the position is invalid.
@@ -1319,11 +1329,11 @@ class JSValidIteratorWrapper
 // "reject", in that order.
 class JSPromiseWithResolversResult : public JSObject {
  public:
-  DECL_ACCESSORS(promise, Object)
+  DECL_ACCESSORS(promise, Tagged<Object>)
 
-  DECL_ACCESSORS(resolve, Object)
+  DECL_ACCESSORS(resolve, Tagged<Object>)
 
-  DECL_ACCESSORS(reject, Object)
+  DECL_ACCESSORS(reject, Tagged<Object>)
 
   // Layout description.
 #define JS_PROMISE_WITHRESOLVERS_RESULT_FIELDS(V) \
diff --git a/src/objects/js-plural-rules-inl.h b/src/objects/js-plural-rules-inl.h
index fb4a97e4763..0c78418cb0e 100644
--- a/src/objects/js-plural-rules-inl.h
+++ b/src/objects/js-plural-rules-inl.h
@@ -23,10 +23,10 @@ namespace internal {
 
 TQ_OBJECT_CONSTRUCTORS_IMPL(JSPluralRules)
 
-ACCESSORS(JSPluralRules, icu_plural_rules, Managed<icu::PluralRules>,
+ACCESSORS(JSPluralRules, icu_plural_rules, Tagged<Managed<icu::PluralRules>>,
           kIcuPluralRulesOffset)
 ACCESSORS(JSPluralRules, icu_number_formatter,
-          Managed<icu::number::LocalizedNumberFormatter>,
+          Tagged<Managed<icu::number::LocalizedNumberFormatter>>,
           kIcuNumberFormatterOffset)
 
 inline void JSPluralRules::set_type(Type type) {
diff --git a/src/objects/js-plural-rules.h b/src/objects/js-plural-rules.h
index 9cbce2c16c4..eb9c2ce43b7 100644
--- a/src/objects/js-plural-rules.h
+++ b/src/objects/js-plural-rules.h
@@ -69,9 +69,9 @@ class JSPluralRules
   static_assert(Type::CARDINAL <= TypeBit::kMax);
   static_assert(Type::ORDINAL <= TypeBit::kMax);
 
-  DECL_ACCESSORS(icu_plural_rules, Managed<icu::PluralRules>)
+  DECL_ACCESSORS(icu_plural_rules, Tagged<Managed<icu::PluralRules>>)
   DECL_ACCESSORS(icu_number_formatter,
-                 Managed<icu::number::LocalizedNumberFormatter>)
+                 Tagged<Managed<icu::number::LocalizedNumberFormatter>>)
 
   TQ_OBJECT_CONSTRUCTORS(JSPluralRules)
 };
diff --git a/src/objects/js-promise-inl.h b/src/objects/js-promise-inl.h
index f289b342be1..e3ab3b98da0 100644
--- a/src/objects/js-promise-inl.h
+++ b/src/objects/js-promise-inl.h
@@ -24,12 +24,12 @@ BOOL_ACCESSORS(JSPromise, flags, has_handler, HasHandlerBit::kShift)
 BOOL_ACCESSORS(JSPromise, flags, handled_hint, HandledHintBit::kShift)
 BOOL_ACCESSORS(JSPromise, flags, is_silent, IsSilentBit::kShift)
 
-Object JSPromise::result() const {
+Tagged<Object> JSPromise::result() const {
   DCHECK_NE(Promise::kPending, status());
   return reactions_or_result();
 }
 
-Object JSPromise::reactions() const {
+Tagged<Object> JSPromise::reactions() const {
   DCHECK_EQ(Promise::kPending, status());
   return reactions_or_result();
 }
diff --git a/src/objects/js-promise.h b/src/objects/js-promise.h
index ffaaab60265..e4759105a48 100644
--- a/src/objects/js-promise.h
+++ b/src/objects/js-promise.h
@@ -32,10 +32,10 @@ class JSPromise
     : public TorqueGeneratedJSPromise<JSPromise, JSObjectWithEmbedderSlots> {
  public:
   // [result]: Checks that the promise is settled and returns the result.
-  inline Object result() const;
+  inline Tagged<Object> result() const;
 
   // [reactions]: Checks that the promise is pending and returns the reactions.
-  inline Object reactions() const;
+  inline Tagged<Object> reactions() const;
 
   // [has_handler]: Whether this promise has a reject handler or not.
   DECL_BOOLEAN_ACCESSORS(has_handler)
diff --git a/src/objects/js-regexp-inl.h b/src/objects/js-regexp-inl.h
index da2981cd291..f45ebb0ab61 100644
--- a/src/objects/js-regexp-inl.h
+++ b/src/objects/js-regexp-inl.h
@@ -28,9 +28,9 @@ TQ_OBJECT_CONSTRUCTORS_IMPL(JSRegExpResultWithIndices)
 ACCESSORS(JSRegExp, last_index, Tagged<Object>, kLastIndexOffset)
 
 JSRegExp::Type JSRegExp::type_tag() const {
-  Object data = this->data();
+  Tagged<Object> data = this->data();
   if (IsUndefined(data)) return JSRegExp::NOT_COMPILED;
-  Smi smi = Smi::cast(FixedArray::cast(data)->get(kTagIndex));
+  Tagged<Smi> smi = Smi::cast(FixedArray::cast(data)->get(kTagIndex));
   return static_cast<JSRegExp::Type>(smi.value());
 }
 
@@ -51,17 +51,17 @@ int JSRegExp::max_register_count() const {
   return Smi::ToInt(DataAt(kIrregexpMaxRegisterCountIndex));
 }
 
-String JSRegExp::atom_pattern() const {
+Tagged<String> JSRegExp::atom_pattern() const {
   DCHECK_EQ(type_tag(), ATOM);
   return String::cast(DataAt(JSRegExp::kAtomPatternIndex));
 }
 
-String JSRegExp::source() const {
+Tagged<String> JSRegExp::source() const {
   return String::cast(TorqueGeneratedClass::source());
 }
 
 JSRegExp::Flags JSRegExp::flags() const {
-  Smi smi = Smi::cast(TorqueGeneratedClass::flags());
+  Tagged<Smi> smi = Smi::cast(TorqueGeneratedClass::flags());
   return Flags(smi.value());
 }
 
@@ -77,14 +77,14 @@ const char* JSRegExp::FlagsToString(Flags flags, FlagsBuffer* out_buffer) {
   return buffer.begin();
 }
 
-String JSRegExp::EscapedPattern() {
+Tagged<String> JSRegExp::EscapedPattern() {
   DCHECK(IsString(source()));
   return String::cast(source());
 }
 
-Object JSRegExp::capture_name_map() {
+Tagged<Object> JSRegExp::capture_name_map() {
   DCHECK(TypeSupportsCaptures(type_tag()));
-  Object value = DataAt(kIrregexpCaptureNameMapIndex);
+  Tagged<Object> value = DataAt(kIrregexpCaptureNameMapIndex);
   DCHECK_NE(value, Smi::FromInt(JSRegExp::kUninitializedValue));
   return value;
 }
@@ -97,12 +97,12 @@ void JSRegExp::set_capture_name_map(Handle<FixedArray> capture_name_map) {
   }
 }
 
-Object JSRegExp::DataAt(int index) const {
+Tagged<Object> JSRegExp::DataAt(int index) const {
   DCHECK(type_tag() != NOT_COMPILED);
   return FixedArray::cast(data())->get(index);
 }
 
-void JSRegExp::SetDataAt(int index, Object value) {
+void JSRegExp::SetDataAt(int index, Tagged<Object> value) {
   DCHECK(type_tag() != NOT_COMPILED);
   // Only implementation data can be set this way.
   DCHECK_GE(index, kFirstTypeSpecificIndex);
@@ -111,7 +111,7 @@ void JSRegExp::SetDataAt(int index, Object value) {
 
 bool JSRegExp::HasCompiledCode() const {
   if (type_tag() != IRREGEXP) return false;
-  Smi uninitialized = Smi::FromInt(kUninitializedValue);
+  Tagged<Smi> uninitialized = Smi::FromInt(kUninitializedValue);
 #ifdef DEBUG
   DCHECK(IsCode(DataAt(kIrregexpLatin1CodeIndex)) ||
          DataAt(kIrregexpLatin1CodeIndex) == uninitialized);
@@ -128,7 +128,7 @@ bool JSRegExp::HasCompiledCode() const {
 
 void JSRegExp::DiscardCompiledCodeForSerialization() {
   DCHECK(HasCompiledCode());
-  Smi uninitialized = Smi::FromInt(kUninitializedValue);
+  Tagged<Smi> uninitialized = Smi::FromInt(kUninitializedValue);
   SetDataAt(kIrregexpLatin1CodeIndex, uninitialized);
   SetDataAt(kIrregexpUC16CodeIndex, uninitialized);
   SetDataAt(kIrregexpLatin1BytecodeIndex, uninitialized);
diff --git a/src/objects/js-regexp.cc b/src/objects/js-regexp.cc
index da9e1358d32..181a1aeb19e 100644
--- a/src/objects/js-regexp.cc
+++ b/src/objects/js-regexp.cc
@@ -150,9 +150,9 @@ MaybeHandle<JSRegExp> JSRegExp::New(Isolate* isolate, Handle<String> pattern,
   return JSRegExp::Initialize(regexp, pattern, flags, backtrack_limit);
 }
 
-Object JSRegExp::code(bool is_latin1) const {
+Tagged<Object> JSRegExp::code(bool is_latin1) const {
   DCHECK_EQ(type_tag(), JSRegExp::IRREGEXP);
-  Object value = DataAt(code_index(is_latin1));
+  Tagged<Object> value = DataAt(code_index(is_latin1));
   DCHECK(IsSmi(value) || IsCode(value));
   return value;
 }
@@ -161,7 +161,7 @@ void JSRegExp::set_code(bool is_latin1, Handle<Code> code) {
   SetDataAt(code_index(is_latin1), *code);
 }
 
-Object JSRegExp::bytecode(bool is_latin1) const {
+Tagged<Object> JSRegExp::bytecode(bool is_latin1) const {
   DCHECK(type_tag() == JSRegExp::IRREGEXP ||
          type_tag() == JSRegExp::EXPERIMENTAL);
   return DataAt(bytecode_index(is_latin1));
@@ -413,8 +413,8 @@ MaybeHandle<JSRegExp> JSRegExp::Initialize(Handle<JSRegExp> regexp,
   regexp->set_source(*escaped_source);
   regexp->set_flags(Smi::FromInt(flags));
 
-  Map map = regexp->map();
-  Object constructor = map->GetConstructor();
+  Tagged<Map> map = regexp->map();
+  Tagged<Object> constructor = map->GetConstructor();
   if (IsJSFunction(constructor) &&
       JSFunction::cast(constructor)->initial_map() == map) {
     // If we still have the original map, set in-object properties directly.
diff --git a/src/objects/js-regexp.h b/src/objects/js-regexp.h
index 3b55bdec3ad..4315c76ebf3 100644
--- a/src/objects/js-regexp.h
+++ b/src/objects/js-regexp.h
@@ -59,18 +59,18 @@ class JSRegExp : public TorqueGeneratedJSRegExp<JSRegExp, JSObject> {
   DECL_ACCESSORS(last_index, Tagged<Object>)
 
   // Instance fields accessors.
-  inline String source() const;
+  inline Tagged<String> source() const;
   inline Flags flags() const;
 
   // Data array field accessors.
 
   inline Type type_tag() const;
-  inline String atom_pattern() const;
+  inline Tagged<String> atom_pattern() const;
   // This could be a Smi kUninitializedValue or InstructionStream.
-  V8_EXPORT_PRIVATE Object code(bool is_latin1) const;
+  V8_EXPORT_PRIVATE Tagged<Object> code(bool is_latin1) const;
   V8_EXPORT_PRIVATE void set_code(bool is_unicode, Handle<Code> code);
   // This could be a Smi kUninitializedValue or ByteArray.
-  V8_EXPORT_PRIVATE Object bytecode(bool is_latin1) const;
+  V8_EXPORT_PRIVATE Tagged<Object> bytecode(bool is_latin1) const;
   // Sets the bytecode as well as initializing trampoline slots to the
   // RegExpInterpreterTrampoline.
   void set_bytecode_and_trampoline(Isolate* isolate,
@@ -78,7 +78,7 @@ class JSRegExp : public TorqueGeneratedJSRegExp<JSRegExp, JSObject> {
   inline int max_register_count() const;
   // Number of captures (without the match itself).
   inline int capture_count() const;
-  inline Object capture_name_map();
+  inline Tagged<Object> capture_name_map();
   inline void set_capture_name_map(Handle<FixedArray> capture_name_map);
   uint32_t backtrack_limit() const;
 
@@ -122,7 +122,7 @@ class JSRegExp : public TorqueGeneratedJSRegExp<JSRegExp, JSObject> {
   V8_EXPORT_PRIVATE static Handle<String> StringFromFlags(Isolate* isolate,
                                                           Flags flags);
 
-  inline String EscapedPattern();
+  inline Tagged<String> EscapedPattern();
 
   bool CanTierUp();
   bool MarkedForTierUp();
@@ -252,8 +252,8 @@ class JSRegExp : public TorqueGeneratedJSRegExp<JSRegExp, JSObject> {
   using FlagsBuffer = base::EmbeddedVector<char, kFlagCount + 1>;
   inline static const char* FlagsToString(Flags flags, FlagsBuffer* out_buffer);
 
-  inline Object DataAt(int index) const;
-  inline void SetDataAt(int index, Object value);
+  inline Tagged<Object> DataAt(int index) const;
+  inline void SetDataAt(int index, Tagged<Object> value);
 
   TQ_OBJECT_CONSTRUCTORS(JSRegExp)
 };
diff --git a/src/objects/js-relative-time-format-inl.h b/src/objects/js-relative-time-format-inl.h
index 4afdaa3088e..953ba584e55 100644
--- a/src/objects/js-relative-time-format-inl.h
+++ b/src/objects/js-relative-time-format-inl.h
@@ -24,7 +24,7 @@ TQ_OBJECT_CONSTRUCTORS_IMPL(JSRelativeTimeFormat)
 
 // Base relative time format accessors.
 ACCESSORS(JSRelativeTimeFormat, icu_formatter,
-          Managed<icu::RelativeDateTimeFormatter>, kIcuFormatterOffset)
+          Tagged<Managed<icu::RelativeDateTimeFormatter>>, kIcuFormatterOffset)
 
 inline void JSRelativeTimeFormat::set_numeric(Numeric numeric) {
   DCHECK_GE(NumericBit::kMax, numeric);
diff --git a/src/objects/js-relative-time-format.h b/src/objects/js-relative-time-format.h
index 77c059fdd67..ce84698c864 100644
--- a/src/objects/js-relative-time-format.h
+++ b/src/objects/js-relative-time-format.h
@@ -59,7 +59,7 @@ class JSRelativeTimeFormat
   V8_EXPORT_PRIVATE static const std::set<std::string>& GetAvailableLocales();
 
   // RelativeTimeFormat accessors.
-  DECL_ACCESSORS(icu_formatter, Managed<icu::RelativeDateTimeFormatter>)
+  DECL_ACCESSORS(icu_formatter, Tagged<Managed<icu::RelativeDateTimeFormatter>>)
 
   // Numeric: identifying whether numerical descriptions are always used, or
   // used only when no more specific version is available (e.g., "1 day ago" vs
diff --git a/src/objects/js-segment-iterator-inl.h b/src/objects/js-segment-iterator-inl.h
index 979a1c796b0..94e7ef3d6c8 100644
--- a/src/objects/js-segment-iterator-inl.h
+++ b/src/objects/js-segment-iterator-inl.h
@@ -22,10 +22,10 @@ namespace internal {
 TQ_OBJECT_CONSTRUCTORS_IMPL(JSSegmentIterator)
 
 // Base segment iterator accessors.
-ACCESSORS(JSSegmentIterator, icu_break_iterator, Managed<icu::BreakIterator>,
-          kIcuBreakIteratorOffset)
-ACCESSORS(JSSegmentIterator, unicode_string, Managed<icu::UnicodeString>,
-          kUnicodeStringOffset)
+ACCESSORS(JSSegmentIterator, icu_break_iterator,
+          Tagged<Managed<icu::BreakIterator>>, kIcuBreakIteratorOffset)
+ACCESSORS(JSSegmentIterator, unicode_string,
+          Tagged<Managed<icu::UnicodeString>>, kUnicodeStringOffset)
 
 inline void JSSegmentIterator::set_granularity(
     JSSegmenter::Granularity granularity) {
diff --git a/src/objects/js-segment-iterator.h b/src/objects/js-segment-iterator.h
index b8151dc436d..fd9077683e9 100644
--- a/src/objects/js-segment-iterator.h
+++ b/src/objects/js-segment-iterator.h
@@ -44,8 +44,8 @@ class JSSegmentIterator
   Handle<String> GranularityAsString(Isolate* isolate) const;
 
   // SegmentIterator accessors.
-  DECL_ACCESSORS(icu_break_iterator, Managed<icu::BreakIterator>)
-  DECL_ACCESSORS(unicode_string, Managed<icu::UnicodeString>)
+  DECL_ACCESSORS(icu_break_iterator, Tagged<Managed<icu::BreakIterator>>)
+  DECL_ACCESSORS(unicode_string, Tagged<Managed<icu::UnicodeString>>)
 
   DECL_PRINTER(JSSegmentIterator)
 
diff --git a/src/objects/js-segmenter-inl.h b/src/objects/js-segmenter-inl.h
index e6744268c4f..527d95e8947 100644
--- a/src/objects/js-segmenter-inl.h
+++ b/src/objects/js-segmenter-inl.h
@@ -22,7 +22,7 @@ namespace internal {
 TQ_OBJECT_CONSTRUCTORS_IMPL(JSSegmenter)
 
 // Base segmenter accessors.
-ACCESSORS(JSSegmenter, icu_break_iterator, Managed<icu::BreakIterator>,
+ACCESSORS(JSSegmenter, icu_break_iterator, Tagged<Managed<icu::BreakIterator>>,
           kIcuBreakIteratorOffset)
 
 inline void JSSegmenter::set_granularity(Granularity granularity) {
diff --git a/src/objects/js-segmenter.h b/src/objects/js-segmenter.h
index f2e21db5fd7..a67c9aac616 100644
--- a/src/objects/js-segmenter.h
+++ b/src/objects/js-segmenter.h
@@ -46,7 +46,7 @@ class JSSegmenter : public TorqueGeneratedJSSegmenter<JSSegmenter, JSObject> {
   Handle<String> GranularityAsString(Isolate* isolate) const;
 
   // Segmenter accessors.
-  DECL_ACCESSORS(icu_break_iterator, Managed<icu::BreakIterator>)
+  DECL_ACCESSORS(icu_break_iterator, Tagged<Managed<icu::BreakIterator>>)
 
   // Granularity: identifying the segmenter used.
   //
diff --git a/src/objects/js-segments-inl.h b/src/objects/js-segments-inl.h
index 37fc4964e0c..86170eb1d9d 100644
--- a/src/objects/js-segments-inl.h
+++ b/src/objects/js-segments-inl.h
@@ -22,9 +22,9 @@ namespace internal {
 TQ_OBJECT_CONSTRUCTORS_IMPL(JSSegments)
 
 // Base segments accessors.
-ACCESSORS(JSSegments, icu_break_iterator, Managed<icu::BreakIterator>,
+ACCESSORS(JSSegments, icu_break_iterator, Tagged<Managed<icu::BreakIterator>>,
           kIcuBreakIteratorOffset)
-ACCESSORS(JSSegments, unicode_string, Managed<icu::UnicodeString>,
+ACCESSORS(JSSegments, unicode_string, Tagged<Managed<icu::UnicodeString>>,
           kUnicodeStringOffset)
 
 inline void JSSegments::set_granularity(JSSegmenter::Granularity granularity) {
diff --git a/src/objects/js-segments.h b/src/objects/js-segments.h
index aa5759ade87..84fc7d46bbf 100644
--- a/src/objects/js-segments.h
+++ b/src/objects/js-segments.h
@@ -48,8 +48,8 @@ class JSSegments : public TorqueGeneratedJSSegments<JSSegments, JSObject> {
   Handle<String> GranularityAsString(Isolate* isolate) const;
 
   // SegmentIterator accessors.
-  DECL_ACCESSORS(icu_break_iterator, Managed<icu::BreakIterator>)
-  DECL_ACCESSORS(unicode_string, Managed<icu::UnicodeString>)
+  DECL_ACCESSORS(icu_break_iterator, Tagged<Managed<icu::BreakIterator>>)
+  DECL_ACCESSORS(unicode_string, Tagged<Managed<icu::UnicodeString>>)
 
   DECL_PRINTER(JSSegments)
 
diff --git a/src/objects/js-struct.cc b/src/objects/js-struct.cc
index f1a123ee25e..b8a583231ea 100644
--- a/src/objects/js-struct.cc
+++ b/src/objects/js-struct.cc
@@ -29,7 +29,8 @@ void PrepareMapCommon(Map map) {
 }  // namespace
 
 // static
-void AlwaysSharedSpaceJSObject::PrepareMapNoEnumerableProperties(Map map) {
+void AlwaysSharedSpaceJSObject::PrepareMapNoEnumerableProperties(
+    Tagged<Map> map) {
   PrepareMapCommon(map);
   map->SetEnumLength(0);
 }
diff --git a/src/objects/js-struct.h b/src/objects/js-struct.h
index 3f6a8e36570..3e0ca6a908e 100644
--- a/src/objects/js-struct.h
+++ b/src/objects/js-struct.h
@@ -20,7 +20,7 @@ class AlwaysSharedSpaceJSObject
                                                       JSObject> {
  public:
   // Prepare a Map to be used as the instance map for shared JS objects.
-  static void PrepareMapNoEnumerableProperties(Map map);
+  static void PrepareMapNoEnumerableProperties(Tagged<Map> map);
   static void PrepareMapWithEnumerableProperties(
       Isolate* isolate, Handle<Map> map, Handle<DescriptorArray> descriptors,
       int enum_length);
diff --git a/src/objects/js-temporal-objects.cc b/src/objects/js-temporal-objects.cc
index a95bf55432f..57b86a87abc 100644
--- a/src/objects/js-temporal-objects.cc
+++ b/src/objects/js-temporal-objects.cc
@@ -18705,7 +18705,7 @@ MaybeHandle<Oddball> IsInvalidTemporalCalendarField(
   Factory* factory = isolate->factory();
   // iii. iii. If fieldNames contains nextValue, then
   for (int i = 0; i < fields_name->length(); i++) {
-    Object item = fields_name->get(i);
+    Tagged<Object> item = fields_name->get(i);
     DCHECK(IsString(item));
     if (String::Equals(isolate, next_value,
                        handle(String::cast(item), isolate))) {
diff --git a/src/objects/js-weak-refs-inl.h b/src/objects/js-weak-refs-inl.h
index 6b6ef7618fd..efb2ed15be1 100644
--- a/src/objects/js-weak-refs-inl.h
+++ b/src/objects/js-weak-refs-inl.h
@@ -44,8 +44,8 @@ void JSFinalizationRegistry::RegisterWeakCellWithUnregisterToken(
       Object::GetOrCreateHash(weak_cell->unregister_token(), isolate).value();
   InternalIndex entry = key_map->FindEntry(isolate, key);
   if (entry.is_found()) {
-    Object value = key_map->ValueAt(entry);
-    WeakCell existing_weak_cell = WeakCell::cast(value);
+    Tagged<Object> value = key_map->ValueAt(entry);
+    Tagged<WeakCell> existing_weak_cell = WeakCell::cast(value);
     existing_weak_cell->set_key_list_prev(*weak_cell);
     weak_cell->set_key_list_next(existing_weak_cell);
   }
@@ -61,12 +61,12 @@ bool JSFinalizationRegistry::Unregister(
   // its FinalizationRegistry; remove it from there.
   return finalization_registry->RemoveUnregisterToken(
       *unregister_token, isolate, kRemoveMatchedCellsFromRegistry,
-      [](HeapObject, ObjectSlot, Object) {});
+      [](Tagged<HeapObject>, ObjectSlot, Tagged<Object>) {});
 }
 
 template <typename GCNotifyUpdatedSlotCallback>
 bool JSFinalizationRegistry::RemoveUnregisterToken(
-    HeapObject unregister_token, Isolate* isolate,
+    Tagged<HeapObject> unregister_token, Isolate* isolate,
     RemoveUnregisterTokenMode removal_mode,
     GCNotifyUpdatedSlotCallback gc_notify_updated_slot) {
   // This method is called from both FinalizationRegistry#unregister and for
@@ -77,11 +77,11 @@ bool JSFinalizationRegistry::RemoveUnregisterToken(
     return false;
   }
 
-  SimpleNumberDictionary key_map =
+  Tagged<SimpleNumberDictionary> key_map =
       SimpleNumberDictionary::cast(this->key_map());
   // If the token doesn't have a hash, it was not used as a key inside any hash
   // tables.
-  Object hash = Object::GetHash(unregister_token);
+  Tagged<Object> hash = Object::GetHash(unregister_token);
   if (IsUndefined(hash, isolate)) {
     return false;
   }
@@ -91,16 +91,16 @@ bool JSFinalizationRegistry::RemoveUnregisterToken(
     return false;
   }
 
-  Object value = key_map->ValueAt(entry);
+  Tagged<Object> value = key_map->ValueAt(entry);
   bool was_present = false;
-  HeapObject undefined = ReadOnlyRoots(isolate).undefined_value();
-  HeapObject new_key_list_head = undefined;
-  HeapObject new_key_list_prev = undefined;
+  Tagged<HeapObject> undefined = ReadOnlyRoots(isolate).undefined_value();
+  Tagged<HeapObject> new_key_list_head = undefined;
+  Tagged<HeapObject> new_key_list_prev = undefined;
   // Compute a new key list that doesn't have unregister_token. Because
   // unregister tokens are held weakly, key_map is keyed using the tokens'
   // identity hashes, and identity hashes may collide.
   while (!IsUndefined(value, isolate)) {
-    WeakCell weak_cell = WeakCell::cast(value);
+    Tagged<WeakCell> weak_cell = WeakCell::cast(value);
     DCHECK(!ObjectInYoungGeneration(weak_cell));
     value = weak_cell->key_list_next();
     if (weak_cell->unregister_token() == unregister_token) {
@@ -130,7 +130,7 @@ bool JSFinalizationRegistry::RemoveUnregisterToken(
         new_key_list_head = weak_cell;
       } else {
         DCHECK(IsWeakCell(new_key_list_head));
-        WeakCell prev_cell = WeakCell::cast(new_key_list_prev);
+        Tagged<WeakCell> prev_cell = WeakCell::cast(new_key_list_prev);
         prev_cell->set_key_list_next(weak_cell);
         gc_notify_updated_slot(
             prev_cell, prev_cell->RawField(WeakCell::kKeyListNextOffset),
@@ -155,11 +155,11 @@ bool JSFinalizationRegistry::NeedsCleanup() const {
   return IsWeakCell(cleared_cells());
 }
 
-HeapObject WeakCell::relaxed_target() const {
+Tagged<HeapObject> WeakCell::relaxed_target() const {
   return TaggedField<HeapObject>::Relaxed_Load(*this, kTargetOffset);
 }
 
-HeapObject WeakCell::relaxed_unregister_token() const {
+Tagged<HeapObject> WeakCell::relaxed_unregister_token() const {
   return TaggedField<HeapObject>::Relaxed_Load(*this, kUnregisterTokenOffset);
 }
 
@@ -174,11 +174,11 @@ void WeakCell::Nullify(Isolate* isolate,
   DCHECK(Object::CanBeHeldWeakly(target()));
   set_target(ReadOnlyRoots(isolate).undefined_value());
 
-  JSFinalizationRegistry fr =
+  Tagged<JSFinalizationRegistry> fr =
       JSFinalizationRegistry::cast(finalization_registry());
   if (IsWeakCell(prev())) {
     DCHECK_NE(fr->active_cells(), *this);
-    WeakCell prev_cell = WeakCell::cast(prev());
+    Tagged<WeakCell> prev_cell = WeakCell::cast(prev());
     prev_cell->set_next(next());
     gc_notify_updated_slot(prev_cell,
                            prev_cell->RawField(WeakCell::kNextOffset), next());
@@ -189,16 +189,16 @@ void WeakCell::Nullify(Isolate* isolate,
         fr, fr->RawField(JSFinalizationRegistry::kActiveCellsOffset), next());
   }
   if (IsWeakCell(next())) {
-    WeakCell next_cell = WeakCell::cast(next());
+    Tagged<WeakCell> next_cell = WeakCell::cast(next());
     next_cell->set_prev(prev());
     gc_notify_updated_slot(next_cell,
                            next_cell->RawField(WeakCell::kPrevOffset), prev());
   }
 
   set_prev(ReadOnlyRoots(isolate).undefined_value());
-  Object cleared_head = fr->cleared_cells();
+  Tagged<Object> cleared_head = fr->cleared_cells();
   if (IsWeakCell(cleared_head)) {
-    WeakCell cleared_head_cell = WeakCell::cast(cleared_head);
+    Tagged<WeakCell> cleared_head_cell = WeakCell::cast(cleared_head);
     cleared_head_cell->set_prev(*this);
     gc_notify_updated_slot(cleared_head_cell,
                            cleared_head_cell->RawField(WeakCell::kPrevOffset),
@@ -220,7 +220,7 @@ void WeakCell::RemoveFromFinalizationRegistryCells(Isolate* isolate) {
   DCHECK(IsUndefined(target()) || Object::CanBeHeldWeakly(target()));
   set_target(ReadOnlyRoots(isolate).undefined_value());
 
-  JSFinalizationRegistry fr =
+  Tagged<JSFinalizationRegistry> fr =
       JSFinalizationRegistry::cast(finalization_registry());
   if (fr->active_cells() == *this) {
     DCHECK(IsUndefined(prev(), isolate));
@@ -230,11 +230,11 @@ void WeakCell::RemoveFromFinalizationRegistryCells(Isolate* isolate) {
     fr->set_cleared_cells(next());
   } else {
     DCHECK(IsWeakCell(prev()));
-    WeakCell prev_cell = WeakCell::cast(prev());
+    Tagged<WeakCell> prev_cell = WeakCell::cast(prev());
     prev_cell->set_next(next());
   }
   if (IsWeakCell(next())) {
-    WeakCell next_cell = WeakCell::cast(next());
+    Tagged<WeakCell> next_cell = WeakCell::cast(next());
     next_cell->set_prev(prev());
   }
   set_prev(ReadOnlyRoots(isolate).undefined_value());
diff --git a/src/objects/js-weak-refs.h b/src/objects/js-weak-refs.h
index 64ff9573f6b..81b78f046d5 100644
--- a/src/objects/js-weak-refs.h
+++ b/src/objects/js-weak-refs.h
@@ -49,7 +49,7 @@ class JSFinalizationRegistry
   };
   template <typename GCNotifyUpdatedSlotCallback>
   inline bool RemoveUnregisterToken(
-      HeapObject unregister_token, Isolate* isolate,
+      Tagged<HeapObject> unregister_token, Isolate* isolate,
       RemoveUnregisterTokenMode removal_mode,
       GCNotifyUpdatedSlotCallback gc_notify_updated_slot);
 
@@ -80,10 +80,10 @@ class WeakCell : public TorqueGeneratedWeakCell<WeakCell, HeapObject> {
   class BodyDescriptor;
 
   // Provide relaxed load access to target field.
-  inline HeapObject relaxed_target() const;
+  inline Tagged<HeapObject> relaxed_target() const;
 
   // Provide relaxed load access to the unregister token field.
-  inline HeapObject relaxed_unregister_token() const;
+  inline Tagged<HeapObject> relaxed_unregister_token() const;
 
   // Nullify is called during GC and it modifies the pointers in WeakCell and
   // JSFinalizationRegistry. Thus we need to tell the GC about the modified
diff --git a/src/objects/keys.cc b/src/objects/keys.cc
index 7efd4c87dfa..f2f3239d34d 100644
--- a/src/objects/keys.cc
+++ b/src/objects/keys.cc
@@ -44,13 +44,13 @@ namespace {
 static bool ContainsOnlyValidKeys(Handle<FixedArray> array) {
   int len = array->length();
   for (int i = 0; i < len; i++) {
-    Object e = array->get(i);
+    Tagged<Object> e = array->get(i);
     if (!(IsName(e) || IsNumber(e))) return false;
   }
   return true;
 }
 
-static int AddKey(Object key, Handle<FixedArray> combined_keys,
+static int AddKey(Tagged<Object> key, Handle<FixedArray> combined_keys,
                   Handle<DescriptorArray> descs, int nof_descriptors,
                   int target) {
   for (InternalIndex i : InternalIndex::Range(nof_descriptors)) {
@@ -68,7 +68,7 @@ static Handle<FixedArray> CombineKeys(Isolate* isolate,
   int prototype_chain_keys_length = prototype_chain_keys->length();
   if (prototype_chain_keys_length == 0) return own_keys;
 
-  Map map = receiver->map();
+  Tagged<Map> map = receiver->map();
   int nof_descriptors = map->NumberOfOwnDescriptors();
   if (nof_descriptors == 0 && !may_have_elements) return prototype_chain_keys;
 
@@ -122,7 +122,8 @@ Handle<OrderedHashSet> KeyAccumulator::keys() {
   return Handle<OrderedHashSet>::cast(keys_);
 }
 
-ExceptionStatus KeyAccumulator::AddKey(Object key, AddKeyConversion convert) {
+ExceptionStatus KeyAccumulator::AddKey(Tagged<Object> key,
+                                       AddKeyConversion convert) {
   return AddKey(handle(key, isolate_), convert);
 }
 
@@ -296,7 +297,7 @@ bool KeyAccumulator::IsShadowed(Handle<Object> key) {
   return shadowing_keys_->Has(isolate_, key);
 }
 
-void KeyAccumulator::AddShadowingKey(Object key,
+void KeyAccumulator::AddShadowingKey(Tagged<Object> key,
                                      AllowGarbageCollection* allow_gc) {
   if (mode_ == KeyCollectionMode::kOwnOnly) return;
   AddShadowingKey(handle(key, isolate_));
@@ -311,8 +312,8 @@ void KeyAccumulator::AddShadowingKey(Handle<Object> key) {
 
 namespace {
 
-void TrySettingEmptyEnumCache(JSReceiver object) {
-  Map map = object->map();
+void TrySettingEmptyEnumCache(Tagged<JSReceiver> object) {
+  Tagged<Map> map = object->map();
   DCHECK_EQ(kInvalidEnumCacheSentinel, map->EnumLength());
   if (!map->OnlyHasSimpleProperties()) return;
   if (IsJSProxyMap(map)) return;
@@ -321,7 +322,7 @@ void TrySettingEmptyEnumCache(JSReceiver object) {
   map->SetEnumLength(0);
 }
 
-bool CheckAndInitializeEmptyEnumCache(JSReceiver object) {
+bool CheckAndInitializeEmptyEnumCache(Tagged<JSReceiver> object) {
   if (object->map()->EnumLength() == kInvalidEnumCacheSentinel) {
     TrySettingEmptyEnumCache(object);
   }
@@ -340,11 +341,11 @@ void FastKeyAccumulator::Prepare() {
   has_empty_prototype_ = true;
   only_own_has_simple_elements_ =
       !IsCustomElementsReceiverMap(receiver_->map());
-  JSReceiver last_prototype;
+  Tagged<JSReceiver> last_prototype;
   may_have_elements_ = MayHaveElements(*receiver_);
   for (PrototypeIterator iter(isolate_, *receiver_); !iter.IsAtEnd();
        iter.Advance()) {
-    JSReceiver current = iter.GetCurrent<JSReceiver>();
+    Tagged<JSReceiver> current = iter.GetCurrent<JSReceiver>();
     if (!may_have_elements_ || only_own_has_simple_elements_) {
       if (MayHaveElements(current)) {
         may_have_elements_ = true;
@@ -466,7 +467,7 @@ MaybeHandle<FixedArray> FastKeyAccumulator::GetKeys(
 MaybeHandle<FixedArray> FastKeyAccumulator::GetKeysFast(
     GetKeysConversion keys_conversion) {
   bool own_only = has_empty_prototype_ || mode_ == KeyCollectionMode::kOwnOnly;
-  Map map = receiver_->map();
+  Tagged<Map> map = receiver_->map();
   if (!own_only || IsCustomElementsReceiverMap(map)) {
     return MaybeHandle<FixedArray>();
   }
@@ -526,7 +527,7 @@ Handle<FixedArray> FastKeyAccumulator::InitializeFastPropertyEnumCache(
     DisallowGarbageCollection no_gc;
     PropertyDetails details = descriptors->GetDetails(i);
     if (details.IsDontEnum()) continue;
-    Object key = descriptors->GetKey(i);
+    Tagged<Object> key = descriptors->GetKey(i);
     if (IsSymbol(key)) continue;
     keys->set(index, key);
     if (details.location() != PropertyLocation::kField) fields_only = false;
@@ -546,7 +547,7 @@ Handle<FixedArray> FastKeyAccumulator::InitializeFastPropertyEnumCache(
     for (InternalIndex i : raw_map->IterateOwnDescriptors()) {
       PropertyDetails details = raw_descriptors->GetDetails(i);
       if (details.IsDontEnum()) continue;
-      Object key = raw_descriptors->GetKey(i);
+      Tagged<Object> key = raw_descriptors->GetKey(i);
       if (IsSymbol(key)) continue;
       DCHECK_EQ(PropertyKind::kData, details.kind());
       DCHECK_EQ(PropertyLocation::kField, details.location());
@@ -567,7 +568,7 @@ MaybeHandle<FixedArray>
 FastKeyAccumulator::GetOwnKeysWithUninitializedEnumLength() {
   Handle<JSObject> object = Handle<JSObject>::cast(receiver_);
   // Uninitialized enum length
-  Map map = object->map();
+  Tagged<Map> map = object->map();
   if (object->elements() != ReadOnlyRoots(isolate_).empty_fixed_array() &&
       object->elements() !=
           ReadOnlyRoots(isolate_).empty_slow_element_dictionary()) {
@@ -651,9 +652,9 @@ MaybeHandle<FixedArray> FastKeyAccumulator::GetKeysWithPrototypeInfoCache(
   return result;
 }
 
-bool FastKeyAccumulator::MayHaveElements(JSReceiver receiver) {
+bool FastKeyAccumulator::MayHaveElements(Tagged<JSReceiver> receiver) {
   if (!IsJSObject(receiver)) return true;
-  JSObject object = JSObject::cast(receiver);
+  Tagged<JSObject> object = JSObject::cast(receiver);
   if (object->HasEnumerableElements()) return true;
   if (object->HasIndexedInterceptor()) return true;
   return false;
@@ -669,11 +670,11 @@ bool FastKeyAccumulator::TryPrototypeInfoCache(Handle<JSReceiver> receiver) {
     return false;
   }
   DisallowGarbageCollection no_gc;
-  HeapObject prototype = receiver->map(isolate_)->prototype();
+  Tagged<HeapObject> prototype = receiver->map(isolate_)->prototype();
   if (prototype.is_null()) return false;
-  Map maybe_proto_map = prototype->map(isolate_);
+  Tagged<Map> maybe_proto_map = prototype->map(isolate_);
   if (!maybe_proto_map->is_prototype_map()) return false;
-  PrototypeInfo prototype_info;
+  Tagged<PrototypeInfo> prototype_info;
   if (!maybe_proto_map->TryGetPrototypeInfo(&prototype_info)) return false;
 
   first_prototype_ = handle(JSReceiver::cast(prototype), isolate_);
@@ -810,12 +811,12 @@ base::Optional<int> CollectOwnPropertyNamesInternal(
 
     if (filter & ONLY_ALL_CAN_READ) {
       if (details.kind() != PropertyKind::kAccessor) continue;
-      Object accessors = descs->GetStrongValue(i);
+      Tagged<Object> accessors = descs->GetStrongValue(i);
       if (!IsAccessorInfo(accessors)) continue;
       if (!AccessorInfo::cast(accessors)->all_can_read()) continue;
     }
 
-    Name key = descs->GetKey(i);
+    Tagged<Name> key = descs->GetKey(i);
     if (skip_symbols == IsSymbol(key)) {
       if (first_skipped == -1) first_skipped = i.as_int();
       continue;
@@ -847,7 +848,7 @@ void CommonCopyEnumKeysTo(Isolate* isolate, Handle<Dictionary> dictionary,
 
   AllowGarbageCollection allow_gc;
   for (InternalIndex i : dictionary->IterateEntries()) {
-    Object key;
+    Tagged<Object> key;
     if (!dictionary->ToKey(roots, i, &key)) continue;
     bool is_shadowing_key = false;
     if (IsSymbol(key)) continue;
@@ -897,7 +898,7 @@ void CopyEnumKeysTo(Isolate* isolate, Handle<Dictionary> dictionary,
 
   DisallowGarbageCollection no_gc;
   Dictionary raw_dictionary = *dictionary;
-  FixedArray raw_storage = *storage;
+  Tagged<FixedArray> raw_storage = *storage;
   EnumIndexComparator<Dictionary> cmp(raw_dictionary);
   // Use AtomicSlot wrapper to ensure that std::sort uses atomic load and
   // store operations that are safe for concurrent marking.
@@ -905,7 +906,7 @@ void CopyEnumKeysTo(Isolate* isolate, Handle<Dictionary> dictionary,
   std::sort(start, start + length, cmp);
   for (int i = 0; i < length; i++) {
     InternalIndex index(Smi::ToInt(raw_storage->get(i)));
-    raw_storage.set(i, raw_dictionary.NameAt(index));
+    raw_storage->set(i, raw_dictionary.NameAt(index));
   }
 }
 
@@ -955,7 +956,7 @@ ExceptionStatus CollectKeysFromDictionary(Handle<Dictionary> dictionary,
   {
     DisallowGarbageCollection no_gc;
     for (InternalIndex i : dictionary->IterateEntries()) {
-      Object key;
+      Tagged<Object> key;
       Dictionary raw_dictionary = *dictionary;
       if (!raw_dictionary.ToKey(roots, i, &key)) continue;
       if (Object::FilterKey(key, filter)) continue;
@@ -968,7 +969,7 @@ ExceptionStatus CollectKeysFromDictionary(Handle<Dictionary> dictionary,
       }
       if (filter & ONLY_ALL_CAN_READ) {
         if (details.kind() != PropertyKind::kAccessor) continue;
-        Object accessors = raw_dictionary.ValueAt(i);
+        Tagged<Object> accessors = raw_dictionary.ValueAt(i);
         if (!IsAccessorInfo(accessors)) continue;
         if (!AccessorInfo::cast(accessors)->all_can_read()) continue;
       }
@@ -991,7 +992,7 @@ ExceptionStatus CollectKeysFromDictionary(Handle<Dictionary> dictionary,
   bool has_seen_symbol = false;
   for (int i = 0; i < array_size; i++) {
     InternalIndex index(Smi::ToInt(array->get(i)));
-    Object key = dictionary->NameAt(index);
+    Tagged<Object> key = dictionary->NameAt(index);
     if (IsSymbol(key)) {
       has_seen_symbol = true;
       continue;
@@ -1002,7 +1003,7 @@ ExceptionStatus CollectKeysFromDictionary(Handle<Dictionary> dictionary,
   if (has_seen_symbol) {
     for (int i = 0; i < array_size; i++) {
       InternalIndex index(Smi::ToInt(array->get(i)));
-      Object key = dictionary->NameAt(index);
+      Tagged<Object> key = dictionary->NameAt(index);
       if (!IsSymbol(key)) continue;
       ExceptionStatus status = keys->AddKey(key, DO_NOT_CONVERT);
       if (!status) return status;
@@ -1021,7 +1022,7 @@ Maybe<bool> KeyAccumulator::CollectOwnPropertyNames(Handle<JSReceiver> receiver,
       enum_keys = KeyAccumulator::GetOwnEnumPropertyKeys(isolate_, object);
       // If the number of properties equals the length of enumerable properties
       // we do not have to filter out non-enumerable ones
-      Map map = object->map();
+      Tagged<Map> map = object->map();
       int nof_descriptors = map->NumberOfOwnDescriptors();
       if (enum_keys->length() != nof_descriptors) {
         if (map->prototype(isolate_) != ReadOnlyRoots(isolate_).null_value()) {
@@ -1152,7 +1153,8 @@ Maybe<bool> KeyAccumulator::CollectOwnKeys(Handle<JSReceiver> receiver,
     Handle<AccessCheckInfo> access_check_info;
     {
       DisallowGarbageCollection no_gc;
-      AccessCheckInfo maybe_info = AccessCheckInfo::Get(isolate_, object);
+      Tagged<AccessCheckInfo> maybe_info =
+          AccessCheckInfo::Get(isolate_, object);
       if (!maybe_info.is_null()) {
         access_check_info = handle(maybe_info, isolate_);
       }
@@ -1344,7 +1346,7 @@ Maybe<bool> KeyAccumulator::CollectOwnJSProxyKeys(Handle<JSReceiver> receiver,
   // 18. (Done in step 9)
   // 19. Repeat, for each key that is an element of targetNonconfigurableKeys:
   for (int i = 0; i < nonconfigurable_keys_length; ++i) {
-    Object raw_key = target_nonconfigurable_keys->get(i);
+    Tagged<Object> raw_key = target_nonconfigurable_keys->get(i);
     Handle<Name> key(Name::cast(raw_key), isolate_);
     // 19a. If key is not an element of uncheckedResultKeys, throw a
     //      TypeError exception.
@@ -1364,7 +1366,7 @@ Maybe<bool> KeyAccumulator::CollectOwnJSProxyKeys(Handle<JSReceiver> receiver,
   }
   // 21. Repeat, for each key that is an element of targetConfigurableKeys:
   for (int i = 0; i < target_configurable_keys->length(); ++i) {
-    Object raw_key = target_configurable_keys->get(i);
+    Tagged<Object> raw_key = target_configurable_keys->get(i);
     if (IsSmi(raw_key)) continue;  // Zapped entry, was nonconfigurable.
     Handle<Name> key(Name::cast(raw_key), isolate_);
     // 21a. If key is not an element of uncheckedResultKeys, throw a
diff --git a/src/objects/keys.h b/src/objects/keys.h
index 031c407620b..5ff6fe85dc6 100644
--- a/src/objects/keys.h
+++ b/src/objects/keys.h
@@ -74,7 +74,7 @@ class KeyAccumulator final {
                                                    Handle<JSObject> object);
 
   V8_WARN_UNUSED_RESULT ExceptionStatus
-  AddKey(Object key, AddKeyConversion convert = DO_NOT_CONVERT);
+  AddKey(Tagged<Object> key, AddKeyConversion convert = DO_NOT_CONVERT);
   V8_WARN_UNUSED_RESULT ExceptionStatus
   AddKey(Handle<Object> key, AddKeyConversion convert = DO_NOT_CONVERT);
 
@@ -89,7 +89,7 @@ class KeyAccumulator final {
   void set_skip_indices(bool value) { skip_indices_ = value; }
   // Shadowing keys are used to filter keys. This happens when non-enumerable
   // keys appear again on the prototype chain.
-  void AddShadowingKey(Object key, AllowGarbageCollection* allow_gc);
+  void AddShadowingKey(Tagged<Object> key, AllowGarbageCollection* allow_gc);
   void AddShadowingKey(Handle<Object> key);
 
  private:
@@ -222,7 +222,7 @@ class FastKeyAccumulator {
 
   MaybeHandle<FixedArray> GetOwnKeysWithUninitializedEnumLength();
 
-  bool MayHaveElements(JSReceiver receiver);
+  bool MayHaveElements(Tagged<JSReceiver> receiver);
   bool TryPrototypeInfoCache(Handle<JSReceiver> receiver);
 
   Isolate* isolate_;
diff --git a/src/objects/literal-objects-inl.h b/src/objects/literal-objects-inl.h
index 65dbc37b928..a99adc9d973 100644
--- a/src/objects/literal-objects-inl.h
+++ b/src/objects/literal-objects-inl.h
@@ -28,31 +28,31 @@ CAST_ACCESSOR(ObjectBoilerplateDescription)
 SMI_ACCESSORS(ObjectBoilerplateDescription, flags,
               FixedArray::OffsetOfElementAt(kLiteralTypeOffset))
 
-Object ObjectBoilerplateDescription::name(int index) const {
+Tagged<Object> ObjectBoilerplateDescription::name(int index) const {
   PtrComprCageBase cage_base = GetPtrComprCageBase(*this);
   return name(cage_base, index);
 }
 
-Object ObjectBoilerplateDescription::name(PtrComprCageBase cage_base,
-                                          int index) const {
+Tagged<Object> ObjectBoilerplateDescription::name(PtrComprCageBase cage_base,
+                                                  int index) const {
   // get() already checks for out of bounds access, but we do not want to allow
   // access to the last element, if it is the number of properties.
   DCHECK_NE(size(), index);
   return get(cage_base, 2 * index + kDescriptionStartIndex);
 }
 
-Object ObjectBoilerplateDescription::value(int index) const {
+Tagged<Object> ObjectBoilerplateDescription::value(int index) const {
   PtrComprCageBase cage_base = GetPtrComprCageBase(*this);
   return value(cage_base, index);
 }
 
-Object ObjectBoilerplateDescription::value(PtrComprCageBase cage_base,
-                                           int index) const {
+Tagged<Object> ObjectBoilerplateDescription::value(PtrComprCageBase cage_base,
+                                                   int index) const {
   return get(cage_base, 2 * index + 1 + kDescriptionStartIndex);
 }
 
-void ObjectBoilerplateDescription::set_key_value(int index, Object key,
-                                                 Object value) {
+void ObjectBoilerplateDescription::set_key_value(int index, Tagged<Object> key,
+                                                 Tagged<Object> value) {
   DCHECK_LT(index, size());
   DCHECK_GE(index, 0);
   set(2 * index + kDescriptionStartIndex, key);
diff --git a/src/objects/literal-objects.cc b/src/objects/literal-objects.cc
index 1a5225c3da7..04f314bdd85 100644
--- a/src/objects/literal-objects.cc
+++ b/src/objects/literal-objects.cc
@@ -76,8 +76,9 @@ void AddToDescriptorArrayTemplate(
     } else {
       DCHECK(value_kind == ClassBoilerplate::kGetter ||
              value_kind == ClassBoilerplate::kSetter);
-      Object raw_accessor = descriptor_array_template->GetStrongValue(entry);
-      AccessorPair pair;
+      Tagged<Object> raw_accessor =
+          descriptor_array_template->GetStrongValue(entry);
+      Tagged<AccessorPair> pair;
       if (IsAccessorPair(raw_accessor)) {
         pair = AccessorPair::cast(raw_accessor);
       } else {
@@ -147,7 +148,7 @@ constexpr int ComputeEnumerationIndex(int value_index) {
 
 constexpr int kAccessorNotDefined = -1;
 
-inline int GetExistingValueIndex(Object value) {
+inline int GetExistingValueIndex(Tagged<Object> value) {
   return IsSmi(value) ? Smi::ToInt(value) : kAccessorNotDefined;
 }
 
@@ -155,7 +156,7 @@ template <typename IsolateT, typename Dictionary, typename Key>
 void AddToDictionaryTemplate(IsolateT* isolate, Handle<Dictionary> dictionary,
                              Key key, int key_index,
                              ClassBoilerplate::ValueKind value_kind,
-                             Smi value) {
+                             Tagged<Smi> value) {
   InternalIndex entry = dictionary->FindEntry(isolate, key);
 
   const bool is_elements_dictionary =
@@ -204,11 +205,11 @@ void AddToDictionaryTemplate(IsolateT* isolate, Handle<Dictionary> dictionary,
             ? kDummyEnumerationIndex
             : ComputeEnumerationIndex(key_index);
 
-    Object existing_value = dictionary->ValueAt(entry);
+    Tagged<Object> existing_value = dictionary->ValueAt(entry);
     if (value_kind == ClassBoilerplate::kData) {
       // Computed value is a normal method.
       if (IsAccessorPair(existing_value)) {
-        AccessorPair current_pair = AccessorPair::cast(existing_value);
+        Tagged<AccessorPair> current_pair = AccessorPair::cast(existing_value);
 
         int existing_getter_index =
             GetExistingValueIndex(current_pair->getter());
@@ -236,7 +237,7 @@ void AddToDictionaryTemplate(IsolateT* isolate, Handle<Dictionary> dictionary,
           // and then it was overwritten by the current computed method which
           // in turn was later overwritten by the setter method. So we clear
           // the getter.
-          current_pair.set_getter(*isolate->factory()->null_value());
+          current_pair->set_getter(*isolate->factory()->null_value());
 
         } else if (existing_setter_index != kAccessorNotDefined &&
                    existing_setter_index < key_index) {
@@ -245,7 +246,7 @@ void AddToDictionaryTemplate(IsolateT* isolate, Handle<Dictionary> dictionary,
           // and then it was overwritten by the current computed method which
           // in turn was later overwritten by the getter method. So we clear
           // the setter.
-          current_pair.set_setter(*isolate->factory()->null_value());
+          current_pair->set_setter(*isolate->factory()->null_value());
 
         } else {
           // One of the following cases holds:
@@ -309,7 +310,7 @@ void AddToDictionaryTemplate(IsolateT* isolate, Handle<Dictionary> dictionary,
       AccessorComponent component = ToAccessorComponent(value_kind);
       if (IsAccessorPair(existing_value)) {
         // Update respective component of existing AccessorPair.
-        AccessorPair current_pair = AccessorPair::cast(existing_value);
+        Tagged<AccessorPair> current_pair = AccessorPair::cast(existing_value);
 
         int existing_component_index =
             GetExistingValueIndex(current_pair->get(component));
@@ -478,7 +479,7 @@ class ObjectDescriptor {
   void AddNamedProperty(IsolateT* isolate, Handle<Name> name,
                         ClassBoilerplate::ValueKind value_kind,
                         int value_index) {
-    Smi value = Smi::FromInt(value_index);
+    Tagged<Smi> value = Smi::FromInt(value_index);
     if (HasDictionaryProperties()) {
       UpdateNextEnumerationIndex(value_index);
       if (V8_ENABLE_SWISS_NAME_DICTIONARY_BOOL) {
@@ -499,7 +500,7 @@ class ObjectDescriptor {
   void AddIndexedProperty(IsolateT* isolate, uint32_t element,
                           ClassBoilerplate::ValueKind value_kind,
                           int value_index) {
-    Smi value = Smi::FromInt(value_index);
+    Tagged<Smi> value = Smi::FromInt(value_index);
     AddToDictionaryTemplate(isolate, elements_dictionary_template_, element,
                             value_index, value_kind, value);
   }
@@ -560,33 +561,33 @@ class ObjectDescriptor {
 template <typename IsolateT, typename PropertyDict>
 void ClassBoilerplate::AddToPropertiesTemplate(
     IsolateT* isolate, Handle<PropertyDict> dictionary, Handle<Name> name,
-    int key_index, ClassBoilerplate::ValueKind value_kind, Smi value) {
+    int key_index, ClassBoilerplate::ValueKind value_kind, Tagged<Smi> value) {
   AddToDictionaryTemplate(isolate, dictionary, name, key_index, value_kind,
                           value);
 }
 template void ClassBoilerplate::AddToPropertiesTemplate(
     Isolate* isolate, Handle<NameDictionary> dictionary, Handle<Name> name,
-    int key_index, ClassBoilerplate::ValueKind value_kind, Smi value);
+    int key_index, ClassBoilerplate::ValueKind value_kind, Tagged<Smi> value);
 template void ClassBoilerplate::AddToPropertiesTemplate(
     LocalIsolate* isolate, Handle<NameDictionary> dictionary, Handle<Name> name,
-    int key_index, ClassBoilerplate::ValueKind value_kind, Smi value);
+    int key_index, ClassBoilerplate::ValueKind value_kind, Tagged<Smi> value);
 template void ClassBoilerplate::AddToPropertiesTemplate(
     Isolate* isolate, Handle<SwissNameDictionary> dictionary, Handle<Name> name,
-    int key_index, ClassBoilerplate::ValueKind value_kind, Smi value);
+    int key_index, ClassBoilerplate::ValueKind value_kind, Tagged<Smi> value);
 
 template <typename IsolateT>
 void ClassBoilerplate::AddToElementsTemplate(
     IsolateT* isolate, Handle<NumberDictionary> dictionary, uint32_t key,
-    int key_index, ClassBoilerplate::ValueKind value_kind, Smi value) {
+    int key_index, ClassBoilerplate::ValueKind value_kind, Tagged<Smi> value) {
   AddToDictionaryTemplate(isolate, dictionary, key, key_index, value_kind,
                           value);
 }
 template void ClassBoilerplate::AddToElementsTemplate(
     Isolate* isolate, Handle<NumberDictionary> dictionary, uint32_t key,
-    int key_index, ClassBoilerplate::ValueKind value_kind, Smi value);
+    int key_index, ClassBoilerplate::ValueKind value_kind, Tagged<Smi> value);
 template void ClassBoilerplate::AddToElementsTemplate(
     LocalIsolate* isolate, Handle<NumberDictionary> dictionary, uint32_t key,
-    int key_index, ClassBoilerplate::ValueKind value_kind, Smi value);
+    int key_index, ClassBoilerplate::ValueKind value_kind, Tagged<Smi> value);
 
 template <typename IsolateT>
 Handle<ClassBoilerplate> ClassBoilerplate::BuildClassBoilerplate(
diff --git a/src/objects/literal-objects.h b/src/objects/literal-objects.h
index f30ae88c75a..988e3ec1053 100644
--- a/src/objects/literal-objects.h
+++ b/src/objects/literal-objects.h
@@ -28,13 +28,14 @@ class StructBodyDescriptor;
 // TODO(ishell): Don't derive from FixedArray as it already has its own map.
 class ObjectBoilerplateDescription : public FixedArray {
  public:
-  inline Object name(int index) const;
-  inline Object name(PtrComprCageBase cage_base, int index) const;
+  inline Tagged<Object> name(int index) const;
+  inline Tagged<Object> name(PtrComprCageBase cage_base, int index) const;
 
-  inline Object value(int index) const;
-  inline Object value(PtrComprCageBase cage_base, int index) const;
+  inline Tagged<Object> value(int index) const;
+  inline Tagged<Object> value(PtrComprCageBase cage_base, int index) const;
 
-  inline void set_key_value(int index, Object key, Object value);
+  inline void set_key_value(int index, Tagged<Object> key,
+                            Tagged<Object> value);
 
   // The number of boilerplate properties.
   inline int size() const;
@@ -130,13 +131,13 @@ class ClassBoilerplate : public FixedArray {
   static void AddToPropertiesTemplate(IsolateT* isolate,
                                       Handle<Dictionary> dictionary,
                                       Handle<Name> name, int key_index,
-                                      ValueKind value_kind, Smi value);
+                                      ValueKind value_kind, Tagged<Smi> value);
 
   template <typename IsolateT>
   static void AddToElementsTemplate(IsolateT* isolate,
                                     Handle<NumberDictionary> dictionary,
                                     uint32_t key, int key_index,
-                                    ValueKind value_kind, Smi value);
+                                    ValueKind value_kind, Tagged<Smi> value);
 
   template <typename IsolateT>
   static Handle<ClassBoilerplate> BuildClassBoilerplate(IsolateT* isolate,
diff --git a/src/objects/lookup-cache-inl.h b/src/objects/lookup-cache-inl.h
index a3f0f1d6ded..9865f712e19 100644
--- a/src/objects/lookup-cache-inl.h
+++ b/src/objects/lookup-cache-inl.h
@@ -13,7 +13,7 @@ namespace v8 {
 namespace internal {
 
 // static
-int DescriptorLookupCache::Hash(Map source, Name name) {
+int DescriptorLookupCache::Hash(Tagged<Map> source, Tagged<Name> name) {
   DCHECK(IsUniqueName(name));
   // Uses only lower 32 bits if pointers are larger.
   uint32_t source_hash = static_cast<uint32_t>(source.ptr()) >> kTaggedSizeLog2;
@@ -21,7 +21,7 @@ int DescriptorLookupCache::Hash(Map source, Name name) {
   return (source_hash ^ name_hash) % kLength;
 }
 
-int DescriptorLookupCache::Lookup(Map source, Name name) {
+int DescriptorLookupCache::Lookup(Tagged<Map> source, Tagged<Name> name) {
   int index = Hash(source, name);
   Key& key = keys_[index];
   // Pointers in the table might be stale, so use SafeEquals.
@@ -31,7 +31,8 @@ int DescriptorLookupCache::Lookup(Map source, Name name) {
   return kAbsent;
 }
 
-void DescriptorLookupCache::Update(Map source, Name name, int result) {
+void DescriptorLookupCache::Update(Tagged<Map> source, Tagged<Name> name,
+                                   int result) {
   DCHECK_NE(result, kAbsent);
   int index = Hash(source, name);
   Key& key = keys_[index];
diff --git a/src/objects/lookup-cache.h b/src/objects/lookup-cache.h
index 4aa3c5a588f..0949ec90ddb 100644
--- a/src/objects/lookup-cache.h
+++ b/src/objects/lookup-cache.h
@@ -22,10 +22,10 @@ class DescriptorLookupCache {
   DescriptorLookupCache& operator=(const DescriptorLookupCache&) = delete;
   // Lookup descriptor index for (map, name).
   // If absent, kAbsent is returned.
-  inline int Lookup(Map source, Name name);
+  inline int Lookup(Tagged<Map> source, Tagged<Name> name);
 
   // Update an element in the cache.
-  inline void Update(Map source, Name name, int result);
+  inline void Update(Tagged<Map> source, Tagged<Name> name, int result);
 
   // Clear the cache.
   void Clear();
@@ -41,7 +41,7 @@ class DescriptorLookupCache {
     }
   }
 
-  static inline int Hash(Map source, Name name);
+  static inline int Hash(Tagged<Map> source, Tagged<Name> name);
 
   static const int kLength = 64;
   struct Key {
diff --git a/src/objects/lookup-inl.h b/src/objects/lookup-inl.h
index 184bc5a90a0..da1e5936256 100644
--- a/src/objects/lookup-inl.h
+++ b/src/objects/lookup-inl.h
@@ -222,7 +222,7 @@ PropertyKey LookupIterator::GetKey() const {
   return PropertyKey(isolate_, name_, index_);
 }
 
-bool LookupIterator::IsElement(JSReceiver object) const {
+bool LookupIterator::IsElement(Tagged<JSReceiver> object) const {
   return index_ <= JSObject::kMaxElementIndex ||
          (index_ != kInvalidIndex &&
           object->map()->has_any_typed_array_or_wasm_array_elements());
@@ -339,7 +339,7 @@ template <class T>
 Handle<T> LookupIterator::GetStoreTarget() const {
   DCHECK(IsJSReceiver(*receiver_, isolate_));
   if (IsJSGlobalProxy(*receiver_, isolate_)) {
-    HeapObject prototype =
+    Tagged<HeapObject> prototype =
         JSGlobalProxy::cast(*receiver_)->map(isolate_)->prototype(isolate_);
     if (IsJSGlobalObject(prototype, isolate_)) {
       return handle(JSGlobalObject::cast(prototype), isolate_);
@@ -349,7 +349,8 @@ Handle<T> LookupIterator::GetStoreTarget() const {
 }
 
 template <bool is_element>
-InterceptorInfo LookupIterator::GetInterceptor(JSObject holder) const {
+Tagged<InterceptorInfo> LookupIterator::GetInterceptor(
+    Tagged<JSObject> holder) const {
   if (is_element && index_ <= JSObject::kMaxElementIndex) {
     return holder->GetIndexedInterceptor(isolate_);
   } else {
@@ -359,9 +360,10 @@ InterceptorInfo LookupIterator::GetInterceptor(JSObject holder) const {
 
 inline Handle<InterceptorInfo> LookupIterator::GetInterceptor() const {
   DCHECK_EQ(INTERCEPTOR, state_);
-  JSObject holder = JSObject::cast(*holder_);
-  InterceptorInfo result = IsElement(holder) ? GetInterceptor<true>(holder)
-                                             : GetInterceptor<false>(holder);
+  Tagged<JSObject> holder = JSObject::cast(*holder_);
+  Tagged<InterceptorInfo> result = IsElement(holder)
+                                       ? GetInterceptor<true>(holder)
+                                       : GetInterceptor<false>(holder);
   return handle(result, isolate_);
 }
 
diff --git a/src/objects/lookup.cc b/src/objects/lookup.cc
index e16419291a0..cad4cb17bbb 100644
--- a/src/objects/lookup.cc
+++ b/src/objects/lookup.cc
@@ -63,8 +63,8 @@ void LookupIterator::Start() {
     has_property_ = false;
     state_ = NOT_FOUND;
 
-    JSReceiver holder = *holder_;
-    Map map = holder->map(isolate_);
+    Tagged<JSReceiver> holder = *holder_;
+    Tagged<Map> map = holder->map(isolate_);
 
     state_ = LookupInHolder<is_element>(map, holder);
     if (IsFound()) return;
@@ -82,8 +82,8 @@ void LookupIterator::Next() {
   DisallowGarbageCollection no_gc;
   has_property_ = false;
 
-  JSReceiver holder = *holder_;
-  Map map = holder->map(isolate_);
+  Tagged<JSReceiver> holder = *holder_;
+  Tagged<Map> map = holder->map(isolate_);
 
   if (IsSpecialReceiverMap(map)) {
     state_ = IsElement() ? LookupInSpecialHolder<true>(map, holder)
@@ -96,9 +96,9 @@ void LookupIterator::Next() {
 }
 
 template <bool is_element>
-void LookupIterator::NextInternal(Map map, JSReceiver holder) {
+void LookupIterator::NextInternal(Tagged<Map> map, Tagged<JSReceiver> holder) {
   do {
-    JSReceiver maybe_holder = NextHolder(map);
+    Tagged<JSReceiver> maybe_holder = NextHolder(map);
     if (maybe_holder.is_null()) {
       if (interceptor_state_ == InterceptorState::kSkipNonMasking) {
         RestartLookupForNonMaskingInterceptors<is_element>();
@@ -400,14 +400,15 @@ void LookupIterator::PrepareForDataProperty(Handle<Object> value) {
         // Therefore, we need to perform the necessary updates to the property
         // details and the prototype validity cell directly.
         if constexpr (V8_ENABLE_SWISS_NAME_DICTIONARY_BOOL) {
-          SwissNameDictionary dict = holder->property_dictionary_swiss();
+          Tagged<SwissNameDictionary> dict =
+              holder->property_dictionary_swiss();
           dict->DetailsAtPut(dictionary_entry(), property_details_);
         } else {
-          NameDictionary dict = holder->property_dictionary();
+          Tagged<NameDictionary> dict = holder->property_dictionary();
           dict->DetailsAtPut(dictionary_entry(), property_details_);
         }
 
-        Map old_map = holder->map(isolate_);
+        Tagged<Map> old_map = holder->map(isolate_);
         if (old_map->is_prototype_map()) {
           JSObject::InvalidatePrototypeChains(old_map);
         }
@@ -452,10 +453,11 @@ void LookupIterator::PrepareForDataProperty(Handle<Object> value) {
         property_details_.CopyWithConstness(PropertyConstness::kMutable);
 
     if constexpr (V8_ENABLE_SWISS_NAME_DICTIONARY_BOOL) {
-      SwissNameDictionary dict = holder_obj->property_dictionary_swiss();
+      Tagged<SwissNameDictionary> dict =
+          holder_obj->property_dictionary_swiss();
       dict->DetailsAtPut(dictionary_entry(), property_details_);
     } else {
-      NameDictionary dict = holder_obj->property_dictionary();
+      Tagged<NameDictionary> dict = holder_obj->property_dictionary();
       dict->DetailsAtPut(dictionary_entry(), property_details_);
     }
 
@@ -832,7 +834,7 @@ void LookupIterator::TransitionToAccessorPair(Handle<Object> pair,
     receiver->RequireSlowElements(*dictionary);
 
     if (receiver->HasSlowArgumentsElements(isolate_)) {
-      SloppyArgumentsElements parameter_map =
+      Tagged<SloppyArgumentsElements> parameter_map =
           SloppyArgumentsElements::cast(receiver->elements(isolate_));
       uint32_t length = parameter_map->length();
       if (number_.is_found() && number_.as_uint32() < length) {
@@ -882,7 +884,7 @@ bool LookupIterator::HolderIsReceiverOrHiddenPrototype() const {
 
 Handle<Object> LookupIterator::FetchValue(
     AllocationPolicy allocation_policy) const {
-  Object result;
+  Tagged<Object> result;
   DCHECK(!IsWasmObject(*holder_));
   if (IsElement(*holder_)) {
     Handle<JSObject> holder = GetHolder<JSObject>();
@@ -919,7 +921,7 @@ Handle<Object> LookupIterator::FetchValue(
   return handle(result, isolate_);
 }
 
-bool LookupIterator::CanStayConst(Object value) const {
+bool LookupIterator::CanStayConst(Tagged<Object> value) const {
   DCHECK(!holder_.is_null());
   DCHECK(!IsElement(*holder_));
   DCHECK(holder_->HasFastProperties(isolate_));
@@ -937,7 +939,8 @@ bool LookupIterator::CanStayConst(Object value) const {
   if (property_details_.representation().IsDouble()) {
     if (!IsNumber(value, isolate_)) return false;
     uint64_t bits;
-    Object current_value = holder->RawFastPropertyAt(isolate_, field_index);
+    Tagged<Object> current_value =
+        holder->RawFastPropertyAt(isolate_, field_index);
     DCHECK(IsHeapNumber(current_value, isolate_));
     bits = HeapNumber::cast(current_value)->value_as_bits(kRelaxedLoad);
     // Use bit representation of double to check for hole double, since
@@ -949,11 +952,12 @@ bool LookupIterator::CanStayConst(Object value) const {
     return bits == kHoleNanInt64;
   }
 
-  Object current_value = holder->RawFastPropertyAt(isolate_, field_index);
+  Tagged<Object> current_value =
+      holder->RawFastPropertyAt(isolate_, field_index);
   return IsUninitialized(current_value, isolate());
 }
 
-bool LookupIterator::DictCanStayConst(Object value) const {
+bool LookupIterator::DictCanStayConst(Tagged<Object> value) const {
   DCHECK(!holder_.is_null());
   DCHECK(!IsElement(*holder_));
   DCHECK(!holder_->HasFastProperties(isolate_));
@@ -970,12 +974,12 @@ bool LookupIterator::DictCanStayConst(Object value) const {
     return true;
   }
   Handle<JSReceiver> holder = GetHolder<JSReceiver>();
-  Object current_value;
+  Tagged<Object> current_value;
   if constexpr (V8_ENABLE_SWISS_NAME_DICTIONARY_BOOL) {
-    SwissNameDictionary dict = holder->property_dictionary_swiss();
+    Tagged<SwissNameDictionary> dict = holder->property_dictionary_swiss();
     current_value = dict->ValueAt(dictionary_entry());
   } else {
-    NameDictionary dict = holder->property_dictionary();
+    Tagged<NameDictionary> dict = holder->property_dictionary();
     current_value = dict->ValueAt(dictionary_entry());
   }
 
@@ -1079,10 +1083,11 @@ void LookupIterator::WriteDataValue(Handle<Object> value,
     // PropertyCell::PrepareForAndSetValue already wrote the value into the
     // cell.
 #ifdef DEBUG
-    GlobalDictionary dictionary =
+    Tagged<GlobalDictionary> dictionary =
         JSGlobalObject::cast(*holder)->global_dictionary(isolate_,
                                                          kAcquireLoad);
-    PropertyCell cell = dictionary->CellAt(isolate_, dictionary_entry());
+    Tagged<PropertyCell> cell =
+        dictionary->CellAt(isolate_, dictionary_entry());
     DCHECK(cell->value() == *value ||
            (IsString(cell->value()) && IsString(*value) &&
             String::cast(cell->value())->Equals(String::cast(*value))));
@@ -1096,11 +1101,11 @@ void LookupIterator::WriteDataValue(Handle<Object> value,
         IsJSProxy(*holder, isolate_) || DictCanStayConst(*value));
 
     if constexpr (V8_ENABLE_SWISS_NAME_DICTIONARY_BOOL) {
-      SwissNameDictionary dictionary =
+      Tagged<SwissNameDictionary> dictionary =
           holder->property_dictionary_swiss(isolate_);
       dictionary->ValueAtPut(dictionary_entry(), *value);
     } else {
-      NameDictionary dictionary = holder->property_dictionary(isolate_);
+      Tagged<NameDictionary> dictionary = holder->property_dictionary(isolate_);
       dictionary->ValueAtPut(dictionary_entry(), *value);
     }
   }
@@ -1173,8 +1178,8 @@ Handle<Object> LookupIterator::CompareAndSwapDataValue(Handle<Object> expected,
 }
 
 template <bool is_element>
-bool LookupIterator::SkipInterceptor(JSObject holder) {
-  InterceptorInfo info = GetInterceptor<is_element>(holder);
+bool LookupIterator::SkipInterceptor(Tagged<JSObject> holder) {
+  Tagged<InterceptorInfo> info = GetInterceptor<is_element>(holder);
   if (!is_element && IsSymbol(*name_, isolate_) &&
       !info->can_intercept_symbols()) {
     return true;
@@ -1193,7 +1198,7 @@ bool LookupIterator::SkipInterceptor(JSObject holder) {
   return interceptor_state_ == InterceptorState::kProcessNonMasking;
 }
 
-JSReceiver LookupIterator::NextHolder(Map map) {
+Tagged<JSReceiver> LookupIterator::NextHolder(Tagged<Map> map) {
   DisallowGarbageCollection no_gc;
   if (map->prototype(isolate_) == ReadOnlyRoots(isolate_).null_value()) {
     return JSReceiver();
@@ -1204,7 +1209,8 @@ JSReceiver LookupIterator::NextHolder(Map map) {
   return JSReceiver::cast(map->prototype(isolate_));
 }
 
-LookupIterator::State LookupIterator::NotFound(JSReceiver const holder) const {
+LookupIterator::State LookupIterator::NotFound(
+    Tagged<JSReceiver> const holder) const {
   if (!IsJSTypedArray(holder, isolate_)) return NOT_FOUND;
   if (IsElement()) return INTEGER_INDEXED_EXOTIC;
   if (!IsString(*name_, isolate_)) return NOT_FOUND;
@@ -1215,7 +1221,7 @@ LookupIterator::State LookupIterator::NotFound(JSReceiver const holder) const {
 namespace {
 
 template <bool is_element>
-bool HasInterceptor(Map map, size_t index) {
+bool HasInterceptor(Tagged<Map> map, size_t index) {
   if (is_element) {
     if (index > JSObject::kMaxElementIndex) {
       // There is currently no way to install interceptors on an object with
@@ -1233,7 +1239,7 @@ bool HasInterceptor(Map map, size_t index) {
 
 template <bool is_element>
 LookupIterator::State LookupIterator::LookupInSpecialHolder(
-    Map const map, JSReceiver const holder) {
+    Tagged<Map> const map, Tagged<JSReceiver> const holder) {
   static_assert(INTERCEPTOR == BEFORE_PROPERTY);
   switch (state_) {
     case NOT_FOUND:
@@ -1257,11 +1263,12 @@ LookupIterator::State LookupIterator::LookupInSpecialHolder(
       V8_FALLTHROUGH;
     case INTERCEPTOR:
       if (IsJSGlobalObjectMap(map) && !is_js_array_element(is_element)) {
-        GlobalDictionary dict = JSGlobalObject::cast(holder)->global_dictionary(
-            isolate_, kAcquireLoad);
+        Tagged<GlobalDictionary> dict =
+            JSGlobalObject::cast(holder)->global_dictionary(isolate_,
+                                                            kAcquireLoad);
         number_ = dict->FindEntry(isolate(), name_);
         if (number_.is_not_found()) return NOT_FOUND;
-        PropertyCell cell = dict->CellAt(isolate_, number_);
+        Tagged<PropertyCell> cell = dict->CellAt(isolate_, number_);
         if (IsPropertyCellHole(cell->value(isolate_), isolate_)) {
           return NOT_FOUND;
         }
@@ -1289,16 +1296,16 @@ LookupIterator::State LookupIterator::LookupInSpecialHolder(
 
 template <bool is_element>
 LookupIterator::State LookupIterator::LookupInRegularHolder(
-    Map const map, JSReceiver const holder) {
+    Tagged<Map> const map, Tagged<JSReceiver> const holder) {
   DisallowGarbageCollection no_gc;
   if (interceptor_state_ == InterceptorState::kProcessNonMasking) {
     return NOT_FOUND;
   }
   DCHECK(!IsWasmObject(holder, isolate_));
   if (is_element && IsElement(holder)) {
-    JSObject js_object = JSObject::cast(holder);
+    Tagged<JSObject> js_object = JSObject::cast(holder);
     ElementsAccessor* accessor = js_object->GetElementsAccessor(isolate_);
-    FixedArrayBase backing_store = js_object->elements(isolate_);
+    Tagged<FixedArrayBase> backing_store = js_object->elements(isolate_);
     number_ =
         accessor->GetEntryForIndex(isolate_, js_object, backing_store, index_);
     if (number_.is_not_found()) {
@@ -1312,19 +1319,20 @@ LookupIterator::State LookupIterator::LookupInRegularHolder(
       property_details_ = property_details_.CopyAddAttributes(SEALED);
     }
   } else if (!map->is_dictionary_map()) {
-    DescriptorArray descriptors = map->instance_descriptors(isolate_);
+    Tagged<DescriptorArray> descriptors = map->instance_descriptors(isolate_);
     number_ = descriptors->SearchWithCache(isolate_, *name_, map);
     if (number_.is_not_found()) return NotFound(holder);
     property_details_ = descriptors->GetDetails(number_);
   } else {
     DCHECK_IMPLIES(IsJSProxy(holder, isolate_), name()->IsPrivate(isolate_));
     if constexpr (V8_ENABLE_SWISS_NAME_DICTIONARY_BOOL) {
-      SwissNameDictionary dict = holder->property_dictionary_swiss(isolate_);
+      Tagged<SwissNameDictionary> dict =
+          holder->property_dictionary_swiss(isolate_);
       number_ = dict->FindEntry(isolate(), *name_);
       if (number_.is_not_found()) return NotFound(holder);
       property_details_ = dict->DetailsAt(number_);
     } else {
-      NameDictionary dict = holder->property_dictionary(isolate_);
+      Tagged<NameDictionary> dict = holder->property_dictionary(isolate_);
       number_ = dict->FindEntry(isolate(), name_);
       if (number_.is_not_found()) return NotFound(holder);
       property_details_ = dict->DetailsAt(number_);
@@ -1350,15 +1358,15 @@ Handle<InterceptorInfo> LookupIterator::GetInterceptorForFailedAccessCheck()
   }
 
   DisallowGarbageCollection no_gc;
-  AccessCheckInfo access_check_info =
+  Tagged<AccessCheckInfo> access_check_info =
       AccessCheckInfo::Get(isolate_, Handle<JSObject>::cast(holder_));
   if (!access_check_info.is_null()) {
     // There is currently no way to create objects with typed array elements
     // and access checks.
     DCHECK(!holder_->map()->has_typed_array_or_rab_gsab_typed_array_elements());
-    Object interceptor = is_js_array_element(IsElement())
-                             ? access_check_info->indexed_interceptor()
-                             : access_check_info->named_interceptor();
+    Tagged<Object> interceptor = is_js_array_element(IsElement())
+                                     ? access_check_info->indexed_interceptor()
+                                     : access_check_info->named_interceptor();
     if (interceptor != Object()) {
       return handle(InterceptorInfo::cast(interceptor), isolate_);
     }
@@ -1389,7 +1397,7 @@ bool LookupIterator::LookupCachedProperty(Handle<AccessorPair> accessor_pair) {
   DCHECK_EQ(state(), LookupIterator::ACCESSOR);
   DCHECK(IsAccessorPair(*GetAccessors(), isolate_));
 
-  Object getter = accessor_pair->getter(isolate_);
+  Tagged<Object> getter = accessor_pair->getter(isolate_);
   base::Optional<Name> maybe_name =
       FunctionTemplateInfo::TryGetCachedPropertyName(isolate(), getter);
   if (!maybe_name.has_value()) return false;
@@ -1412,9 +1420,9 @@ bool LookupIterator::LookupCachedProperty(Handle<AccessorPair> accessor_pair) {
 }
 
 // static
-base::Optional<Object> ConcurrentLookupIterator::TryGetOwnCowElement(
-    Isolate* isolate, FixedArray array_elements, ElementsKind elements_kind,
-    int array_length, size_t index) {
+base::Optional<Tagged<Object>> ConcurrentLookupIterator::TryGetOwnCowElement(
+    Isolate* isolate, Tagged<FixedArray> array_elements,
+    ElementsKind elements_kind, int array_length, size_t index) {
   DisallowGarbageCollection no_gc;
 
   CHECK_EQ(array_elements->map(), ReadOnlyRoots(isolate).fixed_cow_array_map());
@@ -1437,7 +1445,7 @@ base::Optional<Object> ConcurrentLookupIterator::TryGetOwnCowElement(
   if (index >= static_cast<size_t>(array_length)) return {};
   if (index >= static_cast<size_t>(array_elements->length())) return {};
 
-  Object result = array_elements->get(isolate, static_cast<int>(index));
+  Tagged<Object> result = array_elements->get(isolate, static_cast<int>(index));
 
   //  ______________________________________
   // ( Filter out holes irrespective of the )
@@ -1458,9 +1466,9 @@ base::Optional<Object> ConcurrentLookupIterator::TryGetOwnCowElement(
 // static
 ConcurrentLookupIterator::Result
 ConcurrentLookupIterator::TryGetOwnConstantElement(
-    Object* result_out, Isolate* isolate, LocalIsolate* local_isolate,
-    JSObject holder, FixedArrayBase elements, ElementsKind elements_kind,
-    size_t index) {
+    Tagged<Object>* result_out, Isolate* isolate, LocalIsolate* local_isolate,
+    Tagged<JSObject> holder, Tagged<FixedArrayBase> elements,
+    ElementsKind elements_kind, size_t index) {
   DisallowGarbageCollection no_gc;
 
   DCHECK_LE(index, JSObject::kMaxElementIndex);
@@ -1482,11 +1490,12 @@ ConcurrentLookupIterator::TryGetOwnConstantElement(
 
   if (IsFrozenElementsKind(elements_kind)) {
     if (!IsFixedArray(elements)) return kGaveUp;
-    FixedArray elements_fixed_array = FixedArray::cast(elements);
+    Tagged<FixedArray> elements_fixed_array = FixedArray::cast(elements);
     if (index >= static_cast<uint32_t>(elements_fixed_array->length())) {
       return kGaveUp;
     }
-    Object result = elements_fixed_array->get(isolate, static_cast<int>(index));
+    Tagged<Object> result =
+        elements_fixed_array->get(isolate, static_cast<int>(index));
     if (IsHoleyElementsKindForRead(elements_kind) &&
         result == ReadOnlyRoots(isolate).the_hole_value()) {
       return kNotPresent;
@@ -1505,10 +1514,10 @@ ConcurrentLookupIterator::TryGetOwnConstantElement(
     // In this case we don't care about the actual `elements`. All in-bounds
     // reads are redirected to the wrapped String.
 
-    JSPrimitiveWrapper js_value = JSPrimitiveWrapper::cast(holder);
-    String wrapped_string = String::cast(js_value->value());
+    Tagged<JSPrimitiveWrapper> js_value = JSPrimitiveWrapper::cast(holder);
+    Tagged<String> wrapped_string = String::cast(js_value->value());
     return ConcurrentLookupIterator::TryGetOwnChar(
-        static_cast<String*>(result_out), isolate, local_isolate,
+        reinterpret_cast<Tagged<String>*>(result_out), isolate, local_isolate,
         wrapped_string, index);
   } else {
     DCHECK(!IsFrozenElementsKind(elements_kind));
@@ -1522,13 +1531,13 @@ ConcurrentLookupIterator::TryGetOwnConstantElement(
 
 // static
 ConcurrentLookupIterator::Result ConcurrentLookupIterator::TryGetOwnChar(
-    String* result_out, Isolate* isolate, LocalIsolate* local_isolate,
-    String string, size_t index) {
+    Tagged<String>* result_out, Isolate* isolate, LocalIsolate* local_isolate,
+    Tagged<String> string, size_t index) {
   DisallowGarbageCollection no_gc;
   // The access guard below protects string accesses related to internalized
   // strings.
   // TODO(jgruber): Support other string kinds.
-  Map string_map = string->map(isolate, kAcquireLoad);
+  Tagged<Map> string_map = string->map(isolate, kAcquireLoad);
   InstanceType type = string_map->instance_type();
   if (!(InstanceTypeChecker::IsInternalizedString(type) ||
         InstanceTypeChecker::IsThinString(type))) {
@@ -1547,8 +1556,9 @@ ConcurrentLookupIterator::Result ConcurrentLookupIterator::TryGetOwnChar(
 
   if (charcode > unibrow::Latin1::kMaxChar) return kGaveUp;
 
-  Object value = isolate->factory()->single_character_string_table()->get(
-      charcode, kRelaxedLoad);
+  Tagged<Object> value =
+      isolate->factory()->single_character_string_table()->get(charcode,
+                                                               kRelaxedLoad);
 
   DCHECK_NE(value, ReadOnlyRoots(isolate).undefined_value());
 
@@ -1557,23 +1567,26 @@ ConcurrentLookupIterator::Result ConcurrentLookupIterator::TryGetOwnChar(
 }
 
 // static
-base::Optional<PropertyCell> ConcurrentLookupIterator::TryGetPropertyCell(
-    Isolate* isolate, LocalIsolate* local_isolate,
-    Handle<JSGlobalObject> holder, Handle<Name> name) {
+base::Optional<Tagged<PropertyCell>>
+ConcurrentLookupIterator::TryGetPropertyCell(Isolate* isolate,
+                                             LocalIsolate* local_isolate,
+                                             Handle<JSGlobalObject> holder,
+                                             Handle<Name> name) {
   DisallowGarbageCollection no_gc;
 
-  Map holder_map = holder->map();
+  Tagged<Map> holder_map = holder->map();
   if (holder_map->is_access_check_needed()) return {};
   if (holder_map->has_named_interceptor()) return {};
 
-  GlobalDictionary dict = holder->global_dictionary(kAcquireLoad);
-  base::Optional<PropertyCell> cell =
+  Tagged<GlobalDictionary> dict = holder->global_dictionary(kAcquireLoad);
+  base::Optional<Tagged<PropertyCell>> maybe_cell =
       dict->TryFindPropertyCellForConcurrentLookupIterator(isolate, name,
                                                            kRelaxedLoad);
-  if (!cell.has_value()) return {};
+  if (!maybe_cell.has_value()) return {};
+  Tagged<PropertyCell> cell = maybe_cell.value();
 
   if (cell->property_details(kAcquireLoad).kind() == PropertyKind::kAccessor) {
-    Object maybe_accessor_pair = cell->value(kAcquireLoad);
+    Tagged<Object> maybe_accessor_pair = cell->value(kAcquireLoad);
     if (!IsAccessorPair(maybe_accessor_pair)) return {};
 
     base::Optional<Name> maybe_cached_property_name =
@@ -1582,15 +1595,16 @@ base::Optional<PropertyCell> ConcurrentLookupIterator::TryGetPropertyCell(
                          ->getter(isolate, kAcquireLoad));
     if (!maybe_cached_property_name.has_value()) return {};
 
-    cell = dict->TryFindPropertyCellForConcurrentLookupIterator(
+    maybe_cell = dict->TryFindPropertyCellForConcurrentLookupIterator(
         isolate, handle(*maybe_cached_property_name, local_isolate),
         kRelaxedLoad);
-    if (!cell.has_value()) return {};
+    if (!maybe_cell.has_value()) return {};
+    cell = maybe_cell.value();
     if (cell->property_details(kAcquireLoad).kind() != PropertyKind::kData)
       return {};
   }
 
-  DCHECK(cell.has_value());
+  DCHECK(maybe_cell.has_value());
   DCHECK_EQ(cell->property_details(kAcquireLoad).kind(), PropertyKind::kData);
   return cell;
 }
diff --git a/src/objects/lookup.h b/src/objects/lookup.h
index d55a5544015..4ee8b47f688 100644
--- a/src/objects/lookup.h
+++ b/src/objects/lookup.h
@@ -128,7 +128,7 @@ class V8_EXPORT_PRIVATE LookupIterator final {
   // Returns true if this LookupIterator has an index that counts as an
   // element for the given object (up to kMaxArrayIndex for JSArrays,
   // any integer for JSTypedArrays).
-  inline bool IsElement(JSReceiver object) const;
+  inline bool IsElement(Tagged<JSReceiver> object) const;
 
   inline bool IsPrivateName() const;
 
@@ -248,7 +248,7 @@ class V8_EXPORT_PRIVATE LookupIterator final {
 
   Handle<Map> GetReceiverMap() const;
 
-  V8_WARN_UNUSED_RESULT inline JSReceiver NextHolder(Map map);
+  V8_WARN_UNUSED_RESULT inline Tagged<JSReceiver> NextHolder(Tagged<Map> map);
 
   bool is_js_array_element(bool is_element) const {
     return is_element && index_ <= JSArray::kMaxArrayIndex;
@@ -256,17 +256,17 @@ class V8_EXPORT_PRIVATE LookupIterator final {
   template <bool is_element>
   V8_EXPORT_PRIVATE void Start();
   template <bool is_element>
-  void NextInternal(Map map, JSReceiver holder);
+  void NextInternal(Tagged<Map> map, Tagged<JSReceiver> holder);
   template <bool is_element>
-  inline State LookupInHolder(Map map, JSReceiver holder) {
+  inline State LookupInHolder(Tagged<Map> map, Tagged<JSReceiver> holder) {
     return IsSpecialReceiverMap(map)
                ? LookupInSpecialHolder<is_element>(map, holder)
                : LookupInRegularHolder<is_element>(map, holder);
   }
   template <bool is_element>
-  State LookupInRegularHolder(Map map, JSReceiver holder);
+  State LookupInRegularHolder(Tagged<Map> map, Tagged<JSReceiver> holder);
   template <bool is_element>
-  State LookupInSpecialHolder(Map map, JSReceiver holder);
+  State LookupInSpecialHolder(Tagged<Map> map, Tagged<JSReceiver> holder);
   template <bool is_element>
   void RestartLookupForNonMaskingInterceptors() {
     RestartInternal<is_element>(InterceptorState::kProcessNonMasking);
@@ -275,8 +275,8 @@ class V8_EXPORT_PRIVATE LookupIterator final {
   void RestartInternal(InterceptorState interceptor_state);
   Handle<Object> FetchValue(AllocationPolicy allocation_policy =
                                 AllocationPolicy::kAllocationAllowed) const;
-  bool CanStayConst(Object value) const;
-  bool DictCanStayConst(Object value) const;
+  bool CanStayConst(Tagged<Object> value) const;
+  bool DictCanStayConst(Tagged<Object> value) const;
 
   Handle<Object> CompareAndSwapInternal(Handle<Object> desired,
                                         Handle<Object> value,
@@ -286,9 +286,9 @@ class V8_EXPORT_PRIVATE LookupIterator final {
   void ReloadPropertyInformation();
 
   template <bool is_element>
-  bool SkipInterceptor(JSObject holder);
+  bool SkipInterceptor(Tagged<JSObject> holder);
   template <bool is_element>
-  inline InterceptorInfo GetInterceptor(JSObject holder) const;
+  inline Tagged<InterceptorInfo> GetInterceptor(Tagged<JSObject> holder) const;
 
   bool check_interceptor() const {
     return (configuration_ & kInterceptor) != 0;
@@ -307,7 +307,7 @@ class V8_EXPORT_PRIVATE LookupIterator final {
       Isolate* isolate, Handle<Object> lookup_start_object, size_t index,
       Configuration configuration);
 
-  State NotFound(JSReceiver const holder) const;
+  State NotFound(Tagged<JSReceiver> const holder) const;
 
   // If configuration_ becomes mutable, update
   // HolderIsReceiverOrHiddenPrototype.
@@ -354,33 +354,34 @@ class ConcurrentLookupIterator final : public AllStatic {
   // consistent among themselves (e.g. the elements kind may not match the
   // given elements backing store). We are thus extra-careful to handle
   // exceptional situations.
-  V8_EXPORT_PRIVATE static base::Optional<Object> TryGetOwnCowElement(
-      Isolate* isolate, FixedArray array_elements, ElementsKind elements_kind,
-      int array_length, size_t index);
+  V8_EXPORT_PRIVATE static base::Optional<Tagged<Object>> TryGetOwnCowElement(
+      Isolate* isolate, Tagged<FixedArray> array_elements,
+      ElementsKind elements_kind, int array_length, size_t index);
 
   // As above, the contract is that the elements and elements kind should be
   // read from the same holder, but this function is implemented defensively to
   // tolerate concurrency issues.
   V8_EXPORT_PRIVATE static Result TryGetOwnConstantElement(
-      Object* result_out, Isolate* isolate, LocalIsolate* local_isolate,
-      JSObject holder, FixedArrayBase elements, ElementsKind elements_kind,
-      size_t index);
+      Tagged<Object>* result_out, Isolate* isolate, LocalIsolate* local_isolate,
+      Tagged<JSObject> holder, Tagged<FixedArrayBase> elements,
+      ElementsKind elements_kind, size_t index);
 
   // Implements the own data property lookup for the specialized case of
   // strings.
-  V8_EXPORT_PRIVATE static Result TryGetOwnChar(String* result_out,
+  V8_EXPORT_PRIVATE static Result TryGetOwnChar(Tagged<String>* result_out,
                                                 Isolate* isolate,
                                                 LocalIsolate* local_isolate,
-                                                String string, size_t index);
+                                                Tagged<String> string,
+                                                size_t index);
 
   // This method reimplements the following sequence in a concurrent setting:
   //
   // LookupIterator it(holder, isolate, name, LookupIterator::OWN);
   // it.TryLookupCachedProperty();
   // if (it.state() == LookupIterator::DATA) it.GetPropertyCell();
-  V8_EXPORT_PRIVATE static base::Optional<PropertyCell> TryGetPropertyCell(
-      Isolate* isolate, LocalIsolate* local_isolate,
-      Handle<JSGlobalObject> holder, Handle<Name> name);
+  V8_EXPORT_PRIVATE static base::Optional<Tagged<PropertyCell>>
+  TryGetPropertyCell(Isolate* isolate, LocalIsolate* local_isolate,
+                     Handle<JSGlobalObject> holder, Handle<Name> name);
 };
 
 }  // namespace internal
diff --git a/src/objects/managed.h b/src/objects/managed.h
index 3f3c7dfad02..80d18e64ca3 100644
--- a/src/objects/managed.h
+++ b/src/objects/managed.h
@@ -64,10 +64,10 @@ class Managed : public Foreign {
   // Get a reference to the shared pointer to the C++ object.
   V8_INLINE const std::shared_ptr<CppType>& get() { return *GetSharedPtrPtr(); }
 
-  static Tagged<Managed> cast(Object obj) {
+  static Tagged<Managed> cast(Tagged<Object> obj) {
     return Tagged<Managed>(Managed(obj.ptr()).ptr());
   }
-  static constexpr Tagged<Managed> unchecked_cast(Object obj) {
+  static constexpr Tagged<Managed> unchecked_cast(Tagged<Object> obj) {
     return Tagged<Managed>(obj.ptr());
   }
 
diff --git a/src/objects/map-inl.h b/src/objects/map-inl.h
index f18305dd370..4856ecb7a8d 100644
--- a/src/objects/map-inl.h
+++ b/src/objects/map-inl.h
@@ -61,8 +61,9 @@ ACCESSORS_CHECKED2(Map, prototype, Tagged<HeapObject>, kPrototypeOffset, true,
                          value->map()->is_prototype_map())))
 
 DEF_GETTER(Map, prototype_info, Tagged<Object>) {
-  Object value = TaggedField<Object, kTransitionsOrPrototypeInfoOffset>::load(
-      cage_base, *this);
+  Tagged<Object> value =
+      TaggedField<Object, kTransitionsOrPrototypeInfoOffset>::load(cage_base,
+                                                                   *this);
   DCHECK(this->is_prototype_map());
   return value;
 }
@@ -70,7 +71,7 @@ RELEASE_ACQUIRE_ACCESSORS(Map, prototype_info, Tagged<Object>,
                           kTransitionsOrPrototypeInfoOffset)
 
 void Map::init_prototype_and_constructor_or_back_pointer(ReadOnlyRoots roots) {
-  HeapObject null = roots.null_value();
+  Tagged<HeapObject> null = roots.null_value();
   TaggedField<HeapObject,
               kConstructorOrBackPointerOrNativeContextOffset>::store(*this,
                                                                      null);
@@ -129,24 +130,24 @@ BIT_FIELD_ACCESSORS(Map, relaxed_bit_field3, construction_counter,
 
 DEF_GETTER(Map, GetNamedInterceptor, Tagged<InterceptorInfo>) {
   DCHECK(has_named_interceptor());
-  FunctionTemplateInfo info = GetFunctionTemplateInfo(cage_base);
+  Tagged<FunctionTemplateInfo> info = GetFunctionTemplateInfo(cage_base);
   return InterceptorInfo::cast(info->GetNamedPropertyHandler(cage_base));
 }
 
 DEF_GETTER(Map, GetIndexedInterceptor, Tagged<InterceptorInfo>) {
   DCHECK(has_indexed_interceptor());
-  FunctionTemplateInfo info = GetFunctionTemplateInfo(cage_base);
+  Tagged<FunctionTemplateInfo> info = GetFunctionTemplateInfo(cage_base);
   return InterceptorInfo::cast(info->GetIndexedPropertyHandler(cage_base));
 }
 
 // static
 bool Map::IsMostGeneralFieldType(Representation representation,
-                                 FieldType field_type) {
+                                 Tagged<FieldType> field_type) {
   return !representation.IsHeapObject() || IsAny(field_type);
 }
 
 // static
-bool Map::FieldTypeIsCleared(Representation rep, FieldType type) {
+bool Map::FieldTypeIsCleared(Representation rep, Tagged<FieldType> type) {
   return IsNone(type) && rep.IsHeapObject();
 }
 
@@ -189,7 +190,7 @@ Handle<Map> Map::Normalize(Isolate* isolate, Handle<Map> fast_map,
                    kUseCache, reason);
 }
 
-bool Map::EquivalentToForNormalization(const Map other,
+bool Map::EquivalentToForNormalization(const Tagged<Map> other,
                                        PropertyNormalizationMode mode) const {
   return EquivalentToForNormalization(other, elements_kind(), mode);
 }
@@ -213,7 +214,7 @@ bool Map::TooManyFastProperties(StoreOrigin store_origin) const {
   }
 }
 
-Name Map::GetLastDescriptorName(Isolate* isolate) const {
+Tagged<Name> Map::GetLastDescriptorName(Isolate* isolate) const {
   return instance_descriptors(isolate)->GetKey(LastAdded());
 }
 
@@ -257,7 +258,7 @@ void Map::SetEnumLength(int length) {
   set_relaxed_bit_field3(Bits3::EnumLengthBits::update(bit_field3(), length));
 }
 
-FixedArrayBase Map::GetInitialElements() const {
+Tagged<FixedArrayBase> Map::GetInitialElements() const {
   Tagged<FixedArrayBase> result;
   if (has_fast_elements() || has_fast_string_wrapper_elements() ||
       has_any_nonextensible_elements()) {
@@ -448,13 +449,13 @@ void Map::SetOutOfObjectUnusedPropertyFields(int value) {
   DCHECK_EQ(value, UnusedPropertyFields());
 }
 
-void Map::CopyUnusedPropertyFields(Map map) {
+void Map::CopyUnusedPropertyFields(Tagged<Map> map) {
   set_used_or_unused_instance_size_in_words(
       map->used_or_unused_instance_size_in_words());
   DCHECK_EQ(UnusedPropertyFields(), map->UnusedPropertyFields());
 }
 
-void Map::CopyUnusedPropertyFieldsAdjustedForInstanceSize(Map map) {
+void Map::CopyUnusedPropertyFieldsAdjustedForInstanceSize(Tagged<Map> map) {
   int value = map->used_or_unused_instance_size_in_words();
   if (value >= JSPrimitiveWrapper::kFieldsAdded) {
     // Unused in-object fields. Adjust the offset from the object’s start
@@ -584,9 +585,9 @@ bool Map::has_prototype_info() const {
   return PrototypeInfo::IsPrototypeInfoFast(prototype_info());
 }
 
-bool Map::TryGetPrototypeInfo(PrototypeInfo* result) const {
+bool Map::TryGetPrototypeInfo(Tagged<PrototypeInfo>* result) const {
   DCHECK(is_prototype_map());
-  Object maybe_proto_info = prototype_info();
+  Tagged<Object> maybe_proto_info = prototype_info();
   if (!PrototypeInfo::IsPrototypeInfoFast(maybe_proto_info)) return false;
   *result = PrototypeInfo::cast(maybe_proto_info);
   return true;
@@ -739,12 +740,14 @@ bool IsPrimitiveMap(Tagged<Map> map) {
   return map->instance_type() <= LAST_PRIMITIVE_HEAP_OBJECT_TYPE;
 }
 
-void Map::UpdateDescriptors(Isolate* isolate, DescriptorArray descriptors,
+void Map::UpdateDescriptors(Isolate* isolate,
+                            Tagged<DescriptorArray> descriptors,
                             int number_of_own_descriptors) {
   SetInstanceDescriptors(isolate, descriptors, number_of_own_descriptors);
 }
 
-void Map::InitializeDescriptors(Isolate* isolate, DescriptorArray descriptors) {
+void Map::InitializeDescriptors(Isolate* isolate,
+                                Tagged<DescriptorArray> descriptors) {
   SetInstanceDescriptors(isolate, descriptors,
                          descriptors->number_of_descriptors());
 }
@@ -757,7 +760,7 @@ void Map::clear_padding() {
 }
 
 void Map::AppendDescriptor(Isolate* isolate, Descriptor* desc) {
-  DescriptorArray descriptors = instance_descriptors(isolate);
+  Tagged<DescriptorArray> descriptors = instance_descriptors(isolate);
   int number_of_own_descriptors = NumberOfOwnDescriptors();
   DCHECK(descriptors->number_of_descriptors() == number_of_own_descriptors);
   {
@@ -794,7 +797,7 @@ bool Map::ConcurrentIsMap(PtrComprCageBase cage_base,
 }
 
 DEF_GETTER(Map, GetBackPointer, Tagged<HeapObject>) {
-  Map back_pointer;
+  Tagged<Map> back_pointer;
   if (TryGetBackPointer(cage_base, &back_pointer)) {
     return back_pointer;
   }
@@ -802,8 +805,8 @@ DEF_GETTER(Map, GetBackPointer, Tagged<HeapObject>) {
 }
 
 bool Map::TryGetBackPointer(PtrComprCageBase cage_base,
-                            Map* back_pointer) const {
-  Object object = constructor_or_back_pointer(cage_base, kRelaxedLoad);
+                            Tagged<Map>* back_pointer) const {
+  Tagged<Object> object = constructor_or_back_pointer(cage_base, kRelaxedLoad);
   if (ConcurrentIsMap(cage_base, object)) {
     *back_pointer = Map::cast(object);
     return true;
@@ -811,7 +814,7 @@ bool Map::TryGetBackPointer(PtrComprCageBase cage_base,
   return false;
 }
 
-void Map::SetBackPointer(HeapObject value, WriteBarrierMode mode) {
+void Map::SetBackPointer(Tagged<HeapObject> value, WriteBarrierMode mode) {
   CHECK_GE(instance_type(), FIRST_JS_RECEIVER_TYPE);
   CHECK(IsMap(value));
   CHECK(IsUndefined(GetBackPointer()));
@@ -821,13 +824,14 @@ void Map::SetBackPointer(HeapObject value, WriteBarrierMode mode) {
 }
 
 // static
-Map Map::GetMapFor(ReadOnlyRoots roots, InstanceType type) {
+Tagged<Map> Map::GetMapFor(ReadOnlyRoots roots, InstanceType type) {
   RootIndex map_idx = TryGetMapRootIdxFor(type).value();
   return Map::unchecked_cast(roots.object_at(map_idx));
 }
 
 // static
-Map Map::ElementsTransitionMap(Isolate* isolate, ConcurrencyMode cmode) {
+Tagged<Map> Map::ElementsTransitionMap(Isolate* isolate,
+                                       ConcurrencyMode cmode) {
   return TransitionsAccessor(isolate, *this, IsConcurrent(cmode))
       .SearchSpecial(ReadOnlyRoots(isolate).elements_transition_symbol());
 }
@@ -857,18 +861,18 @@ ACCESSORS_CHECKED(Map, wasm_type_info, Tagged<WasmTypeInfo>,
 #endif  // V8_ENABLE_WEBASSEMBLY
 
 bool Map::IsPrototypeValidityCellValid() const {
-  Object validity_cell = prototype_validity_cell(kRelaxedLoad);
+  Tagged<Object> validity_cell = prototype_validity_cell(kRelaxedLoad);
   if (IsSmi(validity_cell)) {
     // Smi validity cells should always be considered valid.
     DCHECK_EQ(Smi::cast(validity_cell).value(), Map::kPrototypeChainValid);
     return true;
   }
-  Smi cell_value = Smi::cast(Cell::cast(validity_cell)->value());
+  Tagged<Smi> cell_value = Smi::cast(Cell::cast(validity_cell)->value());
   return cell_value == Smi::FromInt(Map::kPrototypeChainValid);
 }
 
 DEF_GETTER(Map, GetConstructorRaw, Tagged<Object>) {
-  Object maybe_constructor = constructor_or_back_pointer(cage_base);
+  Tagged<Object> maybe_constructor = constructor_or_back_pointer(cage_base);
   // Follow any back pointers.
   while (ConcurrentIsMap(cage_base, maybe_constructor)) {
     maybe_constructor =
@@ -879,19 +883,19 @@ DEF_GETTER(Map, GetConstructorRaw, Tagged<Object>) {
 
 DEF_GETTER(Map, GetNonInstancePrototype, Tagged<Object>) {
   DCHECK(has_non_instance_prototype());
-  Object raw_constructor = GetConstructorRaw(cage_base);
+  Tagged<Object> raw_constructor = GetConstructorRaw(cage_base);
   CHECK(IsTuple2(raw_constructor));
   // Get prototype from the {constructor, non-instance_prototype} tuple.
-  Tuple2 non_instance_prototype_constructor_tuple =
+  Tagged<Tuple2> non_instance_prototype_constructor_tuple =
       Tuple2::cast(raw_constructor);
-  Object result = non_instance_prototype_constructor_tuple->value2();
+  Tagged<Object> result = non_instance_prototype_constructor_tuple->value2();
   DCHECK(!IsJSReceiver(result));
   DCHECK(!IsFunctionTemplateInfo(result));
   return result;
 }
 
 DEF_GETTER(Map, GetConstructor, Tagged<Object>) {
-  Object maybe_constructor = GetConstructorRaw(cage_base);
+  Tagged<Object> maybe_constructor = GetConstructorRaw(cage_base);
   if (IsTuple2(maybe_constructor)) {
     // Get constructor from the {constructor, non-instance_prototype} tuple.
     maybe_constructor = Tuple2::cast(maybe_constructor)->value1();
@@ -899,8 +903,8 @@ DEF_GETTER(Map, GetConstructor, Tagged<Object>) {
   return maybe_constructor;
 }
 
-Object Map::TryGetConstructor(Isolate* isolate, int max_steps) {
-  Object maybe_constructor = constructor_or_back_pointer(isolate);
+Tagged<Object> Map::TryGetConstructor(Isolate* isolate, int max_steps) {
+  Tagged<Object> maybe_constructor = constructor_or_back_pointer(isolate);
   // Follow any back pointers.
   while (IsMap(maybe_constructor, isolate)) {
     if (max_steps-- == 0) return Smi::FromInt(0);
@@ -915,9 +919,10 @@ Object Map::TryGetConstructor(Isolate* isolate, int max_steps) {
 }
 
 DEF_GETTER(Map, GetFunctionTemplateInfo, Tagged<FunctionTemplateInfo>) {
-  Object constructor = GetConstructor(cage_base);
+  Tagged<Object> constructor = GetConstructor(cage_base);
   if (IsJSFunction(constructor, cage_base)) {
-    SharedFunctionInfo sfi = JSFunction::cast(constructor)->shared(cage_base);
+    Tagged<SharedFunctionInfo> sfi =
+        JSFunction::cast(constructor)->shared(cage_base);
     DCHECK(sfi->IsApiFunction());
     return sfi->api_func_data();
   }
@@ -925,7 +930,7 @@ DEF_GETTER(Map, GetFunctionTemplateInfo, Tagged<FunctionTemplateInfo>) {
   return FunctionTemplateInfo::cast(constructor);
 }
 
-void Map::SetConstructor(Object constructor, WriteBarrierMode mode) {
+void Map::SetConstructor(Tagged<Object> constructor, WriteBarrierMode mode) {
   // Never overwrite a back pointer with a constructor.
   CHECK(!IsMap(constructor_or_back_pointer()));
   // Constructor field must contain {constructor, non-instance_prototype} tuple
diff --git a/src/objects/map-updater.cc b/src/objects/map-updater.cc
index 9c204911dcf..5c71ed95b3f 100644
--- a/src/objects/map-updater.cc
+++ b/src/objects/map-updater.cc
@@ -23,7 +23,7 @@ namespace internal {
 
 namespace {
 
-inline bool EqualImmutableValues(Object obj1, Object obj2) {
+inline bool EqualImmutableValues(Tagged<Object> obj1, Tagged<Object> obj2) {
   if (obj1 == obj2) return true;  // Valid for both kData and kAccessor kinds.
   // TODO(ishell): compare AccessorPairs.
   return false;
@@ -54,7 +54,7 @@ void PrintGeneralization(
     MaybeHandle<Object> new_value) {
   OFStream os(file);
   os << "[generalizing]";
-  Name name = map->instance_descriptors(isolate)->GetKey(modify_index);
+  Tagged<Name> name = map->instance_descriptors(isolate)->GetKey(modify_index);
   if (IsString(name)) {
     String::cast(name)->PrintOn(file);
   } else {
@@ -104,7 +104,7 @@ MapUpdater::MapUpdater(Isolate* isolate, Handle<Map> old_map)
       !IsFunctionTemplateInfo(old_map->FindRootMap(isolate)->GetConstructor()));
 }
 
-Name MapUpdater::GetKey(InternalIndex descriptor) const {
+Tagged<Name> MapUpdater::GetKey(InternalIndex descriptor) const {
   return old_descriptors_->GetKey(descriptor);
 }
 
@@ -129,7 +129,7 @@ PropertyDetails MapUpdater::GetDetails(InternalIndex descriptor) const {
   return old_descriptors_->GetDetails(descriptor);
 }
 
-Object MapUpdater::GetValue(InternalIndex descriptor) const {
+Tagged<Object> MapUpdater::GetValue(InternalIndex descriptor) const {
   DCHECK(descriptor.is_found());
   if (descriptor == modified_descriptor_) {
     DCHECK_EQ(PropertyLocation::kDescriptor, new_location_);
@@ -139,7 +139,7 @@ Object MapUpdater::GetValue(InternalIndex descriptor) const {
   return old_descriptors_->GetStrongValue(descriptor);
 }
 
-FieldType MapUpdater::GetFieldType(InternalIndex descriptor) const {
+Tagged<FieldType> MapUpdater::GetFieldType(InternalIndex descriptor) const {
   DCHECK(descriptor.is_found());
   if (descriptor == modified_descriptor_) {
     DCHECK_EQ(PropertyLocation::kField, new_location_);
@@ -288,24 +288,24 @@ Handle<Map> MapUpdater::UpdateImpl() {
 namespace {
 
 struct IntegrityLevelTransitionInfo {
-  explicit IntegrityLevelTransitionInfo(Map map)
+  explicit IntegrityLevelTransitionInfo(Tagged<Map> map)
       : integrity_level_source_map(map) {}
 
   bool has_integrity_level_transition = false;
   PropertyAttributes integrity_level = NONE;
-  Map integrity_level_source_map;
-  Symbol integrity_level_symbol;
+  Tagged<Map> integrity_level_source_map;
+  Tagged<Symbol> integrity_level_symbol;
 };
 
 IntegrityLevelTransitionInfo DetectIntegrityLevelTransitions(
-    Map map, Isolate* isolate, DisallowGarbageCollection* no_gc,
+    Tagged<Map> map, Isolate* isolate, DisallowGarbageCollection* no_gc,
     ConcurrencyMode cmode) {
   IntegrityLevelTransitionInfo info(map);
 
   // Figure out the most restrictive integrity level transition (it should
   // be the last one in the transition tree).
   DCHECK(!map->is_extensible());
-  Map previous = Map::cast(map->GetBackPointer(isolate));
+  Tagged<Map> previous = Map::cast(map->GetBackPointer(isolate));
   TransitionsAccessor last_transitions(isolate, previous, IsConcurrent(cmode));
   if (!last_transitions.HasIntegrityLevelTransitionTo(
           map, &info.integrity_level_symbol, &info.integrity_level)) {
@@ -318,7 +318,7 @@ IntegrityLevelTransitionInfo DetectIntegrityLevelTransitions(
     return info;
   }
 
-  Map source_map = previous;
+  Tagged<Map> source_map = previous;
   // Now walk up the back pointer chain and skip all integrity level
   // transitions. If we encounter any non-integrity level transition interleaved
   // with integrity level transitions, just bail out.
@@ -342,14 +342,16 @@ IntegrityLevelTransitionInfo DetectIntegrityLevelTransitions(
 }  // namespace
 
 // static
-base::Optional<Map> MapUpdater::TryUpdateNoLock(Isolate* isolate, Map old_map,
+base::Optional<Map> MapUpdater::TryUpdateNoLock(Isolate* isolate,
+                                                Tagged<Map> old_map,
                                                 ConcurrencyMode cmode) {
   DisallowGarbageCollection no_gc;
 
   // Check the state of the root map.
-  Map root_map = old_map->FindRootMap(isolate);
+  Tagged<Map> root_map = old_map->FindRootMap(isolate);
   if (root_map->is_deprecated()) {
-    JSFunction constructor = JSFunction::cast(root_map->GetConstructor());
+    Tagged<JSFunction> constructor =
+        JSFunction::cast(root_map->GetConstructor());
     DCHECK(constructor->has_initial_map());
     DCHECK(constructor->initial_map()->is_dictionary_map());
     if (constructor->initial_map()->elements_kind() !=
@@ -387,7 +389,7 @@ base::Optional<Map> MapUpdater::TryUpdateNoLock(Isolate* isolate, Map old_map,
   }
 
   // Replay the transitions as they were before the integrity level transition.
-  Map result = root_map->TryReplayPropertyTransitions(
+  Tagged<Map> result = root_map->TryReplayPropertyTransitions(
       isolate, info.integrity_level_source_map, cmode);
   if (result.is_null()) return {};
 
@@ -424,7 +426,7 @@ MapUpdater::State MapUpdater::Normalize(const char* reason) {
 
 // static
 void MapUpdater::CompleteInobjectSlackTracking(Isolate* isolate,
-                                               Map initial_map) {
+                                               Tagged<Map> initial_map) {
   // Has to be an initial map.
   DCHECK(IsUndefined(initial_map->GetBackPointer(), isolate));
 
@@ -435,7 +437,7 @@ void MapUpdater::CompleteInobjectSlackTracking(Isolate* isolate,
   TransitionsAccessor::TraverseCallback callback;
   if (slack != 0) {
     // Resize the initial map and all maps in its transition tree.
-    callback = [slack](Map map) {
+    callback = [slack](Tagged<Map> map) {
 #ifdef DEBUG
       int old_visitor_id = Map::GetVisitorId(map);
       int new_unused = map->UnusedPropertyFields() - slack;
@@ -447,7 +449,7 @@ void MapUpdater::CompleteInobjectSlackTracking(Isolate* isolate,
     };
   } else {
     // Stop slack tracking for this map.
-    callback = [](Map map) {
+    callback = [](Tagged<Map> map) {
       map->set_construction_counter(Map::kNoSlackTracking);
     };
   }
@@ -506,7 +508,7 @@ MapUpdater::State MapUpdater::TryReconfigureToDataFieldInplace() {
              .representation()
              .Equals(new_representation_));
   DCHECK(old_descriptors_->GetFieldType(modified_descriptor_)
-             .NowIs(new_field_type_));
+             ->NowIs(new_field_type_));
 
   result_map_ = old_map_;
   state_ = kEnd;
@@ -518,7 +520,7 @@ bool MapUpdater::TrySaveIntegrityLevelTransitions() {
   // be the last one in the transition tree).
   Handle<Map> previous =
       handle(Map::cast(old_map_->GetBackPointer()), isolate_);
-  Symbol integrity_level_symbol;
+  Tagged<Symbol> integrity_level_symbol;
   TransitionsAccessor last_transitions(isolate_, *previous);
   if (!last_transitions.HasIntegrityLevelTransitionTo(
           *old_map_, &integrity_level_symbol, &integrity_level_)) {
@@ -695,7 +697,7 @@ MapUpdater::State MapUpdater::FindTargetMap() {
   if (target_nof == old_nof_) {
 #ifdef DEBUG
     if (modified_descriptor_.is_found()) {
-      DescriptorArray target_descriptors =
+      Tagged<DescriptorArray> target_descriptors =
           target_map_->instance_descriptors(isolate_);
       PropertyDetails details =
           target_descriptors->GetDetails(modified_descriptor_);
@@ -933,15 +935,16 @@ Handle<DescriptorArray> MapUpdater::BuildDescriptorArray() {
 
 Handle<Map> MapUpdater::FindSplitMap(Handle<DescriptorArray> descriptors) {
   int root_nof = root_map_->NumberOfOwnDescriptors();
-  Map current = *root_map_;
+  Tagged<Map> current = *root_map_;
   for (InternalIndex i : InternalIndex::Range(root_nof, old_nof_)) {
-    Name name = descriptors->GetKey(i);
+    Tagged<Name> name = descriptors->GetKey(i);
     PropertyDetails details = descriptors->GetDetails(i);
-    Map next =
+    Tagged<Map> next =
         TransitionsAccessor(isolate_, current)
             .SearchTransition(name, details.kind(), details.attributes());
     if (next.is_null()) break;
-    DescriptorArray next_descriptors = next->instance_descriptors(isolate_);
+    Tagged<DescriptorArray> next_descriptors =
+        next->instance_descriptors(isolate_);
 
     PropertyDetails next_details = next_descriptors->GetDetails(i);
     DCHECK_EQ(details.kind(), next_details.kind());
@@ -951,8 +954,8 @@ Handle<Map> MapUpdater::FindSplitMap(Handle<DescriptorArray> descriptors) {
     if (!details.representation().Equals(next_details.representation())) break;
 
     if (next_details.location() == PropertyLocation::kField) {
-      FieldType next_field_type = next_descriptors->GetFieldType(i);
-      if (!descriptors->GetFieldType(i).NowIs(next_field_type)) {
+      Tagged<FieldType> next_field_type = next_descriptors->GetFieldType(i);
+      if (!descriptors->GetFieldType(i)->NowIs(next_field_type)) {
         break;
       }
     } else {
@@ -1081,7 +1084,7 @@ void PrintReconfiguration(Isolate* isolate, Handle<Map> map, FILE* file,
                           PropertyAttributes attributes) {
   OFStream os(file);
   os << "[reconfiguring]";
-  Name name = map->instance_descriptors(isolate)->GetKey(modify_index);
+  Tagged<Name> name = map->instance_descriptors(isolate)->GetKey(modify_index);
   if (IsString(name)) {
     String::cast(name)->PrintOn(file);
   } else {
@@ -1142,16 +1145,17 @@ void MapUpdater::UpdateFieldType(Isolate* isolate, Handle<Map> map,
   backlog.push(*map);
 
   while (!backlog.empty()) {
-    Map current = backlog.front();
+    Tagged<Map> current = backlog.front();
     backlog.pop();
 
     TransitionsAccessor transitions(isolate, current);
     int num_transitions = transitions.NumberOfTransitions();
     for (int i = 0; i < num_transitions; ++i) {
-      Map target = transitions.GetTarget(i);
+      Tagged<Map> target = transitions.GetTarget(i);
       backlog.push(target);
     }
-    DescriptorArray descriptors = current->instance_descriptors(isolate);
+    Tagged<DescriptorArray> descriptors =
+        current->instance_descriptors(isolate);
     details = descriptors->GetDetails(descriptor);
 
     // It is allowed to change representation here only from None
diff --git a/src/objects/map-updater.h b/src/objects/map-updater.h
index 6b241a8602c..f590ed38bbb 100644
--- a/src/objects/map-updater.h
+++ b/src/objects/map-updater.h
@@ -69,9 +69,9 @@ class V8_EXPORT_PRIVATE MapUpdater {
 
   // As above but does not mutate maps; instead, we attempt to replay existing
   // transitions to find an updated map. No lock is taken.
-  static base::Optional<Map> TryUpdateNoLock(Isolate* isolate, Map old_map,
-                                             ConcurrencyMode cmode)
-      V8_WARN_UNUSED_RESULT;
+  static base::Optional<Map> TryUpdateNoLock(
+      Isolate* isolate, Tagged<Map> old_map,
+      ConcurrencyMode cmode) V8_WARN_UNUSED_RESULT;
 
   static Handle<Map> ReconfigureExistingProperty(Isolate* isolate,
                                                  Handle<Map> map,
@@ -88,7 +88,8 @@ class V8_EXPORT_PRIVATE MapUpdater {
 
   // Completes inobject slack tracking for the transition tree starting at the
   // initial map.
-  static void CompleteInobjectSlackTracking(Isolate* isolate, Map initial_map);
+  static void CompleteInobjectSlackTracking(Isolate* isolate,
+                                            Tagged<Map> initial_map);
 
  private:
   enum State {
@@ -160,7 +161,7 @@ class V8_EXPORT_PRIVATE MapUpdater {
   State Normalize(const char* reason);
 
   // Returns name of a |descriptor| property.
-  inline Name GetKey(InternalIndex descriptor) const;
+  inline Tagged<Name> GetKey(InternalIndex descriptor) const;
 
   // Returns property details of a |descriptor| in "updated" |old_descriptors_|
   // array.
@@ -168,11 +169,11 @@ class V8_EXPORT_PRIVATE MapUpdater {
 
   // Returns value of a |descriptor| with kDescriptor location in "updated"
   // |old_descriptors_| array.
-  inline Object GetValue(InternalIndex descriptor) const;
+  inline Tagged<Object> GetValue(InternalIndex descriptor) const;
 
   // Returns field type for a |descriptor| with kField location in "updated"
   // |old_descriptors_| array.
-  inline FieldType GetFieldType(InternalIndex descriptor) const;
+  inline Tagged<FieldType> GetFieldType(InternalIndex descriptor) const;
 
   // If a |descriptor| property in "updated" |old_descriptors_| has kField
   // location then returns its field type, otherwise computes the optimal field
diff --git a/src/objects/map.cc b/src/objects/map.cc
index 7726d6db458..0a16e67fd0b 100644
--- a/src/objects/map.cc
+++ b/src/objects/map.cc
@@ -32,15 +32,15 @@
 namespace v8 {
 namespace internal {
 
-Map Map::GetPrototypeChainRootMap(Isolate* isolate) const {
+Tagged<Map> Map::GetPrototypeChainRootMap(Isolate* isolate) const {
   DisallowGarbageCollection no_alloc;
   if (IsJSReceiverMap(*this)) {
     return *this;
   }
   int constructor_function_index = GetConstructorFunctionIndex();
   if (constructor_function_index != Map::kNoConstructorFunctionIndex) {
-    Context native_context = isolate->context()->native_context();
-    JSFunction constructor_function =
+    Tagged<Context> native_context = isolate->context()->native_context();
+    Tagged<JSFunction> constructor_function =
         JSFunction::cast(native_context->get(constructor_function_index));
     return constructor_function->initial_map();
   }
@@ -48,8 +48,8 @@ Map Map::GetPrototypeChainRootMap(Isolate* isolate) const {
 }
 
 // static
-base::Optional<JSFunction> Map::GetConstructorFunction(Map map,
-                                                       Context native_context) {
+base::Optional<JSFunction> Map::GetConstructorFunction(
+    Tagged<Map> map, Tagged<Context> native_context) {
   DisallowGarbageCollection no_gc;
   if (IsPrimitiveMap(map)) {
     int const constructor_function_index = map->GetConstructorFunctionIndex();
@@ -60,7 +60,7 @@ base::Optional<JSFunction> Map::GetConstructorFunction(Map map,
   return {};
 }
 
-VisitorId Map::GetVisitorId(Map map) {
+VisitorId Map::GetVisitorId(Tagged<Map> map) {
   static_assert(kVisitorIdCount <= 256);
 
   const int instance_type = map->instance_type();
@@ -432,11 +432,11 @@ MaybeObjectHandle Map::WrapFieldType(Isolate* isolate, Handle<FieldType> type) {
 }
 
 // static
-FieldType Map::UnwrapFieldType(MaybeObject wrapped_type) {
+Tagged<FieldType> Map::UnwrapFieldType(MaybeObject wrapped_type) {
   if (wrapped_type->IsCleared()) {
     return FieldType::None();
   }
-  HeapObject heap_object;
+  Tagged<HeapObject> heap_object;
   if (wrapped_type->GetHeapObjectIfWeak(&heap_object)) {
     return FieldType::cast(heap_object);
   }
@@ -497,7 +497,8 @@ MaybeHandle<Map> Map::CopyWithConstant(Isolate* isolate, Handle<Map> map,
                        PropertyConstness::kConst, representation, flag);
 }
 
-bool Map::InstancesNeedRewriting(Map target, ConcurrencyMode cmode) const {
+bool Map::InstancesNeedRewriting(Tagged<Map> target,
+                                 ConcurrencyMode cmode) const {
   int target_number_of_fields = target->NumberOfFields(cmode);
   int target_inobject = target->GetInObjectProperties();
   int target_unused = target->UnusedPropertyFields();
@@ -508,7 +509,8 @@ bool Map::InstancesNeedRewriting(Map target, ConcurrencyMode cmode) const {
                                 &old_number_of_fields, cmode);
 }
 
-bool Map::InstancesNeedRewriting(Map target, int target_number_of_fields,
+bool Map::InstancesNeedRewriting(Tagged<Map> target,
+                                 int target_number_of_fields,
                                  int target_inobject, int target_unused,
                                  int* old_number_of_fields,
                                  ConcurrencyMode cmode) const {
@@ -518,12 +520,12 @@ bool Map::InstancesNeedRewriting(Map target, int target_number_of_fields,
   if (target_number_of_fields != *old_number_of_fields) return true;
 
   // If smi descriptors were replaced by double descriptors, rewrite.
-  DescriptorArray old_desc = IsConcurrent(cmode)
-                                 ? instance_descriptors(kAcquireLoad)
-                                 : instance_descriptors();
-  DescriptorArray new_desc = IsConcurrent(cmode)
-                                 ? target->instance_descriptors(kAcquireLoad)
-                                 : target->instance_descriptors();
+  Tagged<DescriptorArray> old_desc = IsConcurrent(cmode)
+                                         ? instance_descriptors(kAcquireLoad)
+                                         : instance_descriptors();
+  Tagged<DescriptorArray> new_desc =
+      IsConcurrent(cmode) ? target->instance_descriptors(kAcquireLoad)
+                          : target->instance_descriptors();
   for (InternalIndex i : IterateOwnDescriptors()) {
     if (new_desc->GetDetails(i).representation().IsDouble() !=
         old_desc->GetDetails(i).representation().IsDouble()) {
@@ -547,9 +549,9 @@ bool Map::InstancesNeedRewriting(Map target, int target_number_of_fields,
 }
 
 int Map::NumberOfFields(ConcurrencyMode cmode) const {
-  DescriptorArray descriptors = IsConcurrent(cmode)
-                                    ? instance_descriptors(kAcquireLoad)
-                                    : instance_descriptors();
+  Tagged<DescriptorArray> descriptors = IsConcurrent(cmode)
+                                            ? instance_descriptors(kAcquireLoad)
+                                            : instance_descriptors();
   int result = 0;
   for (InternalIndex i : IterateOwnDescriptors()) {
     if (descriptors->GetDetails(i).location() == PropertyLocation::kField)
@@ -559,7 +561,7 @@ int Map::NumberOfFields(ConcurrencyMode cmode) const {
 }
 
 Map::FieldCounts Map::GetFieldCounts() const {
-  DescriptorArray descriptors = instance_descriptors();
+  Tagged<DescriptorArray> descriptors = instance_descriptors();
   int mutable_count = 0;
   int const_count = 0;
   for (InternalIndex i : IterateOwnDescriptors()) {
@@ -599,7 +601,7 @@ void Map::DeprecateTransitionTree(Isolate* isolate) {
 // Installs |new_descriptors| over the current instance_descriptors to ensure
 // proper sharing of descriptor arrays.
 void Map::ReplaceDescriptors(Isolate* isolate,
-                             DescriptorArray new_descriptors) {
+                             Tagged<DescriptorArray> new_descriptors) {
   PtrComprCageBase cage_base(isolate);
   // Don't overwrite the empty descriptor array or initial map's descriptors.
   if (NumberOfOwnDescriptors() == 0 ||
@@ -607,16 +609,16 @@ void Map::ReplaceDescriptors(Isolate* isolate,
     return;
   }
 
-  DescriptorArray to_replace = instance_descriptors(cage_base);
+  Tagged<DescriptorArray> to_replace = instance_descriptors(cage_base);
   // Replace descriptors by new_descriptors in all maps that share it. The old
   // descriptors will not be trimmed in the mark-compactor, we need to mark
   // all its elements.
-  Map current = *this;
+  Tagged<Map> current = *this;
 #ifndef V8_DISABLE_WRITE_BARRIERS
   WriteBarrier::Marking(to_replace, to_replace->number_of_descriptors());
 #endif
   while (current->instance_descriptors(cage_base) == to_replace) {
-    Map next;
+    Tagged<Map> next;
     if (!current->TryGetBackPointer(cage_base, &next)) {
       break;  // Stop overwriting at initial map.
     }
@@ -628,11 +630,11 @@ void Map::ReplaceDescriptors(Isolate* isolate,
   set_owns_descriptors(false);
 }
 
-Map Map::FindRootMap(PtrComprCageBase cage_base) const {
+Tagged<Map> Map::FindRootMap(PtrComprCageBase cage_base) const {
   DisallowGarbageCollection no_gc;
-  Map result = *this;
+  Tagged<Map> result = *this;
   while (true) {
-    Map parent;
+    Tagged<Map> parent;
     if (!result->TryGetBackPointer(cage_base, &parent)) {
       // Initial map must not contain descriptors in the descriptors array
       // that do not belong to the map.
@@ -645,16 +647,16 @@ Map Map::FindRootMap(PtrComprCageBase cage_base) const {
   }
 }
 
-Map Map::FindFieldOwner(PtrComprCageBase cage_base,
-                        InternalIndex descriptor) const {
+Tagged<Map> Map::FindFieldOwner(PtrComprCageBase cage_base,
+                                InternalIndex descriptor) const {
   DisallowGarbageCollection no_gc;
   DCHECK_EQ(PropertyLocation::kField,
             instance_descriptors(cage_base, kRelaxedLoad)
                 ->GetDetails(descriptor)
                 .location());
-  Map result = *this;
+  Tagged<Map> result = *this;
   while (true) {
-    Map parent;
+    Tagged<Map> parent;
     if (!result->TryGetBackPointer(cage_base, &parent)) break;
     if (parent->NumberOfOwnDescriptors() <= descriptor.as_int()) break;
     result = parent;
@@ -664,10 +666,10 @@ Map Map::FindFieldOwner(PtrComprCageBase cage_base,
 
 namespace {
 
-Map SearchMigrationTarget(Isolate* isolate, Map old_map) {
+Tagged<Map> SearchMigrationTarget(Isolate* isolate, Tagged<Map> old_map) {
   DisallowGarbageCollection no_gc;
 
-  Map target = old_map;
+  Tagged<Map> target = old_map;
   do {
     target = TransitionsAccessor(isolate, target).GetMigrationTarget();
   } while (!target.is_null() && target->is_deprecated());
@@ -681,12 +683,13 @@ Map SearchMigrationTarget(Isolate* isolate, Map old_map) {
   // types instead of old_map's types.
   // Go to slow map updating if the old_map has fast properties with cleared
   // field types.
-  DescriptorArray old_descriptors = old_map->instance_descriptors(isolate);
+  Tagged<DescriptorArray> old_descriptors =
+      old_map->instance_descriptors(isolate);
   for (InternalIndex i : old_map->IterateOwnDescriptors()) {
     PropertyDetails old_details = old_descriptors->GetDetails(i);
     if (old_details.location() == PropertyLocation::kField &&
         old_details.kind() == PropertyKind::kData) {
-      FieldType old_type = old_descriptors->GetFieldType(i);
+      Tagged<FieldType> old_type = old_descriptors->GetFieldType(i);
       if (Map::FieldTypeIsCleared(old_details.representation(), old_type)) {
         return Map();
       }
@@ -707,7 +710,7 @@ MaybeHandle<Map> Map::TryUpdate(Isolate* isolate, Handle<Map> old_map) {
   if (!old_map->is_deprecated()) return old_map;
 
   if (v8_flags.fast_map_update) {
-    Map target_map = SearchMigrationTarget(isolate, *old_map);
+    Tagged<Map> target_map = SearchMigrationTarget(isolate, *old_map);
     if (!target_map.is_null()) {
       return handle(target_map, isolate);
     }
@@ -722,8 +725,9 @@ MaybeHandle<Map> Map::TryUpdate(Isolate* isolate, Handle<Map> old_map) {
   return handle(new_map.value(), isolate);
 }
 
-Map Map::TryReplayPropertyTransitions(Isolate* isolate, Map old_map,
-                                      ConcurrencyMode cmode) {
+Tagged<Map> Map::TryReplayPropertyTransitions(Isolate* isolate,
+                                              Tagged<Map> old_map,
+                                              ConcurrencyMode cmode) {
   DisallowGarbageCollection no_gc;
 
   const int root_nof = NumberOfOwnDescriptors();
@@ -731,19 +735,19 @@ Map Map::TryReplayPropertyTransitions(Isolate* isolate, Map old_map,
   // TODO(jgruber,chromium:1239009): The main thread should use non-atomic
   // reads, but this currently leads to odd behavior (see the linked bug).
   // Investigate and fix this properly. Also below and in called functions.
-  DescriptorArray old_descriptors =
+  Tagged<DescriptorArray> old_descriptors =
       old_map->instance_descriptors(isolate, kAcquireLoad);
 
-  Map new_map = *this;
+  Tagged<Map> new_map = *this;
   for (InternalIndex i : InternalIndex::Range(root_nof, old_nof)) {
     PropertyDetails old_details = old_descriptors->GetDetails(i);
-    Map transition =
+    Tagged<Map> transition =
         TransitionsAccessor(isolate, new_map, IsConcurrent(cmode))
             .SearchTransition(old_descriptors->GetKey(i), old_details.kind(),
                               old_details.attributes());
     if (transition.is_null()) return Map();
     new_map = transition;
-    DescriptorArray new_descriptors =
+    Tagged<DescriptorArray> new_descriptors =
         new_map->instance_descriptors(isolate, kAcquireLoad);
 
     PropertyDetails new_details = new_descriptors->GetDetails(i);
@@ -758,7 +762,7 @@ Map Map::TryReplayPropertyTransitions(Isolate* isolate, Map old_map,
     }
     if (new_details.location() == PropertyLocation::kField) {
       if (new_details.kind() == PropertyKind::kData) {
-        FieldType new_type = new_descriptors->GetFieldType(i);
+        Tagged<FieldType> new_type = new_descriptors->GetFieldType(i);
         // Cleared field types need special treatment. They represent lost
         // knowledge, so we must first generalize the new_type to "Any".
         if (FieldTypeIsCleared(new_details.representation(), new_type)) {
@@ -766,15 +770,15 @@ Map Map::TryReplayPropertyTransitions(Isolate* isolate, Map old_map,
         }
         DCHECK_EQ(PropertyKind::kData, old_details.kind());
         DCHECK_EQ(PropertyLocation::kField, old_details.location());
-        FieldType old_type = old_descriptors->GetFieldType(i);
+        Tagged<FieldType> old_type = old_descriptors->GetFieldType(i);
         if (FieldTypeIsCleared(old_details.representation(), old_type) ||
-            !old_type.NowIs(new_type)) {
+            !old_type->NowIs(new_type)) {
           return Map();
         }
       } else {
         DCHECK_EQ(PropertyKind::kAccessor, new_details.kind());
 #ifdef DEBUG
-        FieldType new_type = new_descriptors->GetFieldType(i);
+        Tagged<FieldType> new_type = new_descriptors->GetFieldType(i);
         DCHECK(IsAny(new_type));
 #endif
         UNREACHABLE();
@@ -796,7 +800,7 @@ Map Map::TryReplayPropertyTransitions(Isolate* isolate, Map old_map,
 Handle<Map> Map::Update(Isolate* isolate, Handle<Map> map) {
   if (!map->is_deprecated()) return map;
   if (v8_flags.fast_map_update) {
-    Map target_map = SearchMigrationTarget(isolate, *map);
+    Tagged<Map> target_map = SearchMigrationTarget(isolate, *map);
     if (!target_map.is_null()) {
       return handle(target_map, isolate);
     }
@@ -842,10 +846,10 @@ void Map::EnsureDescriptorSlack(Isolate* isolate, Handle<Map> map, int slack) {
   // (exclusive). In the case that {map} is the initial map, update it.
   map->UpdateDescriptors(isolate, *new_descriptors,
                          map->NumberOfOwnDescriptors());
-  Object next = map->GetBackPointer();
+  Tagged<Object> next = map->GetBackPointer();
   if (IsUndefined(next, isolate)) return;
 
-  Map current = Map::cast(next);
+  Tagged<Map> current = Map::cast(next);
   while (current->instance_descriptors(isolate) == *descriptors) {
     next = current->GetBackPointer();
     if (IsUndefined(next, isolate)) break;
@@ -885,7 +889,7 @@ Handle<Map> Map::GetObjectCreateMap(Isolate* isolate,
   return Map::TransitionToPrototype(isolate, map, prototype);
 }
 
-static bool ContainsMap(MapHandles const& maps, Map map) {
+static bool ContainsMap(MapHandles const& maps, Tagged<Map> map) {
   DCHECK(!map.is_null());
   for (Handle<Map> current : maps) {
     if (!current.is_null() && *current == map) return true;
@@ -902,9 +906,9 @@ static bool HasElementsKind(MapHandles const& maps,
   return false;
 }
 
-Map Map::FindElementsKindTransitionedMap(Isolate* isolate,
-                                         MapHandles const& candidates,
-                                         ConcurrencyMode cmode) {
+Tagged<Map> Map::FindElementsKindTransitionedMap(Isolate* isolate,
+                                                 MapHandles const& candidates,
+                                                 ConcurrencyMode cmode) {
   DisallowGarbageCollection no_gc;
 
   if (IsDetached(isolate)) return Map();
@@ -912,10 +916,10 @@ Map Map::FindElementsKindTransitionedMap(Isolate* isolate,
   ElementsKind kind = elements_kind();
   bool is_packed = IsFastPackedElementsKind(kind);
 
-  Map transition;
+  Tagged<Map> transition;
   if (IsTransitionableFastElementsKind(kind)) {
     // Check the state of the root map.
-    Map root_map = FindRootMap(isolate);
+    Tagged<Map> root_map = FindRootMap(isolate);
     if (!EquivalentToForElementsKindTransition(root_map, cmode)) return Map();
     root_map = root_map->LookupElementsTransitionMap(isolate, kind, cmode);
     DCHECK(!root_map.is_null());
@@ -928,7 +932,7 @@ Map Map::FindElementsKindTransitionedMap(Isolate* isolate,
       // If root_map's elements kind doesn't match any of the elements kind in
       // the candidates there is no need to do any additional work.
       if (!HasElementsKind(candidates, root_map->elements_kind())) continue;
-      Map current =
+      Tagged<Map> current =
           root_map->TryReplayPropertyTransitions(isolate, *this, cmode);
       if (current.is_null()) continue;
       if (InstancesNeedRewriting(current, cmode)) continue;
@@ -945,18 +949,19 @@ Map Map::FindElementsKindTransitionedMap(Isolate* isolate,
   return transition;
 }
 
-static Map FindClosestElementsTransition(Isolate* isolate, Map map,
-                                         ElementsKind to_kind,
-                                         ConcurrencyMode cmode) {
+static Tagged<Map> FindClosestElementsTransition(Isolate* isolate,
+                                                 Tagged<Map> map,
+                                                 ElementsKind to_kind,
+                                                 ConcurrencyMode cmode) {
   DisallowGarbageCollection no_gc;
   // Ensure we are requested to search elements kind transition "near the root".
   DCHECK_EQ(map->FindRootMap(isolate)->NumberOfOwnDescriptors(),
             map->NumberOfOwnDescriptors());
-  Map current_map = map;
+  Tagged<Map> current_map = map;
 
   ElementsKind kind = map->elements_kind();
   while (kind != to_kind) {
-    Map next_map = current_map->ElementsTransitionMap(isolate, cmode);
+    Tagged<Map> next_map = current_map->ElementsTransitionMap(isolate, cmode);
     if (next_map.is_null()) return current_map;
     kind = next_map->elements_kind();
     current_map = next_map;
@@ -966,9 +971,11 @@ static Map FindClosestElementsTransition(Isolate* isolate, Map map,
   return current_map;
 }
 
-Map Map::LookupElementsTransitionMap(Isolate* isolate, ElementsKind to_kind,
-                                     ConcurrencyMode cmode) {
-  Map to_map = FindClosestElementsTransition(isolate, *this, to_kind, cmode);
+Tagged<Map> Map::LookupElementsTransitionMap(Isolate* isolate,
+                                             ElementsKind to_kind,
+                                             ConcurrencyMode cmode) {
+  Tagged<Map> to_map =
+      FindClosestElementsTransition(isolate, *this, to_kind, cmode);
   if (to_map->elements_kind() == to_kind) return to_map;
   return Map();
 }
@@ -990,7 +997,7 @@ Handle<Map> Map::TransitionElementsTo(Isolate* isolate, Handle<Map> map,
   ElementsKind from_kind = map->elements_kind();
   if (from_kind == to_kind) return map;
 
-  Context native_context = isolate->context()->native_context();
+  Tagged<Context> native_context = isolate->context()->native_context();
   if (from_kind == FAST_SLOPPY_ARGUMENTS_ELEMENTS) {
     if (*map == native_context->fast_aliased_arguments_map()) {
       DCHECK_EQ(SLOW_SLOPPY_ARGUMENTS_ELEMENTS, to_kind);
@@ -1005,7 +1012,7 @@ Handle<Map> Map::TransitionElementsTo(Isolate* isolate, Handle<Map> map,
     // Reuse map transitions for JSArrays.
     DisallowGarbageCollection no_gc;
     if (native_context->GetInitialJSArrayMap(from_kind) == *map) {
-      Object maybe_transitioned_map =
+      Tagged<Object> maybe_transitioned_map =
           native_context->get(Context::ArrayMapIndex(to_kind));
       if (IsMap(maybe_transitioned_map)) {
         return handle(Map::cast(maybe_transitioned_map), isolate);
@@ -1072,7 +1079,8 @@ static Handle<Map> AddMissingElementsTransitions(Isolate* isolate,
 base::Optional<Map> Map::TryAsElementsKind(Isolate* isolate, Handle<Map> map,
                                            ElementsKind kind,
                                            ConcurrencyMode cmode) {
-  Map closest_map = FindClosestElementsTransition(isolate, *map, kind, cmode);
+  Tagged<Map> closest_map =
+      FindClosestElementsTransition(isolate, *map, kind, cmode);
   if (closest_map->elements_kind() != kind) return {};
   return closest_map;
 }
@@ -1094,7 +1102,7 @@ Handle<Map> Map::AsElementsKind(Isolate* isolate, Handle<Map> map,
 
 int Map::NumberOfEnumerableProperties() const {
   int result = 0;
-  DescriptorArray descs = instance_descriptors(kRelaxedLoad);
+  Tagged<DescriptorArray> descs = instance_descriptors(kRelaxedLoad);
   for (InternalIndex i : IterateOwnDescriptors()) {
     if ((int{descs->GetDetails(i).attributes()} & ONLY_ENUMERABLE) == 0 &&
         !Object::FilterKey(descs->GetKey(i), ENUMERABLE_STRINGS)) {
@@ -1106,7 +1114,7 @@ int Map::NumberOfEnumerableProperties() const {
 
 int Map::NextFreePropertyIndex() const {
   int number_of_own_descriptors = NumberOfOwnDescriptors();
-  DescriptorArray descs = instance_descriptors(kRelaxedLoad);
+  Tagged<DescriptorArray> descs = instance_descriptors(kRelaxedLoad);
   // Search properties backwards to find the last field.
   for (int i = number_of_own_descriptors - 1; i >= 0; --i) {
     PropertyDetails details = descs->GetDetails(InternalIndex(i));
@@ -1132,7 +1140,7 @@ bool Map::MayHaveReadOnlyElementsInPrototypeChain(Isolate* isolate) {
     // non-configurable, non-writable elements), API objects, etc.
     if (IsCustomElementsReceiverMap(iter.GetCurrent()->map())) return true;
 
-    JSObject current = iter.GetCurrent<JSObject>();
+    Tagged<JSObject> current = iter.GetCurrent<JSObject>();
     ElementsKind elements_kind = current->GetElementsKind(isolate);
     if (IsFrozenElementsKind(elements_kind)) return true;
 
@@ -1142,9 +1150,9 @@ bool Map::MayHaveReadOnlyElementsInPrototypeChain(Isolate* isolate) {
     }
 
     if (IsSlowArgumentsElementsKind(elements_kind)) {
-      SloppyArgumentsElements elements =
+      Tagged<SloppyArgumentsElements> elements =
           SloppyArgumentsElements::cast(current->elements(isolate));
-      Object arguments = elements->arguments();
+      Tagged<Object> arguments = elements->arguments();
       if (NumberDictionary::cast(arguments)->requires_slow_elements()) {
         return true;
       }
@@ -1163,8 +1171,8 @@ Handle<Map> Map::RawCopy(Isolate* isolate, Handle<Map> src_handle,
   // heap verification might fail otherwise.
   {
     DisallowGarbageCollection no_gc;
-    Map src = *src_handle;
-    Map raw = *result;
+    Tagged<Map> src = *src_handle;
+    Tagged<Map> raw = *result;
     raw->set_constructor_or_back_pointer(src->GetConstructorRaw());
     raw->set_bit_field(src->bit_field());
     raw->set_bit_field2(src->bit_field2());
@@ -1279,7 +1287,7 @@ Handle<Map> Map::CopyNormalized(Isolate* isolate, Handle<Map> map,
       mode == CLEAR_INOBJECT_PROPERTIES ? 0 : map->GetInObjectProperties());
   {
     DisallowGarbageCollection no_gc;
-    Map raw = *result;
+    Tagged<Map> raw = *result;
     // Clear the unused_property_fields explicitly as this field should not
     // be accessed for normalized maps.
     raw->SetInObjectUnusedPropertyFields(0);
@@ -1311,7 +1319,7 @@ Handle<Map> Map::TransitionToImmutableProto(Isolate* isolate, Handle<Map> map) {
 namespace {
 void EnsureInitialMap(Isolate* isolate, Handle<Map> map) {
 #ifdef DEBUG
-  Object maybe_constructor = map->GetConstructor();
+  Tagged<Object> maybe_constructor = map->GetConstructor();
   DCHECK((IsJSFunction(maybe_constructor) &&
           *map == JSFunction::cast(maybe_constructor)->initial_map()) ||
          // Below are the exceptions to the check above.
@@ -1357,7 +1365,7 @@ Handle<Map> Map::CopyInitialMap(Isolate* isolate, Handle<Map> map,
   int number_of_own_descriptors = map->NumberOfOwnDescriptors();
   if (number_of_own_descriptors > 0) {
     // The copy will use the same descriptors array without ownership.
-    DescriptorArray descriptors = map->instance_descriptors(isolate);
+    Tagged<DescriptorArray> descriptors = map->instance_descriptors(isolate);
     result->set_owns_descriptors(false);
     result->UpdateDescriptors(isolate, descriptors, number_of_own_descriptors);
 
@@ -1567,7 +1575,7 @@ Handle<Map> Map::CopyAsElementsKind(Isolate* isolate, Handle<Map> map,
       !map->CanHaveFastTransitionableElementsKind(),
       IsDictionaryElementsKind(kind) || IsTerminalElementsKind(kind));
 
-  Map maybe_elements_transition_map;
+  Tagged<Map> maybe_elements_transition_map;
   if (flag == INSERT_TRANSITION) {
     // Ensure we are requested to add elements kind transition "near the root".
     DCHECK_EQ(map->FindRootMap(isolate)->NumberOfOwnDescriptors(),
@@ -1683,7 +1691,7 @@ Handle<Map> Map::Create(Isolate* isolate, int inobject_properties) {
       Copy(isolate, handle(isolate->object_function()->initial_map(), isolate),
            "MapCreate");
   DisallowGarbageCollection no_gc;
-  Map copy = *copy_handle;
+  Tagged<Map> copy = *copy_handle;
 
   // Check that we do not overflow the instance size when adding the extra
   // inobject properties. If the instance size overflows, we allocate as many
@@ -1780,14 +1788,14 @@ Handle<Map> Map::CopyForPreventExtensions(
 
 namespace {
 
-bool CanHoldValue(DescriptorArray descriptors, InternalIndex descriptor,
-                  PropertyConstness constness, Object value) {
+bool CanHoldValue(Tagged<DescriptorArray> descriptors, InternalIndex descriptor,
+                  PropertyConstness constness, Tagged<Object> value) {
   PropertyDetails details = descriptors->GetDetails(descriptor);
   if (details.location() == PropertyLocation::kField) {
     if (details.kind() == PropertyKind::kData) {
       return IsGeneralizableTo(constness, details.constness()) &&
              Object::FitsRepresentation(value, details.representation()) &&
-             descriptors->GetFieldType(descriptor).NowContains(value);
+             descriptors->GetFieldType(descriptor)->NowContains(value);
     } else {
       DCHECK_EQ(PropertyKind::kAccessor, details.kind());
       return false;
@@ -1953,7 +1961,8 @@ Handle<Map> Map::TransitionToAccessorProperty(Isolate* isolate, Handle<Map> map,
       isolate, map, *name, PropertyKind::kAccessor, attributes);
   Handle<Map> transition;
   if (maybe_transition.ToHandle(&transition)) {
-    DescriptorArray descriptors = transition->instance_descriptors(isolate);
+    Tagged<DescriptorArray> descriptors =
+        transition->instance_descriptors(isolate);
     InternalIndex last_descriptor = transition->LastAdded();
     DCHECK(descriptors->GetKey(last_descriptor)->Equals(*name));
 
@@ -1979,7 +1988,7 @@ Handle<Map> Map::TransitionToAccessorProperty(Isolate* isolate, Handle<Map> map,
   }
 
   Handle<AccessorPair> pair;
-  DescriptorArray old_descriptors = map->instance_descriptors(isolate);
+  Tagged<DescriptorArray> old_descriptors = map->instance_descriptors(isolate);
   if (descriptor.is_found()) {
     if (descriptor != map->LastAdded()) {
       return Map::Normalize(isolate, map, mode, "AccessorsOverwritingNonLast");
@@ -2106,14 +2115,14 @@ int Map::Hash() {
   // For performance reasons we only hash the 2 most variable fields of a map:
   // prototype and bit_field2.
 
-  HeapObject prototype = this->prototype();
+  Tagged<HeapObject> prototype = this->prototype();
   int prototype_hash;
 
   if (IsNull(prototype)) {
     // No identity hash for null, so just pick a random number.
     prototype_hash = 1;
   } else {
-    JSReceiver receiver = JSReceiver::cast(prototype);
+    Tagged<JSReceiver> receiver = JSReceiver::cast(prototype);
     Isolate* isolate = GetIsolateFromWritableObject(receiver);
     prototype_hash = receiver->GetOrCreateIdentityHash(isolate).value();
   }
@@ -2123,7 +2132,7 @@ int Map::Hash() {
 
 namespace {
 
-bool CheckEquivalent(const Map first, const Map second) {
+bool CheckEquivalent(const Tagged<Map> first, const Tagged<Map> second) {
   return first->GetConstructorRaw() == second->GetConstructorRaw() &&
          first->prototype() == second->prototype() &&
          first->instance_type() == second->instance_type() &&
@@ -2134,7 +2143,7 @@ bool CheckEquivalent(const Map first, const Map second) {
 
 }  // namespace
 
-bool Map::EquivalentToForTransition(const Map other,
+bool Map::EquivalentToForTransition(const Tagged<Map> other,
                                     ConcurrencyMode cmode) const {
   CHECK_EQ(GetConstructor(), other->GetConstructor());
   CHECK_EQ(instance_type(), other->instance_type());
@@ -2147,10 +2156,10 @@ bool Map::EquivalentToForTransition(const Map other,
     // not equivalent to strict function.
     int nof =
         std::min(NumberOfOwnDescriptors(), other->NumberOfOwnDescriptors());
-    DescriptorArray this_descriptors = IsConcurrent(cmode)
-                                           ? instance_descriptors(kAcquireLoad)
-                                           : instance_descriptors();
-    DescriptorArray that_descriptors =
+    Tagged<DescriptorArray> this_descriptors =
+        IsConcurrent(cmode) ? instance_descriptors(kAcquireLoad)
+                            : instance_descriptors();
+    Tagged<DescriptorArray> that_descriptors =
         IsConcurrent(cmode) ? other->instance_descriptors(kAcquireLoad)
                             : other->instance_descriptors();
     return this_descriptors->IsEqualUpTo(that_descriptors, nof);
@@ -2158,16 +2167,16 @@ bool Map::EquivalentToForTransition(const Map other,
   return true;
 }
 
-bool Map::EquivalentToForElementsKindTransition(const Map other,
+bool Map::EquivalentToForElementsKindTransition(const Tagged<Map> other,
                                                 ConcurrencyMode cmode) const {
   if (!EquivalentToForTransition(other, cmode)) return false;
 #ifdef DEBUG
   // Ensure that we don't try to generate elements kind transitions from maps
   // with fields that may be generalized in-place. This must already be handled
   // during addition of a new field.
-  DescriptorArray descriptors = IsConcurrent(cmode)
-                                    ? instance_descriptors(kAcquireLoad)
-                                    : instance_descriptors();
+  Tagged<DescriptorArray> descriptors = IsConcurrent(cmode)
+                                            ? instance_descriptors(kAcquireLoad)
+                                            : instance_descriptors();
   for (InternalIndex i : IterateOwnDescriptors()) {
     PropertyDetails details = descriptors->GetDetails(i);
     if (details.location() == PropertyLocation::kField) {
@@ -2179,7 +2188,7 @@ bool Map::EquivalentToForElementsKindTransition(const Map other,
   return true;
 }
 
-bool Map::EquivalentToForNormalization(const Map other,
+bool Map::EquivalentToForNormalization(const Tagged<Map> other,
                                        ElementsKind elements_kind,
                                        PropertyNormalizationMode mode) const {
   int properties =
@@ -2202,14 +2211,15 @@ int Map::ComputeMinObjectSlack(Isolate* isolate) {
 
   int slack = UnusedPropertyFields();
   TransitionsAccessor transitions(isolate, *this);
-  TransitionsAccessor::TraverseCallback callback = [&](Map map) {
+  TransitionsAccessor::TraverseCallback callback = [&](Tagged<Map> map) {
     slack = std::min(slack, map->UnusedPropertyFields());
   };
   transitions.TraverseTransitionTree(callback);
   return slack;
 }
 
-void Map::SetInstanceDescriptors(Isolate* isolate, DescriptorArray descriptors,
+void Map::SetInstanceDescriptors(Isolate* isolate,
+                                 Tagged<DescriptorArray> descriptors,
                                  int number_of_own_descriptors) {
   set_instance_descriptors(descriptors, kReleaseStore);
   SetNumberOfOwnDescriptors(number_of_own_descriptors);
@@ -2223,7 +2233,7 @@ Handle<PrototypeInfo> Map::GetOrCreatePrototypeInfo(Handle<JSObject> prototype,
                                                     Isolate* isolate) {
   DCHECK(IsJSObjectThatCanBeTrackedAsPrototype(*prototype));
   {
-    PrototypeInfo prototype_info;
+    Tagged<PrototypeInfo> prototype_info;
     if (prototype->map()->TryGetPrototypeInfo(&prototype_info)) {
       return handle(prototype_info, isolate);
     }
@@ -2237,7 +2247,7 @@ Handle<PrototypeInfo> Map::GetOrCreatePrototypeInfo(Handle<JSObject> prototype,
 Handle<PrototypeInfo> Map::GetOrCreatePrototypeInfo(Handle<Map> prototype_map,
                                                     Isolate* isolate) {
   {
-    Object maybe_proto_info = prototype_map->prototype_info();
+    Tagged<Object> maybe_proto_info = prototype_map->prototype_info();
     if (PrototypeInfo::IsPrototypeInfoFast(maybe_proto_info)) {
       return handle(PrototypeInfo::cast(maybe_proto_info), isolate);
     }
@@ -2280,10 +2290,11 @@ Handle<Object> Map::GetOrCreatePrototypeChainValidityCell(Handle<Map> map,
   JSObject::LazyRegisterPrototypeUser(handle(prototype->map(), isolate),
                                       isolate);
 
-  Object maybe_cell = prototype->map()->prototype_validity_cell(kRelaxedLoad);
+  Tagged<Object> maybe_cell =
+      prototype->map()->prototype_validity_cell(kRelaxedLoad);
   // Return existing cell if it's still valid.
   if (IsCell(maybe_cell)) {
-    Cell cell = Cell::cast(maybe_cell);
+    Tagged<Cell> cell = Cell::cast(maybe_cell);
     if (cell->value() == Smi::FromInt(Map::kPrototypeChainValid)) {
       return handle(cell, isolate);
     }
@@ -2296,11 +2307,11 @@ Handle<Object> Map::GetOrCreatePrototypeChainValidityCell(Handle<Map> map,
 }
 
 // static
-bool Map::IsPrototypeChainInvalidated(Map map) {
+bool Map::IsPrototypeChainInvalidated(Tagged<Map> map) {
   DCHECK(map->is_prototype_map());
-  Object maybe_cell = map->prototype_validity_cell(kRelaxedLoad);
+  Tagged<Object> maybe_cell = map->prototype_validity_cell(kRelaxedLoad);
   if (IsCell(maybe_cell)) {
-    Cell cell = Cell::cast(maybe_cell);
+    Tagged<Cell> cell = Cell::cast(maybe_cell);
     return cell->value() != Smi::FromInt(Map::kPrototypeChainValid);
   }
   return true;
@@ -2373,12 +2384,12 @@ MaybeHandle<Map> NormalizedMapCache::Get(Handle<Map> fast_map,
                                          PropertyNormalizationMode mode) {
   DisallowGarbageCollection no_gc;
   MaybeObject value = WeakFixedArray::Get(GetIndex(fast_map));
-  HeapObject heap_object;
+  Tagged<HeapObject> heap_object;
   if (!value->GetHeapObjectIfWeak(&heap_object)) {
     return MaybeHandle<Map>();
   }
 
-  Map normalized_map = Map::cast(heap_object);
+  Tagged<Map> normalized_map = Map::cast(heap_object);
   if (!normalized_map->EquivalentToForNormalization(*fast_map, elements_kind,
                                                     mode)) {
     return MaybeHandle<Map>();
diff --git a/src/objects/map.h b/src/objects/map.h
index 8dccd07abd3..2853bdc738f 100644
--- a/src/objects/map.h
+++ b/src/objects/map.h
@@ -235,7 +235,7 @@ class Map : public TorqueGeneratedMap<Map, HeapObject> {
   inline int GetConstructorFunctionIndex() const;
   inline void SetConstructorFunctionIndex(int value);
   static base::Optional<JSFunction> GetConstructorFunction(
-      Map map, Context native_context);
+      Tagged<Map> map, Tagged<Context> native_context);
 
   // Retrieve interceptors.
   DECL_GETTER(GetNamedInterceptor, Tagged<InterceptorInfo>)
@@ -260,8 +260,8 @@ class Map : public TorqueGeneratedMap<Map, HeapObject> {
   inline void SetInObjectUnusedPropertyFields(int unused_property_fields);
   // Updates the counters tracking unused fields in the property array.
   inline void SetOutOfObjectUnusedPropertyFields(int unused_property_fields);
-  inline void CopyUnusedPropertyFields(Map map);
-  inline void CopyUnusedPropertyFieldsAdjustedForInstanceSize(Map map);
+  inline void CopyUnusedPropertyFields(Tagged<Map> map);
+  inline void CopyUnusedPropertyFieldsAdjustedForInstanceSize(Tagged<Map> map);
   inline void AccountAddedPropertyField();
   inline void AccountAddedOutOfObjectPropertyField(
       int unused_in_property_array);
@@ -416,7 +416,7 @@ class Map : public TorqueGeneratedMap<Map, HeapObject> {
   DECL_BOOLEAN_ACCESSORS(is_prototype_map)
   inline bool is_abandoned_prototype_map() const;
   inline bool has_prototype_info() const;
-  inline bool TryGetPrototypeInfo(PrototypeInfo* result) const;
+  inline bool TryGetPrototypeInfo(Tagged<PrototypeInfo>* result) const;
 
   // Whether the instance has been added to the retained map list by
   // Heap::AddRetainedMap.
@@ -456,9 +456,10 @@ class Map : public TorqueGeneratedMap<Map, HeapObject> {
   // elements or an object with any frozen elements, or a slow arguments object.
   bool MayHaveReadOnlyElementsInPrototypeChain(Isolate* isolate);
 
-  inline Map ElementsTransitionMap(Isolate* isolate, ConcurrencyMode cmode);
+  inline Tagged<Map> ElementsTransitionMap(Isolate* isolate,
+                                           ConcurrencyMode cmode);
 
-  inline FixedArrayBase GetInitialElements() const;
+  inline Tagged<FixedArrayBase> GetInitialElements() const;
 
   // [raw_transitions]: Provides access to the transitions storage field.
   // Don't call set_raw_transitions() directly to overwrite transitions, use
@@ -490,14 +491,14 @@ class Map : public TorqueGeneratedMap<Map, HeapObject> {
   static const int kPrototypeChainValid = 0;
   static const int kPrototypeChainInvalid = 1;
 
-  static bool IsPrototypeChainInvalidated(Map map);
+  static bool IsPrototypeChainInvalidated(Tagged<Map> map);
 
   // Return the map of the root of object's prototype chain.
-  Map GetPrototypeChainRootMap(Isolate* isolate) const;
+  Tagged<Map> GetPrototypeChainRootMap(Isolate* isolate) const;
 
-  V8_EXPORT_PRIVATE Map FindRootMap(PtrComprCageBase cage_base) const;
-  V8_EXPORT_PRIVATE Map FindFieldOwner(PtrComprCageBase cage_base,
-                                       InternalIndex descriptor) const;
+  V8_EXPORT_PRIVATE Tagged<Map> FindRootMap(PtrComprCageBase cage_base) const;
+  V8_EXPORT_PRIVATE Tagged<Map> FindFieldOwner(PtrComprCageBase cage_base,
+                                               InternalIndex descriptor) const;
 
   inline int GetInObjectPropertyOffset(int index) const;
 
@@ -520,16 +521,17 @@ class Map : public TorqueGeneratedMap<Map, HeapObject> {
   int NumberOfFields(ConcurrencyMode cmode) const;
 
   // TODO(ishell): candidate with JSObject::MigrateToMap().
-  bool InstancesNeedRewriting(Map target, ConcurrencyMode cmode) const;
-  bool InstancesNeedRewriting(Map target, int target_number_of_fields,
+  bool InstancesNeedRewriting(Tagged<Map> target, ConcurrencyMode cmode) const;
+  bool InstancesNeedRewriting(Tagged<Map> target, int target_number_of_fields,
                               int target_inobject, int target_unused,
                               int* old_number_of_fields,
                               ConcurrencyMode cmode) const;
   // Returns true if the |field_type| is the most general one for
   // given |representation|.
   static inline bool IsMostGeneralFieldType(Representation representation,
-                                            FieldType field_type);
-  static inline bool FieldTypeIsCleared(Representation rep, FieldType type);
+                                            Tagged<FieldType> field_type);
+  static inline bool FieldTypeIsCleared(Representation rep,
+                                        Tagged<FieldType> type);
 
   // Generalizes representation and field_type if objects with given
   // instance type can have fast elements that can be transitioned by
@@ -609,12 +611,12 @@ class Map : public TorqueGeneratedMap<Map, HeapObject> {
   // The result returned might be null, JSFunction or FunctionTemplateInfo.
   DECL_GETTER(GetConstructor, Tagged<Object>)
   DECL_GETTER(GetFunctionTemplateInfo, Tagged<FunctionTemplateInfo>)
-  inline void SetConstructor(Object constructor,
+  inline void SetConstructor(Tagged<Object> constructor,
                              WriteBarrierMode mode = UPDATE_WRITE_BARRIER);
   // Constructor getter that performs at most the given number of steps
   // in the transition tree. Returns either the constructor or the map at
   // which the walk has stopped.
-  inline Object TryGetConstructor(Isolate* isolate, int max_steps);
+  inline Tagged<Object> TryGetConstructor(Isolate* isolate, int max_steps);
 
   // Gets non-instance prototype value which is stored in Tuple2 in a
   // root map's |constructor_or_back_pointer| field.
@@ -623,23 +625,24 @@ class Map : public TorqueGeneratedMap<Map, HeapObject> {
   // [back pointer]: points back to the parent map from which a transition
   // leads to this map. The field overlaps with the constructor (see above).
   DECL_GETTER(GetBackPointer, Tagged<HeapObject>)
-  inline void SetBackPointer(HeapObject value,
+  inline void SetBackPointer(Tagged<HeapObject> value,
                              WriteBarrierMode mode = UPDATE_WRITE_BARRIER);
   inline bool TryGetBackPointer(PtrComprCageBase cage_base,
-                                Map* back_pointer) const;
+                                Tagged<Map>* back_pointer) const;
 
   // [instance descriptors]: describes the object.
   DECL_ACCESSORS(instance_descriptors, Tagged<DescriptorArray>)
   DECL_RELAXED_ACCESSORS(instance_descriptors, Tagged<DescriptorArray>)
   DECL_ACQUIRE_GETTER(instance_descriptors, Tagged<DescriptorArray>)
-  V8_EXPORT_PRIVATE void SetInstanceDescriptors(Isolate* isolate,
-                                                DescriptorArray descriptors,
-                                                int number_of_own_descriptors);
+  V8_EXPORT_PRIVATE void SetInstanceDescriptors(
+      Isolate* isolate, Tagged<DescriptorArray> descriptors,
+      int number_of_own_descriptors);
 
-  inline void UpdateDescriptors(Isolate* isolate, DescriptorArray descriptors,
+  inline void UpdateDescriptors(Isolate* isolate,
+                                Tagged<DescriptorArray> descriptors,
                                 int number_of_own_descriptors);
   inline void InitializeDescriptors(Isolate* isolate,
-                                    DescriptorArray descriptors);
+                                    Tagged<DescriptorArray> descriptors);
 
   // [dependent code]: list of optimized codes that weakly embed this map.
   DECL_ACCESSORS(dependent_code, Tagged<DependentCode>)
@@ -662,7 +665,7 @@ class Map : public TorqueGeneratedMap<Map, HeapObject> {
   // chain state.
   inline bool IsPrototypeValidityCellValid() const;
 
-  inline Name GetLastDescriptorName(Isolate* isolate) const;
+  inline Tagged<Name> GetLastDescriptorName(Isolate* isolate) const;
   inline PropertyDetails GetLastDescriptorDetails(Isolate* isolate) const;
 
   inline InternalIndex LastAdded() const;
@@ -671,7 +674,7 @@ class Map : public TorqueGeneratedMap<Map, HeapObject> {
   inline void SetNumberOfOwnDescriptors(int number);
   inline InternalIndex::Range IterateOwnDescriptors() const;
 
-  inline Cell RetrieveDescriptorsPointer();
+  inline Tagged<Cell> RetrieveDescriptorsPointer();
 
   // Checks whether all properties are stored either in the map or on the object
   // (inobject, properties, or elements backing store), requiring no special
@@ -729,7 +732,8 @@ class Map : public TorqueGeneratedMap<Map, HeapObject> {
 
   static MaybeObjectHandle WrapFieldType(Isolate* isolate,
                                          Handle<FieldType> type);
-  V8_EXPORT_PRIVATE static FieldType UnwrapFieldType(MaybeObject wrapped_type);
+  V8_EXPORT_PRIVATE static Tagged<FieldType> UnwrapFieldType(
+      MaybeObject wrapped_type);
 
   V8_EXPORT_PRIVATE V8_WARN_UNUSED_RESULT static MaybeHandle<Map> CopyWithField(
       Isolate* isolate, Handle<Map> map, Handle<Name> name,
@@ -817,7 +821,7 @@ class Map : public TorqueGeneratedMap<Map, HeapObject> {
   // Returns the transitioned map for this map with the most generic
   // elements_kind that's found in |candidates|, or |nullptr| if no match is
   // found at all.
-  V8_EXPORT_PRIVATE Map FindElementsKindTransitionedMap(
+  V8_EXPORT_PRIVATE Tagged<Map> FindElementsKindTransitionedMap(
       Isolate* isolate, MapHandles const& candidates, ConcurrencyMode cmode);
 
   inline bool CanTransition() const;
@@ -836,7 +840,7 @@ class Map : public TorqueGeneratedMap<Map, HeapObject> {
     }
     return {};
   }
-  static inline Map GetMapFor(ReadOnlyRoots roots, InstanceType type);
+  static inline Tagged<Map> GetMapFor(ReadOnlyRoots roots, InstanceType type);
 
   bool IsMapInArrayPrototypeChain(Isolate* isolate) const;
 
@@ -876,10 +880,11 @@ class Map : public TorqueGeneratedMap<Map, HeapObject> {
   // If |mode| is set to CLEAR_INOBJECT_PROPERTIES, |other| is treated as if
   // it had exactly zero inobject properties.
   // The "shared" flags of both this map and |other| are ignored.
-  bool EquivalentToForNormalization(const Map other, ElementsKind elements_kind,
+  bool EquivalentToForNormalization(const Tagged<Map> other,
+                                    ElementsKind elements_kind,
                                     PropertyNormalizationMode mode) const;
   inline bool EquivalentToForNormalization(
-      const Map other, PropertyNormalizationMode mode) const;
+      const Tagged<Map> other, PropertyNormalizationMode mode) const;
 
   void PrintMapDetails(std::ostream& os);
 
@@ -892,7 +897,7 @@ class Map : public TorqueGeneratedMap<Map, HeapObject> {
   // the descriptor array.
   inline void NotifyLeafMapLayoutChange(Isolate* isolate);
 
-  V8_EXPORT_PRIVATE static VisitorId GetVisitorId(Map map);
+  V8_EXPORT_PRIVATE static VisitorId GetVisitorId(Tagged<Map> map);
 
   // Returns true if objects with given instance type are allowed to have
   // fast transitionable elements kinds. This predicate is used to ensure
@@ -926,23 +931,25 @@ class Map : public TorqueGeneratedMap<Map, HeapObject> {
 
   // Returns the map that this (root) map transitions to if its elements_kind
   // is changed to |elements_kind|, or |nullptr| if no such map is cached yet.
-  Map LookupElementsTransitionMap(Isolate* isolate, ElementsKind elements_kind,
-                                  ConcurrencyMode cmode);
+  Tagged<Map> LookupElementsTransitionMap(Isolate* isolate,
+                                          ElementsKind elements_kind,
+                                          ConcurrencyMode cmode);
 
   // Tries to replay property transitions starting from this (root) map using
   // the descriptor array of the |map|. The |root_map| is expected to have
   // proper elements kind and therefore elements kinds transitions are not
   // taken by this function. Returns |nullptr| if matching transition map is
   // not found.
-  Map TryReplayPropertyTransitions(Isolate* isolate, Map map,
-                                   ConcurrencyMode cmode);
+  Tagged<Map> TryReplayPropertyTransitions(Isolate* isolate, Tagged<Map> map,
+                                           ConcurrencyMode cmode);
 
   static void ConnectTransition(Isolate* isolate, Handle<Map> parent,
                                 Handle<Map> child, Handle<Name> name,
                                 SimpleTransitionFlag flag);
 
-  bool EquivalentToForTransition(const Map other, ConcurrencyMode cmode) const;
-  bool EquivalentToForElementsKindTransition(const Map other,
+  bool EquivalentToForTransition(const Tagged<Map> other,
+                                 ConcurrencyMode cmode) const;
+  bool EquivalentToForElementsKindTransition(const Tagged<Map> other,
                                              ConcurrencyMode cmode) const;
   static Handle<Map> RawCopy(Isolate* isolate, Handle<Map> map,
                              int instance_size, int inobject_properties);
@@ -975,7 +982,8 @@ class Map : public TorqueGeneratedMap<Map, HeapObject> {
 
   void DeprecateTransitionTree(Isolate* isolate);
 
-  void ReplaceDescriptors(Isolate* isolate, DescriptorArray new_descriptors);
+  void ReplaceDescriptors(Isolate* isolate,
+                          Tagged<DescriptorArray> new_descriptors);
 
   // This is the equivalent of IsMap() but avoids reading the instance type so
   // it can be used concurrently without acquire load.
@@ -1024,8 +1032,8 @@ class NormalizedMapCache : public WeakFixedArray {
   static inline int GetIndex(Handle<Map> map);
 
   // The following declarations hide base class methods.
-  Object get(int index);
-  void set(int index, Object value);
+  Tagged<Object> get(int index);
+  void set(int index, Tagged<Object> value);
 
   OBJECT_CONSTRUCTORS(NormalizedMapCache, WeakFixedArray);
 };
diff --git a/src/objects/maybe-object-inl.h b/src/objects/maybe-object-inl.h
index 5965d3767f7..6acb5daae56 100644
--- a/src/objects/maybe-object-inl.h
+++ b/src/objects/maybe-object-inl.h
@@ -18,13 +18,13 @@ namespace internal {
 //
 
 // static
-MaybeObject MaybeObject::FromSmi(Smi smi) {
+MaybeObject MaybeObject::FromSmi(Tagged<Smi> smi) {
   DCHECK(HAS_SMI_TAG(smi.ptr()));
   return MaybeObject(smi.ptr());
 }
 
 // static
-MaybeObject MaybeObject::FromObject(Object object) {
+MaybeObject MaybeObject::FromObject(Tagged<Object> object) {
   DCHECK(!HAS_WEAK_HEAP_OBJECT_TAG(object.ptr()));
   return MaybeObject(object.ptr());
 }
@@ -38,36 +38,36 @@ MaybeObject MaybeObject::MakeWeak(MaybeObject object) {
 MaybeObject MaybeObject::Create(MaybeObject o) { return o; }
 
 // static
-MaybeObject MaybeObject::Create(Object o) { return FromObject(o); }
+MaybeObject MaybeObject::Create(Tagged<Object> o) { return FromObject(o); }
 
 // static
-MaybeObject MaybeObject::Create(Smi smi) { return FromSmi(smi); }
+MaybeObject MaybeObject::Create(Tagged<Smi> smi) { return FromSmi(smi); }
 
 //
 // HeapObjectReference implementation.
 //
 
-HeapObjectReference::HeapObjectReference(Object object)
+HeapObjectReference::HeapObjectReference(Tagged<Object> object)
     : MaybeObject(object.ptr()) {}
 
 // static
-HeapObjectReference HeapObjectReference::Strong(Object object) {
-  DCHECK(!i::IsSmi(object));
+HeapObjectReference HeapObjectReference::Strong(Tagged<Object> object) {
+  DCHECK(!object.IsSmi());
   DCHECK(!HasWeakHeapObjectTag(object));
   return HeapObjectReference(object);
 }
 
 // static
-HeapObjectReference HeapObjectReference::Weak(Object object) {
-  DCHECK(!i::IsSmi(object));
+HeapObjectReference HeapObjectReference::Weak(Tagged<Object> object) {
+  DCHECK(!object.IsSmi());
   DCHECK(!HasWeakHeapObjectTag(object));
   return HeapObjectReference(object.ptr() | kWeakHeapObjectMask);
 }
 
 // static
-HeapObjectReference HeapObjectReference::From(Object object,
+HeapObjectReference HeapObjectReference::From(Tagged<Object> object,
                                               HeapObjectReferenceType type) {
-  DCHECK(!i::IsSmi(object));
+  DCHECK(!object.IsSmi());
   DCHECK(!HasWeakHeapObjectTag(object));
   switch (type) {
     case HeapObjectReferenceType::STRONG:
@@ -95,7 +95,8 @@ HeapObjectReference HeapObjectReference::ClearedValue(
 }
 
 template <typename THeapObjectSlot>
-void HeapObjectReference::Update(THeapObjectSlot slot, HeapObject value) {
+void HeapObjectReference::Update(THeapObjectSlot slot,
+                                 Tagged<HeapObject> value) {
   static_assert(std::is_same<THeapObjectSlot, FullHeapObjectSlot>::value ||
                     std::is_same<THeapObjectSlot, HeapObjectSlot>::value,
                 "Only FullHeapObjectSlot and HeapObjectSlot are expected here");
diff --git a/src/objects/maybe-object.h b/src/objects/maybe-object.h
index 1005693ae1d..3266abb790f 100644
--- a/src/objects/maybe-object.h
+++ b/src/objects/maybe-object.h
@@ -21,15 +21,15 @@ class MaybeObject : public TaggedImpl<HeapObjectReferenceType::WEAK, Address> {
   // These operator->() overloads are required for handlified code.
   constexpr const MaybeObject* operator->() const { return this; }
 
-  V8_INLINE static MaybeObject FromSmi(Smi smi);
+  V8_INLINE static MaybeObject FromSmi(Tagged<Smi> smi);
 
-  V8_INLINE static MaybeObject FromObject(Object object);
+  V8_INLINE static MaybeObject FromObject(Tagged<Object> object);
 
   V8_INLINE static MaybeObject MakeWeak(MaybeObject object);
 
   V8_INLINE static MaybeObject Create(MaybeObject o);
-  V8_INLINE static MaybeObject Create(Object o);
-  V8_INLINE static MaybeObject Create(Smi smi);
+  V8_INLINE static MaybeObject Create(Tagged<Object> o);
+  V8_INLINE static MaybeObject Create(Tagged<Smi> smi);
 
 #ifdef VERIFY_HEAP
   static void VerifyMaybeObjectPointer(Isolate* isolate, MaybeObject p);
@@ -45,19 +45,19 @@ class MaybeObject : public TaggedImpl<HeapObjectReferenceType::WEAK, Address> {
 class HeapObjectReference : public MaybeObject {
  public:
   explicit HeapObjectReference(Address address) : MaybeObject(address) {}
-  V8_INLINE explicit HeapObjectReference(Object object);
+  V8_INLINE explicit HeapObjectReference(Tagged<Object> object);
 
-  V8_INLINE static HeapObjectReference Strong(Object object);
+  V8_INLINE static HeapObjectReference Strong(Tagged<Object> object);
 
-  V8_INLINE static HeapObjectReference Weak(Object object);
+  V8_INLINE static HeapObjectReference Weak(Tagged<Object> object);
 
-  V8_INLINE static HeapObjectReference From(Object object,
+  V8_INLINE static HeapObjectReference From(Tagged<Object> object,
                                             HeapObjectReferenceType type);
 
   V8_INLINE static HeapObjectReference ClearedValue(PtrComprCageBase cage_base);
 
   template <typename THeapObjectSlot>
-  V8_INLINE static void Update(THeapObjectSlot slot, HeapObject value);
+  V8_INLINE static void Update(THeapObjectSlot slot, Tagged<HeapObject> value);
 };
 
 }  // namespace internal
diff --git a/src/objects/module-inl.h b/src/objects/module-inl.h
index e5a20bdd35a..347ccdc33a8 100644
--- a/src/objects/module-inl.h
+++ b/src/objects/module-inl.h
@@ -41,35 +41,35 @@ struct Module::Hash {
   }
 };
 
-SourceTextModuleInfo SourceTextModule::info() const {
+Tagged<SourceTextModuleInfo> SourceTextModule::info() const {
   return GetSharedFunctionInfo()->scope_info()->ModuleDescriptorInfo();
 }
 
 OBJECT_CONSTRUCTORS_IMPL(SourceTextModuleInfo, FixedArray)
 CAST_ACCESSOR(SourceTextModuleInfo)
 
-FixedArray SourceTextModuleInfo::module_requests() const {
+Tagged<FixedArray> SourceTextModuleInfo::module_requests() const {
   return FixedArray::cast(get(kModuleRequestsIndex));
 }
 
-FixedArray SourceTextModuleInfo::special_exports() const {
+Tagged<FixedArray> SourceTextModuleInfo::special_exports() const {
   return FixedArray::cast(get(kSpecialExportsIndex));
 }
 
-FixedArray SourceTextModuleInfo::regular_exports() const {
+Tagged<FixedArray> SourceTextModuleInfo::regular_exports() const {
   return FixedArray::cast(get(kRegularExportsIndex));
 }
 
-FixedArray SourceTextModuleInfo::regular_imports() const {
+Tagged<FixedArray> SourceTextModuleInfo::regular_imports() const {
   return FixedArray::cast(get(kRegularImportsIndex));
 }
 
-FixedArray SourceTextModuleInfo::namespace_imports() const {
+Tagged<FixedArray> SourceTextModuleInfo::namespace_imports() const {
   return FixedArray::cast(get(kNamespaceImportsIndex));
 }
 
 #ifdef DEBUG
-bool SourceTextModuleInfo::Equals(SourceTextModuleInfo other) const {
+bool SourceTextModuleInfo::Equals(Tagged<SourceTextModuleInfo> other) const {
   return regular_exports() == other->regular_exports() &&
          regular_imports() == other->regular_imports() &&
          special_exports() == other->special_exports() &&
diff --git a/src/objects/module.cc b/src/objects/module.cc
index 766a28399a0..d4b170e2f63 100644
--- a/src/objects/module.cc
+++ b/src/objects/module.cc
@@ -27,7 +27,7 @@ namespace internal {
 
 namespace {
 #ifdef DEBUG
-void PrintModuleName(Module module, std::ostream& os) {
+void PrintModuleName(Tagged<Module> module, std::ostream& os) {
   if (IsSourceTextModule(module)) {
     Print(SourceTextModule::cast(module)->GetScript()->GetNameOrSourceURL(),
           os);
@@ -39,7 +39,7 @@ void PrintModuleName(Module module, std::ostream& os) {
 #endif  // OBJECT_PRINT
 }
 
-void PrintStatusTransition(Module module, Module::Status old_status) {
+void PrintStatusTransition(Tagged<Module> module, Module::Status old_status) {
   if (!v8_flags.trace_module_status) return;
   StdoutStream os;
   os << "Changing module status from " << old_status << " to "
@@ -47,7 +47,7 @@ void PrintStatusTransition(Module module, Module::Status old_status) {
   PrintModuleName(module, os);
 }
 
-void PrintStatusMessage(Module module, const char* message) {
+void PrintStatusMessage(Tagged<Module> module, const char* message) {
   if (!v8_flags.trace_module_status) return;
   StdoutStream os;
   os << "Instantiating module ";
@@ -55,14 +55,14 @@ void PrintStatusMessage(Module module, const char* message) {
 }
 #endif  // DEBUG
 
-void SetStatusInternal(Module module, Module::Status new_status) {
+void SetStatusInternal(Tagged<Module> module, Module::Status new_status) {
   DisallowGarbageCollection no_gc;
 #ifdef DEBUG
   Module::Status old_status = static_cast<Module::Status>(module->status());
   module->set_status(new_status);
   PrintStatusTransition(module, old_status);
 #else
-  module.set_status(new_status);
+  module->set_status(new_status);
 #endif  // DEBUG
 }
 
@@ -75,7 +75,7 @@ void Module::SetStatus(Status new_status) {
   SetStatusInternal(*this, new_status);
 }
 
-void Module::RecordError(Isolate* isolate, Object error) {
+void Module::RecordError(Isolate* isolate, Tagged<Object> error) {
   DisallowGarbageCollection no_gc;
   // Allow overriding exceptions with termination exceptions.
   DCHECK_IMPLIES(isolate->is_catchable_by_javascript(error),
@@ -144,7 +144,7 @@ void Module::Reset(Isolate* isolate, Handle<Module> module) {
   SetStatusInternal(*module, kUnlinked);
 }
 
-Object Module::GetException() {
+Tagged<Object> Module::GetException() {
   DisallowGarbageCollection no_gc;
   DCHECK_EQ(status(), Module::kErrored);
   DCHECK(!IsTheHole(exception()));
@@ -308,7 +308,7 @@ Handle<JSModuleNamespace> Module::GetModuleNamespace(Isolate* isolate,
   ZoneVector<Handle<String>> names(&zone);
   names.reserve(exports->NumberOfElements());
   for (InternalIndex i : exports->IterateEntries()) {
-    Object key;
+    Tagged<Object> key;
     if (!exports->ToKey(roots, i, &key)) continue;
     names.push_back(handle(String::cast(key), isolate));
   }
@@ -455,7 +455,7 @@ bool Module::IsGraphAsync(Isolate* isolate) const {
 
   // Only SourceTextModules may be async.
   if (!IsSourceTextModule(*this)) return false;
-  SourceTextModule root = SourceTextModule::cast(*this);
+  Tagged<SourceTextModule> root = SourceTextModule::cast(*this);
 
   Zone zone(isolate->allocator(), ZONE_NAME);
   const size_t bucket_count = 2;
@@ -465,14 +465,14 @@ bool Module::IsGraphAsync(Isolate* isolate) const {
   worklist.push_back(root);
 
   do {
-    SourceTextModule current = worklist.back();
+    Tagged<SourceTextModule> current = worklist.back();
     worklist.pop_back();
     DCHECK_GE(current->status(), kLinked);
 
     if (current->async()) return true;
-    FixedArray requested_modules = current->requested_modules();
+    Tagged<FixedArray> requested_modules = current->requested_modules();
     for (int i = 0, length = requested_modules->length(); i < length; ++i) {
-      Module descendant = Module::cast(requested_modules->get(i));
+      Tagged<Module> descendant = Module::cast(requested_modules->get(i));
       if (IsSourceTextModule(descendant)) {
         const bool cycle = !visited.insert(descendant).second;
         if (!cycle) worklist.push_back(SourceTextModule::cast(descendant));
diff --git a/src/objects/module.h b/src/objects/module.h
index 20d5b390a73..4c47c9d0ae8 100644
--- a/src/objects/module.h
+++ b/src/objects/module.h
@@ -47,7 +47,7 @@ class Module : public TorqueGeneratedModule<Module, HeapObject> {
   };
 
   // The exception in the case {status} is kErrored.
-  Object GetException();
+  Tagged<Object> GetException();
 
   // Returns if this module or any transitively requested module is [[Async]],
   // i.e. has a top-level await.
@@ -119,7 +119,7 @@ class Module : public TorqueGeneratedModule<Module, HeapObject> {
 
   // To set status to kErrored, RecordError should be used.
   void SetStatus(Status status);
-  void RecordError(Isolate* isolate, Object error);
+  void RecordError(Isolate* isolate, Tagged<Object> error);
 
   TQ_OBJECT_CONSTRUCTORS(Module)
 };
diff --git a/src/objects/name-inl.h b/src/objects/name-inl.h
index 92f622eb7d9..a9d92a5ebda 100644
--- a/src/objects/name-inl.h
+++ b/src/objects/name-inl.h
@@ -67,7 +67,7 @@ DEF_HEAP_OBJECT_PREDICATE(Name, IsUniqueName) {
   return result;
 }
 
-bool Name::Equals(Name other) {
+bool Name::Equals(Tagged<Name> other) {
   if (other == *this) return true;
   if ((IsInternalizedString(*this) && IsInternalizedString(other)) ||
       IsSymbol(*this) || IsSymbol(other)) {
diff --git a/src/objects/name.h b/src/objects/name.h
index c3cbd9fd025..6793f4976e7 100644
--- a/src/objects/name.h
+++ b/src/objects/name.h
@@ -62,7 +62,7 @@ class Name : public TorqueGeneratedName<Name, PrimitiveHeapObject> {
   inline bool TryGetHash(uint32_t* hash) const;
 
   // Equality operations.
-  inline bool Equals(Name other);
+  inline bool Equals(Tagged<Name> other);
   inline static bool Equals(Isolate* isolate, Handle<Name> one,
                             Handle<Name> two);
 
diff --git a/src/objects/object-macros.h b/src/objects/object-macros.h
index 4b5103bfe93..ab2467ef41e 100644
--- a/src/objects/object-macros.h
+++ b/src/objects/object-macros.h
@@ -177,12 +177,12 @@
   V8_INLINE static Tagged<Type> cast(Tagged<Object> object); \
   V8_INLINE static constexpr Tagged<Type> unchecked_cast(    \
       Tagged<Object> object) {                               \
-    return Tagged<Type>(object.ptr());                       \
+    return Tagged<Type>::unchecked_cast(object);             \
   }
 
 #define CAST_ACCESSOR(Type)                        \
   Tagged<Type> Type::cast(Tagged<Object> object) { \
-    return Tagged<Type>(Type(object.ptr()).ptr()); \
+    return Tagged<Type>(Type(object.ptr()));       \
   }
 
 #define DEF_PRIMITIVE_ACCESSORS(holder, name, offset, type)     \
diff --git a/src/objects/objects-body-descriptors-inl.h b/src/objects/objects-body-descriptors-inl.h
index 08fe7154b59..9b1d7b00a8b 100644
--- a/src/objects/objects-body-descriptors-inl.h
+++ b/src/objects/objects-body-descriptors-inl.h
@@ -48,18 +48,20 @@ namespace v8 {
 namespace internal {
 
 template <int start_offset>
-int FlexibleBodyDescriptor<start_offset>::SizeOf(Map map, HeapObject object) {
+int FlexibleBodyDescriptor<start_offset>::SizeOf(Tagged<Map> map,
+                                                 Tagged<HeapObject> object) {
   return object->SizeFromMap(map);
 }
 
 template <int start_offset>
-int FlexibleWeakBodyDescriptor<start_offset>::SizeOf(Map map,
-                                                     HeapObject object) {
+int FlexibleWeakBodyDescriptor<start_offset>::SizeOf(
+    Tagged<Map> map, Tagged<HeapObject> object) {
   return object->SizeFromMap(map);
 }
 
 template <typename ObjectVisitor>
-void BodyDescriptorBase::IterateJSObjectBodyImpl(Map map, HeapObject obj,
+void BodyDescriptorBase::IterateJSObjectBodyImpl(Tagged<Map> map,
+                                                 Tagged<HeapObject> obj,
                                                  int start_offset,
                                                  int end_offset,
                                                  ObjectVisitor* v) {
@@ -95,10 +97,9 @@ void BodyDescriptorBase::IterateJSObjectBodyImpl(Map map, HeapObject obj,
 }
 
 template <typename ObjectVisitor>
-DISABLE_CFI_PERF void BodyDescriptorBase::IteratePointers(HeapObject obj,
-                                                          int start_offset,
-                                                          int end_offset,
-                                                          ObjectVisitor* v) {
+DISABLE_CFI_PERF void BodyDescriptorBase::IteratePointers(
+    Tagged<HeapObject> obj, int start_offset, int end_offset,
+    ObjectVisitor* v) {
   if (start_offset == HeapObject::kMapOffset) {
     v->VisitMapPointer(obj);
     start_offset += kTaggedSize;
@@ -107,7 +108,7 @@ DISABLE_CFI_PERF void BodyDescriptorBase::IteratePointers(HeapObject obj,
 }
 
 template <typename ObjectVisitor>
-void BodyDescriptorBase::IteratePointer(HeapObject obj, int offset,
+void BodyDescriptorBase::IteratePointer(Tagged<HeapObject> obj, int offset,
                                         ObjectVisitor* v) {
   DCHECK_NE(offset, HeapObject::kMapOffset);
   v->VisitPointer(obj, obj->RawField(offset));
@@ -115,47 +116,49 @@ void BodyDescriptorBase::IteratePointer(HeapObject obj, int offset,
 
 template <typename ObjectVisitor>
 DISABLE_CFI_PERF void BodyDescriptorBase::IterateMaybeWeakPointers(
-    HeapObject obj, int start_offset, int end_offset, ObjectVisitor* v) {
+    Tagged<HeapObject> obj, int start_offset, int end_offset,
+    ObjectVisitor* v) {
   v->VisitPointers(obj, obj->RawMaybeWeakField(start_offset),
                    obj->RawMaybeWeakField(end_offset));
 }
 
 template <typename ObjectVisitor>
-void BodyDescriptorBase::IterateMaybeWeakPointer(HeapObject obj, int offset,
-                                                 ObjectVisitor* v) {
+void BodyDescriptorBase::IterateMaybeWeakPointer(Tagged<HeapObject> obj,
+                                                 int offset, ObjectVisitor* v) {
   DCHECK_NE(offset, HeapObject::kMapOffset);
   v->VisitPointer(obj, obj->RawMaybeWeakField(offset));
 }
 
 template <typename ObjectVisitor>
 DISABLE_CFI_PERF void BodyDescriptorBase::IterateCustomWeakPointers(
-    HeapObject obj, int start_offset, int end_offset, ObjectVisitor* v) {
+    Tagged<HeapObject> obj, int start_offset, int end_offset,
+    ObjectVisitor* v) {
   v->VisitCustomWeakPointers(obj, obj->RawField(start_offset),
                              obj->RawField(end_offset));
 }
 
 template <typename ObjectVisitor>
-DISABLE_CFI_PERF void BodyDescriptorBase::IterateEphemeron(HeapObject obj,
-                                                           int index,
-                                                           int key_offset,
-                                                           int value_offset,
-                                                           ObjectVisitor* v) {
+DISABLE_CFI_PERF void BodyDescriptorBase::IterateEphemeron(
+    Tagged<HeapObject> obj, int index, int key_offset, int value_offset,
+    ObjectVisitor* v) {
   v->VisitEphemeron(obj, index, obj->RawField(key_offset),
                     obj->RawField(value_offset));
 }
 
 template <typename ObjectVisitor>
-void BodyDescriptorBase::IterateCustomWeakPointer(HeapObject obj, int offset,
+void BodyDescriptorBase::IterateCustomWeakPointer(Tagged<HeapObject> obj,
+                                                  int offset,
                                                   ObjectVisitor* v) {
   v->VisitCustomWeakPointer(obj, obj->RawField(offset));
 }
 
 template <typename ObjectVisitor>
-void BodyDescriptorBase::IterateMaybeIndirectPointer(HeapObject obj, int offset,
+void BodyDescriptorBase::IterateMaybeIndirectPointer(Tagged<HeapObject> obj,
+                                                     int offset,
                                                      ObjectVisitor* v,
                                                      IndirectPointerMode mode) {
 #ifdef V8_CODE_POINTER_SANDBOXING
-  v->VisitIndirectPointer(obj, obj.RawIndirectPointerField(offset), mode);
+  v->VisitIndirectPointer(obj, obj->RawIndirectPointerField(offset), mode);
 #else
   if (mode == IndirectPointerMode::kStrong) {
     IteratePointer(obj, offset, v);
@@ -168,10 +171,10 @@ void BodyDescriptorBase::IterateMaybeIndirectPointer(HeapObject obj, int offset,
 class HeapNumber::BodyDescriptor final : public BodyDescriptorBase {
  public:
   template <typename ObjectVisitor>
-  static inline void IterateBody(Map map, HeapObject obj, int object_size,
-                                 ObjectVisitor* v) {}
+  static inline void IterateBody(Tagged<Map> map, Tagged<HeapObject> obj,
+                                 int object_size, ObjectVisitor* v) {}
 
-  static inline int SizeOf(Map map, HeapObject object) {
+  static inline int SizeOf(Tagged<Map> map, Tagged<HeapObject> object) {
     return HeapNumber::kSize;
   }
 };
@@ -179,14 +182,14 @@ class HeapNumber::BodyDescriptor final : public BodyDescriptorBase {
 // This is a descriptor for one/two pointer fillers.
 class FreeSpaceFillerBodyDescriptor final : public DataOnlyBodyDescriptor {
  public:
-  static inline int SizeOf(Map map, HeapObject raw_object) {
+  static inline int SizeOf(Tagged<Map> map, Tagged<HeapObject> raw_object) {
     return map->instance_size();
   }
 };
 
 class FreeSpace::BodyDescriptor final : public DataOnlyBodyDescriptor {
  public:
-  static inline int SizeOf(Map map, HeapObject raw_object) {
+  static inline int SizeOf(Tagged<Map> map, Tagged<HeapObject> raw_object) {
     return FreeSpace::unchecked_cast(raw_object)->Size();
   }
 };
@@ -196,12 +199,12 @@ class JSObject::BodyDescriptor final : public BodyDescriptorBase {
   static const int kStartOffset = JSReceiver::kPropertiesOrHashOffset;
 
   template <typename ObjectVisitor>
-  static inline void IterateBody(Map map, HeapObject obj, int object_size,
-                                 ObjectVisitor* v) {
+  static inline void IterateBody(Tagged<Map> map, Tagged<HeapObject> obj,
+                                 int object_size, ObjectVisitor* v) {
     IterateJSObjectBodyImpl(map, obj, kStartOffset, object_size, v);
   }
 
-  static inline int SizeOf(Map map, HeapObject object) {
+  static inline int SizeOf(Tagged<Map> map, Tagged<HeapObject> object) {
     return map->instance_size();
   }
 };
@@ -211,12 +214,12 @@ class JSObject::FastBodyDescriptor final : public BodyDescriptorBase {
   static const int kStartOffset = JSReceiver::kPropertiesOrHashOffset;
 
   template <typename ObjectVisitor>
-  static inline void IterateBody(Map map, HeapObject obj, int object_size,
-                                 ObjectVisitor* v) {
+  static inline void IterateBody(Tagged<Map> map, Tagged<HeapObject> obj,
+                                 int object_size, ObjectVisitor* v) {
     IteratePointers(obj, kStartOffset, object_size, v);
   }
 
-  static inline int SizeOf(Map map, HeapObject object) {
+  static inline int SizeOf(Tagged<Map> map, Tagged<HeapObject> object) {
     return map->instance_size();
   }
 };
@@ -224,15 +227,15 @@ class JSObject::FastBodyDescriptor final : public BodyDescriptorBase {
 class WeakCell::BodyDescriptor final : public BodyDescriptorBase {
  public:
   template <typename ObjectVisitor>
-  static inline void IterateBody(Map map, HeapObject obj, int object_size,
-                                 ObjectVisitor* v) {
+  static inline void IterateBody(Tagged<Map> map, Tagged<HeapObject> obj,
+                                 int object_size, ObjectVisitor* v) {
     IteratePointers(obj, HeapObject::kHeaderSize, kTargetOffset, v);
     IterateCustomWeakPointer(obj, kTargetOffset, v);
     IterateCustomWeakPointer(obj, kUnregisterTokenOffset, v);
     IteratePointers(obj, kUnregisterTokenOffset + kTaggedSize, object_size, v);
   }
 
-  static inline int SizeOf(Map map, HeapObject object) {
+  static inline int SizeOf(Tagged<Map> map, Tagged<HeapObject> object) {
     return map->instance_size();
   }
 };
@@ -240,15 +243,15 @@ class WeakCell::BodyDescriptor final : public BodyDescriptorBase {
 class JSWeakRef::BodyDescriptor final : public BodyDescriptorBase {
  public:
   template <typename ObjectVisitor>
-  static inline void IterateBody(Map map, HeapObject obj, int object_size,
-                                 ObjectVisitor* v) {
+  static inline void IterateBody(Tagged<Map> map, Tagged<HeapObject> obj,
+                                 int object_size, ObjectVisitor* v) {
     IteratePointers(obj, JSReceiver::kPropertiesOrHashOffset, kTargetOffset, v);
     IterateCustomWeakPointer(obj, kTargetOffset, v);
     IterateJSObjectBodyImpl(map, obj, kTargetOffset + kTaggedSize, object_size,
                             v);
   }
 
-  static inline int SizeOf(Map map, HeapObject object) {
+  static inline int SizeOf(Tagged<Map> map, Tagged<HeapObject> object) {
     return map->instance_size();
   }
 };
@@ -256,8 +259,8 @@ class JSWeakRef::BodyDescriptor final : public BodyDescriptorBase {
 class JSFinalizationRegistry::BodyDescriptor final : public BodyDescriptorBase {
  public:
   template <typename ObjectVisitor>
-  static inline void IterateBody(Map map, HeapObject obj, int object_size,
-                                 ObjectVisitor* v) {
+  static inline void IterateBody(Tagged<Map> map, Tagged<HeapObject> obj,
+                                 int object_size, ObjectVisitor* v) {
     IteratePointers(obj, JSObject::BodyDescriptor::kStartOffset,
                     kNextDirtyOffset, v);
     IterateCustomWeakPointer(obj, kNextDirtyOffset, v);
@@ -265,7 +268,7 @@ class JSFinalizationRegistry::BodyDescriptor final : public BodyDescriptorBase {
                             object_size, v);
   }
 
-  static inline int SizeOf(Map map, HeapObject object) {
+  static inline int SizeOf(Tagged<Map> map, Tagged<HeapObject> object) {
     return map->instance_size();
   }
 };
@@ -280,8 +283,8 @@ class AllocationSite::BodyDescriptor final : public BodyDescriptorBase {
                 AllocationSite::kWeakNextOffset);
 
   template <typename ObjectVisitor>
-  static inline void IterateBody(Map map, HeapObject obj, int object_size,
-                                 ObjectVisitor* v) {
+  static inline void IterateBody(Tagged<Map> map, Tagged<HeapObject> obj,
+                                 int object_size, ObjectVisitor* v) {
     // Iterate over all the common pointer fields
     IteratePointers(obj, AllocationSite::kStartOffset,
                     AllocationSite::kCommonPointerFieldEndOffset, v);
@@ -293,7 +296,7 @@ class AllocationSite::BodyDescriptor final : public BodyDescriptorBase {
     }
   }
 
-  static inline int SizeOf(Map map, HeapObject object) {
+  static inline int SizeOf(Tagged<Map> map, Tagged<HeapObject> object) {
     return map->instance_size();
   }
 };
@@ -303,8 +306,8 @@ class JSFunction::BodyDescriptor final : public BodyDescriptorBase {
   static const int kStartOffset = JSObject::BodyDescriptor::kStartOffset;
 
   template <typename ObjectVisitor>
-  static inline void IterateBody(Map map, HeapObject obj, int object_size,
-                                 ObjectVisitor* v) {
+  static inline void IterateBody(Tagged<Map> map, Tagged<HeapObject> obj,
+                                 int object_size, ObjectVisitor* v) {
     // Iterate JSFunction header fields first.
     int header_size = JSFunction::GetHeaderSize(map->has_prototype_slot());
     DCHECK_GE(object_size, header_size);
@@ -328,7 +331,7 @@ class JSFunction::BodyDescriptor final : public BodyDescriptorBase {
     IterateJSObjectBodyImpl(map, obj, header_size, object_size, v);
   }
 
-  static inline int SizeOf(Map map, HeapObject object) {
+  static inline int SizeOf(Tagged<Map> map, Tagged<HeapObject> object) {
     return map->instance_size();
   }
 };
@@ -336,8 +339,8 @@ class JSFunction::BodyDescriptor final : public BodyDescriptorBase {
 class JSArrayBuffer::BodyDescriptor final : public BodyDescriptorBase {
  public:
   template <typename ObjectVisitor>
-  static inline void IterateBody(Map map, HeapObject obj, int object_size,
-                                 ObjectVisitor* v) {
+  static inline void IterateBody(Tagged<Map> map, Tagged<HeapObject> obj,
+                                 int object_size, ObjectVisitor* v) {
     // JSArrayBuffer instances contain raw data that the GC does not know about.
     IteratePointers(obj, kPropertiesOrHashOffset, kEndOfTaggedFieldsOffset, v);
     IterateJSObjectBodyImpl(map, obj, kHeaderSize, object_size, v);
@@ -345,7 +348,7 @@ class JSArrayBuffer::BodyDescriptor final : public BodyDescriptorBase {
                             kArrayBufferExtensionTag);
   }
 
-  static inline int SizeOf(Map map, HeapObject object) {
+  static inline int SizeOf(Tagged<Map> map, Tagged<HeapObject> object) {
     return map->instance_size();
   }
 };
@@ -353,8 +356,8 @@ class JSArrayBuffer::BodyDescriptor final : public BodyDescriptorBase {
 class JSTypedArray::BodyDescriptor final : public BodyDescriptorBase {
  public:
   template <typename ObjectVisitor>
-  static inline void IterateBody(Map map, HeapObject obj, int object_size,
-                                 ObjectVisitor* v) {
+  static inline void IterateBody(Tagged<Map> map, Tagged<HeapObject> obj,
+                                 int object_size, ObjectVisitor* v) {
     // JSTypedArray contains raw data that the GC does not know about.
     IteratePointers(obj, kPropertiesOrHashOffset, kEndOfTaggedFieldsOffset, v);
     // TODO(v8:4153): Remove this.
@@ -362,7 +365,7 @@ class JSTypedArray::BodyDescriptor final : public BodyDescriptorBase {
     IterateJSObjectBodyImpl(map, obj, kHeaderSize, object_size, v);
   }
 
-  static inline int SizeOf(Map map, HeapObject object) {
+  static inline int SizeOf(Tagged<Map> map, Tagged<HeapObject> object) {
     return map->instance_size();
   }
 };
@@ -371,15 +374,15 @@ class JSDataViewOrRabGsabDataView::BodyDescriptor final
     : public BodyDescriptorBase {
  public:
   template <typename ObjectVisitor>
-  static inline void IterateBody(Map map, HeapObject obj, int object_size,
-                                 ObjectVisitor* v) {
+  static inline void IterateBody(Tagged<Map> map, Tagged<HeapObject> obj,
+                                 int object_size, ObjectVisitor* v) {
     // JSDataViewOrRabGsabDataView contains raw data that the GC does not know
     // about.
     IteratePointers(obj, kPropertiesOrHashOffset, kEndOfTaggedFieldsOffset, v);
     IterateJSObjectBodyImpl(map, obj, kHeaderSize, object_size, v);
   }
 
-  static inline int SizeOf(Map map, HeapObject object) {
+  static inline int SizeOf(Tagged<Map> map, Tagged<HeapObject> object) {
     return map->instance_size();
   }
 };
@@ -387,14 +390,14 @@ class JSDataViewOrRabGsabDataView::BodyDescriptor final
 class JSExternalObject::BodyDescriptor final : public BodyDescriptorBase {
  public:
   template <typename ObjectVisitor>
-  static inline void IterateBody(Map map, HeapObject obj, int object_size,
-                                 ObjectVisitor* v) {
+  static inline void IterateBody(Tagged<Map> map, Tagged<HeapObject> obj,
+                                 int object_size, ObjectVisitor* v) {
     IteratePointers(obj, kPropertiesOrHashOffset, kEndOfTaggedFieldsOffset, v);
     v->VisitExternalPointer(obj, obj->RawExternalPointerField(kValueOffset),
                             kExternalObjectValueTag);
   }
 
-  static inline int SizeOf(Map map, HeapObject object) {
+  static inline int SizeOf(Tagged<Map> map, Tagged<HeapObject> object) {
     return map->instance_size();
   }
 };
@@ -404,15 +407,15 @@ class V8_EXPORT_PRIVATE SmallOrderedHashTable<Derived>::BodyDescriptor final
     : public BodyDescriptorBase {
  public:
   template <typename ObjectVisitor>
-  static inline void IterateBody(Map map, HeapObject obj, int object_size,
-                                 ObjectVisitor* v) {
+  static inline void IterateBody(Tagged<Map> map, Tagged<HeapObject> obj,
+                                 int object_size, ObjectVisitor* v) {
     Derived table = Derived::cast(obj);
     int start_offset = DataTableStartOffset();
     int end_offset = table.GetBucketsStartOffset();
     IteratePointers(obj, start_offset, end_offset, v);
   }
 
-  static inline int SizeOf(Map map, HeapObject obj) {
+  static inline int SizeOf(Tagged<Map> map, Tagged<HeapObject> obj) {
     Derived table = Derived::cast(obj);
     return Derived::SizeFor(table.Capacity());
   }
@@ -422,18 +425,20 @@ class V8_EXPORT_PRIVATE SwissNameDictionary::BodyDescriptor final
     : public BodyDescriptorBase {
  public:
   template <typename ObjectVisitor>
-  static inline void IterateBody(Map map, HeapObject obj, int object_size,
-                                 ObjectVisitor* v) {
-    SwissNameDictionary table = SwissNameDictionary::unchecked_cast(obj);
+  static inline void IterateBody(Tagged<Map> map, Tagged<HeapObject> obj,
+                                 int object_size, ObjectVisitor* v) {
+    Tagged<SwissNameDictionary> table =
+        SwissNameDictionary::unchecked_cast(obj);
     static_assert(MetaTablePointerOffset() + kTaggedSize ==
                   DataTableStartOffset());
     int start_offset = MetaTablePointerOffset();
-    int end_offset = table.DataTableEndOffset(table->Capacity());
+    int end_offset = table->DataTableEndOffset(table->Capacity());
     IteratePointers(obj, start_offset, end_offset, v);
   }
 
-  static inline int SizeOf(Map map, HeapObject obj) {
-    SwissNameDictionary table = SwissNameDictionary::unchecked_cast(obj);
+  static inline int SizeOf(Tagged<Map> map, Tagged<HeapObject> obj) {
+    Tagged<SwissNameDictionary> table =
+        SwissNameDictionary::unchecked_cast(obj);
     return SwissNameDictionary::SizeFor(table->Capacity());
   }
 };
@@ -441,10 +446,10 @@ class V8_EXPORT_PRIVATE SwissNameDictionary::BodyDescriptor final
 class ByteArray::BodyDescriptor final : public BodyDescriptorBase {
  public:
   template <typename ObjectVisitor>
-  static inline void IterateBody(Map map, HeapObject obj, int object_size,
-                                 ObjectVisitor* v) {}
+  static inline void IterateBody(Tagged<Map> map, Tagged<HeapObject> obj,
+                                 int object_size, ObjectVisitor* v) {}
 
-  static inline int SizeOf(Map map, HeapObject obj) {
+  static inline int SizeOf(Tagged<Map> map, Tagged<HeapObject> obj) {
     return ByteArray::SizeFor(
         ByteArray::unchecked_cast(obj)->length(kAcquireLoad));
   }
@@ -453,14 +458,14 @@ class ByteArray::BodyDescriptor final : public BodyDescriptorBase {
 class BytecodeArray::BodyDescriptor final : public BodyDescriptorBase {
  public:
   template <typename ObjectVisitor>
-  static inline void IterateBody(Map map, HeapObject obj, int object_size,
-                                 ObjectVisitor* v) {
+  static inline void IterateBody(Tagged<Map> map, Tagged<HeapObject> obj,
+                                 int object_size, ObjectVisitor* v) {
     IteratePointer(obj, kConstantPoolOffset, v);
     IteratePointer(obj, kHandlerTableOffset, v);
     IteratePointer(obj, kSourcePositionTableOffset, v);
   }
 
-  static inline int SizeOf(Map map, HeapObject obj) {
+  static inline int SizeOf(Tagged<Map> map, Tagged<HeapObject> obj) {
     return BytecodeArray::SizeFor(
         BytecodeArray::cast(obj)->length(kAcquireLoad));
   }
@@ -469,9 +474,10 @@ class BytecodeArray::BodyDescriptor final : public BodyDescriptorBase {
 class ExternalPointerArray::BodyDescriptor final : public BodyDescriptorBase {
  public:
   template <typename ObjectVisitor>
-  static inline void IterateBody(Map map, HeapObject obj, int object_size,
-                                 ObjectVisitor* v) {
-    ExternalPointerArray array = ExternalPointerArray::unchecked_cast(obj);
+  static inline void IterateBody(Tagged<Map> map, Tagged<HeapObject> obj,
+                                 int object_size, ObjectVisitor* v) {
+    Tagged<ExternalPointerArray> array =
+        ExternalPointerArray::unchecked_cast(obj);
     for (int i = 0; i < array->length(); i++) {
       // We don't currently track the (expected) tag of the elements of this
       // array, so we have to use the generic tag here. This is ok as long as
@@ -486,7 +492,7 @@ class ExternalPointerArray::BodyDescriptor final : public BodyDescriptorBase {
     }
   }
 
-  static inline int SizeOf(Map map, HeapObject obj) {
+  static inline int SizeOf(Tagged<Map> map, Tagged<HeapObject> obj) {
     return ExternalPointerArray::SizeFor(
         ExternalPointerArray::cast(obj)->length(kAcquireLoad));
   }
@@ -495,10 +501,10 @@ class ExternalPointerArray::BodyDescriptor final : public BodyDescriptorBase {
 class BigInt::BodyDescriptor final : public BodyDescriptorBase {
  public:
   template <typename ObjectVisitor>
-  static inline void IterateBody(Map map, HeapObject obj, int object_size,
-                                 ObjectVisitor* v) {}
+  static inline void IterateBody(Tagged<Map> map, Tagged<HeapObject> obj,
+                                 int object_size, ObjectVisitor* v) {}
 
-  static inline int SizeOf(Map map, HeapObject obj) {
+  static inline int SizeOf(Tagged<Map> map, Tagged<HeapObject> obj) {
     return BigInt::SizeFor(BigInt::unchecked_cast(obj)->length(kAcquireLoad));
   }
 };
@@ -506,10 +512,10 @@ class BigInt::BodyDescriptor final : public BodyDescriptorBase {
 class FixedDoubleArray::BodyDescriptor final : public BodyDescriptorBase {
  public:
   template <typename ObjectVisitor>
-  static inline void IterateBody(Map map, HeapObject obj, int object_size,
-                                 ObjectVisitor* v) {}
+  static inline void IterateBody(Tagged<Map> map, Tagged<HeapObject> obj,
+                                 int object_size, ObjectVisitor* v) {}
 
-  static inline int SizeOf(Map map, HeapObject obj) {
+  static inline int SizeOf(Tagged<Map> map, Tagged<HeapObject> obj) {
     return FixedDoubleArray::SizeFor(
         FixedDoubleArray::unchecked_cast(obj)->length(kAcquireLoad));
   }
@@ -518,10 +524,10 @@ class FixedDoubleArray::BodyDescriptor final : public BodyDescriptorBase {
 class FeedbackMetadata::BodyDescriptor final : public BodyDescriptorBase {
  public:
   template <typename ObjectVisitor>
-  static inline void IterateBody(Map map, HeapObject obj, int object_size,
-                                 ObjectVisitor* v) {}
+  static inline void IterateBody(Tagged<Map> map, Tagged<HeapObject> obj,
+                                 int object_size, ObjectVisitor* v) {}
 
-  static inline int SizeOf(Map map, HeapObject obj) {
+  static inline int SizeOf(Tagged<Map> map, Tagged<HeapObject> obj) {
     return FeedbackMetadata::SizeFor(
         FeedbackMetadata::unchecked_cast(obj)->slot_count(kAcquireLoad));
   }
@@ -530,16 +536,16 @@ class FeedbackMetadata::BodyDescriptor final : public BodyDescriptorBase {
 class PreparseData::BodyDescriptor final : public BodyDescriptorBase {
  public:
   template <typename ObjectVisitor>
-  static inline void IterateBody(Map map, HeapObject obj, int object_size,
-                                 ObjectVisitor* v) {
-    PreparseData data = PreparseData::unchecked_cast(obj);
+  static inline void IterateBody(Tagged<Map> map, Tagged<HeapObject> obj,
+                                 int object_size, ObjectVisitor* v) {
+    Tagged<PreparseData> data = PreparseData::unchecked_cast(obj);
     int start_offset = data->inner_start_offset();
     int end_offset = start_offset + data->children_length() * kTaggedSize;
     IteratePointers(obj, start_offset, end_offset, v);
   }
 
-  static inline int SizeOf(Map map, HeapObject obj) {
-    PreparseData data = PreparseData::unchecked_cast(obj);
+  static inline int SizeOf(Tagged<Map> map, Tagged<HeapObject> obj) {
+    Tagged<PreparseData> data = PreparseData::unchecked_cast(obj);
     return PreparseData::SizeFor(data->data_length(), data->children_length());
   }
 };
@@ -547,28 +553,30 @@ class PreparseData::BodyDescriptor final : public BodyDescriptorBase {
 class SharedFunctionInfo::BodyDescriptor final : public BodyDescriptorBase {
  public:
   template <typename ObjectVisitor>
-  static inline void IterateBody(Map map, HeapObject obj, int object_size,
-                                 ObjectVisitor* v) {
+  static inline void IterateBody(Tagged<Map> map, Tagged<HeapObject> obj,
+                                 int object_size, ObjectVisitor* v) {
     IterateCustomWeakPointers(obj, kStartOfWeakFieldsOffset,
                               kEndOfWeakFieldsOffset, v);
     IteratePointers(obj, kStartOfStrongFieldsOffset, kEndOfStrongFieldsOffset,
                     v);
   }
 
-  static inline int SizeOf(Map map, HeapObject raw_object) { return kSize; }
+  static inline int SizeOf(Tagged<Map> map, Tagged<HeapObject> raw_object) {
+    return kSize;
+  }
 };
 
 class PromiseOnStack::BodyDescriptor final : public BodyDescriptorBase {
  public:
   template <typename ObjectVisitor>
-  static inline void IterateBody(Map map, HeapObject obj, int object_size,
-                                 ObjectVisitor* v) {
+  static inline void IterateBody(Tagged<Map> map, Tagged<HeapObject> obj,
+                                 int object_size, ObjectVisitor* v) {
     IteratePointers(obj, Struct::kHeaderSize, kPromiseOffset, v);
     IterateMaybeWeakPointer(obj, kPromiseOffset, v);
     static_assert(kPromiseOffset + kTaggedSize == kHeaderSize);
   }
 
-  static inline int SizeOf(Map map, HeapObject obj) {
+  static inline int SizeOf(Tagged<Map> map, Tagged<HeapObject> obj) {
     return obj->SizeFromMap(map);
   }
 };
@@ -576,14 +584,14 @@ class PromiseOnStack::BodyDescriptor final : public BodyDescriptorBase {
 class PrototypeInfo::BodyDescriptor final : public BodyDescriptorBase {
  public:
   template <typename ObjectVisitor>
-  static inline void IterateBody(Map map, HeapObject obj, int object_size,
-                                 ObjectVisitor* v) {
+  static inline void IterateBody(Tagged<Map> map, Tagged<HeapObject> obj,
+                                 int object_size, ObjectVisitor* v) {
     IteratePointers(obj, HeapObject::kHeaderSize, kObjectCreateMapOffset, v);
     IterateMaybeWeakPointer(obj, kObjectCreateMapOffset, v);
     static_assert(kObjectCreateMapOffset + kTaggedSize == kHeaderSize);
   }
 
-  static inline int SizeOf(Map map, HeapObject obj) {
+  static inline int SizeOf(Tagged<Map> map, Tagged<HeapObject> obj) {
     return obj->SizeFromMap(map);
   }
 };
@@ -593,12 +601,12 @@ class JSWeakCollection::BodyDescriptorImpl final : public BodyDescriptorBase {
   static_assert(kTableOffset + kTaggedSize == kHeaderSizeOfAllWeakCollections);
 
   template <typename ObjectVisitor>
-  static inline void IterateBody(Map map, HeapObject obj, int object_size,
-                                 ObjectVisitor* v) {
+  static inline void IterateBody(Tagged<Map> map, Tagged<HeapObject> obj,
+                                 int object_size, ObjectVisitor* v) {
     IterateJSObjectBodyImpl(map, obj, kPropertiesOrHashOffset, object_size, v);
   }
 
-  static inline int SizeOf(Map map, HeapObject object) {
+  static inline int SizeOf(Tagged<Map> map, Tagged<HeapObject> object) {
     return map->instance_size();
   }
 };
@@ -607,12 +615,12 @@ class JSSynchronizationPrimitive::BodyDescriptor final
     : public BodyDescriptorBase {
  public:
   template <typename ObjectVisitor>
-  static inline void IterateBody(Map map, HeapObject obj, int object_size,
-                                 ObjectVisitor* v) {
+  static inline void IterateBody(Tagged<Map> map, Tagged<HeapObject> obj,
+                                 int object_size, ObjectVisitor* v) {
     IteratePointers(obj, kPropertiesOrHashOffset, kEndOfTaggedFieldsOffset, v);
   }
 
-  static inline int SizeOf(Map map, HeapObject object) {
+  static inline int SizeOf(Tagged<Map> map, Tagged<HeapObject> object) {
     return map->instance_size();
   }
 };
@@ -620,22 +628,24 @@ class JSSynchronizationPrimitive::BodyDescriptor final
 class Foreign::BodyDescriptor final : public BodyDescriptorBase {
  public:
   template <typename ObjectVisitor>
-  static inline void IterateBody(Map map, HeapObject obj, int object_size,
-                                 ObjectVisitor* v) {
+  static inline void IterateBody(Tagged<Map> map, Tagged<HeapObject> obj,
+                                 int object_size, ObjectVisitor* v) {
     v->VisitExternalPointer(obj,
                             obj->RawExternalPointerField(kForeignAddressOffset),
                             kForeignForeignAddressTag);
   }
 
-  static inline int SizeOf(Map map, HeapObject object) { return kSize; }
+  static inline int SizeOf(Tagged<Map> map, Tagged<HeapObject> object) {
+    return kSize;
+  }
 };
 
 #if V8_ENABLE_WEBASSEMBLY
 class WasmTypeInfo::BodyDescriptor final : public BodyDescriptorBase {
  public:
   template <typename ObjectVisitor>
-  static inline void IterateBody(Map map, HeapObject obj, int object_size,
-                                 ObjectVisitor* v) {
+  static inline void IterateBody(Tagged<Map> map, Tagged<HeapObject> obj,
+                                 int object_size, ObjectVisitor* v) {
     IteratePointer(obj, kInstanceOffset, v);
     IteratePointers(obj, kSupertypesOffset, SizeOf(map, obj), v);
 
@@ -644,7 +654,7 @@ class WasmTypeInfo::BodyDescriptor final : public BodyDescriptorBase {
                             kWasmTypeInfoNativeTypeTag);
   }
 
-  static inline int SizeOf(Map map, HeapObject object) {
+  static inline int SizeOf(Tagged<Map> map, Tagged<HeapObject> object) {
     return kSupertypesOffset +
            WasmTypeInfo::cast(object)->supertypes_length() * kTaggedSize;
   }
@@ -653,21 +663,23 @@ class WasmTypeInfo::BodyDescriptor final : public BodyDescriptorBase {
 class WasmApiFunctionRef::BodyDescriptor final : public BodyDescriptorBase {
  public:
   template <typename ObjectVisitor>
-  static inline void IterateBody(Map map, HeapObject obj, int object_size,
-                                 ObjectVisitor* v) {
+  static inline void IterateBody(Tagged<Map> map, Tagged<HeapObject> obj,
+                                 int object_size, ObjectVisitor* v) {
     IteratePointers(obj, kStartOfStrongFieldsOffset, kEndOfStrongFieldsOffset,
                     v);
   }
 
-  static inline int SizeOf(Map map, HeapObject object) { return kSize; }
+  static inline int SizeOf(Tagged<Map> map, Tagged<HeapObject> object) {
+    return kSize;
+  }
 };
 
 class WasmExportedFunctionData::BodyDescriptor final
     : public BodyDescriptorBase {
  public:
   template <typename ObjectVisitor>
-  static inline void IterateBody(Map map, HeapObject obj, int object_size,
-                                 ObjectVisitor* v) {
+  static inline void IterateBody(Tagged<Map> map, Tagged<HeapObject> obj,
+                                 int object_size, ObjectVisitor* v) {
     WasmFunctionData::BodyDescriptor::IterateBody<ObjectVisitor>(
         map, obj, object_size, v);
     IteratePointers(obj, kStartOfStrongFieldsOffset, kEndOfStrongFieldsOffset,
@@ -676,14 +688,16 @@ class WasmExportedFunctionData::BodyDescriptor final
                             kWasmExportedFunctionDataSignatureTag);
   }
 
-  static inline int SizeOf(Map map, HeapObject object) { return kSize; }
+  static inline int SizeOf(Tagged<Map> map, Tagged<HeapObject> object) {
+    return kSize;
+  }
 };
 
 class WasmInternalFunction::BodyDescriptor final : public BodyDescriptorBase {
  public:
   template <typename ObjectVisitor>
-  static inline void IterateBody(Map map, HeapObject obj, int object_size,
-                                 ObjectVisitor* v) {
+  static inline void IterateBody(Tagged<Map> map, Tagged<HeapObject> obj,
+                                 int object_size, ObjectVisitor* v) {
     IteratePointers(obj, kStartOfStrongFieldsOffset, kEndOfStrongFieldsOffset,
                     v);
     v->VisitExternalPointer(obj,
@@ -691,14 +705,16 @@ class WasmInternalFunction::BodyDescriptor final : public BodyDescriptorBase {
                             kWasmInternalFunctionCallTargetTag);
   }
 
-  static inline int SizeOf(Map map, HeapObject object) { return kSize; }
+  static inline int SizeOf(Tagged<Map> map, Tagged<HeapObject> object) {
+    return kSize;
+  }
 };
 
 class WasmInstanceObject::BodyDescriptor final : public BodyDescriptorBase {
  public:
   template <typename ObjectVisitor>
-  static inline void IterateBody(Map map, HeapObject obj, int object_size,
-                                 ObjectVisitor* v) {
+  static inline void IterateBody(Tagged<Map> map, Tagged<HeapObject> obj,
+                                 int object_size, ObjectVisitor* v) {
     IteratePointers(obj, kPropertiesOrHashOffset, JSObject::kHeaderSize, v);
     for (uint16_t offset : kTaggedFieldOffsets) {
       IteratePointer(obj, offset, v);
@@ -706,7 +722,7 @@ class WasmInstanceObject::BodyDescriptor final : public BodyDescriptorBase {
     IterateJSObjectBodyImpl(map, obj, kHeaderSize, object_size, v);
   }
 
-  static inline int SizeOf(Map map, HeapObject object) {
+  static inline int SizeOf(Tagged<Map> map, Tagged<HeapObject> object) {
     return map->instance_size();
   }
 };
@@ -714,15 +730,15 @@ class WasmInstanceObject::BodyDescriptor final : public BodyDescriptorBase {
 class WasmArray::BodyDescriptor final : public BodyDescriptorBase {
  public:
   template <typename ObjectVisitor>
-  static inline void IterateBody(Map map, HeapObject obj, int object_size,
-                                 ObjectVisitor* v) {
+  static inline void IterateBody(Tagged<Map> map, Tagged<HeapObject> obj,
+                                 int object_size, ObjectVisitor* v) {
     // The type is safe to use because it's kept alive by the {map}'s
     // WasmTypeInfo.
     if (!WasmArray::GcSafeType(map)->element_type().is_reference()) return;
     IteratePointers(obj, WasmArray::kHeaderSize, object_size, v);
   }
 
-  static inline int SizeOf(Map map, HeapObject object) {
+  static inline int SizeOf(Tagged<Map> map, Tagged<HeapObject> object) {
     return WasmArray::SizeFor(map, WasmArray::unchecked_cast(object)->length());
   }
 };
@@ -730,23 +746,25 @@ class WasmArray::BodyDescriptor final : public BodyDescriptorBase {
 class WasmContinuationObject::BodyDescriptor final : public BodyDescriptorBase {
  public:
   template <typename ObjectVisitor>
-  static inline void IterateBody(Map map, HeapObject obj, int object_size,
-                                 ObjectVisitor* v) {
+  static inline void IterateBody(Tagged<Map> map, Tagged<HeapObject> obj,
+                                 int object_size, ObjectVisitor* v) {
     IteratePointers(obj, kStartOfStrongFieldsOffset, kEndOfStrongFieldsOffset,
                     v);
     v->VisitExternalPointer(obj, obj->RawExternalPointerField(kJmpbufOffset),
                             kWasmContinuationJmpbufTag);
   }
 
-  static inline int SizeOf(Map map, HeapObject object) { return kSize; }
+  static inline int SizeOf(Tagged<Map> map, Tagged<HeapObject> object) {
+    return kSize;
+  }
 };
 
 class WasmStruct::BodyDescriptor final : public BodyDescriptorBase {
  public:
   template <typename ObjectVisitor>
-  static inline void IterateBody(Map map, HeapObject obj, int object_size,
-                                 ObjectVisitor* v) {
-    WasmStruct wasm_struct = WasmStruct::unchecked_cast(obj);
+  static inline void IterateBody(Tagged<Map> map, Tagged<HeapObject> obj,
+                                 int object_size, ObjectVisitor* v) {
+    Tagged<WasmStruct> wasm_struct = WasmStruct::unchecked_cast(obj);
     // The {type} is safe to use because it's kept alive by the {map}'s
     // WasmTypeInfo.
     wasm::StructType* type = WasmStruct::GcSafeType(map);
@@ -757,7 +775,7 @@ class WasmStruct::BodyDescriptor final : public BodyDescriptorBase {
     }
   }
 
-  static inline int SizeOf(Map map, HeapObject object) {
+  static inline int SizeOf(Tagged<Map> map, Tagged<HeapObject> object) {
     return WasmStruct::GcSafeSize(map);
   }
 };
@@ -765,19 +783,21 @@ class WasmStruct::BodyDescriptor final : public BodyDescriptorBase {
 class WasmNull::BodyDescriptor final : public BodyDescriptorBase {
  public:
   template <typename ObjectVisitor>
-  static inline void IterateBody(Map map, HeapObject obj, int object_size,
-                                 ObjectVisitor* v) {}
+  static inline void IterateBody(Tagged<Map> map, Tagged<HeapObject> obj,
+                                 int object_size, ObjectVisitor* v) {}
 
-  static inline int SizeOf(Map map, HeapObject obj) { return WasmNull::kSize; }
+  static inline int SizeOf(Tagged<Map> map, Tagged<HeapObject> obj) {
+    return WasmNull::kSize;
+  }
 };
 #endif  // V8_ENABLE_WEBASSEMBLY
 
 class ExternalString::BodyDescriptor final : public BodyDescriptorBase {
  public:
   template <typename ObjectVisitor>
-  static inline void IterateBody(Map map, HeapObject obj, int object_size,
-                                 ObjectVisitor* v) {
-    ExternalString string = ExternalString::unchecked_cast(obj);
+  static inline void IterateBody(Tagged<Map> map, Tagged<HeapObject> obj,
+                                 int object_size, ObjectVisitor* v) {
+    Tagged<ExternalString> string = ExternalString::unchecked_cast(obj);
     v->VisitExternalPointer(obj,
                             string->RawExternalPointerField(kResourceOffset),
                             kExternalStringResourceTag);
@@ -787,7 +807,7 @@ class ExternalString::BodyDescriptor final : public BodyDescriptorBase {
         kExternalStringResourceDataTag);
   }
 
-  static inline int SizeOf(Map map, HeapObject object) {
+  static inline int SizeOf(Tagged<Map> map, Tagged<HeapObject> object) {
     InstanceType type = map->instance_type();
     const auto is_uncached =
         (type & kUncachedExternalStringMask) == kUncachedExternalStringTag;
@@ -798,11 +818,11 @@ class ExternalString::BodyDescriptor final : public BodyDescriptorBase {
 class CoverageInfo::BodyDescriptor final : public BodyDescriptorBase {
  public:
   template <typename ObjectVisitor>
-  static inline void IterateBody(Map map, HeapObject obj, int object_size,
-                                 ObjectVisitor* v) {}
+  static inline void IterateBody(Tagged<Map> map, Tagged<HeapObject> obj,
+                                 int object_size, ObjectVisitor* v) {}
 
-  static inline int SizeOf(Map map, HeapObject object) {
-    CoverageInfo info = CoverageInfo::cast(object);
+  static inline int SizeOf(Tagged<Map> map, Tagged<HeapObject> object) {
+    Tagged<CoverageInfo> info = CoverageInfo::cast(object);
     return CoverageInfo::SizeFor(info->slot_count());
   }
 };
@@ -826,13 +846,14 @@ class InstructionStream::BodyDescriptor final : public BodyDescriptorBase {
       RelocInfo::ModeMask(RelocInfo::WASM_STUB_CALL);
 
   template <typename ObjectVisitor>
-  static inline void IterateBody(Map map, HeapObject obj, ObjectVisitor* v) {
+  static inline void IterateBody(Tagged<Map> map, Tagged<HeapObject> obj,
+                                 ObjectVisitor* v) {
     // GC does not visit data/code in the header and in the body directly.
     IteratePointers(obj, kStartOfStrongFieldsOffset, kEndOfStrongFieldsOffset,
                     v);
 
-    InstructionStream istream = InstructionStream::unchecked_cast(obj);
-    Code code;
+    Tagged<InstructionStream> istream = InstructionStream::unchecked_cast(obj);
+    Tagged<Code> code;
     if (istream->TryGetCodeUnchecked(&code, kAcquireLoad)) {
       RelocIterator it(code, istream, istream->unchecked_relocation_info(),
                        kRelocModeMask);
@@ -841,12 +862,12 @@ class InstructionStream::BodyDescriptor final : public BodyDescriptorBase {
   }
 
   template <typename ObjectVisitor>
-  static inline void IterateBody(Map map, HeapObject obj, int object_size,
-                                 ObjectVisitor* v) {
+  static inline void IterateBody(Tagged<Map> map, Tagged<HeapObject> obj,
+                                 int object_size, ObjectVisitor* v) {
     IterateBody(map, obj, v);
   }
 
-  static inline int SizeOf(Map map, HeapObject object) {
+  static inline int SizeOf(Tagged<Map> map, Tagged<HeapObject> object) {
     return InstructionStream::unchecked_cast(object)->Size();
   }
 };
@@ -854,21 +875,23 @@ class InstructionStream::BodyDescriptor final : public BodyDescriptorBase {
 class Map::BodyDescriptor final : public BodyDescriptorBase {
  public:
   template <typename ObjectVisitor>
-  static inline void IterateBody(Map map, HeapObject obj, int object_size,
-                                 ObjectVisitor* v) {
+  static inline void IterateBody(Tagged<Map> map, Tagged<HeapObject> obj,
+                                 int object_size, ObjectVisitor* v) {
     IteratePointers(obj, Map::kStartOfStrongFieldsOffset,
                     Map::kEndOfStrongFieldsOffset, v);
     IterateMaybeWeakPointer(obj, kTransitionsOrPrototypeInfoOffset, v);
   }
 
-  static inline int SizeOf(Map map, HeapObject obj) { return Map::kSize; }
+  static inline int SizeOf(Tagged<Map> map, Tagged<HeapObject> obj) {
+    return Map::kSize;
+  }
 };
 
 class DataHandler::BodyDescriptor final : public BodyDescriptorBase {
  public:
   template <typename ObjectVisitor>
-  static inline void IterateBody(Map map, HeapObject obj, int object_size,
-                                 ObjectVisitor* v) {
+  static inline void IterateBody(Tagged<Map> map, Tagged<HeapObject> obj,
+                                 int object_size, ObjectVisitor* v) {
     static_assert(kSmiHandlerOffset < kData1Offset,
                   "Field order must be in sync with this iteration code");
     static_assert(kData1Offset < kSizeWithData1,
@@ -877,7 +900,7 @@ class DataHandler::BodyDescriptor final : public BodyDescriptorBase {
     IterateMaybeWeakPointers(obj, kData1Offset, object_size, v);
   }
 
-  static inline int SizeOf(Map map, HeapObject object) {
+  static inline int SizeOf(Tagged<Map> map, Tagged<HeapObject> object) {
     return object->SizeFromMap(map);
   }
 };
@@ -885,8 +908,8 @@ class DataHandler::BodyDescriptor final : public BodyDescriptorBase {
 class NativeContext::BodyDescriptor final : public BodyDescriptorBase {
  public:
   template <typename ObjectVisitor>
-  static inline void IterateBody(Map map, HeapObject obj, int object_size,
-                                 ObjectVisitor* v) {
+  static inline void IterateBody(Tagged<Map> map, Tagged<HeapObject> obj,
+                                 int object_size, ObjectVisitor* v) {
     IteratePointers(obj, NativeContext::kStartOfStrongFieldsOffset,
                     NativeContext::kEndOfStrongFieldsOffset, v);
     IterateCustomWeakPointers(obj, NativeContext::kStartOfWeakFieldsOffset,
@@ -896,7 +919,7 @@ class NativeContext::BodyDescriptor final : public BodyDescriptorBase {
                             kNativeContextMicrotaskQueueTag);
   }
 
-  static inline int SizeOf(Map map, HeapObject object) {
+  static inline int SizeOf(Tagged<Map> map, Tagged<HeapObject> object) {
     return NativeContext::kSize;
   }
 };
@@ -904,8 +927,8 @@ class NativeContext::BodyDescriptor final : public BodyDescriptorBase {
 class Code::BodyDescriptor final : public BodyDescriptorBase {
  public:
   template <typename ObjectVisitor>
-  static inline void IterateBody(Map map, HeapObject obj, int object_size,
-                                 ObjectVisitor* v) {
+  static inline void IterateBody(Tagged<Map> map, Tagged<HeapObject> obj,
+                                 int object_size, ObjectVisitor* v) {
     IteratePointers(obj, Code::kStartOfStrongFieldsOffset,
                     Code::kEndOfStrongFieldsWithMainCageBaseOffset, v);
 
@@ -915,21 +938,23 @@ class Code::BodyDescriptor final : public BodyDescriptorBase {
                   Code::kEndOfStrongFieldsOffset);
     v->VisitInstructionStreamPointer(
         Code::cast(obj),
-        obj.RawInstructionStreamField(kInstructionStreamOffset));
+        obj->RawInstructionStreamField(kInstructionStreamOffset));
 #ifdef V8_CODE_POINTER_SANDBOXING
     v->VisitIndirectPointerTableEntry(
-        obj, obj.RawIndirectPointerField(kCodePointerTableEntryOffset));
+        obj, obj->RawIndirectPointerField(kCodePointerTableEntryOffset));
 #endif
   }
 
-  static inline int SizeOf(Map map, HeapObject object) { return Code::kSize; }
+  static inline int SizeOf(Tagged<Map> map, Tagged<HeapObject> object) {
+    return Code::kSize;
+  }
 };
 
 class EmbedderDataArray::BodyDescriptor final : public BodyDescriptorBase {
  public:
   template <typename ObjectVisitor>
-  static inline void IterateBody(Map map, HeapObject obj, int object_size,
-                                 ObjectVisitor* v) {
+  static inline void IterateBody(Tagged<Map> map, Tagged<HeapObject> obj,
+                                 int object_size, ObjectVisitor* v) {
 #ifdef V8_COMPRESS_POINTERS
     static_assert(kEmbedderDataSlotSize == 2 * kTaggedSize);
     for (int offset = EmbedderDataArray::OffsetOfElementAt(0);
@@ -950,7 +975,7 @@ class EmbedderDataArray::BodyDescriptor final : public BodyDescriptorBase {
 #endif
   }
 
-  static inline int SizeOf(Map map, HeapObject object) {
+  static inline int SizeOf(Tagged<Map> map, Tagged<HeapObject> object) {
     return object->SizeFromMap(map);
   }
 };
@@ -1270,33 +1295,35 @@ void HeapObject::IterateFast(PtrComprCageBase cage_base, ObjectVisitor* v) {
 }
 
 template <typename ObjectVisitor>
-void HeapObject::IterateFast(Map map, ObjectVisitor* v) {
+void HeapObject::IterateFast(Tagged<Map> map, ObjectVisitor* v) {
   v->VisitMapPointer(*this);
   IterateBodyFast(map, SizeFromMap(map), v);
 }
 
 template <typename ObjectVisitor>
-void HeapObject::IterateFast(Map map, int object_size, ObjectVisitor* v) {
+void HeapObject::IterateFast(Tagged<Map> map, int object_size,
+                             ObjectVisitor* v) {
   v->VisitMapPointer(*this);
   IterateBodyFast(map, object_size, v);
 }
 
 template <typename ObjectVisitor>
 void HeapObject::IterateBodyFast(PtrComprCageBase cage_base, ObjectVisitor* v) {
-  Map m = map(cage_base);
+  Tagged<Map> m = map(cage_base);
   IterateBodyFast(m, SizeFromMap(m), v);
 }
 
 struct CallIterateBody {
   template <typename BodyDescriptor, typename ObjectVisitor>
-  static void apply(Map map, HeapObject obj, int object_size,
+  static void apply(Tagged<Map> map, Tagged<HeapObject> obj, int object_size,
                     ObjectVisitor* v) {
     BodyDescriptor::IterateBody(map, obj, object_size, v);
   }
 };
 
 template <typename ObjectVisitor>
-void HeapObject::IterateBodyFast(Map map, int object_size, ObjectVisitor* v) {
+void HeapObject::IterateBodyFast(Tagged<Map> map, int object_size,
+                                 ObjectVisitor* v) {
   BodyDescriptorApply<CallIterateBody>(map->instance_type(), map, *this,
                                        object_size, v);
 }
@@ -1304,12 +1331,12 @@ void HeapObject::IterateBodyFast(Map map, int object_size, ObjectVisitor* v) {
 class EphemeronHashTable::BodyDescriptor final : public BodyDescriptorBase {
  public:
   template <typename ObjectVisitor>
-  static inline void IterateBody(Map map, HeapObject obj, int object_size,
-                                 ObjectVisitor* v) {
+  static inline void IterateBody(Tagged<Map> map, Tagged<HeapObject> obj,
+                                 int object_size, ObjectVisitor* v) {
     int entries_start = EphemeronHashTable::kHeaderSize +
                         EphemeronHashTable::kElementsStartIndex * kTaggedSize;
     IteratePointers(obj, EphemeronHashTable::kHeaderSize, entries_start, v);
-    EphemeronHashTable table = EphemeronHashTable::unchecked_cast(obj);
+    Tagged<EphemeronHashTable> table = EphemeronHashTable::unchecked_cast(obj);
     for (InternalIndex i : table->IterateEntries()) {
       const int key_index = EphemeronHashTable::EntryToIndex(i);
       const int value_index = EphemeronHashTable::EntryToValueIndex(i);
@@ -1318,7 +1345,7 @@ class EphemeronHashTable::BodyDescriptor final : public BodyDescriptorBase {
     }
   }
 
-  static inline int SizeOf(Map map, HeapObject object) {
+  static inline int SizeOf(Tagged<Map> map, Tagged<HeapObject> object) {
     return object->SizeFromMap(map);
   }
 };
@@ -1333,8 +1360,8 @@ class AccessorInfo::BodyDescriptor final : public BodyDescriptorBase {
   static_assert(AccessorInfo::kFlagsOffset < AccessorInfo::kSize);
 
   template <typename ObjectVisitor>
-  static inline void IterateBody(Map map, HeapObject obj, int object_size,
-                                 ObjectVisitor* v) {
+  static inline void IterateBody(Tagged<Map> map, Tagged<HeapObject> obj,
+                                 int object_size, ObjectVisitor* v) {
     IteratePointers(obj, HeapObject::kHeaderSize,
                     AccessorInfo::kEndOfStrongFieldsOffset, v);
     v->VisitExternalPointer(obj,
@@ -1346,7 +1373,9 @@ class AccessorInfo::BodyDescriptor final : public BodyDescriptorBase {
         kAccessorInfoSetterTag);
   }
 
-  static inline int SizeOf(Map map, HeapObject object) { return kSize; }
+  static inline int SizeOf(Tagged<Map> map, Tagged<HeapObject> object) {
+    return kSize;
+  }
 };
 
 class CallHandlerInfo::BodyDescriptor final : public BodyDescriptorBase {
@@ -1355,8 +1384,8 @@ class CallHandlerInfo::BodyDescriptor final : public BodyDescriptorBase {
                 CallHandlerInfo::kMaybeRedirectedCallbackOffset);
 
   template <typename ObjectVisitor>
-  static inline void IterateBody(Map map, HeapObject obj, int object_size,
-                                 ObjectVisitor* v) {
+  static inline void IterateBody(Tagged<Map> map, Tagged<HeapObject> obj,
+                                 int object_size, ObjectVisitor* v) {
     IteratePointers(obj, HeapObject::kHeaderSize,
                     CallHandlerInfo::kEndOfStrongFieldsOffset, v);
     v->VisitExternalPointer(
@@ -1366,7 +1395,9 @@ class CallHandlerInfo::BodyDescriptor final : public BodyDescriptorBase {
         kCallHandlerInfoCallbackTag);
   }
 
-  static inline int SizeOf(Map map, HeapObject object) { return kSize; }
+  static inline int SizeOf(Tagged<Map> map, Tagged<HeapObject> object) {
+    return kSize;
+  }
 };
 
 #include "torque-generated/objects-body-descriptors-inl.inc"
diff --git a/src/objects/objects-body-descriptors.h b/src/objects/objects-body-descriptors.h
index d8901fe3f7f..eaeca1e5a3b 100644
--- a/src/objects/objects-body-descriptors.h
+++ b/src/objects/objects-body-descriptors.h
@@ -23,49 +23,53 @@ namespace internal {
 class BodyDescriptorBase {
  public:
   template <typename ObjectVisitor>
-  static inline void IteratePointers(HeapObject obj, int start_offset,
+  static inline void IteratePointers(Tagged<HeapObject> obj, int start_offset,
                                      int end_offset, ObjectVisitor* v);
 
   template <typename ObjectVisitor>
-  static inline void IteratePointer(HeapObject obj, int offset,
+  static inline void IteratePointer(Tagged<HeapObject> obj, int offset,
                                     ObjectVisitor* v);
 
   template <typename ObjectVisitor>
-  static inline void IterateCustomWeakPointers(HeapObject obj, int start_offset,
-                                               int end_offset,
+  static inline void IterateCustomWeakPointers(Tagged<HeapObject> obj,
+                                               int start_offset, int end_offset,
                                                ObjectVisitor* v);
 
   template <typename ObjectVisitor>
-  static inline void IterateCustomWeakPointer(HeapObject obj, int offset,
-                                              ObjectVisitor* v);
+  static inline void IterateCustomWeakPointer(Tagged<HeapObject> obj,
+                                              int offset, ObjectVisitor* v);
 
   template <typename ObjectVisitor>
-  static inline void IterateEphemeron(HeapObject obj, int index, int key_offset,
-                                      int value_offset, ObjectVisitor* v);
+  static inline void IterateEphemeron(Tagged<HeapObject> obj, int index,
+                                      int key_offset, int value_offset,
+                                      ObjectVisitor* v);
 
   template <typename ObjectVisitor>
-  static inline void IterateMaybeWeakPointers(HeapObject obj, int start_offset,
-                                              int end_offset, ObjectVisitor* v);
+  static inline void IterateMaybeWeakPointers(Tagged<HeapObject> obj,
+                                              int start_offset, int end_offset,
+                                              ObjectVisitor* v);
 
   template <typename ObjectVisitor>
-  static inline void IterateMaybeWeakPointer(HeapObject obj, int offset,
+  static inline void IterateMaybeWeakPointer(Tagged<HeapObject> obj, int offset,
                                              ObjectVisitor* v);
 
   // Visits a field that contains either an indirect pointer (if the sandbox is
   // enabled) or a regular/tagged pointer (otherwise).
   template <typename ObjectVisitor>
-  static void IterateMaybeIndirectPointer(HeapObject obj, int offset,
+  static void IterateMaybeIndirectPointer(Tagged<HeapObject> obj, int offset,
                                           ObjectVisitor* visitor,
                                           IndirectPointerMode mode);
 
  protected:
   // Returns true for all header and embedder fields.
-  static inline bool IsValidEmbedderJSObjectSlotImpl(Map map, HeapObject obj,
+  static inline bool IsValidEmbedderJSObjectSlotImpl(Tagged<Map> map,
+                                                     Tagged<HeapObject> obj,
                                                      int offset);
 
   // Treats all header and embedder fields in the range as tagged.
   template <typename ObjectVisitor>
-  static inline void IterateJSObjectBodyImpl(Map map, HeapObject obj,
+  static inline void IterateJSObjectBodyImpl(Tagged<Map> map,
+                                             Tagged<HeapObject> obj,
                                              int start_offset, int end_offset,
                                              ObjectVisitor* v);
 };
@@ -80,18 +84,19 @@ class FixedRangeBodyDescriptor : public BodyDescriptorBase {
   static const int kEndOffset = end_offset;
 
   template <typename ObjectVisitor>
-  static inline void IterateBody(Map map, HeapObject obj, ObjectVisitor* v) {
+  static inline void IterateBody(Tagged<Map> map, Tagged<HeapObject> obj,
+                                 ObjectVisitor* v) {
     IteratePointers(obj, start_offset, end_offset, v);
   }
 
   template <typename ObjectVisitor>
-  static inline void IterateBody(Map map, HeapObject obj, int object_size,
-                                 ObjectVisitor* v) {
+  static inline void IterateBody(Tagged<Map> map, Tagged<HeapObject> obj,
+                                 int object_size, ObjectVisitor* v) {
     IterateBody(map, obj, v);
   }
 
  private:
-  static inline int SizeOf(Map map, HeapObject object) {
+  static inline int SizeOf(Tagged<Map> map, Tagged<HeapObject> object) {
     // Has to be implemented by the subclass.
     UNREACHABLE();
   }
@@ -106,7 +111,9 @@ class FixedBodyDescriptor
     : public FixedRangeBodyDescriptor<start_offset, end_offset> {
  public:
   static const int kSize = size;
-  static inline int SizeOf(Map map, HeapObject object) { return kSize; }
+  static inline int SizeOf(Tagged<Map> map, Tagged<HeapObject> object) {
+    return kSize;
+  }
 };
 
 // This class describes a body of an object in which all pointer fields are
@@ -118,13 +125,13 @@ class SuffixRangeBodyDescriptor : public BodyDescriptorBase {
   static const int kStartOffset = start_offset;
 
   template <typename ObjectVisitor>
-  static inline void IterateBody(Map map, HeapObject obj, int object_size,
-                                 ObjectVisitor* v) {
+  static inline void IterateBody(Tagged<Map> map, Tagged<HeapObject> obj,
+                                 int object_size, ObjectVisitor* v) {
     IteratePointers(obj, start_offset, object_size, v);
   }
 
  private:
-  static inline int SizeOf(Map map, HeapObject object) {
+  static inline int SizeOf(Tagged<Map> map, Tagged<HeapObject> object) {
     // Has to be implemented by the subclass.
     UNREACHABLE();
   }
@@ -137,7 +144,7 @@ class SuffixRangeBodyDescriptor : public BodyDescriptorBase {
 template <int start_offset>
 class FlexibleBodyDescriptor : public SuffixRangeBodyDescriptor<start_offset> {
  public:
-  static inline int SizeOf(Map map, HeapObject object);
+  static inline int SizeOf(Tagged<Map> map, Tagged<HeapObject> object);
 };
 
 // A forward-declacable descriptor body alias for most of the Struct successors.
@@ -155,13 +162,13 @@ class SuffixRangeWeakBodyDescriptor : public BodyDescriptorBase {
   static const int kStartOffset = start_offset;
 
   template <typename ObjectVisitor>
-  static inline void IterateBody(Map map, HeapObject obj, int object_size,
-                                 ObjectVisitor* v) {
+  static inline void IterateBody(Tagged<Map> map, Tagged<HeapObject> obj,
+                                 int object_size, ObjectVisitor* v) {
     IterateMaybeWeakPointers(obj, start_offset, object_size, v);
   }
 
  private:
-  static inline int SizeOf(Map map, HeapObject object) {
+  static inline int SizeOf(Tagged<Map> map, Tagged<HeapObject> object) {
     // Has to be implemented by the subclass.
     UNREACHABLE();
   }
@@ -175,18 +182,18 @@ template <int start_offset>
 class FlexibleWeakBodyDescriptor
     : public SuffixRangeWeakBodyDescriptor<start_offset> {
  public:
-  static inline int SizeOf(Map map, HeapObject object);
+  static inline int SizeOf(Tagged<Map> map, Tagged<HeapObject> object);
 };
 
 // This class describes a body of an object without any pointers.
 class DataOnlyBodyDescriptor : public BodyDescriptorBase {
  public:
   template <typename ObjectVisitor>
-  static inline void IterateBody(Map map, HeapObject obj, int object_size,
-                                 ObjectVisitor* v) {}
+  static inline void IterateBody(Tagged<Map> map, Tagged<HeapObject> obj,
+                                 int object_size, ObjectVisitor* v) {}
 
  private:
-  static inline int SizeOf(Map map, HeapObject object) {
+  static inline int SizeOf(Tagged<Map> map, Tagged<HeapObject> object) {
     // Has to be implemented by the subclass.
     UNREACHABLE();
   }
@@ -206,19 +213,20 @@ class SubclassBodyDescriptor final : public BodyDescriptorBase {
                 ChildBodyDescriptor::kStartOffset);
 
   template <typename ObjectVisitor>
-  static inline void IterateBody(Map map, HeapObject obj, ObjectVisitor* v) {
+  static inline void IterateBody(Tagged<Map> map, Tagged<HeapObject> obj,
+                                 ObjectVisitor* v) {
     ParentBodyDescriptor::IterateBody(map, obj, v);
     ChildBodyDescriptor::IterateBody(map, obj, v);
   }
 
   template <typename ObjectVisitor>
-  static inline void IterateBody(Map map, HeapObject obj, int object_size,
-                                 ObjectVisitor* v) {
+  static inline void IterateBody(Tagged<Map> map, Tagged<HeapObject> obj,
+                                 int object_size, ObjectVisitor* v) {
     ParentBodyDescriptor::IterateBody(map, obj, object_size, v);
     ChildBodyDescriptor::IterateBody(map, obj, object_size, v);
   }
 
-  static inline int SizeOf(Map map, HeapObject object) {
+  static inline int SizeOf(Tagged<Map> map, Tagged<HeapObject> object) {
     // The child should know its full size.
     return ChildBodyDescriptor::SizeOf(map, object);
   }
diff --git a/src/objects/objects-inl.h b/src/objects/objects-inl.h
index f1c3522ef35..ae0f0a77278 100644
--- a/src/objects/objects-inl.h
+++ b/src/objects/objects-inl.h
@@ -58,7 +58,7 @@
 namespace v8 {
 namespace internal {
 
-PropertyDetails::PropertyDetails(Smi smi) { value_ = smi.value(); }
+PropertyDetails::PropertyDetails(Tagged<Smi> smi) { value_ = smi.value(); }
 
 Tagged<Smi> PropertyDetails::AsSmi() const {
   // Ensure the upper 2 bits have the same value by sign extending it. This is
@@ -72,14 +72,6 @@ int PropertyDetails::field_width_in_words() const {
   return 1;
 }
 
-inline bool IsSmi(Tagged<Object> obj) { return obj.IsSmi(); }
-inline bool IsSmi(Tagged<HeapObject> obj) { return false; }
-inline bool IsSmi(Tagged<Smi> obj) { return true; }
-
-inline bool IsHeapObject(Tagged<Object> obj) { return obj.IsHeapObject(); }
-inline bool IsHeapObject(Tagged<HeapObject> obj) { return true; }
-inline bool IsHeapObject(Tagged<Smi> obj) { return false; }
-
 bool IsTaggedIndex(Tagged<Object> obj) {
   return IsSmi(obj) && TaggedIndex::IsValid(TaggedIndex(obj.ptr()).value());
 }
@@ -230,11 +222,12 @@ void HeapObject::Relaxed_WriteField(size_t offset, T value) {
 
 // static
 template <typename CompareAndSwapImpl>
-Object HeapObject::SeqCst_CompareAndSwapField(
-    Object expected, Object value, CompareAndSwapImpl compare_and_swap_impl) {
-  Object actual_expected = expected;
+Tagged<Object> HeapObject::SeqCst_CompareAndSwapField(
+    Tagged<Object> expected, Tagged<Object> value,
+    CompareAndSwapImpl compare_and_swap_impl) {
+  Tagged<Object> actual_expected = expected;
   do {
-    Object old_value = compare_and_swap_impl(actual_expected, value);
+    Tagged<Object> old_value = compare_and_swap_impl(actual_expected, value);
     if (old_value == actual_expected || !IsNumber(old_value) ||
         !IsNumber(actual_expected)) {
       return old_value;
@@ -487,7 +480,7 @@ DEF_HEAP_OBJECT_PREDICATE(HeapObject, IsHashTableBase) {
 // static
 bool IsPrimitive(Tagged<Object> obj) {
   if (obj.IsSmi()) return true;
-  HeapObject this_heap_object = HeapObject::cast(obj);
+  Tagged<HeapObject> this_heap_object = HeapObject::cast(obj);
   PtrComprCageBase cage_base = GetPtrComprCageBase(this_heap_object);
   return IsPrimitiveMap(this_heap_object->map(cage_base));
 }
@@ -512,8 +505,9 @@ DEF_HEAP_OBJECT_PREDICATE(HeapObject, IsUndetectable) {
 
 DEF_HEAP_OBJECT_PREDICATE(HeapObject, IsAccessCheckNeeded) {
   if (IsJSGlobalProxy(obj, cage_base)) {
-    const JSGlobalProxy proxy = JSGlobalProxy::cast(obj);
-    JSGlobalObject global = proxy->GetIsolate()->context()->global_object();
+    const Tagged<JSGlobalProxy> proxy = JSGlobalProxy::cast(obj);
+    Tagged<JSGlobalObject> global =
+        proxy->GetIsolate()->context()->global_object();
     return proxy->IsDetachedFrom(global);
   }
   return obj->map(cage_base)->is_access_check_needed();
@@ -605,7 +599,7 @@ Representation Object::OptimalRepresentation(Tagged<Object> obj,
   if (IsSmi(obj)) {
     return Representation::Smi();
   }
-  HeapObject heap_object = HeapObject::cast(obj);
+  Tagged<HeapObject> heap_object = HeapObject::cast(obj);
   if (IsHeapNumber(heap_object, cage_base)) {
     return Representation::Double();
   } else if (IsUninitialized(heap_object,
@@ -815,12 +809,12 @@ void HeapObject::ResetLazilyInitializedExternalPointerField(size_t offset) {
   i::ResetLazilyInitializedExternalPointerField(field_address(offset));
 }
 
-Object HeapObject::ReadIndirectPointerField(size_t offset) const {
+Tagged<Object> HeapObject::ReadIndirectPointerField(size_t offset) const {
   return i::ReadIndirectPointerField(field_address(offset));
 }
 
 void HeapObject::InitCodePointerTableEntryField(size_t offset, Isolate* isolate,
-                                                Code owning_code,
+                                                Tagged<Code> owning_code,
                                                 Address entrypoint) {
   i::InitCodePointerTableEntryField(field_address(offset), isolate, owning_code,
                                     entrypoint);
@@ -855,7 +849,7 @@ IndirectPointerSlot HeapObject::RawIndirectPointerField(int byte_offset) const {
   return IndirectPointerSlot(field_address(byte_offset));
 }
 
-MapWord MapWord::FromMap(const Map map) {
+MapWord MapWord::FromMap(const Tagged<Map> map) {
   DCHECK(map.is_null() || !MapWord::IsPacked(map.ptr()));
 #ifdef V8_MAP_PACKING
   return MapWord(Pack(map.ptr()));
@@ -883,8 +877,8 @@ bool MapWord::IsForwardingAddress() const {
 #endif  // V8_EXTERNAL_CODE_SPACE
 }
 
-MapWord MapWord::FromForwardingAddress(HeapObject map_word_host,
-                                       HeapObject object) {
+MapWord MapWord::FromForwardingAddress(Tagged<HeapObject> map_word_host,
+                                       Tagged<HeapObject> object) {
 #ifdef V8_EXTERNAL_CODE_SPACE
   // When external code space is enabled forwarding pointers are encoded as
   // Smi representing a diff from the source object address in kObjectAlignment
@@ -899,7 +893,8 @@ MapWord MapWord::FromForwardingAddress(HeapObject map_word_host,
 #endif  // V8_EXTERNAL_CODE_SPACE
 }
 
-HeapObject MapWord::ToForwardingAddress(HeapObject map_word_host) {
+Tagged<HeapObject> MapWord::ToForwardingAddress(
+    Tagged<HeapObject> map_word_host) {
   DCHECK(IsForwardingAddress());
 #ifdef V8_EXTERNAL_CODE_SPACE
   // When external code space is enabled forwarding pointers are encoded as
@@ -958,51 +953,55 @@ Tagged<Map> HeapObject::map(PtrComprCageBase cage_base) const {
   return map_word(cage_base, kRelaxedLoad).ToMap();
 }
 
-void HeapObject::set_map(Map value) {
+void HeapObject::set_map(Tagged<Map> value) {
   set_map<EmitWriteBarrier::kYes>(value, kRelaxedStore,
                                   VerificationMode::kPotentialLayoutChange);
 }
 
-void HeapObject::set_map(Map value, ReleaseStoreTag tag) {
+void HeapObject::set_map(Tagged<Map> value, ReleaseStoreTag tag) {
   set_map<EmitWriteBarrier::kYes>(value, kReleaseStore,
                                   VerificationMode::kPotentialLayoutChange);
 }
 
-void HeapObject::set_map_safe_transition(Map value) {
+void HeapObject::set_map_safe_transition(Tagged<Map> value) {
   set_map<EmitWriteBarrier::kYes>(value, kRelaxedStore,
                                   VerificationMode::kSafeMapTransition);
 }
 
-void HeapObject::set_map_safe_transition(Map value, ReleaseStoreTag tag) {
+void HeapObject::set_map_safe_transition(Tagged<Map> value,
+                                         ReleaseStoreTag tag) {
   set_map<EmitWriteBarrier::kYes>(value, kReleaseStore,
                                   VerificationMode::kSafeMapTransition);
 }
 
-void HeapObject::set_map_safe_transition_no_write_barrier(Map value,
+void HeapObject::set_map_safe_transition_no_write_barrier(Tagged<Map> value,
                                                           RelaxedStoreTag tag) {
   set_map<EmitWriteBarrier::kNo>(value, kRelaxedStore,
                                  VerificationMode::kSafeMapTransition);
 }
 
-void HeapObject::set_map_safe_transition_no_write_barrier(Map value,
+void HeapObject::set_map_safe_transition_no_write_barrier(Tagged<Map> value,
                                                           ReleaseStoreTag tag) {
   set_map<EmitWriteBarrier::kNo>(value, kReleaseStore,
                                  VerificationMode::kSafeMapTransition);
 }
 
 // Unsafe accessor omitting write barrier.
-void HeapObject::set_map_no_write_barrier(Map value, RelaxedStoreTag tag) {
+void HeapObject::set_map_no_write_barrier(Tagged<Map> value,
+                                          RelaxedStoreTag tag) {
   set_map<EmitWriteBarrier::kNo>(value, kRelaxedStore,
                                  VerificationMode::kPotentialLayoutChange);
 }
 
-void HeapObject::set_map_no_write_barrier(Map value, ReleaseStoreTag tag) {
+void HeapObject::set_map_no_write_barrier(Tagged<Map> value,
+                                          ReleaseStoreTag tag) {
   set_map<EmitWriteBarrier::kNo>(value, kReleaseStore,
                                  VerificationMode::kPotentialLayoutChange);
 }
 
 template <HeapObject::EmitWriteBarrier emit_write_barrier, typename MemoryOrder>
-void HeapObject::set_map(Map value, MemoryOrder order, VerificationMode mode) {
+void HeapObject::set_map(Tagged<Map> value, MemoryOrder order,
+                         VerificationMode mode) {
 #if V8_ENABLE_WEBASSEMBLY
   // In {WasmGraphBuilder::SetMap} and {WasmGraphBuilder::LoadMap}, we treat
   // maps as immutable. Therefore we are not allowed to mutate them here.
@@ -1036,14 +1035,20 @@ void HeapObject::set_map(Map value, MemoryOrder order, VerificationMode mode) {
 #endif
 }
 
-void HeapObject::set_map_after_allocation(Map value, WriteBarrierMode mode) {
+void HeapObject::set_map_after_allocation(Tagged<Map> value,
+                                          WriteBarrierMode mode) {
   set_map_word(value, kRelaxedStore);
 #ifndef V8_DISABLE_WRITE_BARRIERS
   if (mode != SKIP_WRITE_BARRIER) {
     DCHECK(!value.is_null());
     CombinedWriteBarrier(*this, map_slot(), value, mode);
   } else {
-    SLOW_DCHECK(!WriteBarrier::IsRequired(*this, value));
+    SLOW_DCHECK(
+        // We allow writes of a null map before root initialisation.
+        value->is_null() ? !GetIsolateFromWritableObject(*this)
+                                ->read_only_heap()
+                                ->roots_init_complete()
+                         : !WriteBarrier::IsRequired(*this, value));
   }
 #endif
 }
@@ -1069,11 +1074,11 @@ MapWord HeapObject::map_word(PtrComprCageBase cage_base,
   return MapField::Relaxed_Load_Map_Word(cage_base, *this);
 }
 
-void HeapObject::set_map_word(Map map, RelaxedStoreTag) {
+void HeapObject::set_map_word(Tagged<Map> map, RelaxedStoreTag) {
   MapField::Relaxed_Store_Map_Word(*this, MapWord::FromMap(map));
 }
 
-void HeapObject::set_map_word_forwarded(HeapObject target_object,
+void HeapObject::set_map_word_forwarded(Tagged<HeapObject> target_object,
                                         RelaxedStoreTag) {
   MapField::Relaxed_Store_Map_Word(
       *this, MapWord::FromForwardingAddress(*this, target_object));
@@ -1092,18 +1097,18 @@ MapWord HeapObject::map_word(PtrComprCageBase cage_base,
   return MapField::Acquire_Load_No_Unpack(cage_base, *this);
 }
 
-void HeapObject::set_map_word(Map map, ReleaseStoreTag) {
+void HeapObject::set_map_word(Tagged<Map> map, ReleaseStoreTag) {
   MapField::Release_Store_Map_Word(*this, MapWord::FromMap(map));
 }
 
-void HeapObject::set_map_word_forwarded(HeapObject target_object,
+void HeapObject::set_map_word_forwarded(Tagged<HeapObject> target_object,
                                         ReleaseStoreTag) {
   MapField::Release_Store_Map_Word(
       *this, MapWord::FromForwardingAddress(*this, target_object));
 }
 
 bool HeapObject::release_compare_and_swap_map_word_forwarded(
-    MapWord old_map_word, HeapObject new_target_object) {
+    MapWord old_map_word, Tagged<HeapObject> new_target_object) {
   Tagged_t result = MapField::Release_CompareAndSwap(
       *this, old_map_word,
       MapWord::FromForwardingAddress(*this, new_target_object));
@@ -1183,7 +1188,7 @@ WriteBarrierMode HeapObject::GetWriteBarrierMode(
 }
 
 // static
-AllocationAlignment HeapObject::RequiredAlignment(Map map) {
+AllocationAlignment HeapObject::RequiredAlignment(Tagged<Map> map) {
   // TODO(v8:4153): We should think about requiring double alignment
   // in general for ByteArray, since they are used as backing store for typed
   // arrays now.
@@ -1302,7 +1307,7 @@ MaybeHandle<Object> Object::GetPropertyOrElement(Handle<Object> receiver,
 }
 
 // static
-Object Object::GetSimpleHash(Object object) {
+Tagged<Object> Object::GetSimpleHash(Tagged<Object> object) {
   DisallowGarbageCollection no_gc;
   if (IsSmi(object)) {
     uint32_t hash = ComputeUnseededHash(Smi::ToInt(object));
@@ -1346,13 +1351,13 @@ Object Object::GetSimpleHash(Object object) {
 }
 
 // static
-Object Object::GetHash(Tagged<Object> obj) {
+Tagged<Object> Object::GetHash(Tagged<Object> obj) {
   DisallowGarbageCollection no_gc;
-  Object hash = GetSimpleHash(obj);
+  Tagged<Object> hash = GetSimpleHash(obj);
   if (IsSmi(hash)) return hash;
 
   DCHECK(IsJSReceiver(obj));
-  JSReceiver receiver = JSReceiver::cast(obj);
+  Tagged<JSReceiver> receiver = JSReceiver::cast(obj);
   return receiver->GetIdentityHash();
 }
 
@@ -1473,7 +1478,8 @@ static inline Handle<Object> MakeEntryPair(Isolate* isolate, Handle<Object> key,
                                                     PACKED_ELEMENTS, 2);
 }
 
-FreshlyAllocatedBigInt FreshlyAllocatedBigInt::cast(Object object) {
+Tagged<FreshlyAllocatedBigInt> FreshlyAllocatedBigInt::cast(
+    Tagged<Object> object) {
   SLOW_DCHECK(IsBigInt(object));
   return FreshlyAllocatedBigInt(object.ptr());
 }
diff --git a/src/objects/objects.cc b/src/objects/objects.cc
index ed6ad5d9603..f36466beac0 100644
--- a/src/objects/objects.cc
+++ b/src/objects/objects.cc
@@ -495,7 +495,8 @@ MaybeHandle<String> Object::NoSideEffectsToMaybeString(Isolate* isolate,
   } else if (IsJSProxy(*input)) {
     Handle<Object> currInput = input;
     do {
-      HeapObject target = Handle<JSProxy>::cast(currInput)->target(isolate);
+      Tagged<HeapObject> target =
+          Handle<JSProxy>::cast(currInput)->target(isolate);
       currInput = Handle<Object>(target, isolate);
     } while (IsJSProxy(*currInput));
     return NoSideEffectsToString(isolate, currInput);
@@ -686,7 +687,7 @@ template bool Object::BooleanValue(Tagged<Object>, Isolate*);
 template bool Object::BooleanValue(Tagged<Object>, LocalIsolate*);
 
 // static
-Object Object::ToBoolean(Tagged<Object> obj, Isolate* isolate) {
+Tagged<Object> Object::ToBoolean(Tagged<Object> obj, Isolate* isolate) {
   if (IsBoolean(obj)) return obj;
   return isolate->heap()->ToBoolean(Object::BooleanValue(obj, isolate));
 }
@@ -715,7 +716,7 @@ bool StrictNumberEquals(double x, double y) {
   return x == y;
 }
 
-bool StrictNumberEquals(const Object x, const Object y) {
+bool StrictNumberEquals(const Tagged<Object> x, const Tagged<Object> y) {
   return StrictNumberEquals(Object::Number(x), Object::Number(y));
 }
 
@@ -885,7 +886,7 @@ Maybe<bool> Object::Equals(Isolate* isolate, Handle<Object> x,
 }
 
 // static
-bool Object::StrictEquals(Tagged<Object> obj, Object that) {
+bool Object::StrictEquals(Tagged<Object> obj, Tagged<Object> that) {
   if (IsNumber(obj)) {
     if (!IsNumber(that)) return false;
     return StrictNumberEquals(obj, that);
@@ -1619,21 +1620,22 @@ Maybe<bool> Object::SetPropertyWithDefinedSetter(
 }
 
 // static
-Map Object::GetPrototypeChainRootMap(Tagged<Object> obj, Isolate* isolate) {
+Tagged<Map> Object::GetPrototypeChainRootMap(Tagged<Object> obj,
+                                             Isolate* isolate) {
   DisallowGarbageCollection no_alloc;
   if (IsSmi(obj)) {
-    Context native_context = isolate->context()->native_context();
+    Tagged<Context> native_context = isolate->context()->native_context();
     return native_context->number_function()->initial_map();
   }
 
-  const HeapObject heap_object = HeapObject::cast(obj);
+  const Tagged<HeapObject> heap_object = HeapObject::cast(obj);
   return heap_object->map()->GetPrototypeChainRootMap(isolate);
 }
 
 // static
-Smi Object::GetOrCreateHash(Tagged<Object> obj, Isolate* isolate) {
+Tagged<Smi> Object::GetOrCreateHash(Tagged<Object> obj, Isolate* isolate) {
   DisallowGarbageCollection no_gc;
-  Object hash = Object::GetSimpleHash(obj);
+  Tagged<Object> hash = Object::GetSimpleHash(obj);
   if (IsSmi(hash)) return Smi::cast(hash);
 
   DCHECK(IsJSReceiver(obj));
@@ -1641,7 +1643,7 @@ Smi Object::GetOrCreateHash(Tagged<Object> obj, Isolate* isolate) {
 }
 
 // static
-bool Object::SameValue(Tagged<Object> obj, Object other) {
+bool Object::SameValue(Tagged<Object> obj, Tagged<Object> other) {
   if (other == obj) return true;
 
   if (IsNumber(obj) && IsNumber(other)) {
@@ -1657,7 +1659,7 @@ bool Object::SameValue(Tagged<Object> obj, Object other) {
 }
 
 // static
-bool Object::SameValueZero(Tagged<Object> obj, Object other) {
+bool Object::SameValueZero(Tagged<Object> obj, Tagged<Object> other) {
   if (other == obj) return true;
 
   if (IsNumber(obj) && IsNumber(other)) {
@@ -1771,7 +1773,7 @@ V8_WARN_UNUSED_RESULT MaybeHandle<Object> Object::SpeciesConstructor(
 bool Object::IterationHasObservableEffects(Tagged<Object> obj) {
   // Check that this object is an array.
   if (!IsJSArray(obj)) return true;
-  JSArray array = JSArray::cast(obj);
+  Tagged<JSArray> array = JSArray::cast(obj);
   Isolate* isolate = array->GetIsolate();
 
   // Check that we have the original ArrayPrototype.
@@ -1779,7 +1781,7 @@ bool Object::IterationHasObservableEffects(Tagged<Object> obj) {
   i::Handle<i::Context> context;
   if (!array->GetCreationContext().ToHandle(&context)) return false;
   if (!IsJSObject(array->map()->prototype())) return true;
-  JSObject array_proto = JSObject::cast(array->map()->prototype());
+  Tagged<JSObject> array_proto = JSObject::cast(array->map()->prototype());
   auto initial_array_prototype =
       context->native_context()->initial_array_prototype();
   if (initial_array_prototype != array_proto) return true;
@@ -1809,18 +1811,18 @@ bool Object::IsCodeLike(Tagged<Object> obj, Isolate* isolate) {
   return IsJSReceiver(obj) && JSReceiver::cast(obj)->IsCodeLike(isolate);
 }
 
-void ShortPrint(Object obj, FILE* out) {
+void ShortPrint(Tagged<Object> obj, FILE* out) {
   OFStream os(out);
   os << Brief(obj);
 }
 
-void ShortPrint(Object obj, StringStream* accumulator) {
+void ShortPrint(Tagged<Object> obj, StringStream* accumulator) {
   std::ostringstream os;
   os << Brief(obj);
   accumulator->Add(os.str().c_str());
 }
 
-void ShortPrint(Object obj, std::ostream& os) { os << Brief(obj); }
+void ShortPrint(Tagged<Object> obj, std::ostream& os) { os << Brief(obj); }
 
 std::ostream& operator<<(std::ostream& os, const Object& obj) {
   ShortPrint(obj, os);
@@ -1829,8 +1831,8 @@ std::ostream& operator<<(std::ostream& os, const Object& obj) {
 
 std::ostream& operator<<(std::ostream& os, const Brief& v) {
   MaybeObject maybe_object(v.value);
-  Smi smi;
-  HeapObject heap_object;
+  Tagged<Smi> smi;
+  Tagged<HeapObject> heap_object;
   if (maybe_object->ToSmi(&smi)) {
     Smi::SmiPrint(smi, os);
   } else if (maybe_object->IsCleared()) {
@@ -1872,15 +1874,16 @@ void HeapObject::Iterate(PtrComprCageBase cage_base, ObjectVisitor* v) {
 }
 
 void HeapObject::IterateBody(PtrComprCageBase cage_base, ObjectVisitor* v) {
-  Map m = map(cage_base);
+  Tagged<Map> m = map(cage_base);
   IterateBodyFast<ObjectVisitor>(m, SizeFromMap(m), v);
 }
 
-void HeapObject::IterateBody(Map map, int object_size, ObjectVisitor* v) {
+void HeapObject::IterateBody(Tagged<Map> map, int object_size,
+                             ObjectVisitor* v) {
   IterateBodyFast<ObjectVisitor>(map, object_size, v);
 }
 
-int HeapObject::SizeFromMap(Map map) const {
+int HeapObject::SizeFromMap(Tagged<Map> map) const {
   int instance_size = map->instance_size();
   if (instance_size != kVariableSizeSentinel) return instance_size;
   // Only inline the most frequent cases.
@@ -1975,7 +1978,7 @@ int HeapObject::SizeFromMap(Map map) const {
     return BigInt::SizeFor(BigInt::unchecked_cast(*this)->length());
   }
   if (instance_type == PREPARSE_DATA_TYPE) {
-    PreparseData data = PreparseData::unchecked_cast(*this);
+    Tagged<PreparseData> data = PreparseData::unchecked_cast(*this);
     return PreparseData::SizeFor(data->data_length(), data->children_length());
   }
 #define MAKE_TORQUE_SIZE_FOR(TYPE, TypeName)                 \
@@ -2640,26 +2643,26 @@ Maybe<bool> Object::TransitionAndWriteDataProperty(
   }
 #endif
 
-    return Just(true);
+  return Just(true);
 }
 // static
 MaybeHandle<Object> Object::ShareSlow(Isolate* isolate,
                                       Handle<HeapObject> value,
                                       ShouldThrow throw_if_cannot_be_shared) {
   // Use Object::Share() if value might already be shared.
-    DCHECK(!IsShared(*value));
+  DCHECK(!IsShared(*value));
 
-    SharedObjectSafePublishGuard publish_guard;
+  SharedObjectSafePublishGuard publish_guard;
 
-    if (IsString(*value)) {
+  if (IsString(*value)) {
     return String::Share(isolate, Handle<String>::cast(value));
-    }
+  }
 
-    if (IsHeapNumber(*value)) {
+  if (IsHeapNumber(*value)) {
     uint64_t bits = HeapNumber::cast(*value)->value_as_bits(kRelaxedLoad);
     return isolate->factory()
         ->NewHeapNumberFromBits<AllocationType::kSharedOld>(bits);
-    }
+  }
 
   if (throw_if_cannot_be_shared == kThrowOnError) {
     THROW_NEW_ERROR(
@@ -3585,13 +3588,13 @@ Handle<DescriptorArray> DescriptorArray::CopyUpToAddAttributes(
   if (attributes != NONE) {
     for (InternalIndex i : InternalIndex::Range(size)) {
       MaybeObject value_or_field_type = source->GetValue(i);
-      Name key = source->GetKey(i);
+      Tagged<Name> key = source->GetKey(i);
       PropertyDetails details = source->GetDetails(i);
       // Bulk attribute changes never affect private properties.
       if (!key->IsPrivate()) {
         int mask = DONT_DELETE | DONT_ENUM;
         // READ_ONLY is an invalid attribute for JS setters/getters.
-        HeapObject heap_object;
+        Tagged<HeapObject> heap_object;
         if (details.kind() != PropertyKind::kAccessor ||
             !(value_or_field_type->GetHeapObjectIfStrong(&heap_object) &&
               IsAccessorPair(heap_object))) {
@@ -3613,7 +3616,8 @@ Handle<DescriptorArray> DescriptorArray::CopyUpToAddAttributes(
   return copy_handle;
 }
 
-bool DescriptorArray::IsEqualUpTo(DescriptorArray desc, int nof_descriptors) {
+bool DescriptorArray::IsEqualUpTo(Tagged<DescriptorArray> desc,
+                                  int nof_descriptors) {
   for (InternalIndex i : InternalIndex::Range(nof_descriptors)) {
     if (GetKey(i) != desc->GetKey(i) || GetValue(i) != desc->GetValue(i)) {
       return false;
@@ -3685,7 +3689,7 @@ Handle<WeakArrayList> PrototypeUsers::Add(Isolate* isolate,
 }
 
 // static
-void PrototypeUsers::ScanForEmptySlots(WeakArrayList array) {
+void PrototypeUsers::ScanForEmptySlots(Tagged<WeakArrayList> array) {
   for (int i = kFirstIndex; i < array->length(); i++) {
     if (array->Get(i)->IsCleared()) {
       PrototypeUsers::MarkSlotEmpty(array, i);
@@ -3693,9 +3697,10 @@ void PrototypeUsers::ScanForEmptySlots(WeakArrayList array) {
   }
 }
 
-WeakArrayList PrototypeUsers::Compact(Handle<WeakArrayList> array, Heap* heap,
-                                      CompactionCallback callback,
-                                      AllocationType allocation) {
+Tagged<WeakArrayList> PrototypeUsers::Compact(Handle<WeakArrayList> array,
+                                              Heap* heap,
+                                              CompactionCallback callback,
+                                              AllocationType allocation) {
   if (array->length() == 0) {
     return *array;
   }
@@ -3713,7 +3718,7 @@ WeakArrayList PrototypeUsers::Compact(Handle<WeakArrayList> array, Heap* heap,
   int copy_to = kFirstIndex;
   for (int i = kFirstIndex; i < array->length(); i++) {
     MaybeObject element = array->Get(i);
-    HeapObject value;
+    Tagged<HeapObject> value;
     if (element->GetHeapObjectIfWeak(&value)) {
       callback(value, i, copy_to);
       new_array->Set(copy_to++, element);
@@ -3743,8 +3748,8 @@ template Handle<DescriptorArray> DescriptorArray::Allocate(
     LocalIsolate* isolate, int nof_descriptors, int slack,
     AllocationType allocation);
 
-void DescriptorArray::Initialize(EnumCache empty_enum_cache,
-                                 HeapObject undefined_value,
+void DescriptorArray::Initialize(Tagged<EnumCache> empty_enum_cache,
+                                 Tagged<HeapObject> undefined_value,
                                  int nof_descriptors, int slack,
                                  uint32_t raw_gc_state) {
   DCHECK_GE(nof_descriptors, 0);
@@ -3772,7 +3777,7 @@ void DescriptorArray::InitializeOrChangeEnumCache(
     Handle<DescriptorArray> descriptors, Isolate* isolate,
     Handle<FixedArray> keys, Handle<FixedArray> indices,
     AllocationType allocation_if_initialize) {
-  EnumCache enum_cache = descriptors->enum_cache();
+  Tagged<EnumCache> enum_cache = descriptors->enum_cache();
   if (enum_cache == ReadOnlyRoots(isolate).empty_enum_cache()) {
     enum_cache = *isolate->factory()->NewEnumCache(keys, indices,
                                                    allocation_if_initialize);
@@ -3783,7 +3788,8 @@ void DescriptorArray::InitializeOrChangeEnumCache(
   }
 }
 
-void DescriptorArray::CopyFrom(InternalIndex index, DescriptorArray src) {
+void DescriptorArray::CopyFrom(InternalIndex index,
+                               Tagged<DescriptorArray> src) {
   PropertyDetails details = src->GetDetails(index);
   Set(index, src->GetKey(index), src->GetValue(index), details);
 }
@@ -3851,7 +3857,7 @@ void DescriptorArray::CheckNameCollisionDuringInsertion(Descriptor* desc,
   if (insertion_index <= 0) return;
 
   for (int i = insertion_index; i > 0; --i) {
-    Name current_key = GetSortedKey(i - 1);
+    Tagged<Name> current_key = GetSortedKey(i - 1);
     if (current_key->hash() != desc_hash) return;
     CHECK(current_key != *desc->GetKey());
   }
@@ -3890,7 +3896,7 @@ Handle<Object> AccessorPair::GetComponent(Isolate* isolate,
 }
 
 #ifdef DEBUG
-bool DescriptorArray::IsEqualTo(DescriptorArray other) {
+bool DescriptorArray::IsEqualTo(Tagged<DescriptorArray> other) {
   if (number_of_all_descriptors() != other->number_of_all_descriptors()) {
     return false;
   }
@@ -3977,8 +3983,9 @@ void Relocatable::Iterate(RootVisitor* v, Relocatable* top) {
 namespace {
 
 template <typename sinkchar>
-void WriteFixedArrayToFlat(FixedArray fixed_array, int length, String separator,
-                           sinkchar* sink, int sink_length) {
+void WriteFixedArrayToFlat(Tagged<FixedArray> fixed_array, int length,
+                           Tagged<String> separator, sinkchar* sink,
+                           int sink_length) {
   DisallowGarbageCollection no_gc;
   CHECK_GT(length, 0);
   CHECK_LE(length, fixed_array->length());
@@ -4000,7 +4007,7 @@ void WriteFixedArrayToFlat(FixedArray fixed_array, int length, String separator,
   uint32_t num_separators = 0;
   uint32_t repeat_last = 0;
   for (int i = 0; i < length; i++) {
-    Object element = fixed_array->get(i);
+    Tagged<Object> element = fixed_array->get(i);
     const bool element_is_special = IsSmi(element);
 
     // If element is a positive Smi, it represents the number of separators to
@@ -4050,7 +4057,7 @@ void WriteFixedArrayToFlat(FixedArray fixed_array, int length, String separator,
     // Repeat the last written string |repeat_last| times (including
     // separators).
     if (V8_UNLIKELY(repeat_last > 0)) {
-      Object last_element = fixed_array->get(i - 1);
+      Tagged<Object> last_element = fixed_array->get(i - 1);
       int string_length = String::cast(last_element)->length();
       // The implemented logic requires that string length is > 0. Empty strings
       // are handled by repeating the separator (positive smi in the fixed
@@ -4080,7 +4087,7 @@ void WriteFixedArrayToFlat(FixedArray fixed_array, int length, String separator,
 
     if (V8_LIKELY(!element_is_special)) {
       DCHECK(IsString(element));
-      String string = String::cast(element);
+      Tagged<String> string = String::cast(element);
       const int string_length = string->length();
 
       DCHECK(string_length == 0 || sink < sink_end);
@@ -4106,9 +4113,9 @@ Address JSArray::ArrayJoinConcatToSequentialString(Isolate* isolate,
                                                    Address raw_dest) {
   DisallowGarbageCollection no_gc;
   DisallowJavascriptExecution no_js(isolate);
-  FixedArray fixed_array = FixedArray::cast(Object(raw_fixed_array));
-  String separator = String::cast(Object(raw_separator));
-  String dest = String::cast(Object(raw_dest));
+  Tagged<FixedArray> fixed_array = FixedArray::cast(Object(raw_fixed_array));
+  Tagged<String> separator = String::cast(Object(raw_separator));
+  Tagged<String> dest = String::cast(Object(raw_dest));
   DCHECK(IsFixedArray(fixed_array));
   DCHECK(StringShape(dest).IsSequentialOneByte() ||
          StringShape(dest).IsSequentialTwoByte());
@@ -4193,7 +4200,7 @@ template <typename IsolateT>
 void Script::InitLineEndsInternal(IsolateT* isolate, Handle<Script> script) {
   DCHECK(!script->has_line_ends());
   DCHECK(script->CanHaveLineEnds());
-  Object src_obj = script->source();
+  Tagged<Object> src_obj = script->source();
   if (!IsString(src_obj)) {
     DCHECK(IsUndefined(src_obj, isolate));
     script->set_line_ends(ReadOnlyRoots(isolate).empty_fixed_array());
@@ -4259,7 +4266,7 @@ bool Script::IsUserJavaScript() const {
 bool Script::ContainsAsmModule() {
   DisallowGarbageCollection no_gc;
   SharedFunctionInfo::ScriptIterator iter(this->GetIsolate(), *this);
-  for (SharedFunctionInfo sfi = iter.Next(); !sfi.is_null();
+  for (Tagged<SharedFunctionInfo> sfi = iter.Next(); !sfi.is_null();
        sfi = iter.Next()) {
     if (sfi->HasAsmWasmData()) return true;
   }
@@ -4292,7 +4299,7 @@ bool GetPositionInfoSlowImpl(base::Vector<Char> source, int position,
   }
   return false;
 }
-bool GetPositionInfoSlow(const Script script, int position,
+bool GetPositionInfoSlow(const Tagged<Script> script, int position,
                          const DisallowGarbageCollection& no_gc,
                          Script::PositionInfo* info) {
   if (!IsString(script->source())) {
@@ -4333,7 +4340,7 @@ bool Script::GetPositionInfo(int position, PositionInfo* info,
     }
   } else {
     DCHECK(has_line_ends());
-    FixedArray ends = FixedArray::cast(line_ends());
+    Tagged<FixedArray> ends = FixedArray::cast(line_ends());
 
     const int ends_len = ends->length();
     if (ends_len == 0) return false;
@@ -4377,7 +4384,7 @@ bool Script::GetPositionInfo(int position, PositionInfo* info,
     info->line_end = Smi::ToInt(ends->get(info->line));
     if (info->line_end > 0) {
       DCHECK(IsString(source()));
-      String src = String::cast(source());
+      Tagged<String> src = String::cast(source());
       if (src->length() >= info->line_end &&
           src->Get(info->line_end - 1) == '\r') {
         info->line_end--;
@@ -4422,7 +4429,7 @@ int Script::GetLineNumber(int code_pos) const {
   return info.line;
 }
 
-Object Script::GetNameOrSourceURL() {
+Tagged<Object> Script::GetNameOrSourceURL() {
   // Keep in sync with ScriptNameOrSourceURL in messages.js.
   if (!IsUndefined(source_url())) return source_url();
   return name();
@@ -4437,7 +4444,7 @@ Handle<String> Script::GetScriptHash(Isolate* isolate, Handle<Script> script,
 
   PtrComprCageBase cage_base(isolate);
   {
-    Object maybe_source_hash = script->source_hash(cage_base);
+    Tagged<Object> maybe_source_hash = script->source_hash(cage_base);
     if (IsString(maybe_source_hash, cage_base)) {
       Handle<String> precomputed(String::cast(maybe_source_hash), isolate);
       if (precomputed->length() > 0) {
@@ -4448,7 +4455,7 @@ Handle<String> Script::GetScriptHash(Isolate* isolate, Handle<Script> script,
 
   Handle<String> src_text;
   {
-    Object maybe_script_source = script->source(cage_base);
+    Tagged<Object> maybe_script_source = script->source(cage_base);
 
     if (!IsString(maybe_script_source, cage_base)) {
       return isolate->factory()->empty_string();
@@ -4486,7 +4493,7 @@ MaybeHandle<SharedFunctionInfo> Script::FindSharedFunctionInfo(
   CHECK_LT(function_literal_id, script->shared_function_info_count());
   MaybeObject shared =
       script->shared_function_infos()->Get(function_literal_id);
-  HeapObject heap_object;
+  Tagged<HeapObject> heap_object;
   if (!shared->GetHeapObject(&heap_object) ||
       IsUndefined(heap_object, isolate)) {
     return MaybeHandle<SharedFunctionInfo>();
@@ -4502,8 +4509,8 @@ template MaybeHandle<SharedFunctionInfo> Script::FindSharedFunctionInfo(
 Script::Iterator::Iterator(Isolate* isolate)
     : iterator_(isolate->heap()->script_list()) {}
 
-Script Script::Iterator::Next() {
-  Object o = iterator_.Next();
+Tagged<Script> Script::Iterator::Next() {
+  Tagged<Object> o = iterator_.Next();
   if (o != Object()) {
     return Script::cast(o);
   }
@@ -4618,9 +4625,9 @@ AllocationType AllocationSite::GetAllocationType() const {
 
 bool AllocationSite::IsNested() {
   DCHECK(v8_flags.trace_track_allocation_sites);
-  Object current = boilerplate()->GetHeap()->allocation_sites_list();
+  Tagged<Object> current = boilerplate()->GetHeap()->allocation_sites_list();
   while (IsAllocationSite(current)) {
-    AllocationSite current_site = AllocationSite::cast(current);
+    Tagged<AllocationSite> current_site = AllocationSite::cast(current);
     if (current_site->nested_site() == *this) {
       return true;
     }
@@ -4666,7 +4673,7 @@ bool JSArray::MayHaveReadOnlyLength(Tagged<Map> js_array_map) {
 }
 
 bool JSArray::HasReadOnlyLength(Handle<JSArray> array) {
-  Map map = array->map();
+  Tagged<Map> map = array->map();
 
   // If map guarantees that there can't be a read-only length, we are done.
   if (!MayHaveReadOnlyLength(map)) return false;
@@ -4734,7 +4741,7 @@ Handle<Object> JSPromise::Fulfill(Handle<JSPromise> promise,
 
 #ifdef V8_ENABLE_JAVASCRIPT_PROMISE_HOOKS
   if (isolate->HasContextPromiseHooks()) {
-    isolate->raw_native_context().RunPromiseHook(
+    isolate->raw_native_context()->RunPromiseHook(
         PromiseHookType::kResolve, promise,
         isolate->factory()->undefined_value());
   }
@@ -4921,10 +4928,10 @@ Handle<Object> JSPromise::TriggerPromiseReactions(Isolate* isolate,
   // on the JSPromise in the reverse order.
   {
     DisallowGarbageCollection no_gc;
-    Object current = *reactions;
-    Object reversed = Smi::zero();
+    Tagged<Object> current = *reactions;
+    Tagged<Object> reversed = Smi::zero();
     while (!IsSmi(current)) {
-      Object next = PromiseReaction::cast(current)->next();
+      Tagged<Object> next = PromiseReaction::cast(current)->next();
       PromiseReaction::cast(current)->set_next(reversed);
       reversed = current;
       current = next;
@@ -5085,7 +5092,7 @@ void HashTable<Derived, Shape>::Rehash(PtrComprCageBase cage_base,
   ReadOnlyRoots roots = GetReadOnlyRoots(cage_base);
   for (InternalIndex i : this->IterateEntries()) {
     uint32_t from_index = EntryToIndex(i);
-    Object k = this->get(cage_base, from_index);
+    Tagged<Object> k = this->get(cage_base, from_index);
     if (!IsKey(roots, k)) continue;
     uint32_t hash = Shape::HashForObject(roots, k);
     uint32_t insertion_index =
@@ -5101,7 +5108,8 @@ void HashTable<Derived, Shape>::Rehash(PtrComprCageBase cage_base,
 
 template <typename Derived, typename Shape>
 InternalIndex HashTable<Derived, Shape>::EntryForProbe(ReadOnlyRoots roots,
-                                                       Object k, int probe,
+                                                       Tagged<Object> k,
+                                                       int probe,
                                                        InternalIndex expected) {
   uint32_t hash = Shape::HashForObject(roots, k);
   uint32_t capacity = this->Capacity();
@@ -5146,7 +5154,7 @@ void HashTable<Derived, Shape>::Rehash(PtrComprCageBase cage_base) {
     done = true;
     for (InternalIndex current(0); current.raw_value() < capacity;
          /* {current} is advanced manually below, when appropriate.*/) {
-      Object current_key = KeyAt(cage_base, current);
+      Tagged<Object> current_key = KeyAt(cage_base, current);
       if (!IsKey(roots, current_key)) {
         ++current;  // Advance to next entry.
         continue;
@@ -5156,7 +5164,7 @@ void HashTable<Derived, Shape>::Rehash(PtrComprCageBase cage_base) {
         ++current;  // Advance to next entry.
         continue;
       }
-      Object target_key = KeyAt(cage_base, target);
+      Tagged<Object> target_key = KeyAt(cage_base, target);
       if (!IsKey(roots, target_key) ||
           EntryForProbe(roots, target_key, probe, target) != target) {
         // Put the current element into the correct position.
@@ -5172,8 +5180,8 @@ void HashTable<Derived, Shape>::Rehash(PtrComprCageBase cage_base) {
     }
   }
   // Wipe deleted entries.
-  Object the_hole = roots.the_hole_value();
-  HeapObject undefined = roots.undefined_value();
+  Tagged<Object> the_hole = roots.the_hole_value();
+  Tagged<HeapObject> undefined = roots.undefined_value();
   Derived* self = static_cast<Derived*>(this);
   for (InternalIndex current : InternalIndex::Range(capacity)) {
     if (KeyAt(cage_base, current) == the_hole) {
@@ -5278,7 +5286,7 @@ InternalIndex HashTable<Derived, Shape>::FindInsertionEntry(
   }
 }
 
-base::Optional<PropertyCell>
+base::Optional<Tagged<PropertyCell>>
 GlobalDictionary::TryFindPropertyCellForConcurrentLookupIterator(
     Isolate* isolate, Handle<Name> name, RelaxedLoadTag tag) {
   // This reimplements HashTable::FindEntry for use in a concurrent setting.
@@ -5293,12 +5301,12 @@ GlobalDictionary::TryFindPropertyCellForConcurrentLookupIterator(
   const int32_t hash = ShapeT::Hash(roots, name);
   const uint32_t capacity = Capacity();
   uint32_t count = 1;
-  Object undefined = roots.undefined_value();
-  Object the_hole = roots.the_hole_value();
+  Tagged<Object> undefined = roots.undefined_value();
+  Tagged<Object> the_hole = roots.the_hole_value();
   // EnsureCapacity will guarantee the hash table is never full.
   for (InternalIndex entry = FirstProbe(hash, capacity);;
        entry = NextProbe(entry, count++, capacity)) {
-    Object element = KeyAt(cage_base, entry, kRelaxedLoad);
+    Tagged<Object> element = KeyAt(cage_base, entry, kRelaxedLoad);
     if (isolate->heap()->IsPendingAllocation(element)) return {};
     if (element == undefined) return {};
     if (ShapeT::kMatchNeedsHoleCheck && element == the_hole) continue;
@@ -5552,7 +5560,7 @@ void NumberDictionary::UpdateMaxNumberKey(uint32_t key,
     return;
   }
   // Update max key value.
-  Object max_index_object = get(kMaxNumberKeyIndex);
+  Tagged<Object> max_index_object = get(kMaxNumberKeyIndex);
   if (!IsSmi(max_index_object) || max_number_key() < key) {
     FixedArray::set(kMaxNumberKeyIndex,
                     Smi::FromInt(key << kRequiresSlowElementsTagSize));
@@ -5579,13 +5587,13 @@ void NumberDictionary::UncheckedSet(Isolate* isolate,
   UncheckedAtPut(isolate, dictionary, key, value, PropertyDetails::Empty());
 }
 
-void NumberDictionary::CopyValuesTo(FixedArray elements) {
+void NumberDictionary::CopyValuesTo(Tagged<FixedArray> elements) {
   ReadOnlyRoots roots = GetReadOnlyRoots();
   int pos = 0;
   DisallowGarbageCollection no_gc;
   WriteBarrierMode mode = elements->GetWriteBarrierMode(no_gc);
   for (InternalIndex i : this->IterateEntries()) {
-    Object k;
+    Tagged<Object> k;
     if (this->ToKey(roots, i, &k)) {
       elements->set(pos++, this->ValueAt(i), mode);
     }
@@ -5598,7 +5606,7 @@ int Dictionary<Derived, Shape>::NumberOfEnumerableProperties() {
   ReadOnlyRoots roots = this->GetReadOnlyRoots();
   int result = 0;
   for (InternalIndex i : this->IterateEntries()) {
-    Object k;
+    Tagged<Object> k;
     if (!this->ToKey(roots, i, &k)) continue;
     if (Object::FilterKey(k, ENUMERABLE_STRINGS)) continue;
     PropertyDetails details = this->DetailsAt(i);
@@ -5619,7 +5627,7 @@ Handle<FixedArray> BaseNameDictionary<Derived, Shape>::IterationIndices(
     DisallowGarbageCollection no_gc;
     Tagged<Derived> raw_dictionary = *dictionary;
     for (InternalIndex i : dictionary->IterateEntries()) {
-      Object k;
+      Tagged<Object> k;
       if (!raw_dictionary->ToKey(roots, i, &k)) continue;
       array->set(array_size++, Smi::FromInt(i.as_int()));
     }
@@ -5643,13 +5651,14 @@ Handle<FixedArray> BaseNameDictionary<Derived, Shape>::IterationIndices(
 
 // Backwards lookup (slow).
 template <typename Derived, typename Shape>
-Object Dictionary<Derived, Shape>::SlowReverseLookup(Object value) {
+Tagged<Object> Dictionary<Derived, Shape>::SlowReverseLookup(
+    Tagged<Object> value) {
   Derived dictionary = Derived::cast(*this);
   ReadOnlyRoots roots = dictionary.GetReadOnlyRoots();
   for (InternalIndex i : dictionary.IterateEntries()) {
-    Object k;
+    Tagged<Object> k;
     if (!dictionary.ToKey(roots, i, &k)) continue;
-    Object e = dictionary.ValueAt(i);
+    Tagged<Object> e = dictionary.ValueAt(i);
     if (e == value) return k;
   }
   return roots.undefined_value();
@@ -5665,9 +5674,8 @@ void ObjectHashTableBase<Derived, Shape>::FillEntriesWithHoles(
 }
 
 template <typename Derived, typename Shape>
-Object ObjectHashTableBase<Derived, Shape>::Lookup(PtrComprCageBase cage_base,
-                                                   Handle<Object> key,
-                                                   int32_t hash) {
+Tagged<Object> ObjectHashTableBase<Derived, Shape>::Lookup(
+    PtrComprCageBase cage_base, Handle<Object> key, int32_t hash) {
   DisallowGarbageCollection no_gc;
   ReadOnlyRoots roots = this->GetReadOnlyRoots(cage_base);
   DCHECK(this->IsKey(roots, *key));
@@ -5690,7 +5698,7 @@ int NameToIndexHashTable::Lookup(Handle<Name> key) {
 }
 
 template <typename Derived, typename Shape>
-Object ObjectHashTableBase<Derived, Shape>::Lookup(Handle<Object> key) {
+Tagged<Object> ObjectHashTableBase<Derived, Shape>::Lookup(Handle<Object> key) {
   DisallowGarbageCollection no_gc;
 
   PtrComprCageBase cage_base = GetPtrComprCageBase(*this);
@@ -5698,7 +5706,7 @@ Object ObjectHashTableBase<Derived, Shape>::Lookup(Handle<Object> key) {
   DCHECK(this->IsKey(roots, *key));
 
   // If the object does not have an identity hash, it was never used as a key.
-  Object hash = Object::GetHash(*key);
+  Tagged<Object> hash = Object::GetHash(*key);
   if (IsUndefined(hash, roots)) {
     return roots.the_hole_value();
   }
@@ -5706,26 +5714,27 @@ Object ObjectHashTableBase<Derived, Shape>::Lookup(Handle<Object> key) {
 }
 
 template <typename Derived, typename Shape>
-Object ObjectHashTableBase<Derived, Shape>::Lookup(Handle<Object> key,
-                                                   int32_t hash) {
+Tagged<Object> ObjectHashTableBase<Derived, Shape>::Lookup(Handle<Object> key,
+                                                           int32_t hash) {
   return Lookup(GetPtrComprCageBase(*this), key, hash);
 }
 
 template <typename Derived, typename Shape>
-Object ObjectHashTableBase<Derived, Shape>::ValueAt(InternalIndex entry) {
+Tagged<Object> ObjectHashTableBase<Derived, Shape>::ValueAt(
+    InternalIndex entry) {
   return this->get(EntryToValueIndex(entry));
 }
 
-Object RegisteredSymbolTable::ValueAt(InternalIndex entry) {
+Tagged<Object> RegisteredSymbolTable::ValueAt(InternalIndex entry) {
   return this->get(EntryToValueIndex(entry));
 }
 
-Object NameToIndexHashTable::ValueAt(InternalIndex entry) {
+Tagged<Object> NameToIndexHashTable::ValueAt(InternalIndex entry) {
   return this->get(EntryToValueIndex(entry));
 }
 
 int NameToIndexHashTable::IndexAt(InternalIndex entry) {
-  Object value = ValueAt(entry);
+  Tagged<Object> value = ValueAt(entry);
   if (IsSmi(value)) {
     int index = Smi::ToInt(value);
     DCHECK_LE(0, index);
@@ -5807,7 +5816,7 @@ Handle<Derived> ObjectHashTableBase<Derived, Shape>::Remove(
     bool* was_present) {
   DCHECK(table->IsKey(table->GetReadOnlyRoots(), *key));
 
-  Object hash = Object::GetHash(*key);
+  Tagged<Object> hash = Object::GetHash(*key);
   if (IsUndefined(hash)) {
     *was_present = false;
     return table;
@@ -5836,7 +5845,8 @@ Handle<Derived> ObjectHashTableBase<Derived, Shape>::Remove(
 
 template <typename Derived, typename Shape>
 void ObjectHashTableBase<Derived, Shape>::AddEntry(InternalIndex entry,
-                                                   Object key, Object value) {
+                                                   Tagged<Object> key,
+                                                   Tagged<Object> value) {
   Derived* self = static_cast<Derived*>(this);
   self->set_key(Derived::EntryToIndex(entry), key);
   self->set(Derived::EntryToValueIndex(entry), value);
@@ -5864,7 +5874,7 @@ std::array<Object, N> ObjectMultiHashTableBase<Derived, N>::Lookup(
   ReadOnlyRoots roots = this->GetReadOnlyRoots(cage_base);
   DCHECK(this->IsKey(roots, *key));
 
-  Object hash_obj = Object::GetHash(*key);
+  Tagged<Object> hash_obj = Object::GetHash(*key);
   if (IsUndefined(hash_obj, roots)) {
     return {roots.the_hole_value(), roots.the_hole_value()};
   }
@@ -6033,11 +6043,11 @@ Handle<JSArray> JSWeakCollection::GetEntries(Handle<JSWeakCollection> holder,
     int count = 0;
     for (int i = 0;
          count / values_per_entry < max_entries && i < table->Capacity(); i++) {
-      Object key;
+      Tagged<Object> key;
       if (table->ToKey(roots, InternalIndex(i), &key)) {
         entries->set(count++, key);
         if (values_per_entry > 1) {
-          Object value = table->Lookup(handle(key, isolate));
+          Tagged<Object> value = table->Lookup(handle(key, isolate));
           entries->set(count++, value);
         }
       }
@@ -6076,27 +6086,30 @@ Handle<PropertyCell> PropertyCell::InvalidateAndReplaceEntry(
   return new_cell;
 }
 
-static bool RemainsConstantType(PropertyCell cell, Object value) {
+static bool RemainsConstantType(Tagged<PropertyCell> cell,
+                                Tagged<Object> value) {
   DisallowGarbageCollection no_gc;
   // TODO(dcarney): double->smi and smi->double transition from kConstant
   if (IsSmi(cell->value()) && IsSmi(value)) {
     return true;
   } else if (IsHeapObject(cell->value()) && IsHeapObject(value)) {
-    Map map = HeapObject::cast(value)->map();
+    Tagged<Map> map = HeapObject::cast(value)->map();
     return HeapObject::cast(cell->value())->map() == map && map->is_stable();
   }
   return false;
 }
 
 // static
-PropertyCellType PropertyCell::InitialType(Isolate* isolate, Object value) {
+PropertyCellType PropertyCell::InitialType(Isolate* isolate,
+                                           Tagged<Object> value) {
   return IsUndefined(value, isolate) ? PropertyCellType::kUndefined
                                      : PropertyCellType::kConstant;
 }
 
 // static
-PropertyCellType PropertyCell::UpdatedType(Isolate* isolate, PropertyCell cell,
-                                           Object value,
+PropertyCellType PropertyCell::UpdatedType(Isolate* isolate,
+                                           Tagged<PropertyCell> cell,
+                                           Tagged<Object> value,
                                            PropertyDetails details) {
   DisallowGarbageCollection no_gc;
   DCHECK(!IsAnyHole(value, isolate));
@@ -6123,7 +6136,7 @@ Handle<PropertyCell> PropertyCell::PrepareForAndSetValue(
     Isolate* isolate, Handle<GlobalDictionary> dictionary, InternalIndex entry,
     Handle<Object> value, PropertyDetails details) {
   DCHECK(!IsAnyHole(*value, isolate));
-  PropertyCell raw_cell = dictionary->CellAt(entry);
+  Tagged<PropertyCell> raw_cell = dictionary->CellAt(entry);
   CHECK(!IsAnyHole(raw_cell->value(), isolate));
   const PropertyDetails original_details = raw_cell->property_details();
   // Data accesses could be cached in ics or optimized code.
@@ -6172,7 +6185,7 @@ void PropertyCell::InvalidateProtector() {
 
 // static
 bool PropertyCell::CheckDataIsCompatible(PropertyDetails details,
-                                         Object value) {
+                                         Tagged<Object> value) {
   DisallowGarbageCollection no_gc;
   PropertyCellType cell_type = details.cell_type();
   CHECK_NE(cell_type, PropertyCellType::kInTransition);
@@ -6189,7 +6202,7 @@ bool PropertyCell::CheckDataIsCompatible(PropertyDetails details,
 
 #ifdef DEBUG
 bool PropertyCell::CanTransitionTo(PropertyDetails new_details,
-                                   Object new_value) const {
+                                   Tagged<Object> new_value) const {
   // Extending the implementation of PropertyCells with additional states
   // and/or transitions likely requires changes to PropertyCellData::Serialize.
   DisallowGarbageCollection no_gc;
@@ -6233,30 +6246,30 @@ int JSGeneratorObject::source_position() const {
              ->shared()
              ->GetBytecodeArray(isolate)
              ->HasSourcePositionTable());
-  AbstractCode code =
+  Tagged<AbstractCode> code =
       AbstractCode::cast(function()->shared()->GetBytecodeArray(isolate));
   return code->SourcePosition(isolate, code_offset());
 }
 
 // static
-AccessCheckInfo AccessCheckInfo::Get(Isolate* isolate,
-                                     Handle<JSObject> receiver) {
+Tagged<AccessCheckInfo> AccessCheckInfo::Get(Isolate* isolate,
+                                             Handle<JSObject> receiver) {
   DisallowGarbageCollection no_gc;
   DCHECK(receiver->map()->is_access_check_needed());
-  Object maybe_constructor = receiver->map()->GetConstructor();
+  Tagged<Object> maybe_constructor = receiver->map()->GetConstructor();
   if (IsFunctionTemplateInfo(maybe_constructor)) {
-    Object data_obj =
+    Tagged<Object> data_obj =
         FunctionTemplateInfo::cast(maybe_constructor)->GetAccessCheckInfo();
     if (IsUndefined(data_obj, isolate)) return AccessCheckInfo();
     return AccessCheckInfo::cast(data_obj);
   }
   // Might happen for a detached context.
   if (!IsJSFunction(maybe_constructor)) return AccessCheckInfo();
-  JSFunction constructor = JSFunction::cast(maybe_constructor);
+  Tagged<JSFunction> constructor = JSFunction::cast(maybe_constructor);
   // Might happen for the debug context.
   if (!constructor->shared()->IsApiFunction()) return AccessCheckInfo();
 
-  Object data_obj =
+  Tagged<Object> data_obj =
       constructor->shared()->api_func_data()->GetAccessCheckInfo();
   if (IsUndefined(data_obj, isolate)) return AccessCheckInfo();
 
@@ -6349,20 +6362,20 @@ void JSFinalizationRegistry::RemoveCellFromUnregisterTokenMap(
     Isolate* isolate, Address raw_finalization_registry,
     Address raw_weak_cell) {
   DisallowGarbageCollection no_gc;
-  JSFinalizationRegistry finalization_registry =
+  Tagged<JSFinalizationRegistry> finalization_registry =
       JSFinalizationRegistry::cast(Object(raw_finalization_registry));
-  WeakCell weak_cell = WeakCell::cast(Object(raw_weak_cell));
+  Tagged<WeakCell> weak_cell = WeakCell::cast(Object(raw_weak_cell));
   DCHECK(!IsUndefined(weak_cell->unregister_token(), isolate));
-  HeapObject undefined = ReadOnlyRoots(isolate).undefined_value();
+  Tagged<HeapObject> undefined = ReadOnlyRoots(isolate).undefined_value();
 
   // Remove weak_cell from the linked list of other WeakCells with the same
   // unregister token and remove its unregister token from key_map if necessary
   // without shrinking it. Since shrinking may allocate, it is performed by the
   // caller after looping, or on exception.
   if (IsUndefined(weak_cell->key_list_prev(), isolate)) {
-    SimpleNumberDictionary key_map =
+    Tagged<SimpleNumberDictionary> key_map =
         SimpleNumberDictionary::cast(finalization_registry->key_map());
-    HeapObject unregister_token = weak_cell->unregister_token();
+    Tagged<HeapObject> unregister_token = weak_cell->unregister_token();
     uint32_t key = Smi::ToInt(Object::GetHash(unregister_token));
     InternalIndex entry = key_map->FindEntry(isolate, key);
     DCHECK(entry.is_found());
@@ -6375,17 +6388,17 @@ void JSFinalizationRegistry::RemoveCellFromUnregisterTokenMap(
     } else {
       // weak_cell is the list head for its key; we need to change the value
       // of the key in the hash table.
-      WeakCell next = WeakCell::cast(weak_cell->key_list_next());
+      Tagged<WeakCell> next = WeakCell::cast(weak_cell->key_list_next());
       DCHECK_EQ(next->key_list_prev(), weak_cell);
       next->set_key_list_prev(undefined);
       key_map->ValueAtPut(entry, next);
     }
   } else {
     // weak_cell is somewhere in the middle of its key list.
-    WeakCell prev = WeakCell::cast(weak_cell->key_list_prev());
+    Tagged<WeakCell> prev = WeakCell::cast(weak_cell->key_list_prev());
     prev->set_key_list_next(weak_cell->key_list_next());
     if (!IsUndefined(weak_cell->key_list_next())) {
-      WeakCell next = WeakCell::cast(weak_cell->key_list_next());
+      Tagged<WeakCell> next = WeakCell::cast(weak_cell->key_list_next());
       next->set_key_list_prev(weak_cell->key_list_prev());
     }
   }
@@ -6398,7 +6411,7 @@ void JSFinalizationRegistry::RemoveCellFromUnregisterTokenMap(
 }
 
 // static
-bool MapWord::IsMapOrForwarded(Map map) {
+bool MapWord::IsMapOrForwarded(Tagged<Map> map) {
   MapWord map_word = map->map_word(kRelaxedLoad);
 
   if (map_word.IsForwardingAddress()) {
diff --git a/src/objects/objects.h b/src/objects/objects.h
index 7674d4148e8..2a4072721dd 100644
--- a/src/objects/objects.h
+++ b/src/objects/objects.h
@@ -364,7 +364,7 @@ class Object : public TaggedImpl<HeapObjectReferenceType::STRONG, Address> {
   template <typename IsolateT>
   V8_EXPORT_PRIVATE static bool BooleanValue(Tagged<Object> obj,
                                              IsolateT* isolate);
-  static Object ToBoolean(Tagged<Object> obj, Isolate* isolate);
+  static Tagged<Object> ToBoolean(Tagged<Object> obj, Isolate* isolate);
 
   // ES6 section 7.2.11 Abstract Relational Comparison
   V8_EXPORT_PRIVATE V8_WARN_UNUSED_RESULT static Maybe<ComparisonResult>
@@ -375,7 +375,8 @@ class Object : public TaggedImpl<HeapObjectReferenceType::STRONG, Address> {
       Isolate* isolate, Handle<Object> x, Handle<Object> y);
 
   // ES6 section 7.2.13 Strict Equality Comparison
-  V8_EXPORT_PRIVATE static bool StrictEquals(Tagged<Object> obj, Object that);
+  V8_EXPORT_PRIVATE static bool StrictEquals(Tagged<Object> obj,
+                                             Tagged<Object> that);
 
   // ES6 section 7.1.13 ToObject
   // Convert to a JSObject if needed.
@@ -564,18 +565,19 @@ class Object : public TaggedImpl<HeapObjectReferenceType::STRONG, Address> {
 
   // Returns the permanent hash code associated with this object. May return
   // undefined if not yet created.
-  static inline Object GetHash(Tagged<Object> obj);
+  static inline Tagged<Object> GetHash(Tagged<Object> obj);
 
   // Returns the permanent hash code associated with this object depending on
   // the actual object type. May create and store a hash code if needed and none
   // exists.
-  V8_EXPORT_PRIVATE static Smi GetOrCreateHash(Tagged<Object> obj,
-                                               Isolate* isolate);
+  V8_EXPORT_PRIVATE static Tagged<Smi> GetOrCreateHash(Tagged<Object> obj,
+                                                       Isolate* isolate);
 
   // Checks whether this object has the same value as the given one.  This
   // function is implemented according to ES5, section 9.12 and can be used
   // to implement the Object.is function.
-  V8_EXPORT_PRIVATE static bool SameValue(Tagged<Object> obj, Object other);
+  V8_EXPORT_PRIVATE static bool SameValue(Tagged<Object> obj,
+                                          Tagged<Object> other);
 
   // A part of SameValue which handles Number vs. Number case.
   // Treats NaN == NaN and +0 != -0.
@@ -585,7 +587,7 @@ class Object : public TaggedImpl<HeapObjectReferenceType::STRONG, Address> {
   // +0 and -0 are treated equal. Everything else is the same as SameValue.
   // This function is implemented according to ES6, section 7.2.4 and is used
   // by ES6 Map and Set.
-  static bool SameValueZero(Tagged<Object> obj, Object other);
+  static bool SameValueZero(Tagged<Object> obj, Tagged<Object> other);
 
   // ES6 section 9.4.2.3 ArraySpeciesCreate (part of it)
   V8_WARN_UNUSED_RESULT static MaybeHandle<Object> ArraySpeciesConstructor(
@@ -625,15 +627,17 @@ class Object : public TaggedImpl<HeapObjectReferenceType::STRONG, Address> {
   // Verify a pointer is a valid (non-InstructionStream) object pointer.
   // When V8_EXTERNAL_CODE_SPACE is enabled InstructionStream objects are
   // not allowed.
-  static void VerifyPointer(Isolate* isolate, Object p);
+  static void VerifyPointer(Isolate* isolate, Tagged<Object> p);
   // Verify a pointer is a valid object pointer.
   // InstructionStream objects are allowed regardless of the
   // V8_EXTERNAL_CODE_SPACE mode.
-  static void VerifyAnyTagged(Isolate* isolate, Object p);
+  static void VerifyAnyTagged(Isolate* isolate, Tagged<Object> p);
 #endif
 
-  inline static constexpr Object cast(Object object) { return object; }
-  inline static constexpr Object unchecked_cast(Object object) {
+  inline static constexpr Tagged<Object> cast(Tagged<Object> object) {
+    return object;
+  }
+  inline static constexpr Tagged<Object> unchecked_cast(Tagged<Object> object) {
     return object;
   }
 
@@ -642,7 +646,7 @@ class Object : public TaggedImpl<HeapObjectReferenceType::STRONG, Address> {
 
   // For use with std::unordered_set.
   struct Hasher {
-    size_t operator()(const Object o) const {
+    size_t operator()(const Tagged<Object> o) const {
       return std::hash<v8::internal::Address>{}(static_cast<Tagged_t>(o.ptr()));
     }
   };
@@ -650,14 +654,16 @@ class Object : public TaggedImpl<HeapObjectReferenceType::STRONG, Address> {
   // For use with std::unordered_set/unordered_map when using both
   // InstructionStream and non-InstructionStream objects as keys.
   struct KeyEqualSafe {
-    bool operator()(const Object a, const Object b) const {
+    bool operator()(const Tagged<Object> a, const Tagged<Object> b) const {
       return a.SafeEquals(b);
     }
   };
 
   // For use with std::map.
   struct Comparer {
-    bool operator()(const Object a, const Object b) const { return a < b; }
+    bool operator()(const Tagged<Object> a, const Tagged<Object> b) const {
+      return a < b;
+    }
   };
 
   // If the receiver is the JSGlobalObject, the store was contextual. In case
@@ -700,7 +706,8 @@ class Object : public TaggedImpl<HeapObjectReferenceType::STRONG, Address> {
   friend class StringStream;
 
   // Return the map of the root of object's prototype chain.
-  static Map GetPrototypeChainRootMap(Tagged<Object> obj, Isolate* isolate);
+  static Tagged<Map> GetPrototypeChainRootMap(Tagged<Object> obj,
+                                              Isolate* isolate);
 
   // Returns a non-SMI for JSReceivers, but returns the hash code forp
   // simple objects.  This avoids a double lookup in the cases where
@@ -709,7 +716,7 @@ class Object : public TaggedImpl<HeapObjectReferenceType::STRONG, Address> {
   //
   // Despite its size, this needs to be inlined for performance
   // reasons.
-  static inline Object GetSimpleHash(Object object);
+  static inline Tagged<Object> GetSimpleHash(Tagged<Object> object);
 
   // Helper for SetProperty and SetSuperProperty.
   // Return value is only meaningful if [found] is set to true on return.
@@ -776,7 +783,7 @@ V8_EXPORT_PRIVATE std::ostream& operator<<(std::ostream& os, const Brief& v);
 
 // Objects should never have the weak tag; this variant is for overzealous
 // checking.
-V8_INLINE static bool HasWeakHeapObjectTag(const Object value) {
+V8_INLINE static bool HasWeakHeapObjectTag(const Tagged<Object> value) {
   return HAS_WEAK_HEAP_OBJECT_TAG(value.ptr());
 }
 
@@ -798,13 +805,13 @@ V8_INLINE constexpr bool IsHeapObject(TaggedImpl<kRefType, StorageType> obj) {
 
 // TODO(leszeks): These exist both as free functions and members of Tagged. They
 // probably want to be cleaned up at some point.
-V8_INLINE bool IsSmi(Tagged<Object> obj);
-V8_INLINE bool IsSmi(Tagged<HeapObject> obj);
-V8_INLINE bool IsSmi(Tagged<Smi> obj);
+V8_INLINE bool IsSmi(Tagged<Object> obj) { return obj.IsSmi(); }
+V8_INLINE bool IsSmi(Tagged<HeapObject> obj) { return false; }
+V8_INLINE bool IsSmi(Tagged<Smi> obj) { return true; }
 
-V8_INLINE bool IsHeapObject(Tagged<Object> obj);
-V8_INLINE bool IsHeapObject(Tagged<HeapObject> obj);
-V8_INLINE bool IsHeapObject(Tagged<Smi> obj);
+V8_INLINE bool IsHeapObject(Tagged<Object> obj) { return obj.IsHeapObject(); }
+V8_INLINE bool IsHeapObject(Tagged<HeapObject> obj) { return true; }
+V8_INLINE bool IsHeapObject(Tagged<Smi> obj) { return false; }
 
 template <typename T>
 // static
@@ -886,44 +893,20 @@ inline bool IsApiCallResultType(Tagged<Object> obj);
 #endif  // DEBUG
 
 // Prints this object without details.
-// TODO(leszeks): Make these functions work on Tagged<Object>, once there is no
-// implicit conversion between Tagged and Object which makes them ambiguously
-// overload with the TaggedImpl overloads.
-V8_EXPORT_PRIVATE void ShortPrint(Object obj, FILE* out = stdout);
+V8_EXPORT_PRIVATE void ShortPrint(Tagged<Object> obj, FILE* out = stdout);
 
 // Prints this object without details to a message accumulator.
-V8_EXPORT_PRIVATE void ShortPrint(Object obj, StringStream* accumulator);
+V8_EXPORT_PRIVATE void ShortPrint(Tagged<Object> obj,
+                                  StringStream* accumulator);
 
-V8_EXPORT_PRIVATE void ShortPrint(Object obj, std::ostream& os);
-
-inline void ShortPrint(TaggedBase obj, FILE* out = stdout) {
-  static_assert(kTaggedCanConvertToRawObjects);
-  return ShortPrint(Object(obj.ptr()), out);
-}
-inline void ShortPrint(TaggedBase obj, StringStream* accumulator) {
-  static_assert(kTaggedCanConvertToRawObjects);
-  return ShortPrint(Object(obj.ptr()), accumulator);
-}
-inline void ShortPrint(TaggedBase obj, std::ostream& os) {
-  static_assert(kTaggedCanConvertToRawObjects);
-  return ShortPrint(Object(obj.ptr()), os);
-}
+V8_EXPORT_PRIVATE void ShortPrint(Tagged<Object> obj, std::ostream& os);
 
 #ifdef OBJECT_PRINT
 // For our gdb macros, we should perhaps change these in the future.
-V8_EXPORT_PRIVATE void Print(Object obj);
+V8_EXPORT_PRIVATE void Print(Tagged<Object> obj);
 
 // Prints this object with details.
-V8_EXPORT_PRIVATE void Print(Object obj, std::ostream& os);
-
-inline void Print(TaggedBase obj) {
-  static_assert(kTaggedCanConvertToRawObjects);
-  return Print(Object(obj.ptr()));
-}
-inline void Print(TaggedBase obj, std::ostream& os) {
-  static_assert(kTaggedCanConvertToRawObjects);
-  return Print(Object(obj.ptr()), os);
-}
+V8_EXPORT_PRIVATE void Print(Tagged<Object> obj, std::ostream& os);
 
 #else
 inline void Print(Object obj) { ShortPrint(obj); }
@@ -949,7 +932,7 @@ class MapWord {
   // Normal state: the map word contains a map pointer.
 
   // Create a map word from a map pointer.
-  static inline MapWord FromMap(const Map map);
+  static inline MapWord FromMap(const Tagged<Map> map);
 
   // View this map word as a map pointer.
   inline Tagged<Map> ToMap() const;
@@ -962,14 +945,15 @@ class MapWord {
   // when all map words are heap object pointers, i.e. not during a full GC).
   inline bool IsForwardingAddress() const;
 
-  V8_EXPORT_PRIVATE static bool IsMapOrForwarded(Map map);
+  V8_EXPORT_PRIVATE static bool IsMapOrForwarded(Tagged<Map> map);
 
   // Create a map word from a forwarding address.
-  static inline MapWord FromForwardingAddress(HeapObject map_word_host,
-                                              HeapObject object);
+  static inline MapWord FromForwardingAddress(Tagged<HeapObject> map_word_host,
+                                              Tagged<HeapObject> object);
 
   // View this map word as a forwarding address.
-  inline HeapObject ToForwardingAddress(HeapObject map_word_host);
+  inline Tagged<HeapObject> ToForwardingAddress(
+      Tagged<HeapObject> map_word_host);
 
   constexpr inline Address ptr() const { return value_; }
 
diff --git a/src/objects/ordered-hash-table-inl.h b/src/objects/ordered-hash-table-inl.h
index b738fe212b4..84fb8a49d40 100644
--- a/src/objects/ordered-hash-table-inl.h
+++ b/src/objects/ordered-hash-table-inl.h
@@ -35,7 +35,7 @@ OrderedHashTable<Derived, entrysize>::OrderedHashTable(Address ptr)
 
 template <class Derived, int entrysize>
 bool OrderedHashTable<Derived, entrysize>::IsKey(ReadOnlyRoots roots,
-                                                 Object k) {
+                                                 Tagged<Object> k) {
   return k != roots.the_hole_value();
 }
 
@@ -59,15 +59,16 @@ SmallOrderedHashTable<Derived>::SmallOrderedHashTable(Address ptr)
     : HeapObject(ptr) {}
 
 template <class Derived>
-Object SmallOrderedHashTable<Derived>::KeyAt(InternalIndex entry) const {
+Tagged<Object> SmallOrderedHashTable<Derived>::KeyAt(
+    InternalIndex entry) const {
   DCHECK_LT(entry.as_int(), Capacity());
   Offset entry_offset = GetDataEntryOffset(entry.as_int(), Derived::kKeyIndex);
   return TaggedField<Object>::load(*this, entry_offset);
 }
 
 template <class Derived>
-Object SmallOrderedHashTable<Derived>::GetDataEntry(int entry,
-                                                    int relative_index) {
+Tagged<Object> SmallOrderedHashTable<Derived>::GetDataEntry(
+    int entry, int relative_index) {
   DCHECK_LT(entry, Capacity());
   DCHECK_LE(static_cast<unsigned>(relative_index), Derived::kEntrySize);
   Offset entry_offset = GetDataEntryOffset(entry, relative_index);
@@ -105,17 +106,17 @@ Handle<Map> SmallOrderedHashSet::GetMap(ReadOnlyRoots roots) {
   return roots.small_ordered_hash_set_map_handle();
 }
 
-inline Object OrderedHashMap::ValueAt(InternalIndex entry) {
+inline Tagged<Object> OrderedHashMap::ValueAt(InternalIndex entry) {
   DCHECK_LT(entry.as_int(), UsedCapacity());
   return get(EntryToIndex(entry) + kValueOffset);
 }
 
-inline Object OrderedNameDictionary::ValueAt(InternalIndex entry) {
+inline Tagged<Object> OrderedNameDictionary::ValueAt(InternalIndex entry) {
   DCHECK_LT(entry.as_int(), UsedCapacity());
   return get(EntryToIndex(entry) + kValueOffset);
 }
 
-Name OrderedNameDictionary::NameAt(InternalIndex entry) {
+Tagged<Name> OrderedNameDictionary::NameAt(InternalIndex entry) {
   return Name::cast(KeyAt(entry));
 }
 
@@ -123,8 +124,8 @@ Name OrderedNameDictionary::NameAt(InternalIndex entry) {
 template <class Derived, int entrysize>
 bool OrderedHashTable<Derived, entrysize>::ToKey(ReadOnlyRoots roots,
                                                  InternalIndex entry,
-                                                 Object* out_key) {
-  Object k = KeyAt(entry);
+                                                 Tagged<Object>* out_key) {
+  Tagged<Object> k = KeyAt(entry);
   if (!IsKey(roots, k)) return false;
   *out_key = k;
   return true;
@@ -132,7 +133,7 @@ bool OrderedHashTable<Derived, entrysize>::ToKey(ReadOnlyRoots roots,
 
 // Set the value for entry.
 inline void OrderedNameDictionary::ValueAtPut(InternalIndex entry,
-                                              Object value) {
+                                              Tagged<Object> value) {
   DCHECK_LT(entry.as_int(), UsedCapacity());
   this->set(EntryToIndex(entry) + kValueOffset, value);
 }
@@ -152,13 +153,13 @@ inline void OrderedNameDictionary::DetailsAtPut(InternalIndex entry,
   this->set(EntryToIndex(entry) + kPropertyDetailsOffset, value.AsSmi());
 }
 
-inline Object SmallOrderedNameDictionary::ValueAt(InternalIndex entry) {
+inline Tagged<Object> SmallOrderedNameDictionary::ValueAt(InternalIndex entry) {
   return this->GetDataEntry(entry.as_int(), kValueIndex);
 }
 
 // Set the value for entry.
 inline void SmallOrderedNameDictionary::ValueAtPut(InternalIndex entry,
-                                                   Object value) {
+                                                   Tagged<Object> value) {
   this->SetDataEntry(entry.as_int(), kValueIndex, value);
 }
 
@@ -203,7 +204,7 @@ inline bool SmallOrderedHashMap::Is(Handle<HeapObject> table) {
 
 template <class Derived>
 void SmallOrderedHashTable<Derived>::SetDataEntry(int entry, int relative_index,
-                                                  Object value) {
+                                                  Tagged<Object> value) {
   DCHECK_NE(kNotFound, entry);
   int entry_offset = GetDataEntryOffset(entry, relative_index);
   RELAXED_WRITE_FIELD(*this, entry_offset, value);
@@ -211,12 +212,12 @@ void SmallOrderedHashTable<Derived>::SetDataEntry(int entry, int relative_index,
 }
 
 template <class Derived, class TableType>
-Object OrderedHashTableIterator<Derived, TableType>::CurrentKey() {
+Tagged<Object> OrderedHashTableIterator<Derived, TableType>::CurrentKey() {
   TableType table = TableType::cast(this->table());
   int index = Smi::ToInt(this->index());
   DCHECK_LE(0, index);
   InternalIndex entry(index);
-  Object key = table.KeyAt(entry);
+  Tagged<Object> key = table.KeyAt(entry);
   DCHECK(!IsTheHole(key));
   return key;
 }
@@ -238,7 +239,7 @@ inline void OrderedNameDictionary::SetHash(int hash) {
 }
 
 inline int OrderedNameDictionary::Hash() {
-  Object hash_obj = this->get(HashIndex());
+  Tagged<Object> hash_obj = this->get(HashIndex());
   int hash = Smi::ToInt(hash_obj);
   DCHECK(PropertyArray::HashField::is_valid(hash));
   return hash;
diff --git a/src/objects/ordered-hash-table.cc b/src/objects/ordered-hash-table.cc
index add1eac22af..53c805d74a7 100644
--- a/src/objects/ordered-hash-table.cc
+++ b/src/objects/ordered-hash-table.cc
@@ -127,17 +127,18 @@ Handle<Derived> OrderedHashTable<Derived, entrysize>::Clear(
 
 template <class Derived, int entrysize>
 bool OrderedHashTable<Derived, entrysize>::HasKey(Isolate* isolate,
-                                                  Derived table, Object key) {
+                                                  Tagged<Derived> table,
+                                                  Tagged<Object> key) {
   DCHECK_IMPLIES(entrysize == 1, IsOrderedHashSet(table));
   DCHECK_IMPLIES(entrysize == 2, IsOrderedHashMap(table));
   DisallowGarbageCollection no_gc;
-  InternalIndex entry = table.FindEntry(isolate, key);
+  InternalIndex entry = table->FindEntry(isolate, key);
   return entry.is_found();
 }
 
 template <class Derived, int entrysize>
-InternalIndex OrderedHashTable<Derived, entrysize>::FindEntry(Isolate* isolate,
-                                                              Object key) {
+InternalIndex OrderedHashTable<Derived, entrysize>::FindEntry(
+    Isolate* isolate, Tagged<Object> key) {
   if (NumberOfElements() == 0) {
     // This is not just an optimization but also ensures that we do the right
     // thing if Capacity() == 0
@@ -152,7 +153,7 @@ InternalIndex OrderedHashTable<Derived, entrysize>::FindEntry(Isolate* isolate,
     raw_entry = HashToEntryRaw(hash & Smi::kMaxValue);
   } else {
     HandleScope scope(isolate);
-    Object hash = Object::GetHash(key);
+    Tagged<Object> hash = Object::GetHash(key);
     // If the object does not have an identity hash, it was never used as a key
     if (IsUndefined(hash, isolate)) return InternalIndex::NotFound();
     raw_entry = HashToEntryRaw(Smi::ToInt(hash));
@@ -160,7 +161,7 @@ InternalIndex OrderedHashTable<Derived, entrysize>::FindEntry(Isolate* isolate,
 
   // Walk the chain in the bucket to find the key.
   while (raw_entry != kNotFound) {
-    Object candidate_key = KeyAt(InternalIndex(raw_entry));
+    Tagged<Object> candidate_key = KeyAt(InternalIndex(raw_entry));
     if (Object::SameValueZero(candidate_key, key))
       return InternalIndex(raw_entry);
     raw_entry = NextChainEntryRaw(raw_entry);
@@ -182,7 +183,8 @@ MaybeHandle<OrderedHashSet> OrderedHashSet::Add(Isolate* isolate,
       int raw_entry = raw_table->HashToEntryRaw(hash);
       // Walk the chain of the bucket and try finding the key.
       while (raw_entry != kNotFound) {
-        Object candidate_key = raw_table->KeyAt(InternalIndex(raw_entry));
+        Tagged<Object> candidate_key =
+            raw_table->KeyAt(InternalIndex(raw_entry));
         // Do not add if we have the key already
         if (Object::SameValueZero(candidate_key, raw_key)) return table;
         raw_entry = raw_table->NextChainEntryRaw(raw_entry);
@@ -225,7 +227,7 @@ Handle<FixedArray> OrderedHashSet::ConvertToKeysArray(
       isolate->heap()->MaxNumberToStringCacheSize();
   for (int i = 0; i < length; i++) {
     int index = HashTableStartIndex() + nof_buckets + (i * kEntrySize);
-    Object key = table->get(index);
+    Tagged<Object> key = table->get(index);
     uint32_t index_value;
     if (convert == GetKeysConversion::kConvertToString) {
       if (Object::ToArrayIndex(key, &index_value)) {
@@ -243,11 +245,11 @@ Handle<FixedArray> OrderedHashSet::ConvertToKeysArray(
   return FixedArray::ShrinkOrEmpty(isolate, result, length);
 }
 
-HeapObject OrderedHashSet::GetEmpty(ReadOnlyRoots ro_roots) {
+Tagged<HeapObject> OrderedHashSet::GetEmpty(ReadOnlyRoots ro_roots) {
   return ro_roots.empty_ordered_hash_set();
 }
 
-HeapObject OrderedHashMap::GetEmpty(ReadOnlyRoots ro_roots) {
+Tagged<HeapObject> OrderedHashMap::GetEmpty(ReadOnlyRoots ro_roots) {
   return ro_roots.empty_ordered_hash_map();
 }
 
@@ -279,20 +281,20 @@ MaybeHandle<Derived> OrderedHashTable<Derived, entrysize>::Rehash(
 
   for (InternalIndex old_entry : table->IterateEntries()) {
     int old_entry_raw = old_entry.as_int();
-    Object key = table->KeyAt(old_entry);
+    Tagged<Object> key = table->KeyAt(old_entry);
     if (IsTheHole(key, isolate)) {
       table->SetRemovedIndexAt(removed_holes_index++, old_entry_raw);
       continue;
     }
 
-    Object hash = Object::GetHash(key);
+    Tagged<Object> hash = Object::GetHash(key);
     int bucket = Smi::ToInt(hash) & (new_buckets - 1);
-    Object chain_entry = new_table->get(HashTableStartIndex() + bucket);
+    Tagged<Object> chain_entry = new_table->get(HashTableStartIndex() + bucket);
     new_table->set(HashTableStartIndex() + bucket, Smi::FromInt(new_entry));
     int new_index = new_table->EntryToIndexRaw(new_entry);
     int old_index = table->EntryToIndexRaw(old_entry_raw);
     for (int i = 0; i < entrysize; ++i) {
-      Object value = table->get(old_index + i);
+      Tagged<Object> value = table->get(old_index + i);
       new_table->set(new_index + i, value);
     }
     new_table->set(new_index + kChainOffset, chain_entry);
@@ -345,30 +347,31 @@ MaybeHandle<OrderedNameDictionary> OrderedNameDictionary::Rehash(
 
 template <class Derived, int entrysize>
 bool OrderedHashTable<Derived, entrysize>::Delete(Isolate* isolate,
-                                                  Derived table, Object key) {
+                                                  Tagged<Derived> table,
+                                                  Tagged<Object> key) {
   DisallowGarbageCollection no_gc;
-  InternalIndex entry = table.FindEntry(isolate, key);
+  InternalIndex entry = table->FindEntry(isolate, key);
   if (entry.is_not_found()) return false;
 
-  int nof = table.NumberOfElements();
-  int nod = table.NumberOfDeletedElements();
-  int index = table.EntryToIndex(entry);
+  int nof = table->NumberOfElements();
+  int nod = table->NumberOfDeletedElements();
+  int index = table->EntryToIndex(entry);
 
-  Object hole = ReadOnlyRoots(isolate).the_hole_value();
+  Tagged<Object> hole = ReadOnlyRoots(isolate).the_hole_value();
   for (int i = 0; i < entrysize; ++i) {
-    table.set(index + i, hole);
+    table->set(index + i, hole);
   }
 
-  table.SetNumberOfElements(nof - 1);
-  table.SetNumberOfDeletedElements(nod + 1);
+  table->SetNumberOfElements(nof - 1);
+  table->SetNumberOfDeletedElements(nod + 1);
 
   return true;
 }
 
 Address OrderedHashMap::GetHash(Isolate* isolate, Address raw_key) {
   DisallowGarbageCollection no_gc;
-  Object key(raw_key);
-  Object hash = Object::GetHash(key);
+  Tagged<Object> key(raw_key);
+  Tagged<Object> hash = Object::GetHash(key);
   // If the object does not have an identity hash, it was never used as a key
   if (IsUndefined(hash, isolate)) return Smi::FromInt(-1).ptr();
   DCHECK(IsSmi(hash));
@@ -386,9 +389,9 @@ MaybeHandle<OrderedHashMap> OrderedHashMap::Add(Isolate* isolate,
     // Walk the chain of the bucket and try finding the key.
     {
       DisallowGarbageCollection no_gc;
-      Object raw_key = *key;
+      Tagged<Object> raw_key = *key;
       while (raw_entry != kNotFound) {
-        Object candidate_key = table->KeyAt(InternalIndex(raw_entry));
+        Tagged<Object> candidate_key = table->KeyAt(InternalIndex(raw_entry));
         // Do not add if we have the key already
         if (Object::SameValueZero(candidate_key, raw_key)) return table;
         raw_entry = table->NextChainEntryRaw(raw_entry);
@@ -419,7 +422,8 @@ MaybeHandle<OrderedHashMap> OrderedHashMap::Add(Isolate* isolate,
   return table;
 }
 
-void OrderedHashMap::SetEntry(InternalIndex entry, Object key, Object value) {
+void OrderedHashMap::SetEntry(InternalIndex entry, Tagged<Object> key,
+                              Tagged<Object> value) {
   DisallowGarbageCollection no_gc;
   int index = EntryToIndex(entry);
   this->set(index, key);
@@ -427,11 +431,12 @@ void OrderedHashMap::SetEntry(InternalIndex entry, Object key, Object value) {
 }
 
 template <typename IsolateT>
-InternalIndex OrderedNameDictionary::FindEntry(IsolateT* isolate, Object key) {
+InternalIndex OrderedNameDictionary::FindEntry(IsolateT* isolate,
+                                               Tagged<Object> key) {
   DisallowGarbageCollection no_gc;
 
   DCHECK(IsUniqueName(key));
-  Name raw_key = Name::cast(key);
+  Tagged<Name> raw_key = Name::cast(key);
 
   if (NumberOfElements() == 0) {
     // This is not just an optimization but also ensures that we do the right
@@ -442,7 +447,7 @@ InternalIndex OrderedNameDictionary::FindEntry(IsolateT* isolate, Object key) {
   int raw_entry = HashToEntryRaw(raw_key->hash());
   while (raw_entry != kNotFound) {
     InternalIndex entry(raw_entry);
-    Object candidate_key = KeyAt(entry);
+    Tagged<Object> candidate_key = KeyAt(entry);
     DCHECK(IsTheHole(candidate_key) || IsUniqueName(Name::cast(candidate_key)));
     if (candidate_key == raw_key) return entry;
 
@@ -491,8 +496,9 @@ MaybeHandle<OrderedNameDictionary> OrderedNameDictionary::Add(
   return table;
 }
 
-void OrderedNameDictionary::SetEntry(InternalIndex entry, Object key,
-                                     Object value, PropertyDetails details) {
+void OrderedNameDictionary::SetEntry(InternalIndex entry, Tagged<Object> key,
+                                     Tagged<Object> value,
+                                     PropertyDetails details) {
   DisallowGarbageCollection gc;
   DCHECK_IMPLIES(!IsName(key), IsTheHole(key));
   DisallowGarbageCollection no_gc;
@@ -511,7 +517,7 @@ Handle<OrderedNameDictionary> OrderedNameDictionary::DeleteEntry(
     InternalIndex entry) {
   DCHECK(entry.is_found());
 
-  Object hole = ReadOnlyRoots(isolate).the_hole_value();
+  Tagged<Object> hole = ReadOnlyRoots(isolate).the_hole_value();
   PropertyDetails details = PropertyDetails::Empty();
   table->SetEntry(entry, hole, hole, details);
 
@@ -587,13 +593,14 @@ template V8_EXPORT_PRIVATE MaybeHandle<OrderedHashSet> OrderedHashSet::Allocate(
     Isolate* isolate, int capacity, AllocationType allocation);
 
 template V8_EXPORT_PRIVATE bool OrderedHashTable<OrderedHashSet, 1>::HasKey(
-    Isolate* isolate, OrderedHashSet table, Object key);
+    Isolate* isolate, Tagged<OrderedHashSet> table, Tagged<Object> key);
 
 template V8_EXPORT_PRIVATE bool OrderedHashTable<OrderedHashSet, 1>::Delete(
-    Isolate* isolate, OrderedHashSet table, Object key);
+    Isolate* isolate, Tagged<OrderedHashSet> table, Tagged<Object> key);
 
 template V8_EXPORT_PRIVATE InternalIndex
-OrderedHashTable<OrderedHashSet, 1>::FindEntry(Isolate* isolate, Object key);
+OrderedHashTable<OrderedHashSet, 1>::FindEntry(Isolate* isolate,
+                                               Tagged<Object> key);
 
 template V8_EXPORT_PRIVATE MaybeHandle<OrderedHashMap>
 OrderedHashTable<OrderedHashMap, 2>::EnsureCapacityForAdding(
@@ -611,13 +618,14 @@ template V8_EXPORT_PRIVATE MaybeHandle<OrderedHashMap> OrderedHashMap::Allocate(
     Isolate* isolate, int capacity, AllocationType allocation);
 
 template V8_EXPORT_PRIVATE bool OrderedHashTable<OrderedHashMap, 2>::HasKey(
-    Isolate* isolate, OrderedHashMap table, Object key);
+    Isolate* isolate, Tagged<OrderedHashMap> table, Tagged<Object> key);
 
 template V8_EXPORT_PRIVATE bool OrderedHashTable<OrderedHashMap, 2>::Delete(
-    Isolate* isolate, OrderedHashMap table, Object key);
+    Isolate* isolate, Tagged<OrderedHashMap> table, Tagged<Object> key);
 
 template V8_EXPORT_PRIVATE InternalIndex
-OrderedHashTable<OrderedHashMap, 2>::FindEntry(Isolate* isolate, Object key);
+OrderedHashTable<OrderedHashMap, 2>::FindEntry(Isolate* isolate,
+                                               Tagged<Object> key);
 
 template V8_EXPORT_PRIVATE Handle<OrderedNameDictionary>
 OrderedHashTable<OrderedNameDictionary, 3>::Shrink(
@@ -628,10 +636,10 @@ OrderedHashTable<OrderedNameDictionary, 3>::EnsureCapacityForAdding(
     Isolate* isolate, Handle<OrderedNameDictionary> table);
 
 template V8_EXPORT_PRIVATE InternalIndex
-OrderedNameDictionary::FindEntry(Isolate* isolate, Object key);
+OrderedNameDictionary::FindEntry(Isolate* isolate, Tagged<Object> key);
 
 template V8_EXPORT_PRIVATE InternalIndex
-OrderedNameDictionary::FindEntry(LocalIsolate* isolate, Object key);
+OrderedNameDictionary::FindEntry(LocalIsolate* isolate, Tagged<Object> key);
 
 template <>
 Handle<SmallOrderedHashSet>
@@ -727,8 +735,9 @@ MaybeHandle<SmallOrderedHashSet> SmallOrderedHashSet::Add(
   return table;
 }
 
-bool SmallOrderedHashSet::Delete(Isolate* isolate, SmallOrderedHashSet table,
-                                 Object key) {
+bool SmallOrderedHashSet::Delete(Isolate* isolate,
+                                 Tagged<SmallOrderedHashSet> table,
+                                 Tagged<Object> key) {
   return SmallOrderedHashTable<SmallOrderedHashSet>::Delete(isolate, table,
                                                             key);
 }
@@ -772,8 +781,9 @@ MaybeHandle<SmallOrderedHashMap> SmallOrderedHashMap::Add(
   return table;
 }
 
-bool SmallOrderedHashMap::Delete(Isolate* isolate, SmallOrderedHashMap table,
-                                 Object key) {
+bool SmallOrderedHashMap::Delete(Isolate* isolate,
+                                 Tagged<SmallOrderedHashMap> table,
+                                 Tagged<Object> key) {
   return SmallOrderedHashTable<SmallOrderedHashMap>::Delete(isolate, table,
                                                             key);
 }
@@ -784,18 +794,18 @@ bool SmallOrderedHashMap::HasKey(Isolate* isolate, Handle<Object> key) {
 
 template <>
 InternalIndex V8_EXPORT_PRIVATE
-SmallOrderedHashTable<SmallOrderedNameDictionary>::FindEntry(Isolate* isolate,
-                                                             Object key) {
+SmallOrderedHashTable<SmallOrderedNameDictionary>::FindEntry(
+    Isolate* isolate, Tagged<Object> key) {
   DisallowGarbageCollection no_gc;
   DCHECK(IsUniqueName(key));
-  Name raw_key = Name::cast(key);
+  Tagged<Name> raw_key = Name::cast(key);
 
   int raw_entry = HashToFirstEntry(raw_key->hash());
 
   // Walk the chain in the bucket to find the key.
   while (raw_entry != kNotFound) {
     InternalIndex entry(raw_entry);
-    Object candidate_key = KeyAt(entry);
+    Tagged<Object> candidate_key = KeyAt(entry);
     if (candidate_key == key) return entry;
     raw_entry = GetNextEntry(raw_entry);
   }
@@ -845,8 +855,9 @@ MaybeHandle<SmallOrderedNameDictionary> SmallOrderedNameDictionary::Add(
   return table;
 }
 
-void SmallOrderedNameDictionary::SetEntry(InternalIndex entry, Object key,
-                                          Object value,
+void SmallOrderedNameDictionary::SetEntry(InternalIndex entry,
+                                          Tagged<Object> key,
+                                          Tagged<Object> value,
                                           PropertyDetails details) {
   int raw_entry = entry.as_int();
   DCHECK_IMPLIES(!IsName(key), IsTheHole(key));
@@ -867,22 +878,23 @@ bool SmallOrderedHashTable<Derived>::HasKey(Isolate* isolate,
 }
 
 template <class Derived>
-bool SmallOrderedHashTable<Derived>::Delete(Isolate* isolate, Derived table,
-                                            Object key) {
+bool SmallOrderedHashTable<Derived>::Delete(Isolate* isolate,
+                                            Tagged<Derived> table,
+                                            Tagged<Object> key) {
   DisallowGarbageCollection no_gc;
-  InternalIndex entry = table.FindEntry(isolate, key);
+  InternalIndex entry = table->FindEntry(isolate, key);
   if (entry.is_not_found()) return false;
 
-  int nof = table.NumberOfElements();
-  int nod = table.NumberOfDeletedElements();
+  int nof = table->NumberOfElements();
+  int nod = table->NumberOfDeletedElements();
 
-  Object hole = ReadOnlyRoots(isolate).the_hole_value();
+  Tagged<Object> hole = ReadOnlyRoots(isolate).the_hole_value();
   for (int j = 0; j < Derived::kEntrySize; j++) {
-    table.SetDataEntry(entry.as_int(), j, hole);
+    table->SetDataEntry(entry.as_int(), j, hole);
   }
 
-  table.SetNumberOfElements(nof - 1);
-  table.SetNumberOfDeletedElements(nod + 1);
+  table->SetNumberOfElements(nof - 1);
+  table->SetNumberOfDeletedElements(nod + 1);
 
   return true;
 }
@@ -893,7 +905,7 @@ Handle<SmallOrderedNameDictionary> SmallOrderedNameDictionary::DeleteEntry(
   DCHECK(entry.is_found());
   {
     DisallowGarbageCollection no_gc;
-    Object hole = ReadOnlyRoots(isolate).the_hole_value();
+    Tagged<Object> hole = ReadOnlyRoots(isolate).the_hole_value();
     PropertyDetails details = PropertyDetails::Empty();
     table->SetEntry(entry, hole, hole, details);
 
@@ -920,7 +932,7 @@ Handle<Derived> SmallOrderedHashTable<Derived>::Rehash(Isolate* isolate,
   {
     DisallowGarbageCollection no_gc;
     for (InternalIndex old_entry : table->IterateEntries()) {
-      Object key = table->KeyAt(old_entry);
+      Tagged<Object> key = table->KeyAt(old_entry);
       if (IsTheHole(key, isolate)) continue;
 
       int hash = Smi::ToInt(Object::GetHash(key));
@@ -931,7 +943,7 @@ Handle<Derived> SmallOrderedHashTable<Derived>::Rehash(Isolate* isolate,
       new_table->SetNextEntry(new_entry, chain);
 
       for (int i = 0; i < Derived::kEntrySize; ++i) {
-        Object value = table->GetDataEntry(old_entry.as_int(), i);
+        Tagged<Object> value = table->GetDataEntry(old_entry.as_int(), i);
         new_table->SetDataEntry(new_entry, i, value);
       }
 
@@ -1003,9 +1015,9 @@ MaybeHandle<Derived> SmallOrderedHashTable<Derived>::Grow(
 
 template <class Derived>
 InternalIndex SmallOrderedHashTable<Derived>::FindEntry(Isolate* isolate,
-                                                        Object key) {
+                                                        Tagged<Object> key) {
   DisallowGarbageCollection no_gc;
-  Object hash = Object::GetHash(key);
+  Tagged<Object> hash = Object::GetHash(key);
 
   if (IsUndefined(hash, isolate)) return InternalIndex::NotFound();
   int raw_entry = HashToFirstEntry(Smi::ToInt(hash));
@@ -1013,7 +1025,7 @@ InternalIndex SmallOrderedHashTable<Derived>::FindEntry(Isolate* isolate,
   // Walk the chain in the bucket to find the key.
   while (raw_entry != kNotFound) {
     InternalIndex entry(raw_entry);
-    Object candidate_key = KeyAt(entry);
+    Tagged<Object> candidate_key = KeyAt(entry);
     if (Object::SameValueZero(candidate_key, key)) return entry;
     raw_entry = GetNextEntry(raw_entry);
   }
@@ -1036,9 +1048,8 @@ template V8_EXPORT_PRIVATE void
 SmallOrderedHashTable<SmallOrderedHashSet>::Initialize(Isolate* isolate,
                                                        int capacity);
 template V8_EXPORT_PRIVATE bool
-SmallOrderedHashTable<SmallOrderedHashSet>::Delete(Isolate* isolate,
-                                                   SmallOrderedHashSet table,
-                                                   Object key);
+SmallOrderedHashTable<SmallOrderedHashSet>::Delete(
+    Isolate* isolate, Tagged<SmallOrderedHashSet> table, Tagged<Object> key);
 
 template EXPORT_TEMPLATE_DEFINE(V8_EXPORT_PRIVATE) bool SmallOrderedHashTable<
     SmallOrderedHashMap>::HasKey(Isolate* isolate, Handle<Object> key);
@@ -1056,9 +1067,8 @@ SmallOrderedHashTable<SmallOrderedHashMap>::Initialize(Isolate* isolate,
                                                        int capacity);
 
 template V8_EXPORT_PRIVATE bool
-SmallOrderedHashTable<SmallOrderedHashMap>::Delete(Isolate* isolate,
-                                                   SmallOrderedHashMap table,
-                                                   Object key);
+SmallOrderedHashTable<SmallOrderedHashMap>::Delete(
+    Isolate* isolate, Tagged<SmallOrderedHashMap> table, Tagged<Object> key);
 
 template V8_EXPORT_PRIVATE void
 SmallOrderedHashTable<SmallOrderedNameDictionary>::Initialize(Isolate* isolate,
@@ -1284,9 +1294,10 @@ MaybeHandle<HeapObject> OrderedNameDictionaryHandler::Add(
       isolate, Handle<OrderedNameDictionary>::cast(table), key, value, details);
 }
 
-void OrderedNameDictionaryHandler::SetEntry(HeapObject table,
-                                            InternalIndex entry, Object key,
-                                            Object value,
+void OrderedNameDictionaryHandler::SetEntry(Tagged<HeapObject> table,
+                                            InternalIndex entry,
+                                            Tagged<Object> key,
+                                            Tagged<Object> value,
                                             PropertyDetails details) {
   DisallowGarbageCollection no_gc;
   if (IsSmallOrderedNameDictionary(table)) {
@@ -1300,8 +1311,8 @@ void OrderedNameDictionaryHandler::SetEntry(HeapObject table,
 }
 
 InternalIndex OrderedNameDictionaryHandler::FindEntry(Isolate* isolate,
-                                                      HeapObject table,
-                                                      Name key) {
+                                                      Tagged<HeapObject> table,
+                                                      Tagged<Name> key) {
   DisallowGarbageCollection no_gc;
   if (IsSmallOrderedNameDictionary(table)) {
     return SmallOrderedNameDictionary::cast(table)->FindEntry(isolate, key);
@@ -1311,8 +1322,8 @@ InternalIndex OrderedNameDictionaryHandler::FindEntry(Isolate* isolate,
   return OrderedNameDictionary::cast(table)->FindEntry(isolate, key);
 }
 
-Object OrderedNameDictionaryHandler::ValueAt(HeapObject table,
-                                             InternalIndex entry) {
+Tagged<Object> OrderedNameDictionaryHandler::ValueAt(Tagged<HeapObject> table,
+                                                     InternalIndex entry) {
   if (IsSmallOrderedNameDictionary(table)) {
     return SmallOrderedNameDictionary::cast(table)->ValueAt(entry);
   }
@@ -1321,9 +1332,9 @@ Object OrderedNameDictionaryHandler::ValueAt(HeapObject table,
   return OrderedNameDictionary::cast(table)->ValueAt(entry);
 }
 
-void OrderedNameDictionaryHandler::ValueAtPut(HeapObject table,
+void OrderedNameDictionaryHandler::ValueAtPut(Tagged<HeapObject> table,
                                               InternalIndex entry,
-                                              Object value) {
+                                              Tagged<Object> value) {
   if (IsSmallOrderedNameDictionary(table)) {
     return SmallOrderedNameDictionary::cast(table)->ValueAtPut(entry, value);
   }
@@ -1332,8 +1343,8 @@ void OrderedNameDictionaryHandler::ValueAtPut(HeapObject table,
   OrderedNameDictionary::cast(table)->ValueAtPut(entry, value);
 }
 
-PropertyDetails OrderedNameDictionaryHandler::DetailsAt(HeapObject table,
-                                                        InternalIndex entry) {
+PropertyDetails OrderedNameDictionaryHandler::DetailsAt(
+    Tagged<HeapObject> table, InternalIndex entry) {
   if (IsSmallOrderedNameDictionary(table)) {
     return SmallOrderedNameDictionary::cast(table)->DetailsAt(entry);
   }
@@ -1342,7 +1353,7 @@ PropertyDetails OrderedNameDictionaryHandler::DetailsAt(HeapObject table,
   return OrderedNameDictionary::cast(table)->DetailsAt(entry);
 }
 
-void OrderedNameDictionaryHandler::DetailsAtPut(HeapObject table,
+void OrderedNameDictionaryHandler::DetailsAtPut(Tagged<HeapObject> table,
                                                 InternalIndex entry,
                                                 PropertyDetails details) {
   if (IsSmallOrderedNameDictionary(table)) {
@@ -1354,7 +1365,7 @@ void OrderedNameDictionaryHandler::DetailsAtPut(HeapObject table,
   OrderedNameDictionary::cast(table)->DetailsAtPut(entry, details);
 }
 
-int OrderedNameDictionaryHandler::Hash(HeapObject table) {
+int OrderedNameDictionaryHandler::Hash(Tagged<HeapObject> table) {
   if (IsSmallOrderedNameDictionary(table)) {
     return SmallOrderedNameDictionary::cast(table)->Hash();
   }
@@ -1363,7 +1374,7 @@ int OrderedNameDictionaryHandler::Hash(HeapObject table) {
   return OrderedNameDictionary::cast(table)->Hash();
 }
 
-void OrderedNameDictionaryHandler::SetHash(HeapObject table, int hash) {
+void OrderedNameDictionaryHandler::SetHash(Tagged<HeapObject> table, int hash) {
   if (IsSmallOrderedNameDictionary(table)) {
     return SmallOrderedNameDictionary::cast(table)->SetHash(hash);
   }
@@ -1372,8 +1383,8 @@ void OrderedNameDictionaryHandler::SetHash(HeapObject table, int hash) {
   OrderedNameDictionary::cast(table)->SetHash(hash);
 }
 
-Name OrderedNameDictionaryHandler::KeyAt(HeapObject table,
-                                         InternalIndex entry) {
+Tagged<Name> OrderedNameDictionaryHandler::KeyAt(Tagged<HeapObject> table,
+                                                 InternalIndex entry) {
   if (IsSmallOrderedNameDictionary(table)) {
     return Name::cast(SmallOrderedNameDictionary::cast(table)->KeyAt(entry));
   }
@@ -1382,7 +1393,7 @@ Name OrderedNameDictionaryHandler::KeyAt(HeapObject table,
       OrderedNameDictionary::cast(table)->KeyAt(InternalIndex(entry)));
 }
 
-int OrderedNameDictionaryHandler::NumberOfElements(HeapObject table) {
+int OrderedNameDictionaryHandler::NumberOfElements(Tagged<HeapObject> table) {
   if (IsSmallOrderedNameDictionary(table)) {
     return SmallOrderedNameDictionary::cast(table)->NumberOfElements();
   }
@@ -1390,7 +1401,7 @@ int OrderedNameDictionaryHandler::NumberOfElements(HeapObject table) {
   return OrderedNameDictionary::cast(table)->NumberOfElements();
 }
 
-int OrderedNameDictionaryHandler::Capacity(HeapObject table) {
+int OrderedNameDictionaryHandler::Capacity(Tagged<HeapObject> table) {
   if (IsSmallOrderedNameDictionary(table)) {
     return SmallOrderedNameDictionary::cast(table)->Capacity();
   }
@@ -1429,23 +1440,23 @@ Handle<HeapObject> OrderedNameDictionaryHandler::DeleteEntry(
 template <class Derived, class TableType>
 void OrderedHashTableIterator<Derived, TableType>::Transition() {
   DisallowGarbageCollection no_gc;
-  TableType table = TableType::cast(this->table());
-  if (!table.IsObsolete()) return;
+  Tagged<TableType> table = TableType::cast(this->table());
+  if (!table->IsObsolete()) return;
 
   int index = Smi::ToInt(this->index());
   DCHECK_LE(0, index);
-  while (table.IsObsolete()) {
-    TableType next_table = table.NextTable();
+  while (table->IsObsolete()) {
+    Tagged<TableType> next_table = table->NextTable();
 
     if (index > 0) {
-      int nod = table.NumberOfDeletedElements();
+      int nod = table->NumberOfDeletedElements();
 
       if (nod == TableType::kClearedTableSentinel) {
         index = 0;
       } else {
         int old_index = index;
         for (int i = 0; i < nod; ++i) {
-          int removed_index = table.RemovedIndexAt(i);
+          int removed_index = table->RemovedIndexAt(i);
           if (removed_index >= old_index) break;
           --index;
         }
@@ -1468,10 +1479,10 @@ bool OrderedHashTableIterator<Derived, TableType>::HasMore() {
 
   TableType table = TableType::cast(this->table());
   int index = Smi::ToInt(this->index());
-  int used_capacity = table.UsedCapacity();
+  int used_capacity = table->UsedCapacity();
 
   while (index < used_capacity &&
-         IsTheHole(table.KeyAt(InternalIndex(index)), ro_roots)) {
+         IsTheHole(table->KeyAt(InternalIndex(index)), ro_roots)) {
     index++;
   }
 
@@ -1489,7 +1500,7 @@ OrderedHashTableIterator<JSSetIterator, OrderedHashSet>::HasMore();
 template void
 OrderedHashTableIterator<JSSetIterator, OrderedHashSet>::MoveNext();
 
-template Object
+template Tagged<Object>
 OrderedHashTableIterator<JSSetIterator, OrderedHashSet>::CurrentKey();
 
 template void
@@ -1501,7 +1512,7 @@ OrderedHashTableIterator<JSMapIterator, OrderedHashMap>::HasMore();
 template void
 OrderedHashTableIterator<JSMapIterator, OrderedHashMap>::MoveNext();
 
-template Object
+template Tagged<Object>
 OrderedHashTableIterator<JSMapIterator, OrderedHashMap>::CurrentKey();
 
 template void
diff --git a/src/objects/ordered-hash-table.h b/src/objects/ordered-hash-table.h
index d06fd113bed..d07c7d631b8 100644
--- a/src/objects/ordered-hash-table.h
+++ b/src/objects/ordered-hash-table.h
@@ -80,17 +80,19 @@ class OrderedHashTable : public FixedArray {
   static Handle<Derived> Clear(Isolate* isolate, Handle<Derived> table);
 
   // Returns true if the OrderedHashTable contains the key
-  static bool HasKey(Isolate* isolate, Derived table, Object key);
+  static bool HasKey(Isolate* isolate, Tagged<Derived> table,
+                     Tagged<Object> key);
 
   // Returns whether a potential key |k| returned by KeyAt is a real
   // key (meaning that it is not a hole).
-  static inline bool IsKey(ReadOnlyRoots roots, Object k);
+  static inline bool IsKey(ReadOnlyRoots roots, Tagged<Object> k);
 
   // Returns a true value if the OrderedHashTable contains the key and
   // the key has been deleted. This does not shrink the table.
-  static bool Delete(Isolate* isolate, Derived table, Object key);
+  static bool Delete(Isolate* isolate, Tagged<Derived> table,
+                     Tagged<Object> key);
 
-  InternalIndex FindEntry(Isolate* isolate, Object key);
+  InternalIndex FindEntry(Isolate* isolate, Tagged<Object> key);
 
   int NumberOfElements() const {
     return Smi::ToInt(get(NumberOfElementsIndex()));
@@ -117,19 +119,20 @@ class OrderedHashTable : public FixedArray {
   }
 
   // use IsKey to check if this is a deleted entry.
-  Object KeyAt(InternalIndex entry) {
+  Tagged<Object> KeyAt(InternalIndex entry) {
     DCHECK_LT(entry.as_int(), this->UsedCapacity());
     return get(EntryToIndex(entry));
   }
 
   // Similar to KeyAt, but indicates whether the given entry is valid
   // (not deleted one)
-  inline bool ToKey(ReadOnlyRoots roots, InternalIndex entry, Object* out_key);
+  inline bool ToKey(ReadOnlyRoots roots, InternalIndex entry,
+                    Tagged<Object>* out_key);
 
   bool IsObsolete() { return !IsSmi(get(NextTableIndex())); }
 
   // The next newer table. This is only valid if the table is obsolete.
-  Derived NextTable() { return Derived::cast(get(NextTableIndex())); }
+  Tagged<Derived> NextTable() { return Derived::cast(get(NextTableIndex())); }
 
   // When the table is obsolete we store the indexes of the removed holes.
   int RemovedIndexAt(int index) {
@@ -214,7 +217,7 @@ class OrderedHashTable : public FixedArray {
 
   int HashToEntryRaw(int hash) {
     int bucket = HashToBucket(hash);
-    Object entry = this->get(HashTableStartIndex() + bucket);
+    Tagged<Object> entry = this->get(HashTableStartIndex() + bucket);
     int entry_int = Smi::ToInt(entry);
     DCHECK(entry_int == kNotFound || entry_int >= 0);
     return entry_int;
@@ -222,7 +225,7 @@ class OrderedHashTable : public FixedArray {
 
   int NextChainEntryRaw(int entry) {
     DCHECK_LT(entry, this->UsedCapacity());
-    Object next_entry = get(EntryToIndexRaw(entry) + kChainOffset);
+    Tagged<Object> next_entry = get(EntryToIndexRaw(entry) + kChainOffset);
     int next_entry_int = Smi::ToInt(next_entry);
     DCHECK(next_entry_int == kNotFound || next_entry_int >= 0);
     return next_entry_int;
@@ -291,7 +294,7 @@ class V8_EXPORT_PRIVATE OrderedHashSet
   static MaybeHandle<OrderedHashSet> AllocateEmpty(
       Isolate* isolate, AllocationType allocation = AllocationType::kReadOnly);
 
-  static HeapObject GetEmpty(ReadOnlyRoots ro_roots);
+  static Tagged<HeapObject> GetEmpty(ReadOnlyRoots ro_roots);
   static inline Handle<Map> GetMap(ReadOnlyRoots roots);
   static inline bool Is(Handle<HeapObject> table);
   static const int kPrefixSize = 0;
@@ -328,15 +331,15 @@ class V8_EXPORT_PRIVATE OrderedHashMap
   static MaybeHandle<OrderedHashMap> Rehash(Isolate* isolate,
                                             Handle<OrderedHashMap> table);
 
-  void SetEntry(InternalIndex entry, Object key, Object value);
+  void SetEntry(InternalIndex entry, Tagged<Object> key, Tagged<Object> value);
 
-  Object ValueAt(InternalIndex entry);
+  Tagged<Object> ValueAt(InternalIndex entry);
 
   // This takes and returns raw Address values containing tagged Object
   // pointers because it is called via ExternalReference.
   static Address GetHash(Isolate* isolate, Address raw_key);
 
-  static HeapObject GetEmpty(ReadOnlyRoots ro_roots);
+  static Tagged<HeapObject> GetEmpty(ReadOnlyRoots ro_roots);
   static inline Handle<Map> GetMap(ReadOnlyRoots roots);
   static inline bool Is(Handle<HeapObject> table);
 
@@ -414,14 +417,15 @@ class SmallOrderedHashTable : public HeapObject {
 
   // Returns a true value if the table contains the key and
   // the key has been deleted. This does not shrink the table.
-  static bool Delete(Isolate* isolate, Derived table, Object key);
+  static bool Delete(Isolate* isolate, Tagged<Derived> table,
+                     Tagged<Object> key);
 
   // Returns an SmallOrderedHashTable (possibly |table|) with enough
   // space to add at least one new element. Returns empty handle if
   // we've already reached MaxCapacity.
   static MaybeHandle<Derived> Grow(Isolate* isolate, Handle<Derived> table);
 
-  InternalIndex FindEntry(Isolate* isolate, Object key);
+  InternalIndex FindEntry(Isolate* isolate, Tagged<Object> key);
   static Handle<Derived> Shrink(Isolate* isolate, Handle<Derived> table);
 
   // Iterates only fields in the DataTable.
@@ -468,7 +472,7 @@ class SmallOrderedHashTable : public HeapObject {
 
   int NumberOfBuckets() const { return getByte(NumberOfBucketsOffset(), 0); }
 
-  V8_INLINE Object KeyAt(InternalIndex entry) const;
+  V8_INLINE Tagged<Object> KeyAt(InternalIndex entry) const;
 
   InternalIndex::Range IterateEntries() {
     return InternalIndex::Range(UsedCapacity());
@@ -504,7 +508,7 @@ class SmallOrderedHashTable : public HeapObject {
   static Handle<Derived> Rehash(Isolate* isolate, Handle<Derived> table,
                                 int new_capacity);
 
-  void SetDataEntry(int entry, int relative_index, Object value);
+  void SetDataEntry(int entry, int relative_index, Tagged<Object> value);
 
   // TODO(gsathya): Calculate all the various possible values for this
   // at compile time since capacity can only be 4 different values.
@@ -552,7 +556,7 @@ class SmallOrderedHashTable : public HeapObject {
     return getByte(GetChainTableOffset(), entry);
   }
 
-  V8_INLINE Object GetDataEntry(int entry, int relative_index);
+  V8_INLINE Tagged<Object> GetDataEntry(int entry, int relative_index);
 
   int HashToBucket(int hash) const { return hash & (NumberOfBuckets() - 1); }
 
@@ -659,7 +663,8 @@ class SmallOrderedHashSet : public SmallOrderedHashTable<SmallOrderedHashSet> {
   V8_EXPORT_PRIVATE static MaybeHandle<SmallOrderedHashSet> Add(
       Isolate* isolate, Handle<SmallOrderedHashSet> table, Handle<Object> key);
   V8_EXPORT_PRIVATE static bool Delete(Isolate* isolate,
-                                       SmallOrderedHashSet table, Object key);
+                                       Tagged<SmallOrderedHashSet> table,
+                                       Tagged<Object> key);
   V8_EXPORT_PRIVATE bool HasKey(Isolate* isolate, Handle<Object> key);
 
   static inline bool Is(Handle<HeapObject> table);
@@ -693,7 +698,8 @@ class SmallOrderedHashMap : public SmallOrderedHashTable<SmallOrderedHashMap> {
       Isolate* isolate, Handle<SmallOrderedHashMap> table, Handle<Object> key,
       Handle<Object> value);
   V8_EXPORT_PRIVATE static bool Delete(Isolate* isolate,
-                                       SmallOrderedHashMap table, Object key);
+                                       Tagged<SmallOrderedHashMap> table,
+                                       Tagged<Object> key);
   V8_EXPORT_PRIVATE bool HasKey(Isolate* isolate, Handle<Object> key);
   static inline bool Is(Handle<HeapObject> table);
   static inline Handle<Map> GetMap(ReadOnlyRoots roots);
@@ -765,11 +771,11 @@ class V8_EXPORT_PRIVATE OrderedNameDictionary
       Isolate* isolate, Handle<OrderedNameDictionary> table, Handle<Name> key,
       Handle<Object> value, PropertyDetails details);
 
-  void SetEntry(InternalIndex entry, Object key, Object value,
+  void SetEntry(InternalIndex entry, Tagged<Object> key, Tagged<Object> value,
                 PropertyDetails details);
 
   template <typename IsolateT>
-  InternalIndex FindEntry(IsolateT* isolate, Object key);
+  InternalIndex FindEntry(IsolateT* isolate, Tagged<Object> key);
 
   // This is to make the interfaces of NameDictionary::FindEntry and
   // OrderedNameDictionary::FindEntry compatible.
@@ -796,13 +802,13 @@ class V8_EXPORT_PRIVATE OrderedNameDictionary
       Isolate* isolate, Handle<OrderedNameDictionary> table, int new_capacity);
 
   // Returns the value for entry.
-  inline Object ValueAt(InternalIndex entry);
+  inline Tagged<Object> ValueAt(InternalIndex entry);
 
   // Like KeyAt, but casts to Name
-  inline Name NameAt(InternalIndex entry);
+  inline Tagged<Name> NameAt(InternalIndex entry);
 
   // Set the value for entry.
-  inline void ValueAtPut(InternalIndex entry, Object value);
+  inline void ValueAtPut(InternalIndex entry, Tagged<Object> value);
 
   // Returns the property details for the property at entry.
   inline PropertyDetails DetailsAt(InternalIndex entry);
@@ -813,7 +819,7 @@ class V8_EXPORT_PRIVATE OrderedNameDictionary
   inline void SetHash(int hash);
   inline int Hash();
 
-  static HeapObject GetEmpty(ReadOnlyRoots ro_roots);
+  static Tagged<HeapObject> GetEmpty(ReadOnlyRoots ro_roots);
   static inline Handle<Map> GetMap(ReadOnlyRoots roots);
   static inline bool Is(Handle<HeapObject> table);
 
@@ -844,30 +850,34 @@ class V8_EXPORT_PRIVATE OrderedNameDictionaryHandler
   static Handle<HeapObject> DeleteEntry(Isolate* isolate,
                                         Handle<HeapObject> table,
                                         InternalIndex entry);
-  static InternalIndex FindEntry(Isolate* isolate, HeapObject table, Name key);
-  static void SetEntry(HeapObject table, InternalIndex entry, Object key,
-                       Object value, PropertyDetails details);
+  static InternalIndex FindEntry(Isolate* isolate, Tagged<HeapObject> table,
+                                 Tagged<Name> key);
+  static void SetEntry(Tagged<HeapObject> table, InternalIndex entry,
+                       Tagged<Object> key, Tagged<Object> value,
+                       PropertyDetails details);
 
   // Returns the value for entry.
-  static Object ValueAt(HeapObject table, InternalIndex entry);
+  static Tagged<Object> ValueAt(Tagged<HeapObject> table, InternalIndex entry);
 
   // Set the value for entry.
-  static void ValueAtPut(HeapObject table, InternalIndex entry, Object value);
+  static void ValueAtPut(Tagged<HeapObject> table, InternalIndex entry,
+                         Tagged<Object> value);
 
   // Returns the property details for the property at entry.
-  static PropertyDetails DetailsAt(HeapObject table, InternalIndex entry);
+  static PropertyDetails DetailsAt(Tagged<HeapObject> table,
+                                   InternalIndex entry);
 
   // Set the details for entry.
-  static void DetailsAtPut(HeapObject table, InternalIndex entry,
+  static void DetailsAtPut(Tagged<HeapObject> table, InternalIndex entry,
                            PropertyDetails value);
 
-  static Name KeyAt(HeapObject table, InternalIndex entry);
+  static Tagged<Name> KeyAt(Tagged<HeapObject> table, InternalIndex entry);
 
-  static void SetHash(HeapObject table, int hash);
-  static int Hash(HeapObject table);
+  static void SetHash(Tagged<HeapObject> table, int hash);
+  static int Hash(Tagged<HeapObject> table);
 
-  static int NumberOfElements(HeapObject table);
-  static int Capacity(HeapObject table);
+  static int NumberOfElements(Tagged<HeapObject> table);
+  static int Capacity(Tagged<HeapObject> table);
 
  protected:
   static MaybeHandle<OrderedNameDictionary> AdjustRepresentation(
@@ -883,7 +893,7 @@ class SmallOrderedNameDictionary
   DECL_VERIFIER(SmallOrderedNameDictionary)
 
   // Returns the value for entry.
-  inline Object ValueAt(InternalIndex entry);
+  inline Tagged<Object> ValueAt(InternalIndex entry);
 
   static Handle<SmallOrderedNameDictionary> Rehash(
       Isolate* isolate, Handle<SmallOrderedNameDictionary> table,
@@ -894,7 +904,7 @@ class SmallOrderedNameDictionary
       InternalIndex entry);
 
   // Set the value for entry.
-  inline void ValueAtPut(InternalIndex entry, Object value);
+  inline void ValueAtPut(InternalIndex entry, Tagged<Object> value);
 
   // Returns the property details for the property at entry.
   inline PropertyDetails DetailsAt(InternalIndex entry);
@@ -918,7 +928,8 @@ class SmallOrderedNameDictionary
       Isolate* isolate, Handle<SmallOrderedNameDictionary> table,
       Handle<Name> key, Handle<Object> value, PropertyDetails details);
 
-  V8_EXPORT_PRIVATE void SetEntry(InternalIndex entry, Object key, Object value,
+  V8_EXPORT_PRIVATE void SetEntry(InternalIndex entry, Tagged<Object> key,
+                                  Tagged<Object> value,
                                   PropertyDetails details);
 
   static inline Handle<Map> GetMap(ReadOnlyRoots roots);
diff --git a/src/objects/property-array-inl.h b/src/objects/property-array-inl.h
index cddf4bfc366..439a2d38fa1 100644
--- a/src/objects/property-array-inl.h
+++ b/src/objects/property-array-inl.h
@@ -26,32 +26,32 @@ SMI_ACCESSORS(PropertyArray, length_and_hash, kLengthAndHashOffset)
 RELEASE_ACQUIRE_SMI_ACCESSORS(PropertyArray, length_and_hash,
                               kLengthAndHashOffset)
 
-Object PropertyArray::get(int index) const {
+Tagged<Object> PropertyArray::get(int index) const {
   PtrComprCageBase cage_base = GetPtrComprCageBase(*this);
   return get(cage_base, index);
 }
 
-Object PropertyArray::get(PtrComprCageBase cage_base, int index) const {
+Tagged<Object> PropertyArray::get(PtrComprCageBase cage_base, int index) const {
   DCHECK_LT(static_cast<unsigned>(index),
             static_cast<unsigned>(this->length(kAcquireLoad)));
   return TaggedField<Object>::Relaxed_Load(cage_base, *this,
                                            OffsetOfElementAt(index));
 }
 
-Object PropertyArray::get(int index, SeqCstAccessTag tag) const {
+Tagged<Object> PropertyArray::get(int index, SeqCstAccessTag tag) const {
   PtrComprCageBase cage_base = GetPtrComprCageBase(*this);
   return get(cage_base, index, tag);
 }
 
-Object PropertyArray::get(PtrComprCageBase cage_base, int index,
-                          SeqCstAccessTag tag) const {
+Tagged<Object> PropertyArray::get(PtrComprCageBase cage_base, int index,
+                                  SeqCstAccessTag tag) const {
   DCHECK_LT(static_cast<unsigned>(index),
             static_cast<unsigned>(this->length(kAcquireLoad)));
   return TaggedField<Object>::SeqCst_Load(cage_base, *this,
                                           OffsetOfElementAt(index));
 }
 
-void PropertyArray::set(int index, Object value) {
+void PropertyArray::set(int index, Tagged<Object> value) {
   DCHECK(IsPropertyArray(*this));
   DCHECK_LT(static_cast<unsigned>(index),
             static_cast<unsigned>(this->length(kAcquireLoad)));
@@ -60,7 +60,8 @@ void PropertyArray::set(int index, Object value) {
   WRITE_BARRIER(*this, offset, value);
 }
 
-void PropertyArray::set(int index, Object value, WriteBarrierMode mode) {
+void PropertyArray::set(int index, Tagged<Object> value,
+                        WriteBarrierMode mode) {
   DCHECK_LT(static_cast<unsigned>(index),
             static_cast<unsigned>(this->length(kAcquireLoad)));
   int offset = OffsetOfElementAt(index);
@@ -68,7 +69,7 @@ void PropertyArray::set(int index, Object value, WriteBarrierMode mode) {
   CONDITIONAL_WRITE_BARRIER(*this, offset, value, mode);
 }
 
-void PropertyArray::set(int index, Object value, SeqCstAccessTag tag) {
+void PropertyArray::set(int index, Tagged<Object> value, SeqCstAccessTag tag) {
   DCHECK(IsPropertyArray(*this));
   DCHECK_LT(static_cast<unsigned>(index),
             static_cast<unsigned>(this->length(kAcquireLoad)));
@@ -78,31 +79,33 @@ void PropertyArray::set(int index, Object value, SeqCstAccessTag tag) {
   CONDITIONAL_WRITE_BARRIER(*this, offset, value, UPDATE_WRITE_BARRIER);
 }
 
-Object PropertyArray::Swap(int index, Object value, SeqCstAccessTag tag) {
+Tagged<Object> PropertyArray::Swap(int index, Tagged<Object> value,
+                                   SeqCstAccessTag tag) {
   PtrComprCageBase cage_base = GetPtrComprCageBase(*this);
   return Swap(cage_base, index, value, tag);
 }
 
-Object PropertyArray::Swap(PtrComprCageBase cage_base, int index, Object value,
-                           SeqCstAccessTag tag) {
+Tagged<Object> PropertyArray::Swap(PtrComprCageBase cage_base, int index,
+                                   Tagged<Object> value, SeqCstAccessTag tag) {
   DCHECK(IsPropertyArray(*this));
   DCHECK_LT(static_cast<unsigned>(index),
             static_cast<unsigned>(this->length(kAcquireLoad)));
   DCHECK(IsShared(value));
-  Object result = TaggedField<Object>::SeqCst_Swap(
+  Tagged<Object> result = TaggedField<Object>::SeqCst_Swap(
       cage_base, *this, OffsetOfElementAt(index), value);
   CONDITIONAL_WRITE_BARRIER(*this, OffsetOfElementAt(index), value,
                             UPDATE_WRITE_BARRIER);
   return result;
 }
 
-Object PropertyArray::CompareAndSwap(int index, Object expected, Object value,
-                                     SeqCstAccessTag tag) {
+Tagged<Object> PropertyArray::CompareAndSwap(int index, Tagged<Object> expected,
+                                             Tagged<Object> value,
+                                             SeqCstAccessTag tag) {
   DCHECK(IsPropertyArray(*this));
   DCHECK_LT(static_cast<unsigned>(index),
             static_cast<unsigned>(this->length(kAcquireLoad)));
   DCHECK(IsShared(value));
-  Object result = TaggedField<Object>::SeqCst_CompareAndSwap(
+  Tagged<Object> result = TaggedField<Object>::SeqCst_CompareAndSwap(
       *this, OffsetOfElementAt(index), expected, value);
   if (result == expected) {
     CONDITIONAL_WRITE_BARRIER(*this, OffsetOfElementAt(index), value,
@@ -135,8 +138,8 @@ void PropertyArray::SetHash(int hash) {
 }
 
 void PropertyArray::CopyElements(Isolate* isolate, int dst_index,
-                                 PropertyArray src, int src_index, int len,
-                                 WriteBarrierMode mode) {
+                                 Tagged<PropertyArray> src, int src_index,
+                                 int len, WriteBarrierMode mode) {
   if (len == 0) return;
   DisallowGarbageCollection no_gc;
 
diff --git a/src/objects/property-array.h b/src/objects/property-array.h
index 62ce49cb68a..3791c4853ac 100644
--- a/src/objects/property-array.h
+++ b/src/objects/property-array.h
@@ -29,27 +29,30 @@ class PropertyArray
   inline void SetHash(int hash);
   inline int Hash() const;
 
-  inline Object get(int index) const;
-  inline Object get(PtrComprCageBase cage_base, int index) const;
-  inline Object get(int index, SeqCstAccessTag tag) const;
-  inline Object get(PtrComprCageBase cage_base, int index,
-                    SeqCstAccessTag tag) const;
-
-  inline void set(int index, Object value);
-  inline void set(int index, Object value, SeqCstAccessTag tag);
+  inline Tagged<Object> get(int index) const;
+  inline Tagged<Object> get(PtrComprCageBase cage_base, int index) const;
+  inline Tagged<Object> get(int index, SeqCstAccessTag tag) const;
+  inline Tagged<Object> get(PtrComprCageBase cage_base, int index,
+                            SeqCstAccessTag tag) const;
+
+  inline void set(int index, Tagged<Object> value);
+  inline void set(int index, Tagged<Object> value, SeqCstAccessTag tag);
   // Setter with explicit barrier mode.
-  inline void set(int index, Object value, WriteBarrierMode mode);
+  inline void set(int index, Tagged<Object> value, WriteBarrierMode mode);
 
-  inline Object Swap(int index, Object value, SeqCstAccessTag tag);
-  inline Object Swap(PtrComprCageBase cage_base, int index, Object value,
-                     SeqCstAccessTag tag);
+  inline Tagged<Object> Swap(int index, Tagged<Object> value,
+                             SeqCstAccessTag tag);
+  inline Tagged<Object> Swap(PtrComprCageBase cage_base, int index,
+                             Tagged<Object> value, SeqCstAccessTag tag);
 
-  inline Object CompareAndSwap(int index, Object expected, Object value,
-                               SeqCstAccessTag tag);
+  inline Tagged<Object> CompareAndSwap(int index, Tagged<Object> expected,
+                                       Tagged<Object> value,
+                                       SeqCstAccessTag tag);
 
   // Signature must be in sync with FixedArray::CopyElements().
-  inline void CopyElements(Isolate* isolate, int dst_index, PropertyArray src,
-                           int src_index, int len, WriteBarrierMode mode);
+  inline void CopyElements(Isolate* isolate, int dst_index,
+                           Tagged<PropertyArray> src, int src_index, int len,
+                           WriteBarrierMode mode);
 
   // Gives access to raw memory which stores the array's data.
   inline ObjectSlot data_start();
diff --git a/src/objects/property-cell.h b/src/objects/property-cell.h
index 54b5981fd4e..697a3403c8a 100644
--- a/src/objects/property-cell.h
+++ b/src/objects/property-cell.h
@@ -44,12 +44,14 @@ class PropertyCell
   // For protectors:
   void InvalidateProtector();
 
-  static PropertyCellType InitialType(Isolate* isolate, Object value);
+  static PropertyCellType InitialType(Isolate* isolate, Tagged<Object> value);
 
   // Computes the new type of the cell's contents for the given value, but
   // without actually modifying the details.
-  static PropertyCellType UpdatedType(Isolate* isolate, PropertyCell cell,
-                                      Object value, PropertyDetails details);
+  static PropertyCellType UpdatedType(Isolate* isolate,
+                                      Tagged<PropertyCell> cell,
+                                      Tagged<Object> value,
+                                      PropertyDetails details);
 
   // Prepares property cell at given entry for receiving given value and sets
   // that value.  As a result the old cell could be invalidated and/or dependent
@@ -66,7 +68,8 @@ class PropertyCell
 
   // Whether or not the {details} and {value} fit together. This is an
   // approximation with false positives.
-  static bool CheckDataIsCompatible(PropertyDetails details, Object value);
+  static bool CheckDataIsCompatible(PropertyDetails details,
+                                    Tagged<Object> value);
 
   DECL_PRINTER(PropertyCell)
   DECL_VERIFIER(PropertyCell)
@@ -87,7 +90,8 @@ class PropertyCell
 #ifdef DEBUG
   // Whether the property cell can transition to the given state. This is an
   // approximation with false positives.
-  bool CanTransitionTo(PropertyDetails new_details, Object new_value) const;
+  bool CanTransitionTo(PropertyDetails new_details,
+                       Tagged<Object> new_value) const;
 #endif  // DEBUG
 };
 
diff --git a/src/objects/property-descriptor.cc b/src/objects/property-descriptor.cc
index 0cc448ade68..1a5d4e9657e 100644
--- a/src/objects/property-descriptor.cc
+++ b/src/objects/property-descriptor.cc
@@ -47,7 +47,7 @@ bool ToPropertyDescriptorFastPath(Isolate* isolate, Handle<JSReceiver> obj,
     DisallowGarbageCollection no_gc;
     Tagged<JSReceiver> raw_obj = *obj;
     if (!IsJSObject(*raw_obj)) return false;
-    Map raw_map = raw_obj->map(isolate);
+    Tagged<Map> raw_map = raw_obj->map(isolate);
     if (raw_map->instance_type() != JS_OBJECT_TYPE) return false;
     if (raw_map->is_access_check_needed()) return false;
     if (raw_map->prototype() != *isolate->initial_object_prototype())
@@ -92,7 +92,7 @@ bool ToPropertyDescriptorFastPath(Isolate* isolate, Handle<JSReceiver> obj,
         return false;
       }
     }
-    Name key = descs->GetKey(i);
+    Tagged<Name> key = descs->GetKey(i);
     if (key == roots.enumerable_string()) {
       desc->set_enumerable(Object::BooleanValue(*value, isolate));
     } else if (key == roots.configurable_string()) {
diff --git a/src/objects/property-details.h b/src/objects/property-details.h
index 0dacd177e17..4cf715ac50c 100644
--- a/src/objects/property-details.h
+++ b/src/objects/property-details.h
@@ -345,7 +345,7 @@ class PropertyDetails {
   }
 
   // Conversion for storing details as Object.
-  explicit inline PropertyDetails(Smi smi);
+  explicit inline PropertyDetails(Tagged<Smi> smi);
   inline Tagged<Smi> AsSmi() const;
 
   static constexpr uint8_t EncodeRepresentation(Representation representation) {
diff --git a/src/objects/prototype-info-inl.h b/src/objects/prototype-info-inl.h
index 50aac376b03..50e7be7356a 100644
--- a/src/objects/prototype-info-inl.h
+++ b/src/objects/prototype-info-inl.h
@@ -31,7 +31,7 @@ DEF_GETTER(PrototypeInfo, object_create_map, MaybeObject) {
 RELEASE_ACQUIRE_WEAK_ACCESSORS(PrototypeInfo, object_create_map,
                                kObjectCreateMapOffset)
 
-Map PrototypeInfo::ObjectCreateMap() {
+Tagged<Map> PrototypeInfo::ObjectCreateMap() {
   return Map::cast(object_create_map()->GetHeapObjectAssumeWeak());
 }
 
@@ -46,7 +46,7 @@ bool PrototypeInfo::HasObjectCreateMap() {
   return cache->IsWeak();
 }
 
-bool PrototypeInfo::IsPrototypeInfoFast(Object object) {
+bool PrototypeInfo::IsPrototypeInfoFast(Tagged<Object> object) {
   bool is_proto_info = object != Smi::zero();
   DCHECK_EQ(is_proto_info, IsPrototypeInfo(object));
   return is_proto_info;
@@ -55,7 +55,7 @@ bool PrototypeInfo::IsPrototypeInfoFast(Object object) {
 BOOL_ACCESSORS(PrototypeInfo, bit_field, should_be_fast_map,
                ShouldBeFastBit::kShift)
 
-void PrototypeUsers::MarkSlotEmpty(WeakArrayList array, int index) {
+void PrototypeUsers::MarkSlotEmpty(Tagged<WeakArrayList> array, int index) {
   DCHECK_GT(index, 0);
   DCHECK_LT(index, array->length());
   // Chain the empty slots into a linked list (each empty slot contains the
@@ -64,11 +64,12 @@ void PrototypeUsers::MarkSlotEmpty(WeakArrayList array, int index) {
   set_empty_slot_index(array, index);
 }
 
-Smi PrototypeUsers::empty_slot_index(WeakArrayList array) {
+Tagged<Smi> PrototypeUsers::empty_slot_index(Tagged<WeakArrayList> array) {
   return array->Get(kEmptySlotIndex).ToSmi();
 }
 
-void PrototypeUsers::set_empty_slot_index(WeakArrayList array, int index) {
+void PrototypeUsers::set_empty_slot_index(Tagged<WeakArrayList> array,
+                                          int index) {
   array->Set(kEmptySlotIndex, MaybeObject::FromObject(Smi::FromInt(index)));
 }
 
diff --git a/src/objects/prototype-info.h b/src/objects/prototype-info.h
index 5284584a8ab..5a4440dd60a 100644
--- a/src/objects/prototype-info.h
+++ b/src/objects/prototype-info.h
@@ -30,10 +30,10 @@ class PrototypeInfo
 
   static inline void SetObjectCreateMap(Handle<PrototypeInfo> info,
                                         Handle<Map> map);
-  inline Map ObjectCreateMap();
+  inline Tagged<Map> ObjectCreateMap();
   inline bool HasObjectCreateMap();
 
-  static inline bool IsPrototypeInfoFast(Object object);
+  static inline bool IsPrototypeInfoFast(Tagged<Object> object);
 
   DECL_BOOLEAN_ACCESSORS(should_be_fast_map)
 
@@ -57,19 +57,19 @@ class V8_EXPORT_PRIVATE PrototypeUsers : public WeakArrayList {
                                    Handle<WeakArrayList> array,
                                    Handle<Map> value, int* assigned_index);
 
-  static inline void MarkSlotEmpty(WeakArrayList array, int index);
+  static inline void MarkSlotEmpty(Tagged<WeakArrayList> array, int index);
 
   // The callback is called when a weak pointer to HeapObject "object" is moved
   // from index "from_index" to index "to_index" during compaction. The callback
   // must not cause GC.
-  using CompactionCallback = void (*)(HeapObject object, int from_index,
+  using CompactionCallback = void (*)(Tagged<HeapObject> object, int from_index,
                                       int to_index);
-  static WeakArrayList Compact(
+  static Tagged<WeakArrayList> Compact(
       Handle<WeakArrayList> array, Heap* heap, CompactionCallback callback,
       AllocationType allocation = AllocationType::kYoung);
 
 #ifdef VERIFY_HEAP
-  static void Verify(WeakArrayList array);
+  static void Verify(Tagged<WeakArrayList> array);
 #endif  // VERIFY_HEAP
 
   static const int kEmptySlotIndex = 0;
@@ -78,10 +78,11 @@ class V8_EXPORT_PRIVATE PrototypeUsers : public WeakArrayList {
   static const int kNoEmptySlotsMarker = 0;
 
  private:
-  static inline Smi empty_slot_index(WeakArrayList array);
-  static inline void set_empty_slot_index(WeakArrayList array, int index);
+  static inline Tagged<Smi> empty_slot_index(Tagged<WeakArrayList> array);
+  static inline void set_empty_slot_index(Tagged<WeakArrayList> array,
+                                          int index);
 
-  static void ScanForEmptySlots(WeakArrayList array);
+  static void ScanForEmptySlots(Tagged<WeakArrayList> array);
 
   DISALLOW_IMPLICIT_CONSTRUCTORS(PrototypeUsers);
 };
diff --git a/src/objects/prototype-inl.h b/src/objects/prototype-inl.h
index d2eea200466..acfe1a21db4 100644
--- a/src/objects/prototype-inl.h
+++ b/src/objects/prototype-inl.h
@@ -27,7 +27,8 @@ PrototypeIterator::PrototypeIterator(Isolate* isolate,
   if (where_to_start == kStartAtPrototype) Advance();
 }
 
-PrototypeIterator::PrototypeIterator(Isolate* isolate, JSReceiver receiver,
+PrototypeIterator::PrototypeIterator(Isolate* isolate,
+                                     Tagged<JSReceiver> receiver,
                                      WhereToStart where_to_start,
                                      WhereToEnd where_to_end)
     : isolate_(isolate),
@@ -38,7 +39,7 @@ PrototypeIterator::PrototypeIterator(Isolate* isolate, JSReceiver receiver,
   if (where_to_start == kStartAtPrototype) Advance();
 }
 
-PrototypeIterator::PrototypeIterator(Isolate* isolate, Map receiver_map,
+PrototypeIterator::PrototypeIterator(Isolate* isolate, Tagged<Map> receiver_map,
                                      WhereToEnd where_to_end)
     : isolate_(isolate),
       object_(receiver_map->GetPrototypeChainRootMap(isolate_)->prototype()),
@@ -47,7 +48,7 @@ PrototypeIterator::PrototypeIterator(Isolate* isolate, Map receiver_map,
       seen_proxies_(0) {
   if (!is_at_end_ && where_to_end_ == END_AT_NON_HIDDEN) {
     DCHECK(IsJSReceiver(object_));
-    Map map = JSReceiver::cast(object_)->map();
+    Tagged<Map> map = JSReceiver::cast(object_)->map();
     is_at_end_ = !IsJSGlobalProxyMap(map);
   }
 }
@@ -62,7 +63,7 @@ PrototypeIterator::PrototypeIterator(Isolate* isolate, Handle<Map> receiver_map,
       seen_proxies_(0) {
   if (!is_at_end_ && where_to_end_ == END_AT_NON_HIDDEN) {
     DCHECK(IsJSReceiver(*handle_));
-    Map map = JSReceiver::cast(*handle_)->map();
+    Tagged<Map> map = JSReceiver::cast(*handle_)->map();
     is_at_end_ = !IsJSGlobalProxyMap(map);
   }
 }
@@ -92,10 +93,10 @@ void PrototypeIterator::Advance() {
 }
 
 void PrototypeIterator::AdvanceIgnoringProxies() {
-  Object object = handle_.is_null() ? object_ : *handle_;
-  Map map = HeapObject::cast(object)->map();
+  Tagged<Object> object = handle_.is_null() ? object_ : *handle_;
+  Tagged<Map> map = HeapObject::cast(object)->map();
 
-  HeapObject prototype = map->prototype();
+  Tagged<HeapObject> prototype = map->prototype();
   is_at_end_ = IsNull(prototype, isolate_) ||
                (where_to_end_ == END_AT_NON_HIDDEN && !IsJSGlobalProxyMap(map));
 
diff --git a/src/objects/prototype.h b/src/objects/prototype.h
index 0a8f21819ac..d526c33e9ca 100644
--- a/src/objects/prototype.h
+++ b/src/objects/prototype.h
@@ -31,11 +31,11 @@ class PrototypeIterator {
                            WhereToStart where_to_start = kStartAtPrototype,
                            WhereToEnd where_to_end = END_AT_NULL);
 
-  inline PrototypeIterator(Isolate* isolate, JSReceiver receiver,
+  inline PrototypeIterator(Isolate* isolate, Tagged<JSReceiver> receiver,
                            WhereToStart where_to_start = kStartAtPrototype,
                            WhereToEnd where_to_end = END_AT_NULL);
 
-  inline explicit PrototypeIterator(Isolate* isolate, Map receiver_map,
+  inline explicit PrototypeIterator(Isolate* isolate, Tagged<Map> receiver_map,
                                     WhereToEnd where_to_end = END_AT_NULL);
 
   inline explicit PrototypeIterator(Isolate* isolate, Handle<Map> receiver_map,
diff --git a/src/objects/regexp-match-info-inl.h b/src/objects/regexp-match-info-inl.h
index 463bcf9326e..9bca9e7f284 100644
--- a/src/objects/regexp-match-info-inl.h
+++ b/src/objects/regexp-match-info-inl.h
@@ -20,7 +20,7 @@ TQ_OBJECT_CONSTRUCTORS_IMPL(RegExpMatchInfo)
 
 int RegExpMatchInfo::NumberOfCaptureRegisters() {
   DCHECK_GE(length(), kLastMatchOverhead);
-  Object obj = get(kNumberOfCapturesIndex);
+  Tagged<Object> obj = get(kNumberOfCapturesIndex);
   return Smi::ToInt(obj);
 }
 
@@ -29,29 +29,31 @@ void RegExpMatchInfo::SetNumberOfCaptureRegisters(int value) {
   set(kNumberOfCapturesIndex, Smi::FromInt(value));
 }
 
-String RegExpMatchInfo::LastSubject() {
+Tagged<String> RegExpMatchInfo::LastSubject() {
   DCHECK_GE(length(), kLastMatchOverhead);
   return String::cast(get(kLastSubjectIndex));
 }
 
-void RegExpMatchInfo::SetLastSubject(String value, WriteBarrierMode mode) {
+void RegExpMatchInfo::SetLastSubject(Tagged<String> value,
+                                     WriteBarrierMode mode) {
   DCHECK_GE(length(), kLastMatchOverhead);
   set(kLastSubjectIndex, value, mode);
 }
 
-Object RegExpMatchInfo::LastInput() {
+Tagged<Object> RegExpMatchInfo::LastInput() {
   DCHECK_GE(length(), kLastMatchOverhead);
   return get(kLastInputIndex);
 }
 
-void RegExpMatchInfo::SetLastInput(Object value, WriteBarrierMode mode) {
+void RegExpMatchInfo::SetLastInput(Tagged<Object> value,
+                                   WriteBarrierMode mode) {
   DCHECK_GE(length(), kLastMatchOverhead);
   set(kLastInputIndex, value, mode);
 }
 
 int RegExpMatchInfo::Capture(int i) {
   DCHECK_LT(i, NumberOfCaptureRegisters());
-  Object obj = get(kFirstCaptureIndex + i);
+  Tagged<Object> obj = get(kFirstCaptureIndex + i);
   return Smi::ToInt(obj);
 }
 
diff --git a/src/objects/regexp-match-info.h b/src/objects/regexp-match-info.h
index 3ce08262b1b..f1d9ef9fb5c 100644
--- a/src/objects/regexp-match-info.h
+++ b/src/objects/regexp-match-info.h
@@ -36,13 +36,13 @@ class RegExpMatchInfo
   inline void SetNumberOfCaptureRegisters(int value);
 
   // Returns the subject string of the last match.
-  inline String LastSubject();
-  inline void SetLastSubject(String value,
+  inline Tagged<String> LastSubject();
+  inline void SetLastSubject(Tagged<String> value,
                              WriteBarrierMode mode = UPDATE_WRITE_BARRIER);
 
   // Like LastSubject, but modifiable by the user.
-  inline Object LastInput();
-  inline void SetLastInput(Object value,
+  inline Tagged<Object> LastInput();
+  inline void SetLastInput(Tagged<Object> value,
                            WriteBarrierMode mode = UPDATE_WRITE_BARRIER);
 
   // Returns the i'th capture index, 0 <= i < NumberOfCaptures(). Capture(0) and
diff --git a/src/objects/scope-info-inl.h b/src/objects/scope-info-inl.h
index 3b38294d4c4..35dbe1f2c65 100644
--- a/src/objects/scope-info-inl.h
+++ b/src/objects/scope-info-inl.h
@@ -66,16 +66,16 @@ class ScopeInfo::LocalNamesRange {
       return !(a == b);
     }
 
-    String name(PtrComprCageBase cage_base) const {
+    Tagged<String> name(PtrComprCageBase cage_base) const {
       DCHECK_LT(index_, range_->max_index());
       if (range_->inlined()) {
         return scope_info()->ContextInlinedLocalName(cage_base,
                                                      index_.as_int());
       }
-      return String::cast(table().KeyAt(cage_base, index_));
+      return String::cast(table()->KeyAt(cage_base, index_));
     }
 
-    String name() const {
+    Tagged<String> name() const {
       PtrComprCageBase cage_base = GetPtrComprCageBase(*scope_info());
       return name(cage_base);
     }
@@ -84,7 +84,7 @@ class ScopeInfo::LocalNamesRange {
 
     int index() const {
       if (range_->inlined()) return index_.as_int();
-      return table().IndexAt(index_);
+      return table()->IndexAt(index_);
     }
 
    private:
@@ -93,7 +93,7 @@ class ScopeInfo::LocalNamesRange {
 
     ScopeInfoPtr scope_info() const { return range_->scope_info_; }
 
-    NameToIndexHashTable table() const {
+    Tagged<NameToIndexHashTable> table() const {
       return scope_info()->context_local_names_hashtable();
     }
 
@@ -103,8 +103,8 @@ class ScopeInfo::LocalNamesRange {
       InternalIndex max = range_->max_index();
       // Increment until iterator points to a valid key or max.
       while (index_ < max) {
-        Object key = table().KeyAt(index_);
-        if (table().IsKey(roots, key)) break;
+        Tagged<Object> key = table()->KeyAt(index_);
+        if (table()->IsKey(roots, key)) break;
         ++index_;
       }
     }
@@ -138,10 +138,10 @@ ScopeInfo::LocalNamesRange<Handle<ScopeInfo>> ScopeInfo::IterateLocalNames(
 }
 
 // static
-ScopeInfo::LocalNamesRange<ScopeInfo*> ScopeInfo::IterateLocalNames(
-    ScopeInfo* scope_info, const DisallowGarbageCollection& no_gc) {
+ScopeInfo::LocalNamesRange<Tagged<ScopeInfo>> ScopeInfo::IterateLocalNames(
+    Tagged<ScopeInfo> scope_info, const DisallowGarbageCollection& no_gc) {
   USE(no_gc);
-  return LocalNamesRange<ScopeInfo*>(scope_info);
+  return LocalNamesRange<Tagged<ScopeInfo>>(scope_info);
 }
 
 }  // namespace internal
diff --git a/src/objects/scope-info.cc b/src/objects/scope-info.cc
index 792a26a3065..586c7a5d16f 100644
--- a/src/objects/scope-info.cc
+++ b/src/objects/scope-info.cc
@@ -22,15 +22,16 @@ namespace v8 {
 namespace internal {
 
 #ifdef DEBUG
-bool ScopeInfo::Equals(ScopeInfo other, bool is_live_edit_compare) const {
+bool ScopeInfo::Equals(Tagged<ScopeInfo> other,
+                       bool is_live_edit_compare) const {
   if (length() != other->length()) return false;
   for (int index = 0; index < length(); ++index) {
     if (is_live_edit_compare && HasPositionInfo() &&
         index >= PositionInfoIndex() && index <= PositionInfoIndex() + 1) {
       continue;
     }
-    Object entry = get(index);
-    Object other_entry = other->get(index);
+    Tagged<Object> entry = get(index);
+    Tagged<Object> other_entry = other->get(index);
     if (IsSmi(entry)) {
       if (entry != other_entry) return false;
     } else {
@@ -205,7 +206,7 @@ Handle<ScopeInfo> ScopeInfo::Create(IsolateT* isolate, Zone* zone, Scope* scope,
   int index = kVariablePartIndex;
   {
     DisallowGarbageCollection no_gc;
-    ScopeInfo scope_info = *scope_info_handle;
+    Tagged<ScopeInfo> scope_info = *scope_info_handle;
     WriteBarrierMode mode = scope_info->GetWriteBarrierMode(no_gc);
 
     bool has_simple_parameters = false;
@@ -373,7 +374,7 @@ Handle<ScopeInfo> ScopeInfo::Create(IsolateT* isolate, Zone* zone, Scope* scope,
     if (has_function_name) {
       Variable* var = scope->AsDeclarationScope()->function_var();
       int var_index = -1;
-      Object name = Smi::zero();
+      Tagged<Object> name = Smi::zero();
       if (var != nullptr) {
         var_index = var->index();
         name = *var->name();
@@ -598,33 +599,34 @@ Handle<ScopeInfo> ScopeInfo::CreateForBootstrapping(Isolate* isolate,
   return scope_info;
 }
 
-Object ScopeInfo::get(int index) const {
+Tagged<Object> ScopeInfo::get(int index) const {
   PtrComprCageBase cage_base = GetPtrComprCageBase(*this);
   return get(cage_base, index);
 }
 
-Object ScopeInfo::get(PtrComprCageBase cage_base, int index) const {
+Tagged<Object> ScopeInfo::get(PtrComprCageBase cage_base, int index) const {
   DCHECK_LT(static_cast<unsigned>(index), static_cast<unsigned>(length()));
   return TaggedField<Object>::Relaxed_Load(cage_base, *this,
                                            OffsetOfElementAt(index));
 }
 
-void ScopeInfo::set(int index, Smi value) {
+void ScopeInfo::set(int index, Tagged<Smi> value) {
   DCHECK_LT(static_cast<unsigned>(index), static_cast<unsigned>(length()));
   DCHECK(IsSmi(Object(value)));
   int offset = OffsetOfElementAt(index);
   RELAXED_WRITE_FIELD(*this, offset, value);
 }
 
-void ScopeInfo::set(int index, Object value, WriteBarrierMode mode) {
+void ScopeInfo::set(int index, Tagged<Object> value, WriteBarrierMode mode) {
   DCHECK_LT(static_cast<unsigned>(index), static_cast<unsigned>(length()));
   int offset = OffsetOfElementAt(index);
   RELAXED_WRITE_FIELD(*this, offset, value);
   CONDITIONAL_WRITE_BARRIER(*this, offset, value, mode);
 }
 
-void ScopeInfo::CopyElements(Isolate* isolate, int dst_index, ScopeInfo src,
-                             int src_index, int len, WriteBarrierMode mode) {
+void ScopeInfo::CopyElements(Isolate* isolate, int dst_index,
+                             Tagged<ScopeInfo> src, int src_index, int len,
+                             WriteBarrierMode mode) {
   if (len == 0) return;
   DCHECK_LE(src_index + len, src->length());
   DisallowGarbageCollection no_gc;
@@ -677,7 +679,7 @@ Handle<ScopeInfo> ScopeInfo::RecreateWithBlockList(
   return scope_info;
 }
 
-ScopeInfo ScopeInfo::Empty(Isolate* isolate) {
+Tagged<ScopeInfo> ScopeInfo::Empty(Isolate* isolate) {
   return ReadOnlyRoots(isolate).empty_scope_info();
 }
 
@@ -786,14 +788,14 @@ bool ScopeInfo::HasSharedFunctionName() const {
   return FunctionName() != SharedFunctionInfo::kNoSharedNameSentinel;
 }
 
-void ScopeInfo::SetFunctionName(Object name) {
+void ScopeInfo::SetFunctionName(Tagged<Object> name) {
   DCHECK(HasFunctionName());
   DCHECK(IsString(name) || name == SharedFunctionInfo::kNoSharedNameSentinel);
   DCHECK_IMPLIES(HasContextAllocatedFunctionName(), IsInternalizedString(name));
   set_function_variable_info_name(name);
 }
 
-void ScopeInfo::SetInferredFunctionName(String name) {
+void ScopeInfo::SetInferredFunctionName(Tagged<String> name) {
   DCHECK(HasInferredFunctionName());
   set_inferred_function_name(name);
 }
@@ -824,26 +826,26 @@ bool ScopeInfo::HasLocalsBlockList() const {
   return HasLocalsBlockListBit::decode(Flags());
 }
 
-StringSet ScopeInfo::LocalsBlockList() const {
+Tagged<StringSet> ScopeInfo::LocalsBlockList() const {
   DCHECK(HasLocalsBlockList());
   return StringSet::cast(locals_block_list());
 }
 
 bool ScopeInfo::HasContext() const { return ContextLength() > 0; }
 
-Object ScopeInfo::FunctionName() const {
+Tagged<Object> ScopeInfo::FunctionName() const {
   DCHECK(HasFunctionName());
   return function_variable_info_name();
 }
 
-Object ScopeInfo::InferredFunctionName() const {
+Tagged<Object> ScopeInfo::InferredFunctionName() const {
   DCHECK(HasInferredFunctionName());
   return inferred_function_name();
 }
 
-String ScopeInfo::FunctionDebugName() const {
+Tagged<String> ScopeInfo::FunctionDebugName() const {
   if (!HasFunctionName()) return GetReadOnlyRoots().empty_string();
-  Object name = FunctionName();
+  Tagged<Object> name = FunctionName();
   if (IsString(name) && String::cast(name)->length() > 0) {
     return String::cast(name);
   }
@@ -871,23 +873,23 @@ void ScopeInfo::SetPositionInfo(int start, int end) {
   set_position_info_end(end);
 }
 
-ScopeInfo ScopeInfo::OuterScopeInfo() const {
+Tagged<ScopeInfo> ScopeInfo::OuterScopeInfo() const {
   DCHECK(HasOuterScopeInfo());
   return ScopeInfo::cast(outer_scope_info());
 }
 
-SourceTextModuleInfo ScopeInfo::ModuleDescriptorInfo() const {
+Tagged<SourceTextModuleInfo> ScopeInfo::ModuleDescriptorInfo() const {
   DCHECK(scope_type() == MODULE_SCOPE);
   return SourceTextModuleInfo::cast(module_info());
 }
 
-String ScopeInfo::ContextInlinedLocalName(int var) const {
+Tagged<String> ScopeInfo::ContextInlinedLocalName(int var) const {
   DCHECK(HasInlinedLocalNames());
   return context_local_names(var);
 }
 
-String ScopeInfo::ContextInlinedLocalName(PtrComprCageBase cage_base,
-                                          int var) const {
+Tagged<String> ScopeInfo::ContextInlinedLocalName(PtrComprCageBase cage_base,
+                                                  int var) const {
   DCHECK(HasInlinedLocalNames());
   return context_local_names(cage_base, var);
 }
@@ -924,7 +926,7 @@ MaybeAssignedFlag ScopeInfo::ContextLocalMaybeAssignedFlag(int var) const {
 }
 
 // static
-bool ScopeInfo::VariableIsSynthetic(String name) {
+bool ScopeInfo::VariableIsSynthetic(Tagged<String> name) {
   // There's currently no flag stored on the ScopeInfo to indicate that a
   // variable is a compiler-introduced temporary. However, to avoid conflict
   // with user declarations, the current temporaries like .generator_object and
@@ -938,7 +940,7 @@ int ScopeInfo::ModuleVariableCount() const {
   return module_variable_count();
 }
 
-int ScopeInfo::ModuleIndex(String name, VariableMode* mode,
+int ScopeInfo::ModuleIndex(Tagged<String> name, VariableMode* mode,
                            InitializationFlag* init_flag,
                            MaybeAssignedFlag* maybe_assigned_flag) {
   DisallowGarbageCollection no_gc;
@@ -950,7 +952,7 @@ int ScopeInfo::ModuleIndex(String name, VariableMode* mode,
 
   int module_vars_count = module_variable_count();
   for (int i = 0; i < module_vars_count; ++i) {
-    String var_name = module_variables_name(i);
+    Tagged<String> var_name = module_variables_name(i);
     if (name->Equals(var_name)) {
       int index;
       ModuleVariable(i, nullptr, &index, mode, init_flag, maybe_assigned_flag);
@@ -961,7 +963,7 @@ int ScopeInfo::ModuleIndex(String name, VariableMode* mode,
   return 0;
 }
 
-int ScopeInfo::InlinedLocalNamesLookup(String name) {
+int ScopeInfo::InlinedLocalNamesLookup(Tagged<String> name) {
   DisallowGarbageCollection no_gc;
   PtrComprCageBase cage_base = GetPtrComprCageBase(*this);
   int local_count = context_local_count();
@@ -1011,14 +1013,14 @@ std::pair<String, int> ScopeInfo::SavedClassVariable() const {
     int index = saved_class_variable_info() - Context::MIN_CONTEXT_SLOTS;
     DCHECK_GE(index, 0);
     DCHECK_LT(index, ContextLocalCount());
-    String name = ContextInlinedLocalName(index);
+    Tagged<String> name = ContextInlinedLocalName(index);
     return std::make_pair(name, index);
   } else {
     // The saved class variable info corresponds to the offset in the hash
     // table storage.
     InternalIndex entry(saved_class_variable_info());
     NameToIndexHashTable table = context_local_names_hashtable();
-    Object name = table->KeyAt(entry);
+    Tagged<Object> name = table->KeyAt(entry);
     DCHECK(IsString(name));
     return std::make_pair(String::cast(name), table->IndexAt(entry));
   }
@@ -1040,7 +1042,7 @@ int ScopeInfo::ParametersStartIndex() const {
   return ContextHeaderLength();
 }
 
-int ScopeInfo::FunctionContextSlotIndex(String name) const {
+int ScopeInfo::FunctionContextSlotIndex(Tagged<String> name) const {
   DCHECK(IsInternalizedString(name));
   if (HasContextAllocatedFunctionName()) {
     DCHECK_IMPLIES(HasFunctionName(), IsInternalizedString(FunctionName()));
@@ -1099,7 +1101,7 @@ int ScopeInfo::ModuleVariablesIndex() const {
   return ConvertOffsetToIndex(ModuleVariablesOffset());
 }
 
-void ScopeInfo::ModuleVariable(int i, String* name, int* index,
+void ScopeInfo::ModuleVariable(int i, Tagged<String>* name, int* index,
                                VariableMode* mode,
                                InitializationFlag* init_flag,
                                MaybeAssignedFlag* maybe_assigned_flag) {
@@ -1276,7 +1278,7 @@ int SourceTextModuleInfo::RegularExportCount() const {
   return regular_exports()->length() / kRegularExportLength;
 }
 
-String SourceTextModuleInfo::RegularExportLocalName(int i) const {
+Tagged<String> SourceTextModuleInfo::RegularExportLocalName(int i) const {
   return String::cast(regular_exports()->get(i * kRegularExportLength +
                                              kRegularExportLocalNameOffset));
 }
@@ -1286,7 +1288,7 @@ int SourceTextModuleInfo::RegularExportCellIndex(int i) const {
                                            kRegularExportCellIndexOffset));
 }
 
-FixedArray SourceTextModuleInfo::RegularExportExportNames(int i) const {
+Tagged<FixedArray> SourceTextModuleInfo::RegularExportExportNames(int i) const {
   return FixedArray::cast(regular_exports()->get(
       i * kRegularExportLength + kRegularExportExportNamesOffset));
 }
diff --git a/src/objects/scope-info.h b/src/objects/scope-info.h
index 29a11c14b18..6cb045709a9 100644
--- a/src/objects/scope-info.h
+++ b/src/objects/scope-info.h
@@ -108,8 +108,8 @@ class ScopeInfo : public TorqueGeneratedScopeInfo<ScopeInfo, HeapObject> {
 
   V8_EXPORT_PRIVATE bool HasInferredFunctionName() const;
 
-  void SetFunctionName(Object name);
-  void SetInferredFunctionName(String name);
+  void SetFunctionName(Tagged<Object> name);
+  void SetInferredFunctionName(Tagged<String> name);
 
   // Does this scope belong to a function?
   bool HasPositionInfo() const;
@@ -123,22 +123,22 @@ class ScopeInfo : public TorqueGeneratedScopeInfo<ScopeInfo, HeapObject> {
   inline bool HasSimpleParameters() const;
 
   // Return the function_name if present.
-  V8_EXPORT_PRIVATE Object FunctionName() const;
+  V8_EXPORT_PRIVATE Tagged<Object> FunctionName() const;
 
   // The function's name if it is non-empty, otherwise the inferred name or an
   // empty string.
-  String FunctionDebugName() const;
+  Tagged<String> FunctionDebugName() const;
 
   // Return the function's inferred name if present.
   // See SharedFunctionInfo::function_identifier.
-  V8_EXPORT_PRIVATE Object InferredFunctionName() const;
+  V8_EXPORT_PRIVATE Tagged<Object> InferredFunctionName() const;
 
   // Position information accessors.
   int StartPosition() const;
   int EndPosition() const;
   void SetPositionInfo(int start, int end);
 
-  SourceTextModuleInfo ModuleDescriptorInfo() const;
+  Tagged<SourceTextModuleInfo> ModuleDescriptorInfo() const;
 
   // Return true if the local names are inlined in the scope info object.
   inline bool HasInlinedLocalNames() const;
@@ -149,13 +149,14 @@ class ScopeInfo : public TorqueGeneratedScopeInfo<ScopeInfo, HeapObject> {
   static inline LocalNamesRange<Handle<ScopeInfo>> IterateLocalNames(
       Handle<ScopeInfo> scope_info);
 
-  static inline LocalNamesRange<ScopeInfo*> IterateLocalNames(
-      ScopeInfo* scope_info, const DisallowGarbageCollection& no_gc);
+  static inline LocalNamesRange<Tagged<ScopeInfo>> IterateLocalNames(
+      Tagged<ScopeInfo> scope_info, const DisallowGarbageCollection& no_gc);
 
   // Return the name of a given context local.
   // It should only be used if inlined local names.
-  String ContextInlinedLocalName(int var) const;
-  String ContextInlinedLocalName(PtrComprCageBase cage_base, int var) const;
+  Tagged<String> ContextInlinedLocalName(int var) const;
+  Tagged<String> ContextInlinedLocalName(PtrComprCageBase cage_base,
+                                         int var) const;
 
   // Return the mode of the given context local.
   VariableMode ContextLocalMode(int var) const;
@@ -174,7 +175,7 @@ class ScopeInfo : public TorqueGeneratedScopeInfo<ScopeInfo, HeapObject> {
 
   // Return true if this local was introduced by the compiler, and should not be
   // exposed to the user in a debugger.
-  static bool VariableIsSynthetic(String name);
+  static bool VariableIsSynthetic(Tagged<String> name);
 
   // Lookup support for serialized scope info. Returns the local context slot
   // index for a given slot name if the slot is present; otherwise
@@ -188,7 +189,7 @@ class ScopeInfo : public TorqueGeneratedScopeInfo<ScopeInfo, HeapObject> {
   // Lookup metadata of a MODULE-allocated variable.  Return 0 if there is no
   // module variable with the given name (the index value of a MODULE variable
   // is never 0).
-  int ModuleIndex(String name, VariableMode* mode,
+  int ModuleIndex(Tagged<String> name, VariableMode* mode,
                   InitializationFlag* init_flag,
                   MaybeAssignedFlag* maybe_assigned_flag);
 
@@ -198,7 +199,7 @@ class ScopeInfo : public TorqueGeneratedScopeInfo<ScopeInfo, HeapObject> {
   // slot index if the function name is present and context-allocated (named
   // function expressions, only), otherwise returns a value < 0. The name
   // must be an internalized string.
-  int FunctionContextSlotIndex(String name) const;
+  int FunctionContextSlotIndex(Tagged<String> name) const;
 
   // Lookup support for serialized scope info.  Returns the receiver context
   // slot index if scope has a "this" binding, and the binding is
@@ -226,7 +227,7 @@ class ScopeInfo : public TorqueGeneratedScopeInfo<ScopeInfo, HeapObject> {
   void SetIsDebugEvaluateScope();
 
   // Return the outer ScopeInfo if present.
-  ScopeInfo OuterScopeInfo() const;
+  Tagged<ScopeInfo> OuterScopeInfo() const;
 
   bool is_script_scope() const;
 
@@ -236,7 +237,7 @@ class ScopeInfo : public TorqueGeneratedScopeInfo<ScopeInfo, HeapObject> {
   // Returns a list of stack-allocated locals of parent scopes.
   // Used during local debug-evalute to decide whether a context lookup
   // can continue upwards after checking this scope.
-  V8_EXPORT_PRIVATE StringSet LocalsBlockList() const;
+  V8_EXPORT_PRIVATE Tagged<StringSet> LocalsBlockList() const;
 
   // Returns true if this ScopeInfo was created for a scope that skips the
   // closest outer class when resolving private names.
@@ -254,7 +255,7 @@ class ScopeInfo : public TorqueGeneratedScopeInfo<ScopeInfo, HeapObject> {
   //   - outer scope info: LiveEdit already analyses outer scopes of unchanged
   //     functions. Also checking it here will break in really subtle cases
   //     e.g. changing a let to a const in an outer function, which is fine.
-  bool Equals(ScopeInfo other, bool is_live_edit_compare = false) const;
+  bool Equals(Tagged<ScopeInfo> other, bool is_live_edit_compare = false) const;
 #endif
 
   template <typename IsolateT>
@@ -276,7 +277,7 @@ class ScopeInfo : public TorqueGeneratedScopeInfo<ScopeInfo, HeapObject> {
       Handle<StringSet> blocklist);
 
   // Serializes empty scope info.
-  V8_EXPORT_PRIVATE static ScopeInfo Empty(Isolate* isolate);
+  V8_EXPORT_PRIVATE static Tagged<ScopeInfo> Empty(Isolate* isolate);
 
 #define FOR_EACH_SCOPE_INFO_NUMERIC_FIELD(V) \
   V(Flags)                                   \
@@ -310,7 +311,7 @@ class ScopeInfo : public TorqueGeneratedScopeInfo<ScopeInfo, HeapObject> {
   V8_EXPORT_PRIVATE uint32_t Hash();
 
  private:
-  int InlinedLocalNamesLookup(String name);
+  int InlinedLocalNamesLookup(Tagged<String> name);
 
   int ContextLocalNamesIndex() const;
   int ContextLocalInfosIndex() const;
@@ -330,14 +331,14 @@ class ScopeInfo : public TorqueGeneratedScopeInfo<ScopeInfo, HeapObject> {
   // in ScopeInfo is tagged. Each slot is tagged-pointer sized. Slot 0 is
   // 'flags', the first field defined by ScopeInfo after the standard-size
   // HeapObject header.
-  V8_EXPORT_PRIVATE Object get(int index) const;
-  Object get(PtrComprCageBase cage_base, int index) const;
+  V8_EXPORT_PRIVATE Tagged<Object> get(int index) const;
+  Tagged<Object> get(PtrComprCageBase cage_base, int index) const;
   // Setter that doesn't need write barrier.
-  void set(int index, Smi value);
+  void set(int index, Tagged<Smi> value);
   // Setter with explicit barrier mode.
-  void set(int index, Object value,
+  void set(int index, Tagged<Object> value,
            WriteBarrierMode mode = UPDATE_WRITE_BARRIER);
-  void CopyElements(Isolate* isolate, int dst_index, ScopeInfo src,
+  void CopyElements(Isolate* isolate, int dst_index, Tagged<ScopeInfo> src,
                     int src_index, int len, WriteBarrierMode mode);
   ObjectSlot RawFieldOfElementAt(int index);
   // The number of tagged-pointer-sized slots in the ScopeInfo after its
@@ -367,7 +368,7 @@ class ScopeInfo : public TorqueGeneratedScopeInfo<ScopeInfo, HeapObject> {
   // Get metadata of i-th MODULE-allocated variable, where 0 <= i <
   // ModuleVariableCount.  The metadata is returned via out-arguments, which may
   // be nullptr if the corresponding information is not requested
-  void ModuleVariable(int i, String* name, int* index,
+  void ModuleVariable(int i, Tagged<String>* name, int* index,
                       VariableMode* mode = nullptr,
                       InitializationFlag* init_flag = nullptr,
                       MaybeAssignedFlag* maybe_assigned_flag = nullptr);
diff --git a/src/objects/script-inl.h b/src/objects/script-inl.h
index 26689d0c7d0..c8e28128a95 100644
--- a/src/objects/script-inl.h
+++ b/src/objects/script-inl.h
@@ -39,7 +39,7 @@ ACCESSORS_CHECKED(Script, wasm_weak_instance_list, Tagged<WeakArrayList>,
 #endif  // V8_ENABLE_WEBASSEMBLY
 
 Script::Type Script::type() const {
-  Smi value = TaggedField<Smi, kScriptTypeOffset>::load(*this);
+  Tagged<Smi> value = TaggedField<Smi, kScriptTypeOffset>::load(*this);
   return static_cast<Type>(value.value());
 }
 void Script::set_type(Type value) {
diff --git a/src/objects/script.h b/src/objects/script.h
index 0827bcc8b92..8ef4f302cf0 100644
--- a/src/objects/script.h
+++ b/src/objects/script.h
@@ -154,7 +154,7 @@ class Script : public TorqueGeneratedScript<Script, Struct> {
   // unfinalized.
   inline bool IsMaybeUnfinalized(Isolate* isolate) const;
 
-  Object GetNameOrSourceURL();
+  Tagged<Object> GetNameOrSourceURL();
   static Handle<String> GetScriptHash(Isolate* isolate, Handle<Script> script,
                                       bool forceForInspector);
 
@@ -230,7 +230,7 @@ class Script : public TorqueGeneratedScript<Script, Struct> {
     explicit Iterator(Isolate* isolate);
     Iterator(const Iterator&) = delete;
     Iterator& operator=(const Iterator&) = delete;
-    Script Next();
+    Tagged<Script> Next();
 
    private:
     WeakArrayList::Iterator iterator_;
diff --git a/src/objects/shared-function-info-inl.h b/src/objects/shared-function-info-inl.h
index c2de9ba495a..c5a7cb6e29e 100644
--- a/src/objects/shared-function-info-inl.h
+++ b/src/objects/shared-function-info-inl.h
@@ -75,18 +75,18 @@ void PreparseData::copy_in(int index, const uint8_t* buffer, int length) {
   memcpy(reinterpret_cast<void*>(dst_addr), buffer, length);
 }
 
-PreparseData PreparseData::get_child(int index) const {
+Tagged<PreparseData> PreparseData::get_child(int index) const {
   return PreparseData::cast(get_child_raw(index));
 }
 
-Object PreparseData::get_child_raw(int index) const {
+Tagged<Object> PreparseData::get_child_raw(int index) const {
   DCHECK_LE(0, index);
   DCHECK_LT(index, this->children_length());
   int offset = inner_start_offset() + index * kTaggedSize;
   return RELAXED_READ_FIELD(*this, offset);
 }
 
-void PreparseData::set_child(int index, PreparseData value,
+void PreparseData::set_child(int index, Tagged<PreparseData> value,
                              WriteBarrierMode mode) {
   DCHECK_LE(0, index);
   DCHECK_LT(index, this->children_length());
@@ -167,16 +167,16 @@ void SharedFunctionInfo::set_relaxed_flags(int32_t flags) {
 UINT8_ACCESSORS(SharedFunctionInfo, flags2, kFlags2Offset)
 
 bool SharedFunctionInfo::HasSharedName() const {
-  Object value = name_or_scope_info(kAcquireLoad);
+  Tagged<Object> value = name_or_scope_info(kAcquireLoad);
   if (IsScopeInfo(value)) {
     return ScopeInfo::cast(value)->HasSharedFunctionName();
   }
   return value != kNoSharedNameSentinel;
 }
 
-String SharedFunctionInfo::Name() const {
+Tagged<String> SharedFunctionInfo::Name() const {
   if (!HasSharedName()) return GetReadOnlyRoots().empty_string();
-  Object value = name_or_scope_info(kAcquireLoad);
+  Tagged<Object> value = name_or_scope_info(kAcquireLoad);
   if (IsScopeInfo(value)) {
     if (ScopeInfo::cast(value)->HasFunctionName()) {
       return String::cast(ScopeInfo::cast(value)->FunctionName());
@@ -186,8 +186,8 @@ String SharedFunctionInfo::Name() const {
   return String::cast(value);
 }
 
-void SharedFunctionInfo::SetName(String name) {
-  Object maybe_scope_info = name_or_scope_info(kAcquireLoad);
+void SharedFunctionInfo::SetName(Tagged<String> name) {
+  Tagged<Object> maybe_scope_info = name_or_scope_info(kAcquireLoad);
   if (IsScopeInfo(maybe_scope_info)) {
     ScopeInfo::cast(maybe_scope_info)->SetFunctionName(name);
   } else {
@@ -208,7 +208,7 @@ bool SharedFunctionInfo::needs_script_context() const {
   return is_script() && scope_info(kAcquireLoad)->ContextLocalCount() > 0;
 }
 
-AbstractCode SharedFunctionInfo::abstract_code(Isolate* isolate) {
+Tagged<AbstractCode> SharedFunctionInfo::abstract_code(Isolate* isolate) {
   // TODO(v8:11429): Decide if this return bytecode or baseline code, when the
   // latter is present.
   if (HasBytecodeArray(isolate)) {
@@ -440,7 +440,7 @@ bool SharedFunctionInfo::IsDontAdaptArguments() const {
 }
 
 DEF_ACQUIRE_GETTER(SharedFunctionInfo, scope_info, Tagged<ScopeInfo>) {
-  Object maybe_scope_info = name_or_scope_info(cage_base, kAcquireLoad);
+  Tagged<Object> maybe_scope_info = name_or_scope_info(cage_base, kAcquireLoad);
   if (IsScopeInfo(maybe_scope_info, cage_base)) {
     return ScopeInfo::cast(maybe_scope_info);
   }
@@ -451,20 +451,20 @@ DEF_GETTER(SharedFunctionInfo, scope_info, Tagged<ScopeInfo>) {
   return scope_info(cage_base, kAcquireLoad);
 }
 
-ScopeInfo SharedFunctionInfo::EarlyScopeInfo(AcquireLoadTag tag) {
+Tagged<ScopeInfo> SharedFunctionInfo::EarlyScopeInfo(AcquireLoadTag tag) {
   // Keep in sync with the scope_info getter above.
   PtrComprCageBase cage_base = GetPtrComprCageBase(*this);
-  Object maybe_scope_info = name_or_scope_info(cage_base, tag);
+  Tagged<Object> maybe_scope_info = name_or_scope_info(cage_base, tag);
   if (IsScopeInfo(maybe_scope_info, cage_base)) {
     return ScopeInfo::cast(maybe_scope_info);
   }
   return EarlyGetReadOnlyRoots().empty_scope_info();
 }
 
-void SharedFunctionInfo::SetScopeInfo(ScopeInfo scope_info,
+void SharedFunctionInfo::SetScopeInfo(Tagged<ScopeInfo> scope_info,
                                       WriteBarrierMode mode) {
   // Move the existing name onto the ScopeInfo.
-  Object name = name_or_scope_info(kAcquireLoad);
+  Tagged<Object> name = name_or_scope_info(kAcquireLoad);
   if (IsScopeInfo(name)) {
     name = ScopeInfo::cast(name)->FunctionName();
   }
@@ -477,7 +477,7 @@ void SharedFunctionInfo::SetScopeInfo(ScopeInfo scope_info,
   set_name_or_scope_info(scope_info, kReleaseStore, mode);
 }
 
-void SharedFunctionInfo::set_raw_scope_info(ScopeInfo scope_info,
+void SharedFunctionInfo::set_raw_scope_info(Tagged<ScopeInfo> scope_info,
                                             WriteBarrierMode mode) {
   WRITE_FIELD(*this, kNameOrScopeInfoOffset, scope_info);
   CONDITIONAL_WRITE_BARRIER(*this, kNameOrScopeInfoOffset, scope_info, mode);
@@ -490,19 +490,19 @@ DEF_GETTER(SharedFunctionInfo, outer_scope_info, Tagged<HeapObject>) {
 }
 
 bool SharedFunctionInfo::HasOuterScopeInfo() const {
-  ScopeInfo outer_info;
+  Tagged<ScopeInfo> outer_info;
   if (!is_compiled()) {
     if (!IsScopeInfo(outer_scope_info())) return false;
     outer_info = ScopeInfo::cast(outer_scope_info());
   } else {
-    ScopeInfo info = scope_info(kAcquireLoad);
+    Tagged<ScopeInfo> info = scope_info(kAcquireLoad);
     if (!info->HasOuterScopeInfo()) return false;
     outer_info = info->OuterScopeInfo();
   }
   return !outer_info->IsEmpty();
 }
 
-ScopeInfo SharedFunctionInfo::GetOuterScopeInfo() const {
+Tagged<ScopeInfo> SharedFunctionInfo::GetOuterScopeInfo() const {
   DCHECK(HasOuterScopeInfo());
   if (!is_compiled()) return ScopeInfo::cast(outer_scope_info());
   return scope_info(kAcquireLoad)->OuterScopeInfo();
@@ -538,7 +538,7 @@ RELEASE_ACQUIRE_ACCESSORS_CHECKED2(SharedFunctionInfo, feedback_metadata,
                                        IsFeedbackMetadata(value))
 
 bool SharedFunctionInfo::is_compiled() const {
-  Object data = function_data(kAcquireLoad);
+  Tagged<Object> data = function_data(kAcquireLoad);
   return data != Smi::FromEnum(Builtin::kCompileLazy) &&
          !IsUncompiledData(data);
 }
@@ -548,7 +548,7 @@ IsCompiledScope SharedFunctionInfo::is_compiled_scope(IsolateT* isolate) const {
   return IsCompiledScope(*this, isolate);
 }
 
-IsCompiledScope::IsCompiledScope(const SharedFunctionInfo shared,
+IsCompiledScope::IsCompiledScope(const Tagged<SharedFunctionInfo> shared,
                                  Isolate* isolate)
     : is_compiled_(shared->is_compiled()) {
   if (shared->HasBaselineCode()) {
@@ -562,7 +562,7 @@ IsCompiledScope::IsCompiledScope(const SharedFunctionInfo shared,
   DCHECK_IMPLIES(!retain_code_.is_null(), is_compiled());
 }
 
-IsCompiledScope::IsCompiledScope(const SharedFunctionInfo shared,
+IsCompiledScope::IsCompiledScope(const Tagged<SharedFunctionInfo> shared,
                                  LocalIsolate* isolate)
     : is_compiled_(shared->is_compiled()) {
   if (shared->HasBaselineCode()) {
@@ -597,7 +597,7 @@ DEF_GETTER(SharedFunctionInfo, api_func_data, Tagged<FunctionTemplateInfo>) {
 }
 
 DEF_GETTER(SharedFunctionInfo, HasBytecodeArray, bool) {
-  Object data = function_data(cage_base, kAcquireLoad);
+  Tagged<Object> data = function_data(cage_base, kAcquireLoad);
   if (!IsHeapObject(data)) return false;
   InstanceType instance_type =
       HeapObject::cast(data)->map(cage_base)->instance_type();
@@ -624,9 +624,9 @@ Tagged<BytecodeArray> SharedFunctionInfo::GetBytecodeArray(
 }
 
 DEF_GETTER(SharedFunctionInfo, GetActiveBytecodeArray, Tagged<BytecodeArray>) {
-  Object data = function_data(kAcquireLoad);
+  Tagged<Object> data = function_data(kAcquireLoad);
   if (IsCode(data)) {
-    Code baseline_code = Code::cast(data);
+    Tagged<Code> baseline_code = Code::cast(data);
     data = baseline_code->bytecode_or_interpreter_data(cage_base);
   }
   if (IsBytecodeArray(data)) {
@@ -643,7 +643,7 @@ void SharedFunctionInfo::SetActiveBytecodeArray(
   // functions. They should have been flushed earlier.
   DCHECK(!HasBaselineCode());
 
-  Object data = function_data(kAcquireLoad);
+  Tagged<Object> data = function_data(kAcquireLoad);
   if (IsBytecodeArray(data)) {
     set_function_data(bytecode, kReleaseStore);
   } else {
@@ -664,9 +664,9 @@ DEF_GETTER(SharedFunctionInfo, InterpreterTrampoline, Tagged<Code>) {
 }
 
 DEF_GETTER(SharedFunctionInfo, HasInterpreterData, bool) {
-  Object data = function_data(cage_base, kAcquireLoad);
+  Tagged<Object> data = function_data(cage_base, kAcquireLoad);
   if (IsCode(data, cage_base)) {
-    Code baseline_code = Code::cast(data);
+    Tagged<Code> baseline_code = Code::cast(data);
     DCHECK_EQ(baseline_code->kind(), CodeKind::BASELINE);
     data = baseline_code->bytecode_or_interpreter_data(cage_base);
   }
@@ -675,9 +675,9 @@ DEF_GETTER(SharedFunctionInfo, HasInterpreterData, bool) {
 
 DEF_GETTER(SharedFunctionInfo, interpreter_data, Tagged<InterpreterData>) {
   DCHECK(HasInterpreterData(cage_base));
-  Object data = function_data(cage_base, kAcquireLoad);
+  Tagged<Object> data = function_data(cage_base, kAcquireLoad);
   if (IsCode(data, cage_base)) {
-    Code baseline_code = Code::cast(data);
+    Tagged<Code> baseline_code = Code::cast(data);
     DCHECK_EQ(baseline_code->kind(), CodeKind::BASELINE);
     data = baseline_code->bytecode_or_interpreter_data(cage_base);
   }
@@ -692,7 +692,7 @@ void SharedFunctionInfo::set_interpreter_data(
 }
 
 DEF_GETTER(SharedFunctionInfo, HasBaselineCode, bool) {
-  Object data = function_data(cage_base, kAcquireLoad);
+  Tagged<Object> data = function_data(cage_base, kAcquireLoad);
   if (IsCode(data, cage_base)) {
     DCHECK_EQ(Code::cast(data)->kind(), CodeKind::BASELINE);
     return true;
@@ -866,7 +866,7 @@ bool SharedFunctionInfo::HasUncompiledDataWithoutPreparseData() const {
 }
 
 void SharedFunctionInfo::ClearUncompiledDataJobPointer() {
-  UncompiledData uncompiled_data = this->uncompiled_data();
+  Tagged<UncompiledData> uncompiled_data = this->uncompiled_data();
   if (IsUncompiledDataWithPreparseDataAndJob(uncompiled_data)) {
     UncompiledDataWithPreparseDataAndJob::cast(uncompiled_data)
         ->set_job(kNullAddress);
@@ -878,7 +878,8 @@ void SharedFunctionInfo::ClearUncompiledDataJobPointer() {
 
 void SharedFunctionInfo::ClearPreparseData() {
   DCHECK(HasUncompiledDataWithPreparseData());
-  UncompiledDataWithPreparseData data = uncompiled_data_with_preparse_data();
+  Tagged<UncompiledDataWithPreparseData> data =
+      uncompiled_data_with_preparse_data();
 
   // Trim off the pre-parsed scope data from the uncompiled data by swapping the
   // map, leaving only an uncompiled data without pre-parsed scope.
@@ -907,8 +908,9 @@ void SharedFunctionInfo::ClearPreparseData() {
 }
 
 void UncompiledData::InitAfterBytecodeFlush(
-    String inferred_name, int start_position, int end_position,
-    std::function<void(HeapObject object, ObjectSlot slot, HeapObject target)>
+    Tagged<String> inferred_name, int start_position, int end_position,
+    std::function<void(Tagged<HeapObject> object, ObjectSlot slot,
+                       Tagged<HeapObject> target)>
         gc_notify_updated_slot) {
   set_inferred_name(inferred_name);
   gc_notify_updated_slot(*this, RawField(UncompiledData::kInferredNameOffset),
@@ -922,7 +924,7 @@ bool SharedFunctionInfo::is_repl_mode() const {
 }
 
 bool SharedFunctionInfo::HasInferredName() {
-  Object scope_info = name_or_scope_info(kAcquireLoad);
+  Tagged<Object> scope_info = name_or_scope_info(kAcquireLoad);
   if (IsScopeInfo(scope_info)) {
     return ScopeInfo::cast(scope_info)->HasInferredFunctionName();
   }
@@ -930,11 +932,11 @@ bool SharedFunctionInfo::HasInferredName() {
 }
 
 DEF_GETTER(SharedFunctionInfo, inferred_name, Tagged<String>) {
-  Object maybe_scope_info = name_or_scope_info(kAcquireLoad);
+  Tagged<Object> maybe_scope_info = name_or_scope_info(kAcquireLoad);
   if (IsScopeInfo(maybe_scope_info)) {
-    ScopeInfo scope_info = ScopeInfo::cast(maybe_scope_info);
+    Tagged<ScopeInfo> scope_info = ScopeInfo::cast(maybe_scope_info);
     if (scope_info->HasInferredFunctionName()) {
-      Object name = scope_info->InferredFunctionName();
+      Tagged<Object> name = scope_info->InferredFunctionName();
       if (IsString(name)) return String::cast(name);
     }
   } else if (HasUncompiledData()) {
@@ -944,9 +946,9 @@ DEF_GETTER(SharedFunctionInfo, inferred_name, Tagged<String>) {
 }
 
 bool SharedFunctionInfo::IsUserJavaScript() const {
-  Object script_obj = script();
+  Tagged<Object> script_obj = script();
   if (IsUndefined(script_obj)) return false;
-  Script script = Script::cast(script_obj);
+  Tagged<Script> script = Script::cast(script_obj);
   return script->IsUserJavaScript();
 }
 
diff --git a/src/objects/shared-function-info.cc b/src/objects/shared-function-info.cc
index bdc08ea6f82..3664db12f8f 100644
--- a/src/objects/shared-function-info.cc
+++ b/src/objects/shared-function-info.cc
@@ -20,7 +20,8 @@
 namespace v8 {
 namespace internal {
 
-V8_EXPORT_PRIVATE constexpr Smi SharedFunctionInfo::kNoSharedNameSentinel;
+V8_EXPORT_PRIVATE constexpr Tagged<Smi>
+    SharedFunctionInfo::kNoSharedNameSentinel;
 
 uint32_t SharedFunctionInfo::Hash() {
   // Hash SharedFunctionInfo based on its start position and script id. Note: we
@@ -70,13 +71,13 @@ void SharedFunctionInfo::Init(ReadOnlyRoots ro_roots, int unique_id) {
   clear_padding();
 }
 
-Code SharedFunctionInfo::GetCode(Isolate* isolate) const {
+Tagged<Code> SharedFunctionInfo::GetCode(Isolate* isolate) const {
   // ======
   // NOTE: This chain of checks MUST be kept in sync with the equivalent CSA
   // GetSharedFunctionInfoCode method in code-stub-assembler.cc.
   // ======
 
-  Object data = function_data(kAcquireLoad);
+  Tagged<Object> data = function_data(kAcquireLoad);
   if (IsSmi(data)) {
     // Holding a Smi means we are a builtin.
     DCHECK(HasBuiltinId());
@@ -129,7 +130,7 @@ Code SharedFunctionInfo::GetCode(Isolate* isolate) const {
     return isolate->builtins()->code(Builtin::kHandleApiCallOrConstruct);
   }
   if (IsInterpreterData(data)) {
-    Code code = InterpreterTrampoline();
+    Tagged<Code> code = InterpreterTrampoline();
     DCHECK(IsCode(code));
     DCHECK(code->is_interpreter_trampoline_builtin());
     return code;
@@ -138,17 +139,17 @@ Code SharedFunctionInfo::GetCode(Isolate* isolate) const {
 }
 
 SharedFunctionInfo::ScriptIterator::ScriptIterator(Isolate* isolate,
-                                                   Script script)
+                                                   Tagged<Script> script)
     : ScriptIterator(handle(script->shared_function_infos(), isolate)) {}
 
 SharedFunctionInfo::ScriptIterator::ScriptIterator(
     Handle<WeakFixedArray> shared_function_infos)
     : shared_function_infos_(shared_function_infos), index_(0) {}
 
-SharedFunctionInfo SharedFunctionInfo::ScriptIterator::Next() {
+Tagged<SharedFunctionInfo> SharedFunctionInfo::ScriptIterator::Next() {
   while (index_ < shared_function_infos_->length()) {
     MaybeObject raw = shared_function_infos_->Get(index_++);
-    HeapObject heap_object;
+    Tagged<HeapObject> heap_object;
     if (!raw->GetHeapObject(&heap_object) || IsUndefined(heap_object)) {
       continue;
     }
@@ -158,13 +159,13 @@ SharedFunctionInfo SharedFunctionInfo::ScriptIterator::Next() {
 }
 
 void SharedFunctionInfo::ScriptIterator::Reset(Isolate* isolate,
-                                               Script script) {
+                                               Tagged<Script> script) {
   shared_function_infos_ = handle(script->shared_function_infos(), isolate);
   index_ = 0;
 }
 
 void SharedFunctionInfo::SetScript(ReadOnlyRoots roots,
-                                   HeapObject script_object,
+                                   Tagged<HeapObject> script_object,
                                    int function_literal_id,
                                    bool reset_preparsed_scope_data) {
   DisallowGarbageCollection no_gc;
@@ -181,12 +182,12 @@ void SharedFunctionInfo::SetScript(ReadOnlyRoots roots,
   // duplicates.
   if (IsScript(script_object)) {
     DCHECK(!IsScript(script()));
-    Script script = Script::cast(script_object);
-    WeakFixedArray list = script->shared_function_infos();
+    Tagged<Script> script = Script::cast(script_object);
+    Tagged<WeakFixedArray> list = script->shared_function_infos();
 #ifdef DEBUG
     DCHECK_LT(function_literal_id, list->length());
     MaybeObject maybe_object = list->Get(function_literal_id);
-    HeapObject heap_object;
+    Tagged<HeapObject> heap_object;
     if (maybe_object->GetHeapObjectIfWeak(&heap_object)) {
       DCHECK_EQ(heap_object, *this);
     }
@@ -196,15 +197,15 @@ void SharedFunctionInfo::SetScript(ReadOnlyRoots roots,
     DCHECK(IsScript(script()));
 
     // Remove shared function info from old script's list.
-    Script old_script = Script::cast(script());
+    Tagged<Script> old_script = Script::cast(script());
 
     // Due to liveedit, it might happen that the old_script doesn't know
     // about the SharedFunctionInfo, so we have to guard against that.
-    WeakFixedArray infos = old_script->shared_function_infos();
+    Tagged<WeakFixedArray> infos = old_script->shared_function_infos();
     if (function_literal_id < infos->length()) {
       MaybeObject raw =
           old_script->shared_function_infos()->Get(function_literal_id);
-      HeapObject heap_object;
+      Tagged<HeapObject> heap_object;
       if (raw->GetHeapObjectIfWeak(&heap_object) && heap_object == *this) {
         old_script->shared_function_infos()->Set(
             function_literal_id,
@@ -217,7 +218,7 @@ void SharedFunctionInfo::SetScript(ReadOnlyRoots roots,
   set_script(script_object, kReleaseStore);
 }
 
-void SharedFunctionInfo::CopyFrom(SharedFunctionInfo other) {
+void SharedFunctionInfo::CopyFrom(Tagged<SharedFunctionInfo> other) {
   PtrComprCageBase cage_base = GetPtrComprCageBase(*this);
   set_function_data(other->function_data(cage_base, kAcquireLoad),
                     kReleaseStore);
@@ -275,7 +276,8 @@ bool SharedFunctionInfo::HasCoverageInfo(Isolate* isolate) const {
   return isolate->debug()->HasCoverageInfo(*this);
 }
 
-CoverageInfo SharedFunctionInfo::GetCoverageInfo(Isolate* isolate) const {
+Tagged<CoverageInfo> SharedFunctionInfo::GetCoverageInfo(
+    Isolate* isolate) const {
   DCHECK(HasCoverageInfo(isolate));
   return CoverageInfo::cast(GetDebugInfo(isolate)->coverage_info());
 }
@@ -288,7 +290,7 @@ std::unique_ptr<char[]> SharedFunctionInfo::DebugNameCStr() const {
   }
 #endif  // V8_ENABLE_WEBASSEMBLY
   DisallowGarbageCollection no_gc;
-  String function_name = Name();
+  Tagged<String> function_name = Name();
   if (function_name->length() == 0) function_name = inferred_name();
   return function_name->ToCString();
 }
@@ -310,7 +312,7 @@ Handle<String> SharedFunctionInfo::DebugName(
                : isolate->factory()->static_initializer_string();
   }
   DisallowHeapAllocation no_gc;
-  String function_name = shared->Name();
+  Tagged<String> function_name = shared->Name();
   if (function_name->length() == 0) function_name = shared->inferred_name();
   return handle(function_name, isolate);
 }
@@ -330,7 +332,8 @@ bool SharedFunctionInfo::HasSourceCode() const {
 
 void SharedFunctionInfo::DiscardCompiledMetadata(
     Isolate* isolate,
-    std::function<void(HeapObject object, ObjectSlot slot, HeapObject target)>
+    std::function<void(Tagged<HeapObject> object, ObjectSlot slot,
+                       Tagged<HeapObject> target)>
         gc_notify_updated_slot) {
   DisallowGarbageCollection no_gc;
   if (HasFeedbackMetadata()) {
@@ -341,7 +344,7 @@ void SharedFunctionInfo::DiscardCompiledMetadata(
       PrintF(scope.file(), "]\n");
     }
 
-    HeapObject outer_scope_info;
+    Tagged<HeapObject> outer_scope_info;
     if (scope_info()->HasOuterScopeInfo()) {
       outer_scope_info = scope_info()->OuterScopeInfo();
     } else {
@@ -443,21 +446,21 @@ int SharedFunctionInfo::SourceSize() { return EndPosition() - StartPosition(); }
 
 // Output the source code without any allocation in the heap.
 std::ostream& operator<<(std::ostream& os, const SourceCodeOf& v) {
-  const SharedFunctionInfo s = v.value;
+  const Tagged<SharedFunctionInfo> s = v.value;
   // For some native functions there is no source.
   if (!s->HasSourceCode()) return os << "<No Source>";
 
   // Get the source for the script which this function came from.
   // Don't use String::cast because we don't want more assertion errors while
   // we are already creating a stack dump.
-  String script_source =
+  Tagged<String> script_source =
       String::unchecked_cast(Script::cast(s->script())->source());
 
   if (!script_source->LooksValid()) return os << "<Invalid Source>";
 
   if (!s->is_toplevel()) {
     os << "function ";
-    String name = s->Name();
+    Tagged<String> name = s->Name();
     if (name->length() > 0) {
       name->PrintUC16(os);
     }
@@ -671,9 +674,9 @@ void SharedFunctionInfo::SetFunctionTokenPosition(int function_token_position,
 }
 
 int SharedFunctionInfo::StartPosition() const {
-  Object maybe_scope_info = name_or_scope_info(kAcquireLoad);
+  Tagged<Object> maybe_scope_info = name_or_scope_info(kAcquireLoad);
   if (IsScopeInfo(maybe_scope_info)) {
-    ScopeInfo info = ScopeInfo::cast(maybe_scope_info);
+    Tagged<ScopeInfo> info = ScopeInfo::cast(maybe_scope_info);
     if (info->HasPositionInfo()) {
       return info->StartPosition();
     }
@@ -688,7 +691,8 @@ int SharedFunctionInfo::StartPosition() const {
   }
 #if V8_ENABLE_WEBASSEMBLY
   if (HasWasmExportedFunctionData()) {
-    WasmInstanceObject instance = wasm_exported_function_data()->instance();
+    Tagged<WasmInstanceObject> instance =
+        wasm_exported_function_data()->instance();
     int func_index = wasm_exported_function_data()->function_index();
     auto& function = instance->module()->functions[func_index];
     return static_cast<int>(function.code.offset());
@@ -698,9 +702,9 @@ int SharedFunctionInfo::StartPosition() const {
 }
 
 int SharedFunctionInfo::EndPosition() const {
-  Object maybe_scope_info = name_or_scope_info(kAcquireLoad);
+  Tagged<Object> maybe_scope_info = name_or_scope_info(kAcquireLoad);
   if (IsScopeInfo(maybe_scope_info)) {
-    ScopeInfo info = ScopeInfo::cast(maybe_scope_info);
+    Tagged<ScopeInfo> info = ScopeInfo::cast(maybe_scope_info);
     if (info->HasPositionInfo()) {
       return info->EndPosition();
     }
@@ -715,7 +719,8 @@ int SharedFunctionInfo::EndPosition() const {
   }
 #if V8_ENABLE_WEBASSEMBLY
   if (HasWasmExportedFunctionData()) {
-    WasmInstanceObject instance = wasm_exported_function_data()->instance();
+    Tagged<WasmInstanceObject> instance =
+        wasm_exported_function_data()->instance();
     int func_index = wasm_exported_function_data()->function_index();
     auto& function = instance->module()->functions[func_index];
     return static_cast<int>(function.code.end_offset());
@@ -726,11 +731,11 @@ int SharedFunctionInfo::EndPosition() const {
 
 void SharedFunctionInfo::UpdateFromFunctionLiteralForLiveEdit(
     FunctionLiteral* lit) {
-  Object maybe_scope_info = name_or_scope_info(kAcquireLoad);
+  Tagged<Object> maybe_scope_info = name_or_scope_info(kAcquireLoad);
   if (IsScopeInfo(maybe_scope_info)) {
     // Updating the ScopeInfo is safe since they are identical modulo
     // source positions.
-    ScopeInfo new_scope_info = *lit->scope()->scope_info();
+    Tagged<ScopeInfo> new_scope_info = *lit->scope()->scope_info();
     DCHECK(new_scope_info->Equals(ScopeInfo::cast(maybe_scope_info), true));
     SetScopeInfo(new_scope_info);
   } else if (!is_compiled()) {
@@ -791,7 +796,7 @@ void SharedFunctionInfo::InstallDebugBytecode(Handle<SharedFunctionInfo> shared,
     DisallowGarbageCollection no_gc;
     base::SharedMutexGuard<base::kExclusive> mutex_guard(
         isolate->shared_function_info_access());
-    DebugInfo debug_info = shared->GetDebugInfo(isolate);
+    Tagged<DebugInfo> debug_info = shared->GetDebugInfo(isolate);
     debug_info->set_original_bytecode_array(*original_bytecode_array,
                                             kReleaseStore);
     debug_info->set_debug_bytecode_array(*debug_bytecode_array, kReleaseStore);
@@ -800,13 +805,14 @@ void SharedFunctionInfo::InstallDebugBytecode(Handle<SharedFunctionInfo> shared,
 }
 
 // static
-void SharedFunctionInfo::UninstallDebugBytecode(SharedFunctionInfo shared,
-                                                Isolate* isolate) {
+void SharedFunctionInfo::UninstallDebugBytecode(
+    Tagged<SharedFunctionInfo> shared, Isolate* isolate) {
   DisallowGarbageCollection no_gc;
   base::SharedMutexGuard<base::kExclusive> mutex_guard(
       isolate->shared_function_info_access());
-  DebugInfo debug_info = shared->GetDebugInfo(isolate);
-  BytecodeArray original_bytecode_array = debug_info->OriginalBytecodeArray();
+  Tagged<DebugInfo> debug_info = shared->GetDebugInfo(isolate);
+  Tagged<BytecodeArray> original_bytecode_array =
+      debug_info->OriginalBytecodeArray();
   DCHECK(!shared->HasBaselineCode());
   shared->SetActiveBytecodeArray(original_bytecode_array);
   debug_info->set_original_bytecode_array(
@@ -816,7 +822,7 @@ void SharedFunctionInfo::UninstallDebugBytecode(SharedFunctionInfo shared,
 }
 
 // static
-void SharedFunctionInfo::EnsureOldForTesting(SharedFunctionInfo sfi) {
+void SharedFunctionInfo::EnsureOldForTesting(Tagged<SharedFunctionInfo> sfi) {
   if (v8_flags.flush_code_based_on_time ||
       v8_flags.flush_code_based_on_tab_visibility) {
     sfi->set_age(kMaxAge);
@@ -830,7 +836,7 @@ void SharedFunctionInfo::EnsureOldForTesting(SharedFunctionInfo sfi) {
 bool SharedFunctionInfo::UniqueIdsAreUnique(Isolate* isolate) {
   std::unordered_set<uint32_t> ids({isolate->next_unique_sfi_id()});
   CombinedHeapObjectIterator it(isolate->heap());
-  for (HeapObject o = it.Next(); !o.is_null(); o = it.Next()) {
+  for (Tagged<HeapObject> o = it.Next(); !o.is_null(); o = it.Next()) {
     if (!IsSharedFunctionInfo(o)) continue;
     auto result = ids.emplace(SharedFunctionInfo::cast(o)->unique_id());
     // If previously inserted...
diff --git a/src/objects/shared-function-info.h b/src/objects/shared-function-info.h
index e5154f92007..fd4a93d7bce 100644
--- a/src/objects/shared-function-info.h
+++ b/src/objects/shared-function-info.h
@@ -85,8 +85,8 @@ class PreparseData
   inline void set(int index, uint8_t value);
   inline void copy_in(int index, const uint8_t* buffer, int length);
 
-  inline PreparseData get_child(int index) const;
-  inline void set_child(int index, PreparseData value,
+  inline Tagged<PreparseData> get_child(int index) const;
+  inline void set_child(int index, Tagged<PreparseData> value,
                         WriteBarrierMode mode = UPDATE_WRITE_BARRIER);
 
   // Clear uninitialized padding space.
@@ -110,7 +110,7 @@ class PreparseData
   TQ_OBJECT_CONSTRUCTORS(PreparseData)
 
  private:
-  inline Object get_child_raw(int index) const;
+  inline Tagged<Object> get_child_raw(int index) const;
 };
 
 // Abstract class representing extra data for an uncompiled function, which is
@@ -119,8 +119,9 @@ class UncompiledData
     : public TorqueGeneratedUncompiledData<UncompiledData, HeapObject> {
  public:
   inline void InitAfterBytecodeFlush(
-      String inferred_name, int start_position, int end_position,
-      std::function<void(HeapObject object, ObjectSlot slot, HeapObject target)>
+      Tagged<String> inferred_name, int start_position, int end_position,
+      std::function<void(Tagged<HeapObject> object, ObjectSlot slot,
+                         Tagged<HeapObject> target)>
           gc_notify_updated_slot);
 
   TQ_OBJECT_CONSTRUCTORS(UncompiledData)
@@ -198,31 +199,31 @@ class SharedFunctionInfo
   // Important: This function MUST not allocate.
   void Init(ReadOnlyRoots roots, int unique_id);
 
-  V8_EXPORT_PRIVATE static constexpr Smi const kNoSharedNameSentinel =
+  V8_EXPORT_PRIVATE static constexpr Tagged<Smi> const kNoSharedNameSentinel =
       Smi::zero();
 
   // [name]: Returns shared name if it exists or an empty string otherwise.
-  inline String Name() const;
-  inline void SetName(String name);
+  inline Tagged<String> Name() const;
+  inline void SetName(Tagged<String> name);
 
   // Get the code object which represents the execution of this function.
-  V8_EXPORT_PRIVATE Code GetCode(Isolate* isolate) const;
+  V8_EXPORT_PRIVATE Tagged<Code> GetCode(Isolate* isolate) const;
 
   // Get the abstract code associated with the function, which will either be
   // a Code object or a BytecodeArray.
-  inline AbstractCode abstract_code(Isolate* isolate);
+  inline Tagged<AbstractCode> abstract_code(Isolate* isolate);
 
   // Set up the link between shared function info and the script. The shared
   // function info is added to the list on the script.
   V8_EXPORT_PRIVATE void SetScript(ReadOnlyRoots roots,
-                                   HeapObject script_object,
+                                   Tagged<HeapObject> script_object,
                                    int function_literal_id,
                                    bool reset_preparsed_scope_data = true);
 
   // Copy the data from another SharedFunctionInfo. Used for copying data into
   // and out of a placeholder SharedFunctionInfo, for off-thread compilation
   // which is not allowed to touch a main-thread-visible SharedFunctionInfo.
-  void CopyFrom(SharedFunctionInfo other);
+  void CopyFrom(Tagged<SharedFunctionInfo> other);
 
   // Layout description of the optimized code map.
   static const int kEntriesStart = 0;
@@ -240,13 +241,13 @@ class SharedFunctionInfo
   // Deprecated, use the ACQUIRE version instead.
   DECL_GETTER(scope_info, Tagged<ScopeInfo>)
   // Slow but safe:
-  inline ScopeInfo EarlyScopeInfo(AcquireLoadTag tag);
+  inline Tagged<ScopeInfo> EarlyScopeInfo(AcquireLoadTag tag);
 
   // Set scope_info without moving the existing name onto the ScopeInfo.
-  inline void set_raw_scope_info(ScopeInfo scope_info,
+  inline void set_raw_scope_info(Tagged<ScopeInfo> scope_info,
                                  WriteBarrierMode mode = UPDATE_WRITE_BARRIER);
 
-  inline void SetScopeInfo(ScopeInfo scope_info,
+  inline void SetScopeInfo(Tagged<ScopeInfo> scope_info,
                            WriteBarrierMode mode = UPDATE_WRITE_BARRIER);
 
   inline bool is_script() const;
@@ -275,7 +276,7 @@ class SharedFunctionInfo
  public:
   // Get the outer scope info whether this function is compiled or not.
   inline bool HasOuterScopeInfo() const;
-  inline ScopeInfo GetOuterScopeInfo() const;
+  inline Tagged<ScopeInfo> GetOuterScopeInfo() const;
 
   // [feedback metadata] Metadata template for feedback vectors of instances of
   // this function.
@@ -340,7 +341,6 @@ class SharedFunctionInfo
   DECL_GETTER(InterpreterTrampoline, Tagged<Code>)
   DECL_GETTER(HasInterpreterData, bool)
   DECL_ACCESSORS(interpreter_data, Tagged<InterpreterData>)
-  inline void set_interpreter_data(InterpreterData interpreter_data);
   DECL_GETTER(HasBaselineCode, bool)
   DECL_RELEASE_ACQUIRE_ACCESSORS(baseline_code, Tagged<Code>)
   inline void FlushBaselineCode();
@@ -398,7 +398,7 @@ class SharedFunctionInfo
   V8_EXPORT_PRIVATE bool HasBreakInfo(Isolate* isolate) const;
   bool BreakAtEntry(Isolate* isolate) const;
   bool HasCoverageInfo(Isolate* isolate) const;
-  CoverageInfo GetCoverageInfo(Isolate* isolate) const;
+  Tagged<CoverageInfo> GetCoverageInfo(Isolate* isolate) const;
 
   // The function's name if it is non-empty, otherwise the inferred name.
   std::unique_ptr<char[]> DebugNameCStr() const;
@@ -563,9 +563,11 @@ class SharedFunctionInfo
   // |gc_notify_updated_slot| should be used to record any slot updates.
   void DiscardCompiledMetadata(
       Isolate* isolate,
-      std::function<void(HeapObject object, ObjectSlot slot, HeapObject target)>
-          gc_notify_updated_slot =
-              [](HeapObject object, ObjectSlot slot, HeapObject target) {});
+      std::function<void(Tagged<HeapObject> object, ObjectSlot slot,
+                         Tagged<HeapObject> target)>
+          gc_notify_updated_slot = [](Tagged<HeapObject> object,
+                                      ObjectSlot slot,
+                                      Tagged<HeapObject> target) {});
 
   // Returns true if the function has old bytecode that could be flushed. This
   // function shouldn't access any flags as it is used by concurrent marker.
@@ -648,7 +650,8 @@ class SharedFunctionInfo
   inline uint16_t CompareExchangeAge(uint16_t expected_age, uint16_t new_age);
 
   // Bytecode aging
-  V8_EXPORT_PRIVATE static void EnsureOldForTesting(SharedFunctionInfo sfu);
+  V8_EXPORT_PRIVATE static void EnsureOldForTesting(
+      Tagged<SharedFunctionInfo> sfu);
 
   // Dispatched behavior.
   DECL_PRINTER(SharedFunctionInfo)
@@ -663,15 +666,15 @@ class SharedFunctionInfo
   // Iterate over all shared function infos in a given script.
   class ScriptIterator {
    public:
-    V8_EXPORT_PRIVATE ScriptIterator(Isolate* isolate, Script script);
+    V8_EXPORT_PRIVATE ScriptIterator(Isolate* isolate, Tagged<Script> script);
     explicit ScriptIterator(Handle<WeakFixedArray> shared_function_infos);
     ScriptIterator(const ScriptIterator&) = delete;
     ScriptIterator& operator=(const ScriptIterator&) = delete;
-    V8_EXPORT_PRIVATE SharedFunctionInfo Next();
+    V8_EXPORT_PRIVATE Tagged<SharedFunctionInfo> Next();
     int CurrentIndex() const { return index_ - 1; }
 
     // Reset the iterator to run on |script|.
-    void Reset(Isolate* isolate, Script script);
+    void Reset(Isolate* isolate, Tagged<Script> script);
 
    private:
     Handle<WeakFixedArray> shared_function_infos_;
@@ -702,7 +705,7 @@ class SharedFunctionInfo
                                    Isolate* isolate);
   // Removes the debug bytecode and restores the original bytecode to be
   // returned by following calls to GetActiveBytecodeArray.
-  static void UninstallDebugBytecode(SharedFunctionInfo shared,
+  static void UninstallDebugBytecode(Tagged<SharedFunctionInfo> shared,
                                      Isolate* isolate);
 
 #ifdef DEBUG
@@ -752,7 +755,7 @@ static_assert(SharedFunctionInfo::kSize == kStaticRootsSFISize);
 
 // Printing support.
 struct SourceCodeOf {
-  explicit SourceCodeOf(SharedFunctionInfo v, int max = -1)
+  explicit SourceCodeOf(Tagged<SharedFunctionInfo> v, int max = -1)
       : value(v), max_length(max) {}
   const SharedFunctionInfo value;
   int max_length;
@@ -763,8 +766,9 @@ struct SourceCodeOf {
 // the scope is retained.
 class V8_NODISCARD IsCompiledScope {
  public:
-  inline IsCompiledScope(const SharedFunctionInfo shared, Isolate* isolate);
-  inline IsCompiledScope(const SharedFunctionInfo shared,
+  inline IsCompiledScope(const Tagged<SharedFunctionInfo> shared,
+                         Isolate* isolate);
+  inline IsCompiledScope(const Tagged<SharedFunctionInfo> shared,
                          LocalIsolate* isolate);
   inline IsCompiledScope() : retain_code_(), is_compiled_(false) {}
 
diff --git a/src/objects/simd.cc b/src/objects/simd.cc
index dfb6914d6d5..9c2244558bc 100644
--- a/src/objects/simd.cc
+++ b/src/objects/simd.cc
@@ -359,7 +359,8 @@ Address ArrayIndexOfIncludes(Address array_start, uintptr_t array_len,
   }
 
   if constexpr (kind == ArrayIndexOfIncludesKind::DOUBLE) {
-    FixedDoubleArray fixed_array = FixedDoubleArray::cast(Object(array_start));
+    Tagged<FixedDoubleArray> fixed_array =
+        FixedDoubleArray::cast(Object(array_start));
     double* array = static_cast<double*>(
         fixed_array->RawField(FixedDoubleArray::OffsetOfElementAt(0))
             .ToVoidPtr());
@@ -394,7 +395,7 @@ Address ArrayIndexOfIncludes(Address array_start, uintptr_t array_len,
   }
 
   if constexpr (kind == ArrayIndexOfIncludesKind::OBJECTORSMI) {
-    FixedArray fixed_array = FixedArray::cast(Object(array_start));
+    Tagged<FixedArray> fixed_array = FixedArray::cast(Object(array_start));
     Tagged_t* array =
         static_cast<Tagged_t*>(fixed_array->data_start().ToVoidPtr());
 
diff --git a/src/objects/slots-inl.h b/src/objects/slots-inl.h
index c4700235d48..fee88062b35 100644
--- a/src/objects/slots-inl.h
+++ b/src/objects/slots-inl.h
@@ -26,9 +26,6 @@ namespace internal {
 // FullObjectSlot implementation.
 //
 
-FullObjectSlot::FullObjectSlot(Object* object)
-    : SlotBase(reinterpret_cast<Address>(&object->ptr_)) {}
-
 FullObjectSlot::FullObjectSlot(TaggedBase* object)
     : SlotBase(reinterpret_cast<Address>(&object->ptr_)) {}
 
@@ -40,13 +37,17 @@ bool FullObjectSlot::Relaxed_ContainsMapValue(Address raw_value) const {
   return base::AsAtomicPointer::Relaxed_Load(location()) == raw_value;
 }
 
-Object FullObjectSlot::operator*() const { return Object(*location()); }
+Tagged<Object> FullObjectSlot::operator*() const { return Object(*location()); }
 
-Object FullObjectSlot::load(PtrComprCageBase cage_base) const { return **this; }
+Tagged<Object> FullObjectSlot::load(PtrComprCageBase cage_base) const {
+  return **this;
+}
 
-void FullObjectSlot::store(Object value) const { *location() = value.ptr(); }
+void FullObjectSlot::store(Tagged<Object> value) const {
+  *location() = value.ptr();
+}
 
-void FullObjectSlot::store_map(Map map) const {
+void FullObjectSlot::store_map(Tagged<Map> map) const {
 #ifdef V8_MAP_PACKING
   *location() = MapWord::Pack(map.ptr());
 #else
@@ -54,7 +55,7 @@ void FullObjectSlot::store_map(Map map) const {
 #endif
 }
 
-Map FullObjectSlot::load_map() const {
+Tagged<Map> FullObjectSlot::load_map() const {
 #ifdef V8_MAP_PACKING
   return Map::unchecked_cast(Object(MapWord::Unpack(*location())));
 #else
@@ -62,37 +63,39 @@ Map FullObjectSlot::load_map() const {
 #endif
 }
 
-Object FullObjectSlot::Acquire_Load() const {
+Tagged<Object> FullObjectSlot::Acquire_Load() const {
   return Object(base::AsAtomicPointer::Acquire_Load(location()));
 }
 
-Object FullObjectSlot::Acquire_Load(PtrComprCageBase cage_base) const {
+Tagged<Object> FullObjectSlot::Acquire_Load(PtrComprCageBase cage_base) const {
   return Acquire_Load();
 }
 
-Object FullObjectSlot::Relaxed_Load() const {
+Tagged<Object> FullObjectSlot::Relaxed_Load() const {
   return Object(base::AsAtomicPointer::Relaxed_Load(location()));
 }
 
-Object FullObjectSlot::Relaxed_Load(PtrComprCageBase cage_base) const {
+Tagged<Object> FullObjectSlot::Relaxed_Load(PtrComprCageBase cage_base) const {
   return Relaxed_Load();
 }
 
-void FullObjectSlot::Relaxed_Store(Object value) const {
+void FullObjectSlot::Relaxed_Store(Tagged<Object> value) const {
   base::AsAtomicPointer::Relaxed_Store(location(), value.ptr());
 }
 
-void FullObjectSlot::Release_Store(Object value) const {
+void FullObjectSlot::Release_Store(Tagged<Object> value) const {
   base::AsAtomicPointer::Release_Store(location(), value.ptr());
 }
 
-Object FullObjectSlot::Relaxed_CompareAndSwap(Object old, Object target) const {
+Tagged<Object> FullObjectSlot::Relaxed_CompareAndSwap(
+    Tagged<Object> old, Tagged<Object> target) const {
   Address result = base::AsAtomicPointer::Relaxed_CompareAndSwap(
       location(), old.ptr(), target.ptr());
   return Object(result);
 }
 
-Object FullObjectSlot::Release_CompareAndSwap(Object old, Object target) const {
+Tagged<Object> FullObjectSlot::Release_CompareAndSwap(
+    Tagged<Object> old, Tagged<Object> target) const {
   Address result = base::AsAtomicPointer::Release_CompareAndSwap(
       location(), old.ptr(), target.ptr());
   return Object(result);
@@ -149,13 +152,13 @@ void FullHeapObjectSlot::store(HeapObjectReference value) const {
   *location() = value.ptr();
 }
 
-HeapObject FullHeapObjectSlot::ToHeapObject() const {
+Tagged<HeapObject> FullHeapObjectSlot::ToHeapObject() const {
   TData value = *location();
   DCHECK(HAS_STRONG_HEAP_OBJECT_TAG(value));
   return HeapObject::cast(Object(value));
 }
 
-void FullHeapObjectSlot::StoreHeapObject(HeapObject value) const {
+void FullHeapObjectSlot::StoreHeapObject(Tagged<HeapObject> value) const {
   *location() = value.ptr();
 }
 
@@ -290,11 +293,13 @@ ExternalPointerSlot::GetDefaultExternalPointerSpace(Isolate* isolate,
 }
 #endif  // V8_ENABLE_SANDBOX
 
-Object IndirectPointerSlot::load() const { return Relaxed_Load(); }
+Tagged<Object> IndirectPointerSlot::load() const { return Relaxed_Load(); }
 
-void IndirectPointerSlot::store(Code code) const { return Relaxed_Store(code); }
+void IndirectPointerSlot::store(Tagged<Code> code) const {
+  return Relaxed_Store(code);
+}
 
-Object IndirectPointerSlot::Relaxed_Load() const {
+Tagged<Object> IndirectPointerSlot::Relaxed_Load() const {
 #ifdef V8_CODE_POINTER_SANDBOXING
   IndirectPointerHandle handle = Relaxed_LoadHandle();
   // TODO(saelo) Maybe come up with a different CPT encoding scheme that returns
@@ -307,7 +312,7 @@ Object IndirectPointerSlot::Relaxed_Load() const {
 #endif  // V8_CODE_POINTER_SANDBOXING
 }
 
-Object IndirectPointerSlot::Acquire_Load() const {
+Tagged<Object> IndirectPointerSlot::Acquire_Load() const {
 #ifdef V8_CODE_POINTER_SANDBOXING
   IndirectPointerHandle handle = Acquire_LoadHandle();
   // TODO(saelo) Maybe come up with a different CPT encoding scheme that returns
@@ -320,22 +325,22 @@ Object IndirectPointerSlot::Acquire_Load() const {
 #endif  // V8_CODE_POINTER_SANDBOXING
 }
 
-void IndirectPointerSlot::Relaxed_Store(Code value) const {
+void IndirectPointerSlot::Relaxed_Store(Tagged<Code> value) const {
 #ifdef V8_CODE_POINTER_SANDBOXING
   // The Code objects owns its entry in the CodePointerTable, so here we only
   // need to copy the handle.
   IndirectPointerHandle handle =
-      value.ReadField<IndirectPointerHandle>(Code::kInstructionStartOffset);
+      value->ReadField<IndirectPointerHandle>(Code::kInstructionStartOffset);
   Relaxed_StoreHandle(handle);
 #else
   UNREACHABLE();
 #endif  // V8_CODE_POINTER_SANDBOXING
 }
 
-void IndirectPointerSlot::Release_Store(Code value) const {
+void IndirectPointerSlot::Release_Store(Tagged<Code> value) const {
 #ifdef V8_CODE_POINTER_SANDBOXING
   IndirectPointerHandle handle =
-      value.ReadField<IndirectPointerHandle>(Code::kInstructionStartOffset);
+      value->ReadField<IndirectPointerHandle>(Code::kInstructionStartOffset);
   Release_StoreHandle(handle);
 #else
   UNREACHABLE();
@@ -373,7 +378,8 @@ inline void CopyTagged(Address dst, const Address src, size_t num_tagged) {
 }
 
 // Sets |counter| number of kTaggedSize-sized values starting at |start| slot.
-inline void MemsetTagged(Tagged_t* start, Object value, size_t counter) {
+inline void MemsetTagged(Tagged_t* start, Tagged<Object> value,
+                         size_t counter) {
 #ifdef V8_COMPRESS_POINTERS
   // CompressAny since many callers pass values which are not valid objects.
   Tagged_t raw_value = V8HeapCompressionScheme::CompressAny(value.ptr());
@@ -386,14 +392,15 @@ inline void MemsetTagged(Tagged_t* start, Object value, size_t counter) {
 
 // Sets |counter| number of kTaggedSize-sized values starting at |start| slot.
 template <typename T>
-inline void MemsetTagged(SlotBase<T, Tagged_t> start, Object value,
+inline void MemsetTagged(SlotBase<T, Tagged_t> start, Tagged<Object> value,
                          size_t counter) {
   MemsetTagged(start.location(), value, counter);
 }
 
 // Sets |counter| number of kSystemPointerSize-sized values starting at |start|
 // slot.
-inline void MemsetPointer(FullObjectSlot start, Object value, size_t counter) {
+inline void MemsetPointer(FullObjectSlot start, Tagged<Object> value,
+                          size_t counter) {
   MemsetPointer(start.location(), value.ptr(), counter);
 }
 
diff --git a/src/objects/slots.h b/src/objects/slots.h
index 4aa30b5dd2c..867b610c6cf 100644
--- a/src/objects/slots.h
+++ b/src/objects/slots.h
@@ -93,7 +93,7 @@ class SlotBase {
 // The slot's contents can be read and written using operator* and store().
 class FullObjectSlot : public SlotBase<FullObjectSlot, Address> {
  public:
-  using TObject = Object;
+  using TObject = Tagged<Object>;
   using THeapObjectSlot = FullHeapObjectSlot;
 
   // Tagged value stored in this slot is guaranteed to never be a weak pointer.
@@ -103,7 +103,6 @@ class FullObjectSlot : public SlotBase<FullObjectSlot, Address> {
   explicit FullObjectSlot(Address ptr) : SlotBase(ptr) {}
   explicit FullObjectSlot(const Address* ptr)
       : SlotBase(reinterpret_cast<Address>(ptr)) {}
-  inline explicit FullObjectSlot(Object* object);
   inline explicit FullObjectSlot(TaggedBase* object);
   template <typename T>
   explicit FullObjectSlot(SlotBase<T, TData, kSlotDataAlignment> slot)
@@ -114,21 +113,23 @@ class FullObjectSlot : public SlotBase<FullObjectSlot, Address> {
   inline bool contains_map_value(Address raw_value) const;
   inline bool Relaxed_ContainsMapValue(Address raw_value) const;
 
-  inline Object operator*() const;
-  inline Object load(PtrComprCageBase cage_base) const;
-  inline void store(Object value) const;
-  inline void store_map(Map map) const;
-
-  inline Map load_map() const;
-
-  inline Object Acquire_Load() const;
-  inline Object Acquire_Load(PtrComprCageBase cage_base) const;
-  inline Object Relaxed_Load() const;
-  inline Object Relaxed_Load(PtrComprCageBase cage_base) const;
-  inline void Relaxed_Store(Object value) const;
-  inline void Release_Store(Object value) const;
-  inline Object Relaxed_CompareAndSwap(Object old, Object target) const;
-  inline Object Release_CompareAndSwap(Object old, Object target) const;
+  inline Tagged<Object> operator*() const;
+  inline Tagged<Object> load(PtrComprCageBase cage_base) const;
+  inline void store(Tagged<Object> value) const;
+  inline void store_map(Tagged<Map> map) const;
+
+  inline Tagged<Map> load_map() const;
+
+  inline Tagged<Object> Acquire_Load() const;
+  inline Tagged<Object> Acquire_Load(PtrComprCageBase cage_base) const;
+  inline Tagged<Object> Relaxed_Load() const;
+  inline Tagged<Object> Relaxed_Load(PtrComprCageBase cage_base) const;
+  inline void Relaxed_Store(Tagged<Object> value) const;
+  inline void Release_Store(Tagged<Object> value) const;
+  inline Tagged<Object> Relaxed_CompareAndSwap(Tagged<Object> old,
+                                               Tagged<Object> target) const;
+  inline Tagged<Object> Release_CompareAndSwap(Tagged<Object> old,
+                                               Tagged<Object> target) const;
 };
 
 // A FullMaybeObjectSlot instance describes a kSystemPointerSize-sized field
@@ -146,7 +147,7 @@ class FullMaybeObjectSlot
 
   FullMaybeObjectSlot() : SlotBase(kNullAddress) {}
   explicit FullMaybeObjectSlot(Address ptr) : SlotBase(ptr) {}
-  explicit FullMaybeObjectSlot(Object* ptr)
+  explicit FullMaybeObjectSlot(TaggedBase* ptr)
       : SlotBase(reinterpret_cast<Address>(ptr)) {}
   explicit FullMaybeObjectSlot(MaybeObject* ptr)
       : SlotBase(reinterpret_cast<Address>(ptr)) {}
@@ -175,7 +176,7 @@ class FullHeapObjectSlot : public SlotBase<FullHeapObjectSlot, Address> {
  public:
   FullHeapObjectSlot() : SlotBase(kNullAddress) {}
   explicit FullHeapObjectSlot(Address ptr) : SlotBase(ptr) {}
-  explicit FullHeapObjectSlot(Object* ptr)
+  explicit FullHeapObjectSlot(TaggedBase* ptr)
       : SlotBase(reinterpret_cast<Address>(ptr)) {}
   template <typename T>
   explicit FullHeapObjectSlot(SlotBase<T, TData, kSlotDataAlignment> slot)
@@ -185,9 +186,9 @@ class FullHeapObjectSlot : public SlotBase<FullHeapObjectSlot, Address> {
   inline HeapObjectReference load(PtrComprCageBase cage_base) const;
   inline void store(HeapObjectReference value) const;
 
-  inline HeapObject ToHeapObject() const;
+  inline Tagged<HeapObject> ToHeapObject() const;
 
-  inline void StoreHeapObject(HeapObject value) const;
+  inline void StoreHeapObject(Tagged<HeapObject> value) const;
 };
 
 // TODO(ishell, v8:8875): When pointer compression is enabled the [u]intptr_t
@@ -276,10 +277,10 @@ class OffHeapFullObjectSlot : public FullObjectSlot {
   explicit OffHeapFullObjectSlot(Address ptr) : FullObjectSlot(ptr) {}
   explicit OffHeapFullObjectSlot(const Address* ptr) : FullObjectSlot(ptr) {}
 
-  inline Object operator*() const = delete;
+  inline Tagged<Object> operator*() const = delete;
 
   using FullObjectSlot::Relaxed_Load;
-  inline Object Relaxed_Load() const = delete;
+  inline Tagged<Object> Relaxed_Load() const = delete;
 };
 
 // An ExternalPointerSlot instance describes a kExternalPointerSlotSize-sized
@@ -360,16 +361,16 @@ class IndirectPointerSlot
   // see below) can be stored into an IndirectPointerSlot, these slots can be
   // empty (containing kNullIndirectPointerHandle), in which case load() will
   // return Smi::zero().
-  inline Object load() const;
+  inline Tagged<Object> load() const;
   // TODO(saelo) currently, Code objects are the only objects that can be
   // referenced through an indirect pointer. Once we have more, we should
   // generalize this to take ExternalObject or another appropriate base class.
-  inline void store(Code value) const;
+  inline void store(Tagged<Code> value) const;
 
-  inline Object Relaxed_Load() const;
-  inline Object Acquire_Load() const;
-  inline void Relaxed_Store(Code value) const;
-  inline void Release_Store(Code value) const;
+  inline Tagged<Object> Relaxed_Load() const;
+  inline Tagged<Object> Acquire_Load() const;
+  inline void Relaxed_Store(Tagged<Code> value) const;
+  inline void Release_Store(Tagged<Code> value) const;
 
   inline IndirectPointerHandle Relaxed_LoadHandle() const;
   inline IndirectPointerHandle Acquire_LoadHandle() const;
diff --git a/src/objects/smi.h b/src/objects/smi.h
index f2e1d95c322..14738fd595a 100644
--- a/src/objects/smi.h
+++ b/src/objects/smi.h
@@ -39,7 +39,7 @@ class Smi : public Object {
   }
 
   // Convert a Smi object to an int.
-  static inline constexpr int ToInt(const Object object) {
+  static inline constexpr int ToInt(const Tagged<Object> object) {
     return Tagged<Smi>(object.ptr()).value();
   }
 
diff --git a/src/objects/source-text-module.cc b/src/objects/source-text-module.cc
index 0889673bdb4..6d391b2bebc 100644
--- a/src/objects/source-text-module.cc
+++ b/src/objects/source-text-module.cc
@@ -86,7 +86,7 @@ struct SourceTextModule::AsyncEvaluatingOrdinalCompare {
   }
 };
 
-SharedFunctionInfo SourceTextModule::GetSharedFunctionInfo() const {
+Tagged<SharedFunctionInfo> SourceTextModule::GetSharedFunctionInfo() const {
   DisallowGarbageCollection no_gc;
   switch (status()) {
     case kUnlinked:
@@ -105,7 +105,7 @@ SharedFunctionInfo SourceTextModule::GetSharedFunctionInfo() const {
   UNREACHABLE();
 }
 
-Script SourceTextModule::GetScript() const {
+Tagged<Script> SourceTextModule::GetScript() const {
   DisallowGarbageCollection no_gc;
   return Script::cast(GetSharedFunctionInfo()->script());
 }
@@ -147,9 +147,9 @@ void SourceTextModule::CreateExport(Isolate* isolate,
   module->set_exports(*exports);
 }
 
-Cell SourceTextModule::GetCell(int cell_index) {
+Tagged<Cell> SourceTextModule::GetCell(int cell_index) {
   DisallowGarbageCollection no_gc;
-  Object cell;
+  Tagged<Object> cell;
   switch (SourceTextModuleDescriptor::GetCellIndexKind(cell_index)) {
     case SourceTextModuleDescriptor::kImport:
       cell = regular_imports()->get(ImportIndex(cell_index));
@@ -577,7 +577,7 @@ void SourceTextModule::FetchStarExports(Isolate* isolate,
     Handle<ObjectHashTable> requested_exports(requested_module->exports(),
                                               isolate);
     for (InternalIndex index : requested_exports->IterateEntries()) {
-      Object key;
+      Tagged<Object> key;
       if (!requested_exports->ToKey(roots, index, &key)) continue;
       Handle<String> name(String::cast(key), isolate);
 
@@ -686,7 +686,7 @@ MaybeHandle<JSObject> SourceTextModule::GetImportMeta(
 bool SourceTextModule::MaybeHandleEvaluationException(
     Isolate* isolate, ZoneForwardList<Handle<SourceTextModule>>* stack) {
   DisallowGarbageCollection no_gc;
-  Object pending_exception = isolate->pending_exception();
+  Tagged<Object> pending_exception = isolate->pending_exception();
   if (isolate->is_catchable_by_javascript(pending_exception)) {
     //  a. For each Cyclic Module Record m in stack, do
     for (Handle<SourceTextModule>& descendant : *stack) {
@@ -1264,13 +1264,13 @@ void SourceTextModule::InnerGetStalledTopLevelAwaitModule(
     return;
   }
   // The module isn't what we are looking for, continue looking in the graph.
-  FixedArray requested = requested_modules();
+  Tagged<FixedArray> requested = requested_modules();
   int length = requested->length();
   for (int i = 0; i < length; ++i) {
-    Module requested_module = Module::cast(requested->get(i));
+    Tagged<Module> requested_module = Module::cast(requested->get(i));
     if (IsSourceTextModule(requested_module) &&
         visited->insert(handle(requested_module, isolate)).second) {
-      SourceTextModule source_text_module =
+      Tagged<SourceTextModule> source_text_module =
           SourceTextModule::cast(requested_module);
       source_text_module->InnerGetStalledTopLevelAwaitModule(isolate, visited,
                                                              result);
diff --git a/src/objects/source-text-module.h b/src/objects/source-text-module.h
index 5c8e1b1b101..55754c0a7d4 100644
--- a/src/objects/source-text-module.h
+++ b/src/objects/source-text-module.h
@@ -33,18 +33,18 @@ class SourceTextModule
 
   // The shared function info in case {status} is not kEvaluating, kEvaluated or
   // kErrored.
-  SharedFunctionInfo GetSharedFunctionInfo() const;
+  Tagged<SharedFunctionInfo> GetSharedFunctionInfo() const;
 
-  Script GetScript() const;
+  Tagged<Script> GetScript() const;
 
   // Whether or not this module is an async module. Set during module creation
   // and does not change afterwards.
   DECL_BOOLEAN_ACCESSORS(async)
 
   // Get the SourceTextModuleInfo associated with the code.
-  inline SourceTextModuleInfo info() const;
+  inline Tagged<SourceTextModuleInfo> info() const;
 
-  Cell GetCell(int cell_index);
+  Tagged<Cell> GetCell(int cell_index);
   static Handle<Object> LoadVariable(Isolate* isolate,
                                      Handle<SourceTextModule> module,
                                      int cell_index);
@@ -241,20 +241,20 @@ class SourceTextModuleInfo : public FixedArray {
   static Handle<SourceTextModuleInfo> New(IsolateT* isolate, Zone* zone,
                                           SourceTextModuleDescriptor* descr);
 
-  inline FixedArray module_requests() const;
-  inline FixedArray special_exports() const;
-  inline FixedArray regular_exports() const;
-  inline FixedArray regular_imports() const;
-  inline FixedArray namespace_imports() const;
+  inline Tagged<FixedArray> module_requests() const;
+  inline Tagged<FixedArray> special_exports() const;
+  inline Tagged<FixedArray> regular_exports() const;
+  inline Tagged<FixedArray> regular_imports() const;
+  inline Tagged<FixedArray> namespace_imports() const;
 
   // Accessors for [regular_exports].
   int RegularExportCount() const;
-  String RegularExportLocalName(int i) const;
+  Tagged<String> RegularExportLocalName(int i) const;
   int RegularExportCellIndex(int i) const;
-  FixedArray RegularExportExportNames(int i) const;
+  Tagged<FixedArray> RegularExportExportNames(int i) const;
 
 #ifdef DEBUG
-  inline bool Equals(SourceTextModuleInfo other) const;
+  inline bool Equals(Tagged<SourceTextModuleInfo> other) const;
 #endif
 
  private:
diff --git a/src/objects/string-comparator.cc b/src/objects/string-comparator.cc
index a56dd87dc56..0ca4debfb87 100644
--- a/src/objects/string-comparator.cc
+++ b/src/objects/string-comparator.cc
@@ -10,8 +10,10 @@ namespace v8 {
 namespace internal {
 
 void StringComparator::State::Init(
-    String string, const SharedStringAccessGuardIfNeeded& access_guard) {
-  ConsString cons_string = String::VisitFlat(this, string, 0, access_guard);
+    Tagged<String> string,
+    const SharedStringAccessGuardIfNeeded& access_guard) {
+  Tagged<ConsString> cons_string =
+      String::VisitFlat(this, string, 0, access_guard);
   iter_.Reset(cons_string);
   if (!cons_string.is_null()) {
     int offset;
@@ -38,14 +40,14 @@ void StringComparator::State::Advance(
   }
   // Advance state.
   int offset;
-  String next = iter_.Next(&offset);
+  Tagged<String> next = iter_.Next(&offset);
   DCHECK_EQ(0, offset);
   DCHECK(!next.is_null());
   String::VisitFlat(this, next, 0, access_guard);
 }
 
 bool StringComparator::Equals(
-    String string_1, String string_2,
+    Tagged<String> string_1, Tagged<String> string_2,
     const SharedStringAccessGuardIfNeeded& access_guard) {
   int length = string_1->length();
   state_1_.Init(string_1, access_guard);
diff --git a/src/objects/string-comparator.h b/src/objects/string-comparator.h
index 8f3420945a4..953afafe6e2 100644
--- a/src/objects/string-comparator.h
+++ b/src/objects/string-comparator.h
@@ -20,7 +20,7 @@ class StringComparator {
     State(const State&) = delete;
     State& operator=(const State&) = delete;
 
-    void Init(String string,
+    void Init(Tagged<String> string,
               const SharedStringAccessGuardIfNeeded& access_guard);
 
     inline void VisitOneByteString(const uint8_t* chars, int length) {
@@ -59,7 +59,7 @@ class StringComparator {
     return CompareCharsEqual(a, b, to_check);
   }
 
-  bool Equals(String string_1, String string_2,
+  bool Equals(Tagged<String> string_1, Tagged<String> string_2,
               const SharedStringAccessGuardIfNeeded& access_guard);
 
  private:
diff --git a/src/objects/string-forwarding-table-inl.h b/src/objects/string-forwarding-table-inl.h
index 67a0d92272d..7da8dc15680 100644
--- a/src/objects/string-forwarding-table-inl.h
+++ b/src/objects/string-forwarding-table-inl.h
@@ -21,11 +21,11 @@ namespace internal {
 
 class StringForwardingTable::Record final {
  public:
-  String original_string(PtrComprCageBase cage_base) const {
+  Tagged<String> original_string(PtrComprCageBase cage_base) const {
     return String::cast(OriginalStringObject(cage_base));
   }
 
-  String forward_string(PtrComprCageBase cage_base) const {
+  Tagged<String> forward_string(PtrComprCageBase cage_base) const {
     return String::cast(ForwardStringObjectOrHash(cage_base));
   }
 
@@ -33,11 +33,11 @@ class StringForwardingTable::Record final {
   inline v8::String::ExternalStringResourceBase* external_resource(
       bool* is_one_byte) const;
 
-  Object OriginalStringObject(PtrComprCageBase cage_base) const {
+  Tagged<Object> OriginalStringObject(PtrComprCageBase cage_base) const {
     return OriginalStringSlot().Acquire_Load(cage_base);
   }
 
-  Object ForwardStringObjectOrHash(PtrComprCageBase cage_base) const {
+  Tagged<Object> ForwardStringObjectOrHash(PtrComprCageBase cage_base) const {
     return ForwardStringOrHashSlot().Acquire_Load(cage_base);
   }
 
@@ -45,11 +45,11 @@ class StringForwardingTable::Record final {
     return base::AsAtomicPointer::Acquire_Load(&external_resource_);
   }
 
-  void set_original_string(Object object) {
+  void set_original_string(Tagged<Object> object) {
     OriginalStringSlot().Release_Store(object);
   }
 
-  void set_forward_string(Object object) {
+  void set_forward_string(Tagged<Object> object) {
     ForwardStringOrHashSlot().Release_Store(object);
   }
 
@@ -60,8 +60,8 @@ class StringForwardingTable::Record final {
     base::AsAtomicPointer::Release_Store(&external_resource_, address);
   }
 
-  inline void SetInternalized(String string, String forward_to);
-  inline void SetExternal(String string,
+  inline void SetInternalized(Tagged<String> string, Tagged<String> forward_to);
+  inline void SetExternal(Tagged<String> string,
                           v8::String::ExternalStringResourceBase*,
                           bool is_one_byte, uint32_t raw_hash);
   inline bool TryUpdateExternalResource(
@@ -71,7 +71,7 @@ class StringForwardingTable::Record final {
   // Dispose the external resource if the original string has transitioned
   // to an external string and the resource used for the transition is different
   // than the one in the record.
-  inline void DisposeUnusedExternalResource(String original_string);
+  inline void DisposeUnusedExternalResource(Tagged<String> original_string);
 
  private:
   OffHeapObjectSlot OriginalStringSlot() const {
@@ -127,7 +127,7 @@ class StringForwardingTable::Record final {
 
 uint32_t StringForwardingTable::Record::raw_hash(
     PtrComprCageBase cage_base) const {
-  Object hash_or_string = ForwardStringObjectOrHash(cage_base);
+  Tagged<Object> hash_or_string = ForwardStringObjectOrHash(cage_base);
   uint32_t raw_hash;
   if (IsHeapObject(hash_or_string)) {
     raw_hash = String::cast(hash_or_string)->RawHash();
@@ -166,15 +166,15 @@ void StringForwardingTable::Record::set_external_resource(
   set_external_resource(address);
 }
 
-void StringForwardingTable::Record::SetInternalized(String string,
-                                                    String forward_to) {
+void StringForwardingTable::Record::SetInternalized(Tagged<String> string,
+                                                    Tagged<String> forward_to) {
   set_original_string(string);
   set_forward_string(forward_to);
   set_external_resource(kNullExternalPointer);
 }
 
 void StringForwardingTable::Record::SetExternal(
-    String string, v8::String::ExternalStringResourceBase* resource,
+    Tagged<String> string, v8::String::ExternalStringResourceBase* resource,
     bool is_one_byte, uint32_t raw_hash) {
   set_original_string(string);
   set_raw_hash_if_empty(raw_hash);
@@ -208,9 +208,9 @@ void StringForwardingTable::Record::DisposeExternalResource() {
 }
 
 void StringForwardingTable::Record::DisposeUnusedExternalResource(
-    String original) {
+    Tagged<String> original) {
 #ifdef DEBUG
-  String stored_original =
+  Tagged<String> stored_original =
       original_string(GetIsolateFromWritableObject(original));
   if (IsThinString(stored_original)) {
     stored_original = ThinString::cast(stored_original)->actual();
diff --git a/src/objects/string-forwarding-table.cc b/src/objects/string-forwarding-table.cc
index 64e1f268df8..692b317d146 100644
--- a/src/objects/string-forwarding-table.cc
+++ b/src/objects/string-forwarding-table.cc
@@ -70,17 +70,17 @@ void StringForwardingTable::Block::UpdateAfterFullEvacuation(
 
 namespace {
 
-bool UpdateForwardedSlot(HeapObject object, OffHeapObjectSlot slot) {
+bool UpdateForwardedSlot(Tagged<HeapObject> object, OffHeapObjectSlot slot) {
   MapWord map_word = object->map_word(kRelaxedLoad);
   if (map_word.IsForwardingAddress()) {
-    HeapObject forwarded_object = map_word.ToForwardingAddress(object);
+    Tagged<HeapObject> forwarded_object = map_word.ToForwardingAddress(object);
     slot.Release_Store(forwarded_object);
     return true;
   }
   return false;
 }
 
-bool UpdateForwardedSlot(Object object, OffHeapObjectSlot slot) {
+bool UpdateForwardedSlot(Tagged<Object> object, OffHeapObjectSlot slot) {
   if (!IsHeapObject(object)) return false;
   return UpdateForwardedSlot(HeapObject::cast(object), slot);
 }
@@ -91,9 +91,9 @@ void StringForwardingTable::Block::UpdateAfterYoungEvacuation(
     PtrComprCageBase cage_base, int up_to_index) {
   for (int index = 0; index < up_to_index; ++index) {
     OffHeapObjectSlot slot = record(index)->OriginalStringSlot();
-    Object original = slot.Acquire_Load(cage_base);
+    Tagged<Object> original = slot.Acquire_Load(cage_base);
     if (!IsHeapObject(original)) continue;
-    HeapObject object = HeapObject::cast(original);
+    Tagged<HeapObject> object = HeapObject::cast(original);
     if (Heap::InFromPage(object)) {
       DCHECK(!object.InWritableSharedSpace());
       const bool was_forwarded = UpdateForwardedSlot(object, slot);
@@ -107,7 +107,8 @@ void StringForwardingTable::Block::UpdateAfterYoungEvacuation(
 // No need to update forwarded (internalized) strings as they are never
 // in young space.
 #ifdef DEBUG
-    Object forward = record(index)->ForwardStringObjectOrHash(cage_base);
+    Tagged<Object> forward =
+        record(index)->ForwardStringObjectOrHash(cage_base);
     if (IsHeapObject(forward)) {
       DCHECK(!Heap::InYoungGeneration(HeapObject::cast(forward)));
     }
@@ -119,13 +120,13 @@ void StringForwardingTable::Block::UpdateAfterFullEvacuation(
     PtrComprCageBase cage_base, int up_to_index) {
   for (int index = 0; index < up_to_index; ++index) {
     OffHeapObjectSlot original_slot = record(index)->OriginalStringSlot();
-    Object original = original_slot.Acquire_Load(cage_base);
+    Tagged<Object> original = original_slot.Acquire_Load(cage_base);
     if (!IsHeapObject(original)) continue;
     UpdateForwardedSlot(HeapObject::cast(original), original_slot);
     // During mark compact the forwarded (internalized) string may have been
     // evacuated.
     OffHeapObjectSlot forward_slot = record(index)->ForwardStringOrHashSlot();
-    Object forward = forward_slot.Acquire_Load(cage_base);
+    Tagged<Object> forward = forward_slot.Acquire_Load(cage_base);
     UpdateForwardedSlot(forward, forward_slot);
   }
 }
@@ -202,7 +203,8 @@ StringForwardingTable::BlockVector* StringForwardingTable::EnsureCapacity(
   return blocks;
 }
 
-int StringForwardingTable::AddForwardString(String string, String forward_to) {
+int StringForwardingTable::AddForwardString(Tagged<String> string,
+                                            Tagged<String> forward_to) {
   DCHECK_IMPLIES(!v8_flags.always_use_string_forwarding_table,
                  Object::InSharedHeap(string));
   DCHECK_IMPLIES(!v8_flags.always_use_string_forwarding_table,
@@ -217,7 +219,8 @@ int StringForwardingTable::AddForwardString(String string, String forward_to) {
   return index;
 }
 
-void StringForwardingTable::UpdateForwardString(int index, String forward_to) {
+void StringForwardingTable::UpdateForwardString(int index,
+                                                Tagged<String> forward_to) {
   CHECK_LT(index, size());
   uint32_t index_in_block;
   const uint32_t block_index = BlockForIndex(index, &index_in_block);
@@ -227,7 +230,7 @@ void StringForwardingTable::UpdateForwardString(int index, String forward_to) {
 }
 
 template <typename T>
-int StringForwardingTable::AddExternalResourceAndHash(String string,
+int StringForwardingTable::AddExternalResourceAndHash(Tagged<String> string,
                                                       T* resource,
                                                       uint32_t raw_hash) {
   constexpr bool is_one_byte =
@@ -247,11 +250,11 @@ int StringForwardingTable::AddExternalResourceAndHash(String string,
 }
 
 template EXPORT_TEMPLATE_DEFINE(V8_EXPORT_PRIVATE) int StringForwardingTable::
-    AddExternalResourceAndHash(String string,
+    AddExternalResourceAndHash(Tagged<String> string,
                                v8::String::ExternalOneByteStringResource*,
                                uint32_t raw_hash);
 template EXPORT_TEMPLATE_DEFINE(V8_EXPORT_PRIVATE) int StringForwardingTable::
-    AddExternalResourceAndHash(String string,
+    AddExternalResourceAndHash(Tagged<String> string,
                                v8::String::ExternalStringResource*,
                                uint32_t raw_hash);
 
@@ -276,8 +279,8 @@ template EXPORT_TEMPLATE_DEFINE(V8_EXPORT_PRIVATE) bool StringForwardingTable::
     TryUpdateExternalResource(int index,
                               v8::String::ExternalStringResource* resource);
 
-String StringForwardingTable::GetForwardString(PtrComprCageBase cage_base,
-                                               int index) const {
+Tagged<String> StringForwardingTable::GetForwardString(
+    PtrComprCageBase cage_base, int index) const {
   CHECK_LT(index, size());
   uint32_t index_in_block;
   const uint32_t block_index = BlockForIndex(index, &index_in_block);
diff --git a/src/objects/string-forwarding-table.h b/src/objects/string-forwarding-table.h
index a2c2093df78..f92120dd467 100644
--- a/src/objects/string-forwarding-table.h
+++ b/src/objects/string-forwarding-table.h
@@ -32,8 +32,8 @@ class StringForwardingTable {
       kBitsPerInt - base::bits::CountLeadingZeros32(kInitialBlockSize) - 1;
   // Initial capacity in the block vector.
   static constexpr int kInitialBlockVectorCapacity = 4;
-  static constexpr Smi unused_element() { return Smi::FromInt(0); }
-  static constexpr Smi deleted_element() { return Smi::FromInt(1); }
+  static constexpr Tagged<Smi> unused_element() { return Smi::FromInt(0); }
+  static constexpr Tagged<Smi> deleted_element() { return Smi::FromInt(1); }
 
   explicit StringForwardingTable(Isolate* isolate);
   ~StringForwardingTable();
@@ -41,18 +41,19 @@ class StringForwardingTable {
   inline int size() const;
   inline bool empty() const;
   // Returns the index of the added record.
-  int AddForwardString(String string, String forward_to);
+  int AddForwardString(Tagged<String> string, Tagged<String> forward_to);
   template <typename T>
   EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE)
-  int AddExternalResourceAndHash(String string, T* resource, uint32_t raw_hash);
-  void UpdateForwardString(int index, String forward_to);
+  int AddExternalResourceAndHash(Tagged<String> string, T* resource,
+                                 uint32_t raw_hash);
+  void UpdateForwardString(int index, Tagged<String> forward_to);
   // Returns true when the resource was set. When an external resource is
   // already set for the record, false is returned and the resource not stored.
   // The caller is responsible for disposing the resource.
   template <typename T>
   EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE)
   bool TryUpdateExternalResource(int index, T* resource);
-  String GetForwardString(PtrComprCageBase cage_base, int index) const;
+  Tagged<String> GetForwardString(PtrComprCageBase cage_base, int index) const;
   static Address GetForwardStringAddress(Isolate* isolate, int index);
   V8_EXPORT_PRIVATE uint32_t GetRawHash(PtrComprCageBase cage_base,
                                         int index) const;
diff --git a/src/objects/string-inl.h b/src/objects/string-inl.h
index 97ac65dd717..cf47e6fbda7 100644
--- a/src/objects/string-inl.h
+++ b/src/objects/string-inl.h
@@ -47,7 +47,7 @@ class V8_NODISCARD SharedStringAccessGuardIfNeeded {
   }
 
   // Slow version which gets the isolate from the String.
-  explicit SharedStringAccessGuardIfNeeded(String str) {
+  explicit SharedStringAccessGuardIfNeeded(Tagged<String> str) {
     Isolate* isolate = GetIsolateIfNeeded(str);
     if (isolate != nullptr) {
       mutex_guard.emplace(isolate->internalized_string_access());
@@ -58,11 +58,11 @@ class V8_NODISCARD SharedStringAccessGuardIfNeeded {
     return SharedStringAccessGuardIfNeeded();
   }
 
-  static bool IsNeeded(String str, LocalIsolate* local_isolate) {
+  static bool IsNeeded(Tagged<String> str, LocalIsolate* local_isolate) {
     return IsNeeded(local_isolate) && IsNeeded(str, false);
   }
 
-  static bool IsNeeded(String str, bool check_local_heap = true) {
+  static bool IsNeeded(Tagged<String> str, bool check_local_heap = true) {
     if (check_local_heap) {
       LocalHeap* local_heap = LocalHeap::Current();
       if (!local_heap || local_heap->is_main_thread()) {
@@ -94,7 +94,7 @@ class V8_NODISCARD SharedStringAccessGuardIfNeeded {
   }
 
   // Returns the Isolate from the String if we need it for the lock.
-  static Isolate* GetIsolateIfNeeded(String str) {
+  static Isolate* GetIsolateIfNeeded(Tagged<String> str) {
     if (!IsNeeded(str)) return nullptr;
 
     Isolate* isolate;
@@ -133,11 +133,6 @@ TQ_OBJECT_CONSTRUCTORS_IMPL(ExternalTwoByteString)
 
 static_assert(kTaggedCanConvertToRawObjects);
 
-StringShape::StringShape(const String str) : StringShape(Tagged<String>(str)) {}
-StringShape::StringShape(const String str, PtrComprCageBase cage_base)
-    : StringShape(Tagged<String>(str), cage_base) {}
-StringShape::StringShape(Map map) : StringShape(Tagged<Map>(map)) {}
-
 StringShape::StringShape(const Tagged<String> str)
     : type_(str->map(kAcquireLoad)->instance_type()) {
   set_valid();
@@ -291,7 +286,7 @@ inline TResult StringShape::DispatchToSpecificTypeWithoutCast(TArgs&&... args) {
   V(ThinString)
 
 template <typename TDispatcher, typename TResult, typename... TArgs>
-inline TResult StringShape::DispatchToSpecificType(String str,
+inline TResult StringShape::DispatchToSpecificType(Tagged<String> str,
                                                    TArgs&&... args) {
   class CastingDispatcher : public AllStatic {
    public:
@@ -302,7 +297,8 @@ inline TResult StringShape::DispatchToSpecificType(String str,
   }
     STRING_CLASS_TYPES(DEFINE_METHOD)
 #undef DEFINE_METHOD
-    static inline TResult HandleInvalidString(String str, TArgs&&... args) {
+    static inline TResult HandleInvalidString(Tagged<String> str,
+                                              TArgs&&... args) {
       return TDispatcher::HandleInvalidString(str,
                                               std::forward<TArgs>(args)...);
     }
@@ -375,8 +371,8 @@ class SequentialStringKey final : public StringTableKey {
         convert_(convert) {}
 
   template <typename IsolateT>
-  bool IsMatch(IsolateT* isolate, String s) {
-    return s.IsEqualTo<String::EqualityType::kNoLengthCheck>(chars_, isolate);
+  bool IsMatch(IsolateT* isolate, Tagged<String> s) {
+    return s->IsEqualTo<String::EqualityType::kNoLengthCheck>(chars_, isolate);
   }
 
   template <typename IsolateT>
@@ -441,11 +437,11 @@ class SeqSubStringKey final : public StringTableKey {
 #pragma warning(pop)
 #endif
 
-  bool IsMatch(Isolate* isolate, String string) {
+  bool IsMatch(Isolate* isolate, Tagged<String> string) {
     DCHECK(!SharedStringAccessGuardIfNeeded::IsNeeded(string));
     DCHECK(!SharedStringAccessGuardIfNeeded::IsNeeded(*string_));
     DisallowGarbageCollection no_gc;
-    return string.IsEqualTo<String::EqualityType::kNoLengthCheck>(
+    return string->IsEqualTo<String::EqualityType::kNoLengthCheck>(
         base::Vector<const Char>(string_->GetChars(no_gc) + from_, length()),
         isolate);
   }
@@ -485,7 +481,7 @@ class SeqSubStringKey final : public StringTableKey {
 using SeqOneByteSubStringKey = SeqSubStringKey<SeqOneByteString>;
 using SeqTwoByteSubStringKey = SeqSubStringKey<SeqTwoByteString>;
 
-bool String::Equals(String other) const {
+bool String::Equals(Tagged<String> other) const {
   if (other == *this) return true;
   if (IsInternalizedString(*this) && IsInternalizedString(other)) {
     return false;
@@ -571,7 +567,7 @@ bool String::IsEqualToImpl(
 
       case kSlicedStringTag | kOneByteStringTag:
       case kSlicedStringTag | kTwoByteStringTag: {
-        SlicedString slicedString = SlicedString::cast(string);
+        Tagged<SlicedString> slicedString = SlicedString::cast(string);
         slice_offset += slicedString->offset();
         string = slicedString->parent(cage_base);
         continue;
@@ -602,7 +598,8 @@ bool String::IsEqualToImpl(
 // static
 template <typename Char>
 bool String::IsConsStringEqualToImpl(
-    ConsString string, base::Vector<const Char> str, PtrComprCageBase cage_base,
+    Tagged<ConsString> string, base::Vector<const Char> str,
+    PtrComprCageBase cage_base,
     const SharedStringAccessGuardIfNeeded& access_guard) {
   // Already checked the len in IsEqualToImpl. Check GE rather than EQ in case
   // this is a prefix check.
@@ -611,7 +608,7 @@ bool String::IsConsStringEqualToImpl(
   ConsStringIterator iter(ConsString::cast(string));
   base::Vector<const Char> remaining_str = str;
   int offset;
-  for (String segment = iter.Next(&offset); !segment.is_null();
+  for (Tagged<String> segment = iter.Next(&offset); !segment.is_null();
        segment = iter.Next(&offset)) {
     // We create the iterator without an offset, so we should never have a
     // per-segment offset.
@@ -620,8 +617,8 @@ bool String::IsConsStringEqualToImpl(
     // remaining string.
     size_t len = std::min<size_t>(segment->length(), remaining_str.size());
     base::Vector<const Char> sub_str = remaining_str.SubVector(0, len);
-    if (!segment.IsEqualToImpl<EqualityType::kNoLengthCheck>(sub_str, cage_base,
-                                                             access_guard)) {
+    if (!segment->IsEqualToImpl<EqualityType::kNoLengthCheck>(
+            sub_str, cage_base, access_guard)) {
       return false;
     }
     remaining_str += len;
@@ -652,9 +649,10 @@ const Char* String::GetDirectStringChars(
     const SharedStringAccessGuardIfNeeded& access_guard) const {
   DCHECK(StringShape(*this).IsDirect());
   return StringShape(*this, cage_base).IsExternal()
-             ? CharTraits<Char>::ExternalString::cast(*this).GetChars(cage_base)
-             : CharTraits<Char>::String::cast(*this).GetChars(no_gc,
-                                                              access_guard);
+             ? CharTraits<Char>::ExternalString::cast(*this)->GetChars(
+                   cage_base)
+             : CharTraits<Char>::String::cast(*this)->GetChars(no_gc,
+                                                               access_guard);
 }
 
 // static
@@ -662,7 +660,7 @@ Handle<String> String::Flatten(Isolate* isolate, Handle<String> string,
                                AllocationType allocation) {
   DisallowGarbageCollection no_gc;  // Unhandlified code.
   PtrComprCageBase cage_base(isolate);
-  String s = *string;
+  Tagged<String> s = *string;
   StringShape shape(s, cage_base);
 
   // Shortcut already-flat strings.
@@ -670,7 +668,7 @@ Handle<String> String::Flatten(Isolate* isolate, Handle<String> string,
 
   if (shape.IsCons()) {
     DCHECK(!Object::InSharedHeap(s));
-    ConsString cons = ConsString::cast(s);
+    Tagged<ConsString> cons = ConsString::cast(s);
     if (!cons->IsFlat(isolate)) {
       AllowGarbageCollection yes_gc;
       return SlowFlatten(isolate, handle(cons, isolate), allocation);
@@ -698,7 +696,7 @@ Handle<String> String::Flatten(LocalIsolate* isolate, Handle<String> string,
 // static
 base::Optional<String::FlatContent> String::TryGetFlatContentFromDirectString(
     PtrComprCageBase cage_base, const DisallowGarbageCollection& no_gc,
-    String string, int offset, int length,
+    Tagged<String> string, int offset, int length,
     const SharedStringAccessGuardIfNeeded& access_guard) {
   DCHECK_GE(offset, 0);
   DCHECK_GE(length, 0);
@@ -844,7 +842,7 @@ uint16_t String::GetImpl(
     STRING_CLASS_TYPES(DEFINE_METHOD)
 #undef DEFINE_METHOD
     static inline uint16_t HandleInvalidString(
-        String str, int index, PtrComprCageBase cage_base,
+        Tagged<String> str, int index, PtrComprCageBase cage_base,
         const SharedStringAccessGuardIfNeeded& access_guard) {
       UNREACHABLE();
     }
@@ -879,7 +877,7 @@ bool String::IsShared(PtrComprCageBase cage_base) const {
   return result;
 }
 
-String String::GetUnderlying() const {
+Tagged<String> String::GetUnderlying() const {
   // Giving direct access to underlying string only makes sense if the
   // wrapping string is already flattened.
   DCHECK(IsFlat());
@@ -893,15 +891,15 @@ String String::GetUnderlying() const {
 }
 
 template <class Visitor>
-ConsString String::VisitFlat(Visitor* visitor, Tagged<String> string,
-                             const int offset) {
+Tagged<ConsString> String::VisitFlat(Visitor* visitor, Tagged<String> string,
+                                     const int offset) {
   DCHECK(!SharedStringAccessGuardIfNeeded::IsNeeded(string));
   return VisitFlat(visitor, string, offset,
                    SharedStringAccessGuardIfNeeded::NotNeeded());
 }
 
 template <class Visitor>
-ConsString String::VisitFlat(
+Tagged<ConsString> String::VisitFlat(
     Visitor* visitor, Tagged<String> string, const int offset,
     const SharedStringAccessGuardIfNeeded& access_guard) {
   DisallowGarbageCollection no_gc;
@@ -943,7 +941,7 @@ ConsString String::VisitFlat(
 
       case kSlicedStringTag | kOneByteStringTag:
       case kSlicedStringTag | kTwoByteStringTag: {
-        SlicedString slicedString = SlicedString::cast(string);
+        Tagged<SlicedString> slicedString = SlicedString::cast(string);
         slice_offset += slicedString->offset();
         string = slicedString->parent(cage_base);
         continue;
@@ -1101,16 +1099,16 @@ bool SeqTwoByteString::IsCompatibleMap(Tagged<Map> map, ReadOnlyRoots roots) {
          map == roots.shared_seq_two_byte_string_map();
 }
 
-void SlicedString::set_parent(String parent, WriteBarrierMode mode) {
+void SlicedString::set_parent(Tagged<String> parent, WriteBarrierMode mode) {
   DCHECK(IsSeqString(parent) || IsExternalString(parent));
   TorqueGeneratedSlicedString<SlicedString, Super>::set_parent(parent, mode);
 }
 
-Object ConsString::unchecked_first() const {
+Tagged<Object> ConsString::unchecked_first() const {
   return TaggedField<Object, kFirstOffset>::load(*this);
 }
 
-Object ConsString::unchecked_second() const {
+Tagged<Object> ConsString::unchecked_second() const {
   return RELAXED_READ_FIELD(*this, kSecondOffset);
 }
 
@@ -1334,11 +1332,11 @@ const uint16_t* ExternalTwoByteString::ExternalTwoByteStringGetData(
 
 int ConsStringIterator::OffsetForDepth(int depth) { return depth & kDepthMask; }
 
-void ConsStringIterator::PushLeft(ConsString string) {
+void ConsStringIterator::PushLeft(Tagged<ConsString> string) {
   frames_[depth_++ & kDepthMask] = string;
 }
 
-void ConsStringIterator::PushRight(ConsString string) {
+void ConsStringIterator::PushRight(Tagged<ConsString> string) {
   // Inplace update.
   frames_[(depth_ - 1) & kDepthMask] = string;
 }
@@ -1355,12 +1353,12 @@ void ConsStringIterator::Pop() {
 
 class StringCharacterStream {
  public:
-  inline explicit StringCharacterStream(String string, int offset = 0);
+  inline explicit StringCharacterStream(Tagged<String> string, int offset = 0);
   StringCharacterStream(const StringCharacterStream&) = delete;
   StringCharacterStream& operator=(const StringCharacterStream&) = delete;
   inline uint16_t GetNext();
   inline bool HasMore();
-  inline void Reset(String string, int offset = 0);
+  inline void Reset(Tagged<String> string, int offset = 0);
   inline void VisitOneByteString(const uint8_t* chars, int length);
   inline void VisitTwoByteString(const uint16_t* chars, int length);
 
@@ -1386,16 +1384,16 @@ uint16_t StringCharacterStream::GetNext() {
 // TODO(solanes, v8:7790, chromium:1166095): Assess if we need to use
 // Isolate/LocalIsolate and pipe them through, instead of using the slow
 // version of the SharedStringAccessGuardIfNeeded.
-StringCharacterStream::StringCharacterStream(String string, int offset)
+StringCharacterStream::StringCharacterStream(Tagged<String> string, int offset)
     : is_one_byte_(false), access_guard_(string) {
   Reset(string, offset);
 }
 
-void StringCharacterStream::Reset(String string, int offset) {
+void StringCharacterStream::Reset(Tagged<String> string, int offset) {
   buffer8_ = nullptr;
   end_ = nullptr;
 
-  ConsString cons_string =
+  Tagged<ConsString> cons_string =
       String::VisitFlat(this, string, offset, access_guard_);
   iter_.Reset(cons_string, offset);
   if (!cons_string.is_null()) {
@@ -1408,7 +1406,7 @@ void StringCharacterStream::Reset(String string, int offset) {
 bool StringCharacterStream::HasMore() {
   if (buffer8_ != end_) return true;
   int offset;
-  String string = iter_.Next(&offset);
+  Tagged<String> string = iter_.Next(&offset);
   DCHECK_EQ(offset, 0);
   if (string.is_null()) return false;
   String::VisitFlat(this, string, 0, access_guard_);
@@ -1455,7 +1453,7 @@ bool String::AsIntegerIndex(size_t* index) {
   return SlowAsIntegerIndex(index);
 }
 
-SubStringRange::SubStringRange(String string,
+SubStringRange::SubStringRange(Tagged<String> string,
                                const DisallowGarbageCollection& no_gc,
                                int first, int length)
     : string_(string),
@@ -1489,7 +1487,8 @@ class SubStringRange::iterator final {
  private:
   friend class String;
   friend class SubStringRange;
-  iterator(String from, int offset, const DisallowGarbageCollection& no_gc)
+  iterator(Tagged<String> from, int offset,
+           const DisallowGarbageCollection& no_gc)
       : content_(from->GetFlatContent(no_gc)), offset_(offset) {}
   String::FlatContent content_;
   int offset_;
diff --git a/src/objects/string-set-inl.h b/src/objects/string-set-inl.h
index d1bfc4f7c93..088675c9b78 100644
--- a/src/objects/string-set-inl.h
+++ b/src/objects/string-set-inl.h
@@ -20,16 +20,17 @@ StringSet::StringSet(Address ptr) : HashTable<StringSet, StringSetShape>(ptr) {
   SLOW_DCHECK(IsStringSet(*this));
 }
 
-bool StringSetShape::IsMatch(String key, Object value) {
+bool StringSetShape::IsMatch(Tagged<String> key, Tagged<Object> value) {
   DCHECK(IsString(value));
   return key->Equals(String::cast(value));
 }
 
-uint32_t StringSetShape::Hash(ReadOnlyRoots roots, String key) {
+uint32_t StringSetShape::Hash(ReadOnlyRoots roots, Tagged<String> key) {
   return key->EnsureHash();
 }
 
-uint32_t StringSetShape::HashForObject(ReadOnlyRoots roots, Object object) {
+uint32_t StringSetShape::HashForObject(ReadOnlyRoots roots,
+                                       Tagged<Object> object) {
   return String::cast(object)->EnsureHash();
 }
 
diff --git a/src/objects/string-set.h b/src/objects/string-set.h
index a4d589f11cd..7edfa8fd39c 100644
--- a/src/objects/string-set.h
+++ b/src/objects/string-set.h
@@ -15,9 +15,10 @@ namespace internal {
 
 class StringSetShape : public BaseShape<String> {
  public:
-  static inline bool IsMatch(String key, Object value);
-  static inline uint32_t Hash(ReadOnlyRoots roots, String key);
-  static inline uint32_t HashForObject(ReadOnlyRoots roots, Object object);
+  static inline bool IsMatch(Tagged<String> key, Tagged<Object> value);
+  static inline uint32_t Hash(ReadOnlyRoots roots, Tagged<String> key);
+  static inline uint32_t HashForObject(ReadOnlyRoots roots,
+                                       Tagged<Object> object);
 
   static const int kPrefixSize = 0;
   static const int kEntrySize = 1;
diff --git a/src/objects/string-table.h b/src/objects/string-table.h
index 9c1f0d1a975..69998c1a3ad 100644
--- a/src/objects/string-table.h
+++ b/src/objects/string-table.h
@@ -47,8 +47,8 @@ class SeqOneByteString;
 // StringTable::Data for details.
 class V8_EXPORT_PRIVATE StringTable {
  public:
-  static constexpr Smi empty_element() { return Smi::FromInt(0); }
-  static constexpr Smi deleted_element() { return Smi::FromInt(1); }
+  static constexpr Tagged<Smi> empty_element() { return Smi::FromInt(0); }
+  static constexpr Tagged<Smi> deleted_element() { return Smi::FromInt(1); }
 
   explicit StringTable(Isolate* isolate);
   ~StringTable();
diff --git a/src/objects/string.cc b/src/objects/string.cc
index 5102b96ab2f..0e8b6ffb721 100644
--- a/src/objects/string.cc
+++ b/src/objects/string.cc
@@ -139,26 +139,27 @@ Handle<String> String::SlowShare(Isolate* isolate, Handle<String> source) {
 namespace {
 
 template <class StringClass>
-void MigrateExternalStringResource(Isolate* isolate, ExternalString from,
-                                   StringClass to) {
-  Address to_resource_address = to.resource_as_address();
+void MigrateExternalStringResource(Isolate* isolate,
+                                   Tagged<ExternalString> from,
+                                   Tagged<StringClass> to) {
+  Address to_resource_address = to->resource_as_address();
   if (to_resource_address == kNullAddress) {
-    StringClass cast_from = StringClass::cast(from);
+    Tagged<StringClass> cast_from = StringClass::cast(from);
     // |to| is a just-created internalized copy of |from|. Migrate the resource.
-    to.SetResource(isolate, cast_from.resource());
+    to->SetResource(isolate, cast_from->resource());
     // Zap |from|'s resource pointer to reflect the fact that |from| has
     // relinquished ownership of its resource.
     isolate->heap()->UpdateExternalString(
         from, ExternalString::cast(from)->ExternalPayloadSize(), 0);
-    cast_from.SetResource(isolate, nullptr);
+    cast_from->SetResource(isolate, nullptr);
   } else if (to_resource_address != from->resource_as_address()) {
     // |to| already existed and has its own resource. Finalize |from|.
     isolate->heap()->FinalizeExternalString(from);
   }
 }
 
-void MigrateExternalString(Isolate* isolate, String string,
-                           String internalized) {
+void MigrateExternalString(Isolate* isolate, Tagged<String> string,
+                           Tagged<String> internalized) {
   if (IsExternalOneByteString(internalized)) {
     MigrateExternalStringResource(isolate, ExternalString::cast(string),
                                   ExternalOneByteString::cast(internalized));
@@ -173,14 +174,15 @@ void MigrateExternalString(Isolate* isolate, String string,
   }
 }
 
-void InitExternalPointerFieldsDuringExternalization(String string, Map new_map,
+void InitExternalPointerFieldsDuringExternalization(Tagged<String> string,
+                                                    Tagged<Map> new_map,
                                                     Isolate* isolate) {
-  string.InitExternalPointerField<kExternalStringResourceTag>(
+  string->InitExternalPointerField<kExternalStringResourceTag>(
       ExternalString::kResourceOffset, isolate, kNullAddress);
   bool is_uncached = (new_map->instance_type() & kUncachedExternalStringMask) ==
                      kUncachedExternalStringTag;
   if (!is_uncached) {
-    string.InitExternalPointerField<kExternalStringResourceDataTag>(
+    string->InitExternalPointerField<kExternalStringResourceDataTag>(
         ExternalString::kResourceDataOffset, isolate, kNullAddress);
   }
 }
@@ -188,12 +190,12 @@ void InitExternalPointerFieldsDuringExternalization(String string, Map new_map,
 }  // namespace
 
 template <typename IsolateT>
-void String::MakeThin(IsolateT* isolate, String internalized) {
+void String::MakeThin(IsolateT* isolate, Tagged<String> internalized) {
   DisallowGarbageCollection no_gc;
   DCHECK_NE(*this, internalized);
   DCHECK(IsInternalizedString(internalized));
 
-  Map initial_map = map(kAcquireLoad);
+  Tagged<Map> initial_map = map(kAcquireLoad);
   StringShape initial_shape(initial_map);
 
   DCHECK(!initial_shape.IsThin());
@@ -211,9 +213,9 @@ void String::MakeThin(IsolateT* isolate, String internalized) {
   bool may_contain_recorded_slots = initial_shape.IsIndirect();
   int old_size = SizeFromMap(initial_map);
   ReadOnlyRoots roots(isolate);
-  Map target_map = internalized.IsOneByteRepresentation()
-                       ? roots.thin_one_byte_string_map()
-                       : roots.thin_two_byte_string_map();
+  Tagged<Map> target_map = internalized->IsOneByteRepresentation()
+                               ? roots.thin_one_byte_string_map()
+                               : roots.thin_two_byte_string_map();
   if (initial_shape.IsExternal()) {
     // Notify GC about the layout change before the transition to avoid
     // concurrent marking from observing any in-between state (e.g.
@@ -229,7 +231,7 @@ void String::MakeThin(IsolateT* isolate, String internalized) {
   // Update actual first and then do release store on the map word. This ensures
   // that the concurrent marker will read the pointer when visiting a
   // ThinString.
-  ThinString thin = ThinString::unchecked_cast(*this);
+  Tagged<ThinString> thin = ThinString::unchecked_cast(*this);
   thin->set_actual(internalized);
 
   DCHECK_GE(old_size, ThinString::kSize);
@@ -256,9 +258,9 @@ void String::MakeThin(IsolateT* isolate, String internalized) {
 }
 
 template EXPORT_TEMPLATE_DEFINE(V8_EXPORT_PRIVATE) void String::MakeThin(
-    Isolate* isolate, String internalized);
+    Isolate* isolate, Tagged<String> internalized);
 template EXPORT_TEMPLATE_DEFINE(V8_EXPORT_PRIVATE) void String::MakeThin(
-    LocalIsolate* isolate, String internalized);
+    LocalIsolate* isolate, Tagged<String> internalized);
 
 template <typename T>
 bool String::MarkForExternalizationDuringGC(Isolate* isolate, T* resource) {
@@ -296,7 +298,8 @@ bool String::MarkForExternalizationDuringGC(Isolate* isolate, T* resource) {
 namespace {
 
 template <bool is_one_byte>
-Map ComputeExternalStringMap(Isolate* isolate, String string, int size) {
+Tagged<Map> ComputeExternalStringMap(Isolate* isolate, Tagged<String> string,
+                                     int size) {
   ReadOnlyRoots roots(isolate);
   StringShape shape(string, isolate);
   const bool is_internalized = shape.IsInternalized();
@@ -354,7 +357,8 @@ void String::MakeExternalDuringGC(Isolate* isolate, T* resource) {
   // resort to an uncached external string instead, omitting the field caching
   // the address of the backing store.  When we encounter uncached external
   // strings in generated code, we need to bailout to runtime.
-  Map new_map = ComputeExternalStringMap<is_one_byte>(isolate, *this, size);
+  Tagged<Map> new_map =
+      ComputeExternalStringMap<is_one_byte>(isolate, *this, size);
 
   // Byte size of the external String object.
   int new_size = this->SizeFromMap(new_map);
@@ -379,11 +383,11 @@ void String::MakeExternalDuringGC(Isolate* isolate, T* resource) {
   this->set_map(new_map, kReleaseStore);
 
   if constexpr (is_one_byte) {
-    ExternalOneByteString self = ExternalOneByteString::cast(*this);
-    self.SetResource(isolate, resource);
+    Tagged<ExternalOneByteString> self = ExternalOneByteString::cast(*this);
+    self->SetResource(isolate, resource);
   } else {
-    ExternalTwoByteString self = ExternalTwoByteString::cast(*this);
-    self.SetResource(isolate, resource);
+    Tagged<ExternalTwoByteString> self = ExternalTwoByteString::cast(*this);
+    self->SetResource(isolate, resource);
   }
   isolate->heap()->RegisterExternalString(*this);
 }
@@ -437,7 +441,8 @@ bool String::MakeExternal(v8::String::ExternalStringResource* resource) {
   // the address of the backing store.  When we encounter uncached external
   // strings in generated code, we need to bailout to runtime.
   constexpr bool is_one_byte = false;
-  Map new_map = ComputeExternalStringMap<is_one_byte>(isolate, *this, size);
+  Tagged<Map> new_map =
+      ComputeExternalStringMap<is_one_byte>(isolate, *this, size);
 
   // Byte size of the external String object.
   int new_size = this->SizeFromMap(new_map);
@@ -468,7 +473,7 @@ bool String::MakeExternal(v8::String::ExternalStringResource* resource) {
   // the sweeper thread.
   this->set_map(new_map, kReleaseStore);
 
-  ExternalTwoByteString self = ExternalTwoByteString::cast(*this);
+  Tagged<ExternalTwoByteString> self = ExternalTwoByteString::cast(*this);
   self->SetResource(isolate, resource);
   isolate->heap()->RegisterExternalString(*this);
   // Force regeneration of the hash value.
@@ -524,7 +529,8 @@ bool String::MakeExternal(v8::String::ExternalOneByteStringResource* resource) {
   // the address of the backing store.  When we encounter uncached external
   // strings in generated code, we need to bailout to runtime.
   constexpr bool is_one_byte = true;
-  Map new_map = ComputeExternalStringMap<is_one_byte>(isolate, *this, size);
+  Tagged<Map> new_map =
+      ComputeExternalStringMap<is_one_byte>(isolate, *this, size);
 
   if (!isolate->heap()->IsLargeObject(*this)) {
     // Byte size of the external String object.
@@ -555,7 +561,7 @@ bool String::MakeExternal(v8::String::ExternalOneByteStringResource* resource) {
   // the sweeper thread.
   this->set_map(new_map, kReleaseStore);
 
-  ExternalOneByteString self = ExternalOneByteString::cast(*this);
+  Tagged<ExternalOneByteString> self = ExternalOneByteString::cast(*this);
   self->SetResource(isolate, resource);
   isolate->heap()->RegisterExternalString(*this);
   // Force regeneration of the hash value.
@@ -589,7 +595,7 @@ bool String::SupportsExternalization(v8::String::Encoding encoding) {
   }
 
   // Only strings in old space can be externalized.
-  if (Heap::InYoungGeneration(*this)) {
+  if (Heap::InYoungGeneration(Tagged(*this))) {
     return false;
   }
 
@@ -687,7 +693,7 @@ void String::PrintUC16(StringStream* accumulator, int start, int end) {
 
 int32_t String::ToArrayIndex(Address addr) {
   DisallowGarbageCollection no_gc;
-  String key(addr);
+  Tagged<String> key(addr);
 
   uint32_t index;
   if (!key->AsArrayIndex(&index)) return -1;
@@ -794,18 +800,18 @@ String::FlatContent String::SlowGetFlatContent(
     const SharedStringAccessGuardIfNeeded& access_guard) {
   USE(no_gc);
   PtrComprCageBase cage_base = GetPtrComprCageBase(*this);
-  String string = *this;
+  Tagged<String> string = *this;
   StringShape shape(string, cage_base);
   int offset = 0;
 
   // Extract cons- and sliced strings.
   if (shape.IsCons()) {
-    ConsString cons = ConsString::cast(string);
+    Tagged<ConsString> cons = ConsString::cast(string);
     if (!cons->IsFlat(cage_base)) return FlatContent(no_gc);
     string = cons->first(cage_base);
     shape = StringShape(string, cage_base);
   } else if (shape.IsSliced()) {
-    SlicedString slice = SlicedString::cast(string);
+    Tagged<SlicedString> slice = SlicedString::cast(string);
     offset = slice->offset();
     string = slice->parent(cage_base);
     shape = StringShape(string, cage_base);
@@ -816,7 +822,7 @@ String::FlatContent String::SlowGetFlatContent(
 
   // Extract thin strings.
   if (shape.IsThin()) {
-    ThinString thin = ThinString::cast(string);
+    Tagged<ThinString> thin = ThinString::cast(string);
     string = thin->actual(cage_base);
     shape = StringShape(string, cage_base);
   }
@@ -880,7 +886,8 @@ std::unique_ptr<char[]> String::ToCString(AllowNullsFlag allow_nulls,
 
 // static
 template <typename sinkchar>
-void String::WriteToFlat(String source, sinkchar* sink, int start, int length) {
+void String::WriteToFlat(Tagged<String> source, sinkchar* sink, int start,
+                         int length) {
   DCHECK(!SharedStringAccessGuardIfNeeded::IsNeeded(source));
   return WriteToFlat(source, sink, start, length, GetPtrComprCageBase(source),
                      SharedStringAccessGuardIfNeeded::NotNeeded());
@@ -888,8 +895,8 @@ void String::WriteToFlat(String source, sinkchar* sink, int start, int length) {
 
 // static
 template <typename sinkchar>
-void String::WriteToFlat(String source, sinkchar* sink, int start, int length,
-                         PtrComprCageBase cage_base,
+void String::WriteToFlat(Tagged<String> source, sinkchar* sink, int start,
+                         int length, PtrComprCageBase cage_base,
                          const SharedStringAccessGuardIfNeeded& access_guard) {
   DisallowGarbageCollection no_gc;
   if (length == 0) return;
@@ -926,8 +933,8 @@ void String::WriteToFlat(String source, sinkchar* sink, int start, int length,
         return;
       case kOneByteStringTag | kConsStringTag:
       case kTwoByteStringTag | kConsStringTag: {
-        ConsString cons_string = ConsString::cast(source);
-        String first = cons_string->first(cage_base);
+        Tagged<ConsString> cons_string = ConsString::cast(source);
+        Tagged<String> first = cons_string->first(cage_base);
         int boundary = first->length();
         int first_length = boundary - start;
         int second_length = start + length - boundary;
@@ -950,7 +957,7 @@ void String::WriteToFlat(String source, sinkchar* sink, int start, int length,
         } else {
           // Left hand side is longer.  Recurse over right.
           if (second_length > 0) {
-            String second = cons_string->second(cage_base);
+            Tagged<String> second = cons_string->second(cage_base);
             // When repeatedly appending to a string, we get a cons string that
             // is unbalanced to the left, a list, essentially.  We inline the
             // common case of sequential one-byte right child.
@@ -975,7 +982,7 @@ void String::WriteToFlat(String source, sinkchar* sink, int start, int length,
       }
       case kOneByteStringTag | kSlicedStringTag:
       case kTwoByteStringTag | kSlicedStringTag: {
-        SlicedString slice = SlicedString::cast(source);
+        Tagged<SlicedString> slice = SlicedString::cast(source);
         unsigned offset = slice->offset();
         source = slice->parent(cage_base);
         start += offset;
@@ -1059,14 +1066,15 @@ template Handle<FixedArray> String::CalculateLineEnds(LocalIsolate* isolate,
                                                       Handle<String> src,
                                                       bool include_ending_line);
 
-bool String::SlowEquals(String other) const {
+bool String::SlowEquals(Tagged<String> other) const {
   DCHECK(!SharedStringAccessGuardIfNeeded::IsNeeded(*this));
   DCHECK(!SharedStringAccessGuardIfNeeded::IsNeeded(other));
   return SlowEquals(other, SharedStringAccessGuardIfNeeded::NotNeeded());
 }
 
 bool String::SlowEquals(
-    String other, const SharedStringAccessGuardIfNeeded& access_guard) const {
+    Tagged<String> other,
+    const SharedStringAccessGuardIfNeeded& access_guard) const {
   DisallowGarbageCollection no_gc;
   // Fast check: negative check with lengths.
   int len = length();
@@ -1097,7 +1105,7 @@ bool String::SlowEquals(
       if (this_hash != other_hash) {
         bool found_difference = false;
         for (int i = 0; i < len; i++) {
-          if (Get(i) != other.Get(i)) {
+          if (Get(i) != other->Get(i)) {
             found_difference = true;
             break;
           }
@@ -1261,7 +1269,7 @@ ComparisonResult String::Compare(Isolate* isolate, Handle<String> x,
 
 namespace {
 
-uint32_t ToValidIndex(String str, Object number) {
+uint32_t ToValidIndex(Tagged<String> str, Tagged<Object> number) {
   uint32_t index = PositiveNumberToUint32(number);
   uint32_t length_value = static_cast<uint32_t>(str->length());
   if (index > length_value) return length_value;
@@ -1270,8 +1278,8 @@ uint32_t ToValidIndex(String str, Object number) {
 
 }  // namespace
 
-Object String::IndexOf(Isolate* isolate, Handle<Object> receiver,
-                       Handle<Object> search, Handle<Object> position) {
+Tagged<Object> String::IndexOf(Isolate* isolate, Handle<Object> receiver,
+                               Handle<Object> search, Handle<Object> position) {
   if (IsNullOrUndefined(*receiver, isolate)) {
     THROW_NEW_ERROR_RETURN_FAILURE(
         isolate, NewTypeError(MessageTemplate::kCalledOnNullOrUndefined,
@@ -1536,8 +1544,9 @@ int StringMatchBackwards(base::Vector<const schar> subject,
 
 }  // namespace
 
-Object String::LastIndexOf(Isolate* isolate, Handle<Object> receiver,
-                           Handle<Object> search, Handle<Object> position) {
+Tagged<Object> String::LastIndexOf(Isolate* isolate, Handle<Object> receiver,
+                                   Handle<Object> search,
+                                   Handle<Object> position) {
   if (IsNullOrUndefined(*receiver, isolate)) {
     THROW_NEW_ERROR_RETURN_FAILURE(
         isolate, NewTypeError(MessageTemplate::kCalledOnNullOrUndefined,
@@ -1646,8 +1655,8 @@ bool String::IsIdentifier(Isolate* isolate, Handle<String> str) {
 namespace {
 
 template <typename Char>
-uint32_t HashString(String string, size_t start, int length, uint64_t seed,
-                    PtrComprCageBase cage_base,
+uint32_t HashString(Tagged<String> string, size_t start, int length,
+                    uint64_t seed, PtrComprCageBase cage_base,
                     const SharedStringAccessGuardIfNeeded& access_guard) {
   DisallowGarbageCollection no_gc;
 
@@ -1666,7 +1675,7 @@ uint32_t HashString(String string, size_t start, int length, uint64_t seed,
                         access_guard);
     chars = buffer.get();
   } else {
-    chars = string.GetDirectStringChars<Char>(cage_base, no_gc, access_guard) +
+    chars = string->GetDirectStringChars<Char>(cage_base, no_gc, access_guard) +
             start;
   }
 
@@ -1694,11 +1703,11 @@ uint32_t String::ComputeAndSetRawHash(
   // Store the hash code in the object.
   uint64_t seed = HashSeed(EarlyGetReadOnlyRoots());
   size_t start = 0;
-  String string = *this;
+  Tagged<String> string = *this;
   PtrComprCageBase cage_base = GetPtrComprCageBase(string);
   StringShape shape(string, cage_base);
   if (shape.IsSliced()) {
-    SlicedString sliced = SlicedString::cast(string);
+    Tagged<SlicedString> sliced = SlicedString::cast(string);
     start = sliced->offset();
     string = sliced->parent(cage_base);
     shape = StringShape(string, cage_base);
@@ -1862,16 +1871,16 @@ uint16_t ConsString::Get(
 
   // Check for a flattened cons string
   if (second(cage_base)->length() == 0) {
-    String left = first(cage_base);
+    Tagged<String> left = first(cage_base);
     return left->Get(index);
   }
 
-  String string = String::cast(*this);
+  Tagged<String> string = String::cast(*this);
 
   while (true) {
     if (StringShape(string, cage_base).IsCons()) {
-      ConsString cons_string = ConsString::cast(string);
-      String left = cons_string->first();
+      Tagged<ConsString> cons_string = ConsString::cast(string);
+      Tagged<String> left = cons_string->first();
       if (left->length() > index) {
         string = left;
       } else {
@@ -1926,7 +1935,8 @@ void FlatStringReader::PostGarbageCollection() {
   }
 }
 
-void ConsStringIterator::Initialize(ConsString cons_string, int offset) {
+void ConsStringIterator::Initialize(Tagged<ConsString> cons_string,
+                                    int offset) {
   DCHECK(!cons_string.is_null());
   root_ = cons_string;
   consumed_ = offset;
@@ -1936,11 +1946,11 @@ void ConsStringIterator::Initialize(ConsString cons_string, int offset) {
   DCHECK(StackBlown());
 }
 
-String ConsStringIterator::Continue(int* offset_out) {
+Tagged<String> ConsStringIterator::Continue(int* offset_out) {
   DCHECK_NE(depth_, 0);
   DCHECK_EQ(0, *offset_out);
   bool blew_stack = StackBlown();
-  String string;
+  Tagged<String> string;
   // Get the next leaf if there is one.
   if (!blew_stack) string = NextLeaf(&blew_stack);
   // Restart search from root.
@@ -1953,8 +1963,8 @@ String ConsStringIterator::Continue(int* offset_out) {
   return string;
 }
 
-String ConsStringIterator::Search(int* offset_out) {
-  ConsString cons_string = root_;
+Tagged<String> ConsStringIterator::Search(int* offset_out) {
+  Tagged<ConsString> cons_string = root_;
   // Reset the stack, pushing the root string.
   depth_ = 1;
   maximum_depth_ = 1;
@@ -1963,7 +1973,7 @@ String ConsStringIterator::Search(int* offset_out) {
   int offset = 0;
   while (true) {
     // Loop until the string is found which contains the target offset.
-    String string = cons_string->first();
+    Tagged<String> string = cons_string->first();
     int length = string->length();
     int32_t type;
     if (consumed < offset + length) {
@@ -2012,7 +2022,7 @@ String ConsStringIterator::Search(int* offset_out) {
   UNREACHABLE();
 }
 
-String ConsStringIterator::NextLeaf(bool* blew_stack) {
+Tagged<String> ConsStringIterator::NextLeaf(bool* blew_stack) {
   while (true) {
     // Tree traversal complete.
     if (depth_ == 0) {
@@ -2025,8 +2035,8 @@ String ConsStringIterator::NextLeaf(bool* blew_stack) {
       return String();
     }
     // Go right.
-    ConsString cons_string = frames_[OffsetForDepth(depth_ - 1)];
-    String string = cons_string->second();
+    Tagged<ConsString> cons_string = frames_[OffsetForDepth(depth_ - 1)];
+    Tagged<String> string = cons_string->second();
     int32_t type = string->map()->instance_type();
     if ((type & kStringRepresentationMask) != kConsStringTag) {
       // Pop stack so next iteration is in correct place.
@@ -2061,7 +2071,7 @@ String ConsStringIterator::NextLeaf(bool* blew_stack) {
 const uint8_t* String::AddressOfCharacterAt(
     int start_index, const DisallowGarbageCollection& no_gc) {
   DCHECK(IsFlat());
-  String subject = *this;
+  Tagged<String> subject = *this;
   PtrComprCageBase cage_base = GetPtrComprCageBase(subject);
   StringShape shape(subject, cage_base);
   if (IsConsString(subject, cage_base)) {
@@ -2099,15 +2109,15 @@ const uint8_t* String::AddressOfCharacterAt(
 }
 
 template EXPORT_TEMPLATE_DEFINE(V8_EXPORT_PRIVATE) void String::WriteToFlat(
-    String source, uint16_t* sink, int from, int to);
+    Tagged<String> source, uint16_t* sink, int from, int to);
 template EXPORT_TEMPLATE_DEFINE(V8_EXPORT_PRIVATE) void String::WriteToFlat(
-    String source, uint8_t* sink, int from, int to);
+    Tagged<String> source, uint8_t* sink, int from, int to);
 template EXPORT_TEMPLATE_DEFINE(V8_EXPORT_PRIVATE) void String::WriteToFlat(
-    String source, uint16_t* sink, int from, int to, PtrComprCageBase cage_base,
-    const SharedStringAccessGuardIfNeeded&);
+    Tagged<String> source, uint16_t* sink, int from, int to,
+    PtrComprCageBase cage_base, const SharedStringAccessGuardIfNeeded&);
 template EXPORT_TEMPLATE_DEFINE(V8_EXPORT_PRIVATE) void String::WriteToFlat(
-    String source, uint8_t* sink, int from, int to, PtrComprCageBase cage_base,
-    const SharedStringAccessGuardIfNeeded&);
+    Tagged<String> source, uint8_t* sink, int from, int to,
+    PtrComprCageBase cage_base, const SharedStringAccessGuardIfNeeded&);
 
 namespace {
 // Check that the constants defined in src/objects/instance-type.h coincides
diff --git a/src/objects/string.h b/src/objects/string.h
index 79106283692..3d04b0dae7a 100644
--- a/src/objects/string.h
+++ b/src/objects/string.h
@@ -45,9 +45,6 @@ enum RobustnessFlag { ROBUST_STRING_TRAVERSAL, FAST_STRING_TRAVERSAL };
 // concrete performance benefit at that particular point in the code.
 class StringShape {
  public:
-  V8_INLINE explicit StringShape(const String s);
-  V8_INLINE explicit StringShape(const String s, PtrComprCageBase cage_base);
-  V8_INLINE explicit StringShape(Map s);
   V8_INLINE explicit StringShape(const Tagged<String> s);
   V8_INLINE explicit StringShape(const Tagged<String> s,
                                  PtrComprCageBase cage_base);
@@ -84,7 +81,7 @@ class StringShape {
   template <typename TDispatcher, typename TResult, typename... TArgs>
   inline TResult DispatchToSpecificTypeWithoutCast(TArgs&&... args);
   template <typename TDispatcher, typename TResult, typename... TArgs>
-  inline TResult DispatchToSpecificType(String str, TArgs&&... args);
+  inline TResult DispatchToSpecificType(Tagged<String> str, TArgs&&... args);
 
  private:
   uint32_t type_;
@@ -200,7 +197,7 @@ class String : public TorqueGeneratedString<String, Name> {
 
   template <typename IsolateT>
   EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE)
-  void MakeThin(IsolateT* isolate, String canonical);
+  void MakeThin(IsolateT* isolate, Tagged<String> canonical);
 
   template <typename Char>
   V8_INLINE base::Vector<const Char> GetCharVector(
@@ -296,7 +293,7 @@ class String : public TorqueGeneratedString<String, Name> {
 
   // Returns the parent of a sliced string or first part of a flat cons string.
   // Requires: StringShape(this).IsIndirect() && this->IsFlat()
-  inline String GetUnderlying() const;
+  inline Tagged<String> GetUnderlying() const;
 
   // Shares the string. Checks inline if the string is already shared or can be
   // shared by transitioning its map in-place. If neither is possible, flattens
@@ -319,16 +316,17 @@ class String : public TorqueGeneratedString<String, Name> {
                                                         Handle<String> y);
 
   // Perform ES6 21.1.3.8, including checking arguments.
-  static Object IndexOf(Isolate* isolate, Handle<Object> receiver,
-                        Handle<Object> search, Handle<Object> position);
+  static Tagged<Object> IndexOf(Isolate* isolate, Handle<Object> receiver,
+                                Handle<Object> search, Handle<Object> position);
   // Perform string match of pattern on subject, starting at start index.
   // Caller must ensure that 0 <= start_index <= sub->length(), as this does not
   // check any arguments.
   static int IndexOf(Isolate* isolate, Handle<String> receiver,
                      Handle<String> search, int start_index);
 
-  static Object LastIndexOf(Isolate* isolate, Handle<Object> receiver,
-                            Handle<Object> search, Handle<Object> position);
+  static Tagged<Object> LastIndexOf(Isolate* isolate, Handle<Object> receiver,
+                                    Handle<Object> search,
+                                    Handle<Object> position);
 
   // Encapsulates logic related to a match and its capture groups as required
   // by GetSubstitution.
@@ -362,7 +360,7 @@ class String : public TorqueGeneratedString<String, Name> {
       int start_index = 0);
 
   // String equality operations.
-  inline bool Equals(String other) const;
+  inline bool Equals(Tagged<String> other) const;
   inline static bool Equals(Isolate* isolate, Handle<String> one,
                             Handle<String> two);
 
@@ -523,10 +521,11 @@ class String : public TorqueGeneratedString<String, Name> {
   // Helper function for flattening strings.
   template <typename sinkchar>
   EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE)
-  static void WriteToFlat(String source, sinkchar* sink, int from, int to);
+  static void WriteToFlat(Tagged<String> source, sinkchar* sink, int from,
+                          int to);
   template <typename sinkchar>
-  static void WriteToFlat(String source, sinkchar* sink, int from, int to,
-                          PtrComprCageBase cage_base,
+  static void WriteToFlat(Tagged<String> source, sinkchar* sink, int from,
+                          int to, PtrComprCageBase cage_base,
                           const SharedStringAccessGuardIfNeeded&);
 
   // Returns true if this string has no unpaired surrogates and false otherwise.
@@ -588,11 +587,12 @@ class String : public TorqueGeneratedString<String, Name> {
   // May only be called when a SharedStringAccessGuard is not needed (i.e. on
   // the main thread or on read-only strings).
   template <class Visitor>
-  static inline ConsString VisitFlat(Visitor* visitor, Tagged<String> string,
-                                     int offset = 0);
+  static inline Tagged<ConsString> VisitFlat(Visitor* visitor,
+                                             Tagged<String> string,
+                                             int offset = 0);
 
   template <class Visitor>
-  static inline ConsString VisitFlat(
+  static inline Tagged<ConsString> VisitFlat(
       Visitor* visitor, Tagged<String> string, int offset,
       const SharedStringAccessGuardIfNeeded& access_guard);
 
@@ -630,7 +630,7 @@ class String : public TorqueGeneratedString<String, Name> {
   // Out-of-line IsEqualToImpl for ConsString.
   template <typename Char>
   V8_NOINLINE static bool IsConsStringEqualToImpl(
-      ConsString string, base::Vector<const Char> str,
+      Tagged<ConsString> string, base::Vector<const Char> str,
       PtrComprCageBase cage_base,
       const SharedStringAccessGuardIfNeeded& access_guard);
 
@@ -640,7 +640,8 @@ class String : public TorqueGeneratedString<String, Name> {
   V8_EXPORT_PRIVATE V8_INLINE static base::Optional<FlatContent>
   TryGetFlatContentFromDirectString(PtrComprCageBase cage_base,
                                     const DisallowGarbageCollection& no_gc,
-                                    String string, int offset, int length,
+                                    Tagged<String> string, int offset,
+                                    int length,
                                     const SharedStringAccessGuardIfNeeded&);
   V8_EXPORT_PRIVATE FlatContent
   SlowGetFlatContent(const DisallowGarbageCollection& no_gc,
@@ -651,9 +652,9 @@ class String : public TorqueGeneratedString<String, Name> {
 
   // Slow case of String::Equals.  This implementation works on any strings
   // but it is most efficient on strings that are almost flat.
-  V8_EXPORT_PRIVATE bool SlowEquals(String other) const;
+  V8_EXPORT_PRIVATE bool SlowEquals(Tagged<String> other) const;
   V8_EXPORT_PRIVATE bool SlowEquals(
-      String other, const SharedStringAccessGuardIfNeeded&) const;
+      Tagged<String> other, const SharedStringAccessGuardIfNeeded&) const;
 
   V8_EXPORT_PRIVATE static bool SlowEquals(Isolate* isolate, Handle<String> one,
                                            Handle<String> two);
@@ -673,24 +674,26 @@ class String : public TorqueGeneratedString<String, Name> {
 };
 
 // clang-format off
-extern template EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE)
-void String::WriteToFlat(String source, uint8_t* sink, int from, int to);
-extern template EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE)
-void String::WriteToFlat(String source, uint16_t* sink, int from, int to);
-extern template EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE)
-void String::WriteToFlat(String source, uint8_t* sink, int from, int to,
-                         PtrComprCageBase cage_base,
-                         const SharedStringAccessGuardIfNeeded&);
-extern template EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE)
-void String::WriteToFlat(String source, uint16_t* sink, int from, int to,
-                         PtrComprCageBase cage_base,
-                         const SharedStringAccessGuardIfNeeded&);
+extern template EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE) void
+    String::WriteToFlat(Tagged<String> source, uint8_t* sink, int from, int to);
+extern template EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE) void
+    String::WriteToFlat(Tagged<String> source, uint16_t* sink, int from,
+                        int to);
+extern template EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE) void
+    String::WriteToFlat(Tagged<String> source, uint8_t* sink, int from, int to,
+                        PtrComprCageBase cage_base,
+                        const SharedStringAccessGuardIfNeeded&);
+extern template EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE) void
+    String::WriteToFlat(Tagged<String> source, uint16_t* sink, int from, int to,
+                        PtrComprCageBase cage_base,
+                        const SharedStringAccessGuardIfNeeded&);
 // clang-format on
 
 class SubStringRange {
  public:
-  inline SubStringRange(String string, const DisallowGarbageCollection& no_gc,
-                        int first = 0, int length = -1);
+  inline SubStringRange(Tagged<String> string,
+                        const DisallowGarbageCollection& no_gc, int first = 0,
+                        int length = -1);
   class iterator;
   inline iterator begin();
   inline iterator end();
@@ -850,11 +853,11 @@ class ConsString : public TorqueGeneratedConsString<ConsString, String> {
  public:
   // Doesn't check that the result is a string, even in debug mode.  This is
   // useful during GC where the mark bits confuse the checks.
-  inline Object unchecked_first() const;
+  inline Tagged<Object> unchecked_first() const;
 
   // Doesn't check that the result is a string, even in debug mode.  This is
   // useful during GC where the mark bits confuse the checks.
-  inline Object unchecked_second() const;
+  inline Tagged<Object> unchecked_second() const;
 
   V8_INLINE bool IsFlat(PtrComprCageBase cage_base) const;
 
@@ -907,7 +910,7 @@ class ThinString : public TorqueGeneratedThinString<ThinString, String> {
 //  - truncating sliced string to enable otherwise unneeded parent to be GC'ed.
 class SlicedString : public TorqueGeneratedSlicedString<SlicedString, String> {
  public:
-  inline void set_parent(String parent,
+  inline void set_parent(Tagged<String> parent,
                          WriteBarrierMode mode = UPDATE_WRITE_BARRIER);
   // Dispatched behavior.
   V8_EXPORT_PRIVATE uint16_t
@@ -1084,12 +1087,13 @@ class V8_EXPORT_PRIVATE FlatStringReader : public Relocatable {
 class ConsStringIterator {
  public:
   inline ConsStringIterator() = default;
-  inline explicit ConsStringIterator(ConsString cons_string, int offset = 0) {
+  inline explicit ConsStringIterator(Tagged<ConsString> cons_string,
+                                     int offset = 0) {
     Reset(cons_string, offset);
   }
   ConsStringIterator(const ConsStringIterator&) = delete;
   ConsStringIterator& operator=(const ConsStringIterator&) = delete;
-  inline void Reset(ConsString cons_string, int offset = 0) {
+  inline void Reset(Tagged<ConsString> cons_string, int offset = 0) {
     depth_ = 0;
     // Next will always return nullptr.
     if (cons_string.is_null()) return;
@@ -1100,7 +1104,7 @@ class ConsStringIterator {
   // to match the offset passed into the constructor or Reset -- this will only
   // be non-zero immediately after construction or Reset, and only if those had
   // a non-zero offset.
-  inline String Next(int* offset_out) {
+  inline Tagged<String> Next(int* offset_out) {
     *offset_out = 0;
     if (depth_ == 0) return String();
     return Continue(offset_out);
@@ -1114,15 +1118,15 @@ class ConsStringIterator {
                 "kStackSize must be power of two");
   static inline int OffsetForDepth(int depth);
 
-  inline void PushLeft(ConsString string);
-  inline void PushRight(ConsString string);
+  inline void PushLeft(Tagged<ConsString> string);
+  inline void PushRight(Tagged<ConsString> string);
   inline void AdjustMaximumDepth();
   inline void Pop();
   inline bool StackBlown() { return maximum_depth_ - depth_ == kStackSize; }
-  V8_EXPORT_PRIVATE void Initialize(ConsString cons_string, int offset);
-  V8_EXPORT_PRIVATE String Continue(int* offset_out);
-  String NextLeaf(bool* blew_stack);
-  String Search(int* offset_out);
+  V8_EXPORT_PRIVATE void Initialize(Tagged<ConsString> cons_string, int offset);
+  V8_EXPORT_PRIVATE Tagged<String> Continue(int* offset_out);
+  Tagged<String> NextLeaf(bool* blew_stack);
+  Tagged<String> Search(int* offset_out);
 
   // Stack must always contain only frames for which right traversal
   // has not yet been performed.
diff --git a/src/objects/struct-inl.h b/src/objects/struct-inl.h
index 91238bb1c74..61fe0eb91e2 100644
--- a/src/objects/struct-inl.h
+++ b/src/objects/struct-inl.h
@@ -28,11 +28,11 @@ NEVER_READ_ONLY_SPACE_IMPL(AccessorPair)
 
 TQ_OBJECT_CONSTRUCTORS_IMPL(ClassPositions)
 
-Object AccessorPair::get(AccessorComponent component) {
+Tagged<Object> AccessorPair::get(AccessorComponent component) {
   return component == ACCESSOR_GETTER ? getter() : setter();
 }
 
-void AccessorPair::set(AccessorComponent component, Object value) {
+void AccessorPair::set(AccessorComponent component, Tagged<Object> value) {
   if (component == ACCESSOR_GETTER) {
     set_getter(value);
   } else {
@@ -40,7 +40,7 @@ void AccessorPair::set(AccessorComponent component, Object value) {
   }
 }
 
-void AccessorPair::set(AccessorComponent component, Object value,
+void AccessorPair::set(AccessorComponent component, Tagged<Object> value,
                        ReleaseStoreTag tag) {
   if (component == ACCESSOR_GETTER) {
     set_getter(value, tag);
@@ -52,12 +52,13 @@ void AccessorPair::set(AccessorComponent component, Object value,
 RELEASE_ACQUIRE_ACCESSORS(AccessorPair, getter, Tagged<Object>, kGetterOffset)
 RELEASE_ACQUIRE_ACCESSORS(AccessorPair, setter, Tagged<Object>, kSetterOffset)
 
-void AccessorPair::SetComponents(Object getter, Object setter) {
+void AccessorPair::SetComponents(Tagged<Object> getter, Tagged<Object> setter) {
   if (!IsNull(getter)) set_getter(getter);
   if (!IsNull(setter)) set_setter(setter);
 }
 
-bool AccessorPair::Equals(Object getter_value, Object setter_value) {
+bool AccessorPair::Equals(Tagged<Object> getter_value,
+                          Tagged<Object> setter_value) {
   return (getter() == getter_value) && (setter() == setter_value);
 }
 
diff --git a/src/objects/struct.h b/src/objects/struct.h
index 349b780dbe0..4fc00fe45b1 100644
--- a/src/objects/struct.h
+++ b/src/objects/struct.h
@@ -49,9 +49,9 @@ class AccessorPair : public TorqueGeneratedAccessorPair<AccessorPair, Struct> {
   NEVER_READ_ONLY_SPACE
   static Handle<AccessorPair> Copy(Isolate* isolate, Handle<AccessorPair> pair);
 
-  inline Object get(AccessorComponent component);
-  inline void set(AccessorComponent component, Object value);
-  inline void set(AccessorComponent component, Object value,
+  inline Tagged<Object> get(AccessorComponent component);
+  inline void set(AccessorComponent component, Tagged<Object> value);
+  inline void set(AccessorComponent component, Tagged<Object> value,
                   ReleaseStoreTag tag);
 
   using TorqueGeneratedAccessorPair::getter;
@@ -69,9 +69,9 @@ class AccessorPair : public TorqueGeneratedAccessorPair<AccessorPair, Struct> {
                                      AccessorComponent component);
 
   // Set both components, skipping arguments which are a JavaScript null.
-  inline void SetComponents(Object getter, Object setter);
+  inline void SetComponents(Tagged<Object> getter, Tagged<Object> setter);
 
-  inline bool Equals(Object getter_value, Object setter_value);
+  inline bool Equals(Tagged<Object> getter_value, Tagged<Object> setter_value);
 
   using BodyDescriptor = StructBodyDescriptor;
 
diff --git a/src/objects/swiss-name-dictionary-inl.h b/src/objects/swiss-name-dictionary-inl.h
index 5b92ceb277c..ac5ac969e08 100644
--- a/src/objects/swiss-name-dictionary-inl.h
+++ b/src/objects/swiss-name-dictionary-inl.h
@@ -148,8 +148,9 @@ void SwissNameDictionary::SetEntryForEnumerationIndex(int enumeration_index,
 }
 
 template <typename IsolateT>
-InternalIndex SwissNameDictionary::FindEntry(IsolateT* isolate, Object key) {
-  Name name = Name::cast(key);
+InternalIndex SwissNameDictionary::FindEntry(IsolateT* isolate,
+                                             Tagged<Object> key) {
+  Tagged<Name> name = Name::cast(key);
   DCHECK(IsUniqueName(name));
   uint32_t hash = name->hash();
 
@@ -180,7 +181,7 @@ InternalIndex SwissNameDictionary::FindEntry(IsolateT* isolate, Object key) {
     Group g{ctrl + seq.offset()};
     for (int i : g.Match(swiss_table::H2(hash))) {
       int candidate_entry = seq.offset(i);
-      Object candidate_key = KeyAt(candidate_entry);
+      Tagged<Object> candidate_key = KeyAt(candidate_entry);
       // This key matching is SwissNameDictionary specific!
       if (candidate_key == key) return InternalIndex(candidate_entry);
     }
@@ -218,12 +219,13 @@ InternalIndex SwissNameDictionary::FindEntry(IsolateT* isolate,
   return FindEntry(isolate, *key);
 }
 
-Object SwissNameDictionary::LoadFromDataTable(int entry, int data_offset) {
+Tagged<Object> SwissNameDictionary::LoadFromDataTable(int entry,
+                                                      int data_offset) {
   return LoadFromDataTable(GetPtrComprCageBase(*this), entry, data_offset);
 }
 
-Object SwissNameDictionary::LoadFromDataTable(PtrComprCageBase cage_base,
-                                              int entry, int data_offset) {
+Tagged<Object> SwissNameDictionary::LoadFromDataTable(
+    PtrComprCageBase cage_base, int entry, int data_offset) {
   DCHECK_LT(static_cast<unsigned>(entry), static_cast<unsigned>(Capacity()));
   int offset = DataTableStartOffset() +
                (entry * kDataTableEntryCount + data_offset) * kTaggedSize;
@@ -231,7 +233,7 @@ Object SwissNameDictionary::LoadFromDataTable(PtrComprCageBase cage_base,
 }
 
 void SwissNameDictionary::StoreToDataTable(int entry, int data_offset,
-                                           Object data) {
+                                           Tagged<Object> data) {
   DCHECK_LT(static_cast<unsigned>(entry), static_cast<unsigned>(Capacity()));
 
   int offset = DataTableStartOffset() +
@@ -242,7 +244,7 @@ void SwissNameDictionary::StoreToDataTable(int entry, int data_offset,
 }
 
 void SwissNameDictionary::StoreToDataTableNoBarrier(int entry, int data_offset,
-                                                    Object data) {
+                                                    Tagged<Object> data) {
   DCHECK_LT(static_cast<unsigned>(entry), static_cast<unsigned>(Capacity()));
 
   int offset = DataTableStartOffset() +
@@ -258,16 +260,17 @@ void SwissNameDictionary::ClearDataTableEntry(Isolate* isolate, int entry) {
   StoreToDataTable(entry, kDataTableValueEntryIndex, roots.the_hole_value());
 }
 
-void SwissNameDictionary::ValueAtPut(int entry, Object value) {
+void SwissNameDictionary::ValueAtPut(int entry, Tagged<Object> value) {
   DCHECK(!IsTheHole(value));
   StoreToDataTable(entry, kDataTableValueEntryIndex, value);
 }
 
-void SwissNameDictionary::ValueAtPut(InternalIndex entry, Object value) {
+void SwissNameDictionary::ValueAtPut(InternalIndex entry,
+                                     Tagged<Object> value) {
   ValueAtPut(entry.as_int(), value);
 }
 
-void SwissNameDictionary::SetKey(int entry, Object key) {
+void SwissNameDictionary::SetKey(int entry, Tagged<Object> key) {
   DCHECK(!IsTheHole(key));
   StoreToDataTable(entry, kDataTableKeyEntryIndex, key);
 }
@@ -283,34 +286,35 @@ void SwissNameDictionary::DetailsAtPut(InternalIndex entry,
   DetailsAtPut(entry.as_int(), details);
 }
 
-Object SwissNameDictionary::KeyAt(int entry) {
+Tagged<Object> SwissNameDictionary::KeyAt(int entry) {
   return LoadFromDataTable(entry, kDataTableKeyEntryIndex);
 }
 
-Object SwissNameDictionary::KeyAt(InternalIndex entry) {
+Tagged<Object> SwissNameDictionary::KeyAt(InternalIndex entry) {
   return KeyAt(entry.as_int());
 }
 
-Name SwissNameDictionary::NameAt(InternalIndex entry) {
+Tagged<Name> SwissNameDictionary::NameAt(InternalIndex entry) {
   return Name::cast(KeyAt(entry));
 }
 
 // This version can be called on empty buckets.
-Object SwissNameDictionary::ValueAtRaw(int entry) {
+Tagged<Object> SwissNameDictionary::ValueAtRaw(int entry) {
   return LoadFromDataTable(entry, kDataTableValueEntryIndex);
 }
 
-Object SwissNameDictionary::ValueAt(InternalIndex entry) {
+Tagged<Object> SwissNameDictionary::ValueAt(InternalIndex entry) {
   DCHECK(IsFull(GetCtrl(entry.as_int())));
   return ValueAtRaw(entry.as_int());
 }
 
-base::Optional<Object> SwissNameDictionary::TryValueAt(InternalIndex entry) {
+base::Optional<Tagged<Object>> SwissNameDictionary::TryValueAt(
+    InternalIndex entry) {
 #if DEBUG
   Isolate* isolate;
   GetIsolateFromHeapObject(*this, &isolate);
   DCHECK_NE(isolate, nullptr);
-  SLOW_DCHECK(!isolate->heap()->IsPendingAllocation(*this));
+  SLOW_DCHECK(!isolate->heap()->IsPendingAllocation(Tagged(*this)));
 #endif  // DEBUG
   // We can read Capacity() in a non-atomic way since we are reading an
   // initialized object which is not pending allocation.
@@ -407,7 +411,7 @@ void SwissNameDictionary::SetMetaTableField(int field_index, int value) {
   // |kMax2ByteMetaTableCapacity| in the .cc file for an explanation of these
   // constants.
   int capacity = Capacity();
-  ByteArray meta_table = this->meta_table();
+  Tagged<ByteArray> meta_table = this->meta_table();
   if (capacity <= kMax1ByteMetaTableCapacity) {
     SetMetaTableField<uint8_t>(meta_table, field_index, value);
   } else if (capacity <= kMax2ByteMetaTableCapacity) {
@@ -422,7 +426,7 @@ int SwissNameDictionary::GetMetaTableField(int field_index) {
   // |kMax2ByteMetaTableCapacity| in the .cc file for an explanation of these
   // constants.
   int capacity = Capacity();
-  ByteArray meta_table = this->meta_table();
+  Tagged<ByteArray> meta_table = this->meta_table();
   if (capacity <= kMax1ByteMetaTableCapacity) {
     return GetMetaTableField<uint8_t>(meta_table, field_index);
   } else if (capacity <= kMax2ByteMetaTableCapacity) {
@@ -434,7 +438,7 @@ int SwissNameDictionary::GetMetaTableField(int field_index) {
 
 // static
 template <typename T>
-void SwissNameDictionary::SetMetaTableField(ByteArray meta_table,
+void SwissNameDictionary::SetMetaTableField(Tagged<ByteArray> meta_table,
                                             int field_index, int value) {
   static_assert((std::is_same<T, uint8_t>::value) ||
                 (std::is_same<T, uint16_t>::value) ||
@@ -448,7 +452,7 @@ void SwissNameDictionary::SetMetaTableField(ByteArray meta_table,
 
 // static
 template <typename T>
-int SwissNameDictionary::GetMetaTableField(ByteArray meta_table,
+int SwissNameDictionary::GetMetaTableField(Tagged<ByteArray> meta_table,
                                            int field_index) {
   static_assert((std::is_same<T, uint8_t>::value) ||
                 (std::is_same<T, uint16_t>::value) ||
@@ -486,20 +490,21 @@ constexpr int SwissNameDictionary::MetaTableSizeFor(int capacity) {
   return per_entry_size * (MaxUsableCapacity(capacity) + 2);
 }
 
-bool SwissNameDictionary::IsKey(ReadOnlyRoots roots, Object key_candidate) {
+bool SwissNameDictionary::IsKey(ReadOnlyRoots roots,
+                                Tagged<Object> key_candidate) {
   return key_candidate != roots.the_hole_value();
 }
 
 bool SwissNameDictionary::ToKey(ReadOnlyRoots roots, int entry,
-                                Object* out_key) {
-  Object k = KeyAt(entry);
+                                Tagged<Object>* out_key) {
+  Tagged<Object> k = KeyAt(entry);
   if (!IsKey(roots, k)) return false;
   *out_key = k;
   return true;
 }
 
 bool SwissNameDictionary::ToKey(ReadOnlyRoots roots, InternalIndex entry,
-                                Object* out_key) {
+                                Tagged<Object>* out_key) {
   return ToKey(roots, entry.as_int(), out_key);
 }
 
@@ -530,7 +535,7 @@ Handle<SwissNameDictionary> SwissNameDictionary::Add(
   return table;
 }
 
-int SwissNameDictionary::AddInternal(Name key, Object value,
+int SwissNameDictionary::AddInternal(Tagged<Name> key, Tagged<Object> value,
                                      PropertyDetails details) {
   DisallowHeapAllocation no_gc;
 
@@ -556,7 +561,8 @@ int SwissNameDictionary::AddInternal(Name key, Object value,
 }
 
 template <typename IsolateT>
-void SwissNameDictionary::Initialize(IsolateT* isolate, ByteArray meta_table,
+void SwissNameDictionary::Initialize(IsolateT* isolate,
+                                     Tagged<ByteArray> meta_table,
                                      int capacity) {
   DCHECK(IsValidCapacity(capacity));
   DisallowHeapAllocation no_gc;
diff --git a/src/objects/swiss-name-dictionary.cc b/src/objects/swiss-name-dictionary.cc
index 0f1d27593ad..58cea831365 100644
--- a/src/objects/swiss-name-dictionary.cc
+++ b/src/objects/swiss-name-dictionary.cc
@@ -57,10 +57,10 @@ Handle<SwissNameDictionary> SwissNameDictionary::Rehash(
   for (int enum_index = 0; enum_index < table->UsedCapacity(); ++enum_index) {
     int entry = table->EntryForEnumerationIndex(enum_index);
 
-    Object key;
+    Tagged<Object> key;
 
     if (table->ToKey(roots, entry, &key)) {
-      Object value = table->ValueAtRaw(entry);
+      Tagged<Object> value = table->ValueAtRaw(entry);
       PropertyDetails details = table->DetailsAt(entry);
 
       int new_entry = new_table->AddInternal(Name::cast(key), value, details);
@@ -77,7 +77,7 @@ Handle<SwissNameDictionary> SwissNameDictionary::Rehash(
   return new_table;
 }
 
-bool SwissNameDictionary::EqualsForTesting(SwissNameDictionary other) {
+bool SwissNameDictionary::EqualsForTesting(Tagged<SwissNameDictionary> other) {
   if (Capacity() != other->Capacity() ||
       NumberOfElements() != other->NumberOfElements() ||
       NumberOfDeletedElements() != other->NumberOfDeletedElements() ||
@@ -145,8 +145,8 @@ Handle<SwissNameDictionary> SwissNameDictionary::ShallowCopy(
 
     // We may have to trigger write barriers when copying the data table.
     for (int i = 0; i < capacity; ++i) {
-      Object key = table->KeyAt(i);
-      Object value = table->ValueAtRaw(i);
+      Tagged<Object> key = table->KeyAt(i);
+      Tagged<Object> value = table->ValueAtRaw(i);
 
       // Cannot use SetKey/ValueAtPut because they don't accept the hole as data
       // to store.
@@ -226,7 +226,7 @@ void SwissNameDictionary::Rehash(IsolateT* isolate) {
   int data_index = 0;
   for (int enum_index = 0; enum_index < UsedCapacity(); ++enum_index) {
     int entry = EntryForEnumerationIndex(enum_index);
-    Object key;
+    Tagged<Object> key;
     if (!ToKey(roots, entry, &key)) continue;
 
     data[data_index++] =
@@ -255,7 +255,7 @@ int SwissNameDictionary::NumberOfEnumerableProperties() {
   ReadOnlyRoots roots = this->GetReadOnlyRoots();
   int result = 0;
   for (InternalIndex i : this->IterateEntries()) {
-    Object k;
+    Tagged<Object> k;
     if (!this->ToKey(roots, i, &k)) continue;
     if (Object::FilterKey(k, ENUMERABLE_STRINGS)) continue;
     PropertyDetails details = this->DetailsAt(i);
@@ -268,12 +268,13 @@ int SwissNameDictionary::NumberOfEnumerableProperties() {
 // TODO(emrich, v8:11388): This is almost an identical copy of
 // Dictionary<..>::SlowReverseLookup. Consolidate both versions elsewhere (e.g.,
 // hash-table-utils)?
-Object SwissNameDictionary::SlowReverseLookup(Isolate* isolate, Object value) {
+Tagged<Object> SwissNameDictionary::SlowReverseLookup(Isolate* isolate,
+                                                      Tagged<Object> value) {
   ReadOnlyRoots roots(isolate);
   for (InternalIndex i : IterateEntries()) {
-    Object k;
+    Tagged<Object> k;
     if (!ToKey(roots, i, &k)) continue;
-    Object e = this->ValueAt(i);
+    Tagged<Object> e = this->ValueAt(i);
     if (e == value) return k;
   }
   return roots.undefined_value();
@@ -296,9 +297,9 @@ static_assert(SwissNameDictionary::MaxUsableCapacity(
               std::numeric_limits<uint16_t>::max());
 
 template V8_EXPORT_PRIVATE void SwissNameDictionary::Initialize(
-    Isolate* isolate, ByteArray meta_table, int capacity);
+    Isolate* isolate, Tagged<ByteArray> meta_table, int capacity);
 template V8_EXPORT_PRIVATE void SwissNameDictionary::Initialize(
-    LocalIsolate* isolate, ByteArray meta_table, int capacity);
+    LocalIsolate* isolate, Tagged<ByteArray> meta_table, int capacity);
 
 template V8_EXPORT_PRIVATE Handle<SwissNameDictionary>
 SwissNameDictionary::Rehash(LocalIsolate* isolate,
diff --git a/src/objects/swiss-name-dictionary.h b/src/objects/swiss-name-dictionary.h
index 6b51b906a42..12976026d88 100644
--- a/src/objects/swiss-name-dictionary.h
+++ b/src/objects/swiss-name-dictionary.h
@@ -86,7 +86,7 @@ class V8_EXPORT_PRIVATE SwissNameDictionary : public HeapObject {
       Isolate* isolate, Handle<SwissNameDictionary> table, InternalIndex entry);
 
   template <typename IsolateT>
-  inline InternalIndex FindEntry(IsolateT* isolate, Object key);
+  inline InternalIndex FindEntry(IsolateT* isolate, Tagged<Object> key);
 
   // This is to make the interfaces of NameDictionary::FindEntry and
   // OrderedNameDictionary::FindEntry compatible.
@@ -96,17 +96,18 @@ class V8_EXPORT_PRIVATE SwissNameDictionary : public HeapObject {
   template <typename IsolateT>
   inline InternalIndex FindEntry(IsolateT* isolate, Handle<Object> key);
 
-  static inline bool IsKey(ReadOnlyRoots roots, Object key_candidate);
-  inline bool ToKey(ReadOnlyRoots roots, InternalIndex entry, Object* out_key);
+  static inline bool IsKey(ReadOnlyRoots roots, Tagged<Object> key_candidate);
+  inline bool ToKey(ReadOnlyRoots roots, InternalIndex entry,
+                    Tagged<Object>* out_key);
 
-  inline Object KeyAt(InternalIndex entry);
-  inline Name NameAt(InternalIndex entry);
-  inline Object ValueAt(InternalIndex entry);
+  inline Tagged<Object> KeyAt(InternalIndex entry);
+  inline Tagged<Name> NameAt(InternalIndex entry);
+  inline Tagged<Object> ValueAt(InternalIndex entry);
   // Returns {} if we would be reading out of the bounds of the object.
-  inline base::Optional<Object> TryValueAt(InternalIndex entry);
+  inline base::Optional<Tagged<Object>> TryValueAt(InternalIndex entry);
   inline PropertyDetails DetailsAt(InternalIndex entry);
 
-  inline void ValueAtPut(InternalIndex entry, Object value);
+  inline void ValueAtPut(InternalIndex entry, Tagged<Object> value);
   inline void DetailsAtPut(InternalIndex entry, PropertyDetails value);
 
   inline int NumberOfElements();
@@ -128,10 +129,11 @@ class V8_EXPORT_PRIVATE SwissNameDictionary : public HeapObject {
   // |this| and |other| is the same. The only exceptions are the meta table
   // pointer (which must differ  between the two tables) and PropertyDetails of
   // deleted entries (which reside in initialized memory, but are not compared).
-  bool EqualsForTesting(SwissNameDictionary other);
+  bool EqualsForTesting(Tagged<SwissNameDictionary> other);
 
   template <typename IsolateT>
-  void Initialize(IsolateT* isolate, ByteArray meta_table, int capacity);
+  void Initialize(IsolateT* isolate, Tagged<ByteArray> meta_table,
+                  int capacity);
 
   template <typename IsolateT>
   static Handle<SwissNameDictionary> Rehash(IsolateT* isolate,
@@ -143,7 +145,7 @@ class V8_EXPORT_PRIVATE SwissNameDictionary : public HeapObject {
   inline void SetHash(int hash);
   inline int Hash();
 
-  Object SlowReverseLookup(Isolate* isolate, Object value);
+  Tagged<Object> SlowReverseLookup(Isolate* isolate, Tagged<Object> value);
 
   class IndexIterator {
    public:
@@ -280,23 +282,24 @@ class V8_EXPORT_PRIVATE SwissNameDictionary : public HeapObject {
 
   // Sets key and value to the hole for the given entry.
   inline void ClearDataTableEntry(Isolate* isolate, int entry);
-  inline void SetKey(int entry, Object key);
+  inline void SetKey(int entry, Tagged<Object> key);
 
   inline void DetailsAtPut(int entry, PropertyDetails value);
-  inline void ValueAtPut(int entry, Object value);
+  inline void ValueAtPut(int entry, Tagged<Object> value);
 
   inline PropertyDetails DetailsAt(int entry);
-  inline Object ValueAtRaw(int entry);
-  inline Object KeyAt(int entry);
+  inline Tagged<Object> ValueAtRaw(int entry);
+  inline Tagged<Object> KeyAt(int entry);
 
-  inline bool ToKey(ReadOnlyRoots roots, int entry, Object* out_key);
+  inline bool ToKey(ReadOnlyRoots roots, int entry, Tagged<Object>* out_key);
 
   inline int FindFirstEmpty(uint32_t hash);
   // Adds |key| ->  (|value|, |details|) as a new mapping to the table, which
   // must have sufficient room. Returns the entry (= bucket) used by the new
   // mapping. Does not update the number of present entries or the
   // enumeration table.
-  inline int AddInternal(Name key, Object value, PropertyDetails details);
+  inline int AddInternal(Tagged<Name> key, Tagged<Object> value,
+                         PropertyDetails details);
 
   // Use |set_ctrl| for modifications whenever possible, since that function
   // correctly maintains the copy of the first group at the end of the ctrl
@@ -313,12 +316,12 @@ class V8_EXPORT_PRIVATE SwissNameDictionary : public HeapObject {
   inline void SetCtrl(int entry, ctrl_t h);
   inline ctrl_t GetCtrl(int entry);
 
-  inline Object LoadFromDataTable(int entry, int data_offset);
-  inline Object LoadFromDataTable(PtrComprCageBase cage_base, int entry,
-                                  int data_offset);
-  inline void StoreToDataTable(int entry, int data_offset, Object data);
+  inline Tagged<Object> LoadFromDataTable(int entry, int data_offset);
+  inline Tagged<Object> LoadFromDataTable(PtrComprCageBase cage_base, int entry,
+                                          int data_offset);
+  inline void StoreToDataTable(int entry, int data_offset, Tagged<Object> data);
   inline void StoreToDataTableNoBarrier(int entry, int data_offset,
-                                        Object data);
+                                        Tagged<Object> data);
 
   inline void SetCapacity(int capacity);
   inline void SetNumberOfElements(int elements);
@@ -336,10 +339,11 @@ class V8_EXPORT_PRIVATE SwissNameDictionary : public HeapObject {
   inline int GetMetaTableField(int field_index);
 
   template <typename T>
-  inline static void SetMetaTableField(ByteArray meta_table, int field_index,
-                                       int value);
+  inline static void SetMetaTableField(Tagged<ByteArray> meta_table,
+                                       int field_index, int value);
   template <typename T>
-  inline static int GetMetaTableField(ByteArray meta_table, int field_index);
+  inline static int GetMetaTableField(Tagged<ByteArray> meta_table,
+                                      int field_index);
 };
 
 }  // namespace internal
diff --git a/src/objects/symbol-table.cc b/src/objects/symbol-table.cc
index 4dcce78d6da..41200b68ced 100644
--- a/src/objects/symbol-table.cc
+++ b/src/objects/symbol-table.cc
@@ -7,12 +7,12 @@
 namespace v8 {
 namespace internal {
 
-Object RegisteredSymbolTable::SlowReverseLookup(Object value) {
+Tagged<Object> RegisteredSymbolTable::SlowReverseLookup(Tagged<Object> value) {
   ReadOnlyRoots roots = this->GetReadOnlyRoots();
   for (InternalIndex i : this->IterateEntries()) {
-    Object k;
+    Tagged<Object> k;
     if (!this->ToKey(roots, i, &k)) continue;
-    Object e = this->ValueAt(i);
+    Tagged<Object> e = this->ValueAt(i);
     if (e == value) return k;
   }
   return roots.undefined_value();
diff --git a/src/objects/tagged-impl-inl.h b/src/objects/tagged-impl-inl.h
index 9a447499498..af99089bc7b 100644
--- a/src/objects/tagged-impl-inl.h
+++ b/src/objects/tagged-impl-inl.h
@@ -19,7 +19,7 @@ namespace v8 {
 namespace internal {
 
 template <HeapObjectReferenceType kRefType, typename StorageType>
-bool TaggedImpl<kRefType, StorageType>::ToSmi(Smi* value) const {
+bool TaggedImpl<kRefType, StorageType>::ToSmi(Tagged<Smi>* value) const {
   if (HAS_SMI_TAG(ptr_)) {
     *value = ToSmi();
     return true;
@@ -28,23 +28,23 @@ bool TaggedImpl<kRefType, StorageType>::ToSmi(Smi* value) const {
 }
 
 template <HeapObjectReferenceType kRefType, typename StorageType>
-Smi TaggedImpl<kRefType, StorageType>::ToSmi() const {
+Tagged<Smi> TaggedImpl<kRefType, StorageType>::ToSmi() const {
   DCHECK(HAS_SMI_TAG(ptr_));
   if (kIsFull) {
-    return Smi(ptr_);
+    return Tagged<Smi>(ptr_);
   }
   // Implementation for compressed pointers.
-  return Smi(
+  return Tagged<Smi>(
       CompressionScheme::DecompressTaggedSigned(static_cast<Tagged_t>(ptr_)));
 }
 
 //
-// TaggedImpl::GetHeapObject(HeapObject* result) implementation.
+// TaggedImpl::GetHeapObject(Tagged<HeapObject>* result) implementation.
 //
 
 template <HeapObjectReferenceType kRefType, typename StorageType>
 bool TaggedImpl<kRefType, StorageType>::GetHeapObject(
-    HeapObject* result) const {
+    Tagged<HeapObject>* result) const {
   CHECK(kIsFull);
   if (!IsStrongOrWeak()) return false;
   *result = GetHeapObject();
@@ -53,7 +53,7 @@ bool TaggedImpl<kRefType, StorageType>::GetHeapObject(
 
 template <HeapObjectReferenceType kRefType, typename StorageType>
 bool TaggedImpl<kRefType, StorageType>::GetHeapObject(
-    Isolate* isolate, HeapObject* result) const {
+    Isolate* isolate, Tagged<HeapObject>* result) const {
   if (kIsFull) return GetHeapObject(result);
   // Implementation for compressed pointers.
   if (!IsStrongOrWeak()) return false;
@@ -62,14 +62,14 @@ bool TaggedImpl<kRefType, StorageType>::GetHeapObject(
 }
 
 //
-// TaggedImpl::GetHeapObject(HeapObject* result,
+// TaggedImpl::GetHeapObject(Tagged<HeapObject>* result,
 //                           HeapObjectReferenceType* reference_type)
 // implementation.
 //
 
 template <HeapObjectReferenceType kRefType, typename StorageType>
 bool TaggedImpl<kRefType, StorageType>::GetHeapObject(
-    HeapObject* result, HeapObjectReferenceType* reference_type) const {
+    Tagged<HeapObject>* result, HeapObjectReferenceType* reference_type) const {
   CHECK(kIsFull);
   if (!IsStrongOrWeak()) return false;
   *reference_type = IsWeakOrCleared() ? HeapObjectReferenceType::WEAK
@@ -80,7 +80,7 @@ bool TaggedImpl<kRefType, StorageType>::GetHeapObject(
 
 template <HeapObjectReferenceType kRefType, typename StorageType>
 bool TaggedImpl<kRefType, StorageType>::GetHeapObject(
-    Isolate* isolate, HeapObject* result,
+    Isolate* isolate, Tagged<HeapObject>* result,
     HeapObjectReferenceType* reference_type) const {
   if (kIsFull) return GetHeapObject(result, reference_type);
   // Implementation for compressed pointers.
@@ -92,12 +92,12 @@ bool TaggedImpl<kRefType, StorageType>::GetHeapObject(
 }
 
 //
-// TaggedImpl::GetHeapObjectIfStrong(HeapObject* result) implementation.
+// TaggedImpl::GetHeapObjectIfStrong(Tagged<HeapObject>* result) implementation.
 //
 
 template <HeapObjectReferenceType kRefType, typename StorageType>
 bool TaggedImpl<kRefType, StorageType>::GetHeapObjectIfStrong(
-    HeapObject* result) const {
+    Tagged<HeapObject>* result) const {
   CHECK(kIsFull);
   if (IsStrong()) {
     *result = HeapObject::cast(Object(ptr_));
@@ -108,7 +108,7 @@ bool TaggedImpl<kRefType, StorageType>::GetHeapObjectIfStrong(
 
 template <HeapObjectReferenceType kRefType, typename StorageType>
 bool TaggedImpl<kRefType, StorageType>::GetHeapObjectIfStrong(
-    Isolate* isolate, HeapObject* result) const {
+    Isolate* isolate, Tagged<HeapObject>* result) const {
   if (kIsFull) return GetHeapObjectIfStrong(result);
   // Implementation for compressed pointers.
   if (IsStrong()) {
@@ -124,15 +124,15 @@ bool TaggedImpl<kRefType, StorageType>::GetHeapObjectIfStrong(
 //
 
 template <HeapObjectReferenceType kRefType, typename StorageType>
-HeapObject TaggedImpl<kRefType, StorageType>::GetHeapObjectAssumeStrong()
-    const {
+Tagged<HeapObject>
+TaggedImpl<kRefType, StorageType>::GetHeapObjectAssumeStrong() const {
   CHECK(kIsFull);
   DCHECK(IsStrong());
   return HeapObject::cast(Object(ptr_));
 }
 
 template <HeapObjectReferenceType kRefType, typename StorageType>
-HeapObject TaggedImpl<kRefType, StorageType>::GetHeapObjectAssumeStrong(
+Tagged<HeapObject> TaggedImpl<kRefType, StorageType>::GetHeapObjectAssumeStrong(
     Isolate* isolate) const {
   if (kIsFull) return GetHeapObjectAssumeStrong();
   // Implementation for compressed pointers.
@@ -142,12 +142,12 @@ HeapObject TaggedImpl<kRefType, StorageType>::GetHeapObjectAssumeStrong(
 }
 
 //
-// TaggedImpl::GetHeapObjectIfWeak(HeapObject* result) implementation
+// TaggedImpl::GetHeapObjectIfWeak(Tagged<HeapObject>* result) implementation
 //
 
 template <HeapObjectReferenceType kRefType, typename StorageType>
 bool TaggedImpl<kRefType, StorageType>::GetHeapObjectIfWeak(
-    HeapObject* result) const {
+    Tagged<HeapObject>* result) const {
   CHECK(kIsFull);
   if (kCanBeWeak) {
     if (IsWeak()) {
@@ -163,7 +163,7 @@ bool TaggedImpl<kRefType, StorageType>::GetHeapObjectIfWeak(
 
 template <HeapObjectReferenceType kRefType, typename StorageType>
 bool TaggedImpl<kRefType, StorageType>::GetHeapObjectIfWeak(
-    Isolate* isolate, HeapObject* result) const {
+    Isolate* isolate, Tagged<HeapObject>* result) const {
   if (kIsFull) return GetHeapObjectIfWeak(result);
   // Implementation for compressed pointers.
   if (kCanBeWeak) {
@@ -183,14 +183,15 @@ bool TaggedImpl<kRefType, StorageType>::GetHeapObjectIfWeak(
 //
 
 template <HeapObjectReferenceType kRefType, typename StorageType>
-HeapObject TaggedImpl<kRefType, StorageType>::GetHeapObjectAssumeWeak() const {
+Tagged<HeapObject> TaggedImpl<kRefType, StorageType>::GetHeapObjectAssumeWeak()
+    const {
   CHECK(kIsFull);
   DCHECK(IsWeak());
   return GetHeapObject();
 }
 
 template <HeapObjectReferenceType kRefType, typename StorageType>
-HeapObject TaggedImpl<kRefType, StorageType>::GetHeapObjectAssumeWeak(
+Tagged<HeapObject> TaggedImpl<kRefType, StorageType>::GetHeapObjectAssumeWeak(
     Isolate* isolate) const {
   if (kIsFull) return GetHeapObjectAssumeWeak();
   // Implementation for compressed pointers.
@@ -203,7 +204,7 @@ HeapObject TaggedImpl<kRefType, StorageType>::GetHeapObjectAssumeWeak(
 //
 
 template <HeapObjectReferenceType kRefType, typename StorageType>
-HeapObject TaggedImpl<kRefType, StorageType>::GetHeapObject() const {
+Tagged<HeapObject> TaggedImpl<kRefType, StorageType>::GetHeapObject() const {
   CHECK(kIsFull);
   DCHECK(!IsSmi());
   if (kCanBeWeak) {
@@ -216,7 +217,7 @@ HeapObject TaggedImpl<kRefType, StorageType>::GetHeapObject() const {
 }
 
 template <HeapObjectReferenceType kRefType, typename StorageType>
-HeapObject TaggedImpl<kRefType, StorageType>::GetHeapObject(
+Tagged<HeapObject> TaggedImpl<kRefType, StorageType>::GetHeapObject(
     Isolate* isolate) const {
   if (kIsFull) return GetHeapObject();
   // Implementation for compressed pointers.
@@ -237,7 +238,7 @@ HeapObject TaggedImpl<kRefType, StorageType>::GetHeapObject(
 //
 
 template <HeapObjectReferenceType kRefType, typename StorageType>
-Object TaggedImpl<kRefType, StorageType>::GetHeapObjectOrSmi() const {
+Tagged<Object> TaggedImpl<kRefType, StorageType>::GetHeapObjectOrSmi() const {
   CHECK(kIsFull);
   if (IsSmi()) {
     return Object(ptr_);
@@ -246,7 +247,7 @@ Object TaggedImpl<kRefType, StorageType>::GetHeapObjectOrSmi() const {
 }
 
 template <HeapObjectReferenceType kRefType, typename StorageType>
-Object TaggedImpl<kRefType, StorageType>::GetHeapObjectOrSmi(
+Tagged<Object> TaggedImpl<kRefType, StorageType>::GetHeapObjectOrSmi(
     Isolate* isolate) const {
   if (kIsFull) return GetHeapObjectOrSmi();
   // Implementation for compressed pointers.
diff --git a/src/objects/tagged-impl.cc b/src/objects/tagged-impl.cc
index 00359c4fdaf..58c639a62f9 100644
--- a/src/objects/tagged-impl.cc
+++ b/src/objects/tagged-impl.cc
@@ -42,10 +42,10 @@ void ShortPrint(TaggedImpl<kRefType, StorageType> ptr, FILE* out) {
   OFStream os(out);
   os << Brief(ptr);
 }
-template void ShortPrint(
+template EXPORT_TEMPLATE_DEFINE(V8_EXPORT_PRIVATE) void ShortPrint(
     TaggedImpl<HeapObjectReferenceType::STRONG, Address> ptr, FILE* out);
-template void ShortPrint(TaggedImpl<HeapObjectReferenceType::WEAK, Address> ptr,
-                         FILE* out);
+template EXPORT_TEMPLATE_DEFINE(V8_EXPORT_PRIVATE) void ShortPrint(
+    TaggedImpl<HeapObjectReferenceType::WEAK, Address> ptr, FILE* out);
 
 template <HeapObjectReferenceType kRefType, typename StorageType>
 void ShortPrint(TaggedImpl<kRefType, StorageType> ptr,
@@ -54,20 +54,21 @@ void ShortPrint(TaggedImpl<kRefType, StorageType> ptr,
   os << Brief(ptr);
   accumulator->Add(os.str().c_str());
 }
-template void ShortPrint(
+template EXPORT_TEMPLATE_DEFINE(V8_EXPORT_PRIVATE) void ShortPrint(
     TaggedImpl<HeapObjectReferenceType::STRONG, Address> ptr,
     StringStream* accumulator);
-template void ShortPrint(TaggedImpl<HeapObjectReferenceType::WEAK, Address> ptr,
-                         StringStream* accumulator);
+template EXPORT_TEMPLATE_DEFINE(V8_EXPORT_PRIVATE) void ShortPrint(
+    TaggedImpl<HeapObjectReferenceType::WEAK, Address> ptr,
+    StringStream* accumulator);
 
 template <HeapObjectReferenceType kRefType, typename StorageType>
 void ShortPrint(TaggedImpl<kRefType, StorageType> ptr, std::ostream& os) {
   os << Brief(ptr);
 }
-template void ShortPrint(
+template EXPORT_TEMPLATE_DEFINE(V8_EXPORT_PRIVATE) void ShortPrint(
     TaggedImpl<HeapObjectReferenceType::STRONG, Address> ptr, std::ostream& os);
-template void ShortPrint(TaggedImpl<HeapObjectReferenceType::WEAK, Address> ptr,
-                         std::ostream& os);
+template EXPORT_TEMPLATE_DEFINE(V8_EXPORT_PRIVATE) void ShortPrint(
+    TaggedImpl<HeapObjectReferenceType::WEAK, Address> ptr, std::ostream& os);
 
 #ifdef OBJECT_PRINT
 template <HeapObjectReferenceType kRefType, typename StorageType>
@@ -76,13 +77,15 @@ void Print(TaggedImpl<kRefType, StorageType> ptr) {
   Print(ptr, os);
   os << std::flush;
 }
-template void Print(TaggedImpl<HeapObjectReferenceType::STRONG, Address> ptr);
-template void Print(TaggedImpl<HeapObjectReferenceType::WEAK, Address> ptr);
+template EXPORT_TEMPLATE_DEFINE(V8_EXPORT_PRIVATE) void Print(
+    TaggedImpl<HeapObjectReferenceType::STRONG, Address> ptr);
+template EXPORT_TEMPLATE_DEFINE(V8_EXPORT_PRIVATE) void Print(
+    TaggedImpl<HeapObjectReferenceType::WEAK, Address> ptr);
 
 template <HeapObjectReferenceType kRefType, typename StorageType>
 void Print(TaggedImpl<kRefType, StorageType> ptr, std::ostream& os) {
-  Smi smi(0);
-  HeapObject heap_object;
+  Tagged<Smi> smi;
+  Tagged<HeapObject> heap_object;
   if (ptr.ToSmi(&smi)) {
     os << "Smi: " << std::hex << "0x" << smi.value();
     os << std::dec << " (" << smi.value() << ")\n";
@@ -97,10 +100,10 @@ void Print(TaggedImpl<kRefType, StorageType> ptr, std::ostream& os) {
     UNREACHABLE();
   }
 }
-template void Print(TaggedImpl<HeapObjectReferenceType::STRONG, Address> ptr,
-                    std::ostream& os);
-template void Print(TaggedImpl<HeapObjectReferenceType::WEAK, Address> ptr,
-                    std::ostream& os);
+template EXPORT_TEMPLATE_DEFINE(V8_EXPORT_PRIVATE) void Print(
+    TaggedImpl<HeapObjectReferenceType::STRONG, Address> ptr, std::ostream& os);
+template EXPORT_TEMPLATE_DEFINE(V8_EXPORT_PRIVATE) void Print(
+    TaggedImpl<HeapObjectReferenceType::WEAK, Address> ptr, std::ostream& os);
 #endif  // OBJECT_PRINT
 
 // Explicit instantiation declarations.
diff --git a/src/objects/tagged-impl.h b/src/objects/tagged-impl.h
index e13c6d32310..5666035e203 100644
--- a/src/objects/tagged-impl.h
+++ b/src/objects/tagged-impl.h
@@ -6,6 +6,7 @@
 #define V8_OBJECTS_TAGGED_IMPL_H_
 
 #include "include/v8-internal.h"
+#include "src/base/export-template.h"
 #include "src/base/macros.h"
 #include "src/common/checks.h"
 #include "src/common/globals.h"
@@ -120,8 +121,8 @@ class TaggedImpl {
 
   // Returns true if this tagged value is a Smi.
   constexpr bool IsSmi() const { return HAS_SMI_TAG(ptr_); }
-  inline bool ToSmi(Smi* value) const;
-  inline Smi ToSmi() const;
+  inline bool ToSmi(Tagged<Smi>* value) const;
+  inline Tagged<Smi> ToSmi() const;
 
   // Returns true if this tagged value is a strong pointer to a HeapObject.
   constexpr inline bool IsHeapObject() const { return IsStrong(); }
@@ -165,43 +166,45 @@ class TaggedImpl {
 
   // If this tagged value is a strong pointer to a HeapObject, returns true and
   // sets *result. Otherwise returns false.
-  inline bool GetHeapObjectIfStrong(HeapObject* result) const;
-  inline bool GetHeapObjectIfStrong(Isolate* isolate, HeapObject* result) const;
+  inline bool GetHeapObjectIfStrong(Tagged<HeapObject>* result) const;
+  inline bool GetHeapObjectIfStrong(Isolate* isolate,
+                                    Tagged<HeapObject>* result) const;
 
   // DCHECKs that this tagged value is a strong pointer to a HeapObject and
   // returns the HeapObject.
-  inline HeapObject GetHeapObjectAssumeStrong() const;
-  inline HeapObject GetHeapObjectAssumeStrong(Isolate* isolate) const;
+  inline Tagged<HeapObject> GetHeapObjectAssumeStrong() const;
+  inline Tagged<HeapObject> GetHeapObjectAssumeStrong(Isolate* isolate) const;
 
   // If this tagged value is a weak pointer to a HeapObject, returns true and
   // sets *result. Otherwise returns false.
-  inline bool GetHeapObjectIfWeak(HeapObject* result) const;
-  inline bool GetHeapObjectIfWeak(Isolate* isolate, HeapObject* result) const;
+  inline bool GetHeapObjectIfWeak(Tagged<HeapObject>* result) const;
+  inline bool GetHeapObjectIfWeak(Isolate* isolate,
+                                  Tagged<HeapObject>* result) const;
 
   // DCHECKs that this tagged value is a weak pointer to a HeapObject and
   // returns the HeapObject.
-  inline HeapObject GetHeapObjectAssumeWeak() const;
-  inline HeapObject GetHeapObjectAssumeWeak(Isolate* isolate) const;
+  inline Tagged<HeapObject> GetHeapObjectAssumeWeak() const;
+  inline Tagged<HeapObject> GetHeapObjectAssumeWeak(Isolate* isolate) const;
 
   // If this tagged value is a strong or weak pointer to a HeapObject, returns
   // true and sets *result. Otherwise returns false.
-  inline bool GetHeapObject(HeapObject* result) const;
-  inline bool GetHeapObject(Isolate* isolate, HeapObject* result) const;
+  inline bool GetHeapObject(Tagged<HeapObject>* result) const;
+  inline bool GetHeapObject(Isolate* isolate, Tagged<HeapObject>* result) const;
 
-  inline bool GetHeapObject(HeapObject* result,
+  inline bool GetHeapObject(Tagged<HeapObject>* result,
                             HeapObjectReferenceType* reference_type) const;
-  inline bool GetHeapObject(Isolate* isolate, HeapObject* result,
+  inline bool GetHeapObject(Isolate* isolate, Tagged<HeapObject>* result,
                             HeapObjectReferenceType* reference_type) const;
 
   // DCHECKs that this tagged value is a strong or a weak pointer to a
   // HeapObject and returns the HeapObject.
-  inline HeapObject GetHeapObject() const;
-  inline HeapObject GetHeapObject(Isolate* isolate) const;
+  inline Tagged<HeapObject> GetHeapObject() const;
+  inline Tagged<HeapObject> GetHeapObject(Isolate* isolate) const;
 
   // DCHECKs that this tagged value is a strong or a weak pointer to a
   // HeapObject or a Smi and returns the HeapObject or Smi.
-  inline Object GetHeapObjectOrSmi() const;
-  inline Object GetHeapObjectOrSmi(Isolate* isolate) const;
+  inline Tagged<Object> GetHeapObjectOrSmi() const;
+  inline Tagged<Object> GetHeapObjectOrSmi(Isolate* isolate) const;
 
   // Cast operation is available only for full non-weak tagged values.
   template <typename T>
@@ -220,20 +223,25 @@ class TaggedImpl {
 
 // Prints this object without details.
 template <HeapObjectReferenceType kRefType, typename StorageType>
+EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE)
 void ShortPrint(TaggedImpl<kRefType, StorageType> ptr, FILE* out = stdout);
 
 // Prints this object without details to a message accumulator.
 template <HeapObjectReferenceType kRefType, typename StorageType>
+EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE)
 void ShortPrint(TaggedImpl<kRefType, StorageType> ptr,
                 StringStream* accumulator);
 
 template <HeapObjectReferenceType kRefType, typename StorageType>
+EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE)
 void ShortPrint(TaggedImpl<kRefType, StorageType> ptr, std::ostream& os);
 
 #ifdef OBJECT_PRINT
 template <HeapObjectReferenceType kRefType, typename StorageType>
+EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE)
 void Print(TaggedImpl<kRefType, StorageType> ptr);
 template <HeapObjectReferenceType kRefType, typename StorageType>
+EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE)
 void Print(TaggedImpl<kRefType, StorageType> ptr, std::ostream& os);
 #else
 template <HeapObjectReferenceType kRefType, typename StorageType>
diff --git a/src/objects/tagged-index.h b/src/objects/tagged-index.h
index f17c72ba8a9..b59682865d8 100644
--- a/src/objects/tagged-index.h
+++ b/src/objects/tagged-index.h
@@ -50,9 +50,10 @@ class TaggedIndex : public Object {
   }
 
   // Convert a value to a TaggedIndex object.
-  static inline TaggedIndex FromIntptr(intptr_t value) {
+  static inline Tagged<TaggedIndex> FromIntptr(intptr_t value) {
     DCHECK(TaggedIndex::IsValid(value));
-    return TaggedIndex((static_cast<Address>(value) << kSmiTagSize) | kSmiTag);
+    return Tagged<TaggedIndex>((static_cast<Address>(value) << kSmiTagSize) |
+                               kSmiTag);
   }
 
   // Returns whether value can be represented in a TaggedIndex.
@@ -74,6 +75,29 @@ class TaggedIndex : public Object {
 
 CAST_ACCESSOR(TaggedIndex)
 
+// Defined Tagged<TaggedIndex> now that TaggedIndex exists.
+
+// Implicit conversions to/from raw pointers
+// TODO(leszeks): Remove once we're using Tagged everywhere.
+// NOLINTNEXTLINE
+constexpr Tagged<TaggedIndex>::Tagged(TaggedIndex raw) : TaggedBase(raw.ptr()) {
+  static_assert(kTaggedCanConvertToRawObjects);
+}
+// NOLINTNEXTLINE
+constexpr Tagged<TaggedIndex>::operator TaggedIndex() {
+  static_assert(kTaggedCanConvertToRawObjects);
+  return TaggedIndex(ptr());
+}
+
+// Access via ->, remove once TaggedIndex doesn't have its own address.
+constexpr TaggedIndex Tagged<TaggedIndex>::operator*() const {
+  return TaggedIndex(ptr());
+}
+constexpr detail::TaggedOperatorArrowRef<TaggedIndex>
+Tagged<TaggedIndex>::operator->() {
+  return detail::TaggedOperatorArrowRef<TaggedIndex>(TaggedIndex(ptr()));
+}
+
 }  // namespace internal
 }  // namespace v8
 
diff --git a/src/objects/tagged-value-inl.h b/src/objects/tagged-value-inl.h
index 7cdebdf700b..c729aa3988d 100644
--- a/src/objects/tagged-value-inl.h
+++ b/src/objects/tagged-value-inl.h
@@ -18,7 +18,7 @@
 namespace v8 {
 namespace internal {
 
-inline StrongTaggedValue::StrongTaggedValue(Object o)
+inline StrongTaggedValue::StrongTaggedValue(Tagged<Object> o)
     :
 #ifdef V8_COMPRESS_POINTERS
       TaggedImpl(CompressionScheme::CompressObject(o.ptr()))
@@ -28,7 +28,8 @@ inline StrongTaggedValue::StrongTaggedValue(Object o)
 {
 }
 
-Object StrongTaggedValue::ToObject(Isolate* isolate, StrongTaggedValue object) {
+Tagged<Object> StrongTaggedValue::ToObject(Isolate* isolate,
+                                           StrongTaggedValue object) {
 #ifdef V8_COMPRESS_POINTERS
   return Object(CompressionScheme::DecompressTagged(isolate, object.ptr()));
 #else
diff --git a/src/objects/tagged-value.h b/src/objects/tagged-value.h
index 7b6192204a4..7a479638113 100644
--- a/src/objects/tagged-value.h
+++ b/src/objects/tagged-value.h
@@ -21,9 +21,10 @@ class StrongTaggedValue
  public:
   constexpr StrongTaggedValue() : TaggedImpl() {}
   explicit constexpr StrongTaggedValue(Tagged_t ptr) : TaggedImpl(ptr) {}
-  explicit StrongTaggedValue(Object o);
+  explicit StrongTaggedValue(Tagged<Object> o);
 
-  inline static Object ToObject(Isolate* isolate, StrongTaggedValue object);
+  inline static Tagged<Object> ToObject(Isolate* isolate,
+                                        StrongTaggedValue object);
 };
 
 // Almost same as MaybeObject but this one deals with in-heap and potentially
diff --git a/src/objects/tagged.h b/src/objects/tagged.h
index c6df609cc32..60afe6a4060 100644
--- a/src/objects/tagged.h
+++ b/src/objects/tagged.h
@@ -127,6 +127,17 @@ class Tagged<Object> : public TaggedBase {
   // Allow Tagged<Object> to be created from any address.
   constexpr explicit Tagged(Address o) : TaggedBase(o) {}
 
+  // Allow explicit uninitialized initialization. In debug mode this is zapped.
+  // TODO(leszeks): Mark this somehow as uninitialized, so that we get some
+  // warning if it is used before initialization.
+  constexpr Tagged()
+      : TaggedBase(
+#ifdef DEBUG
+            kZapValue
+#endif
+        ) {
+  }
+
   // Implicit conversion for subclasses -- all classes are subclasses of Object,
   // so allow all tagged pointers.
   // NOLINTNEXTLINE
@@ -175,6 +186,9 @@ class Tagged<Smi> : public TaggedBase {
     return Tagged<Smi>(other.ptr());
   }
 
+  constexpr Tagged() = default;
+  constexpr explicit Tagged(Address ptr) : TaggedBase(ptr) {}
+
   // No implicit conversions from other tagged pointers.
 
   constexpr bool IsHeapObject() const { return false; }
@@ -200,8 +214,6 @@ class Tagged<Smi> : public TaggedBase {
 #ifdef V8_ENABLE_DIRECT_HANDLE
   friend class DirectHandle<Smi>;
 #endif
-
-  using TaggedBase::TaggedBase;
 };
 
 // Specialization for TaggedIndex disallowing any implicit creation or access
@@ -225,6 +237,9 @@ class Tagged<TaggedIndex> : public TaggedBase {
     return Tagged<TaggedIndex>(other.ptr());
   }
 
+  constexpr Tagged() = default;
+  constexpr explicit Tagged(Address ptr) : TaggedBase(ptr) {}
+
   // No implicit conversions from other tagged pointers.
 
   constexpr bool IsHeapObject() const { return false; }
@@ -254,8 +269,6 @@ class Tagged<TaggedIndex> : public TaggedBase {
 #ifdef V8_ENABLE_DIRECT_HANDLE
   friend class DirectHandle<TaggedIndex>;
 #endif
-
-  using TaggedBase::TaggedBase;
 };
 
 // Specialization for HeapObject, to group together functions shared between all
@@ -332,6 +345,9 @@ class Tagged<HeapObject> : public TaggedBase {
 
   Address address() const { return this->ptr() - kHeapObjectTag; }
 
+ protected:
+  constexpr explicit Tagged(Address ptr) : Base(ptr) {}
+
  private:
   friend class HeapObject;
   // Handles of the same type are allowed to access the Address constructor.
@@ -340,7 +356,6 @@ class Tagged<HeapObject> : public TaggedBase {
   friend class DirectHandle<HeapObject>;
 #endif
 
-  using Base::Base;
   constexpr HeapObject ToRawPtr() const;
 };
 
@@ -414,13 +429,14 @@ class Tagged : public detail::BaseForTagged<T>::type {
   }
 
  private:
+  friend T;
   // Handles of the same type are allowed to access the Address constructor.
   friend class Handle<T>;
 #ifdef V8_ENABLE_DIRECT_HANDLE
   friend class DirectHandle<T>;
 #endif
 
-  using Base::Base;
+  constexpr explicit Tagged(Address ptr) : Base(ptr) {}
   constexpr T ToRawPtr() const {
     return T(this->ptr(), typename T::SkipTypeCheckTag{});
   }
diff --git a/src/objects/template-objects.cc b/src/objects/template-objects.cc
index 80a851d6608..06e18695dde 100644
--- a/src/objects/template-objects.cc
+++ b/src/objects/template-objects.cc
@@ -16,23 +16,26 @@ namespace v8 {
 namespace internal {
 
 namespace {
-bool CachedTemplateMatches(Isolate* isolate, NativeContext native_context,
-                           JSArray entry, int function_literal_id, int slot_id,
-                           DisallowGarbageCollection& no_gc) {
+bool CachedTemplateMatches(Isolate* isolate,
+                           Tagged<NativeContext> native_context,
+                           Tagged<JSArray> entry, int function_literal_id,
+                           int slot_id, DisallowGarbageCollection& no_gc) {
   if (native_context->is_js_array_template_literal_object_map(
           entry->map(isolate))) {
-    TemplateLiteralObject template_object = TemplateLiteralObject::cast(entry);
+    Tagged<TemplateLiteralObject> template_object =
+        TemplateLiteralObject::cast(entry);
     return template_object->function_literal_id() == function_literal_id &&
            template_object->slot_id() == slot_id;
   }
 
   Handle<JSArray> entry_handle(entry, isolate);
-  Smi cached_function_literal_id = Smi::cast(*JSReceiver::GetDataProperty(
-      isolate, entry_handle,
-      isolate->factory()->template_literal_function_literal_id_symbol()));
+  Tagged<Smi> cached_function_literal_id =
+      Smi::cast(*JSReceiver::GetDataProperty(
+          isolate, entry_handle,
+          isolate->factory()->template_literal_function_literal_id_symbol()));
   if (cached_function_literal_id.value() != function_literal_id) return false;
 
-  Smi cached_slot_id = Smi::cast(*JSReceiver::GetDataProperty(
+  Tagged<Smi> cached_slot_id = Smi::cast(*JSReceiver::GetDataProperty(
       isolate, entry_handle,
       isolate->factory()->template_literal_slot_id_symbol()));
   if (cached_slot_id.value() != slot_id) return false;
@@ -60,19 +63,21 @@ Handle<JSArray> TemplateObjectDescription::GetTemplateObject(
     // CachedTemplateMatches calls JSReceiver::GetDataProperty.
     DisableGCMole no_gcmole;
     ReadOnlyRoots roots(isolate);
-    EphemeronHashTable template_weakmap =
+    Tagged<EphemeronHashTable> template_weakmap =
         EphemeronHashTable::cast(native_context->template_weakmap());
-    Object cached_templates_lookup =
+    Tagged<Object> cached_templates_lookup =
         template_weakmap->Lookup(isolate, script, hash);
     if (!IsTheHole(cached_templates_lookup, roots)) {
-      ArrayList cached_templates = ArrayList::cast(cached_templates_lookup);
+      Tagged<ArrayList> cached_templates =
+          ArrayList::cast(cached_templates_lookup);
       maybe_cached_templates = handle(cached_templates, isolate);
 
       // Linear search over the cached template array list for a template
       // object matching the given function_literal_id + slot_id.
       // TODO(leszeks): Consider keeping this list sorted for faster lookup.
       for (int i = 0; i < cached_templates->Length(); i++) {
-        JSArray template_object = JSArray::cast(cached_templates->Get(i));
+        Tagged<JSArray> template_object =
+            JSArray::cast(cached_templates->Get(i));
         if (CachedTemplateMatches(isolate, *native_context, template_object,
                                   function_literal_id, slot_id, no_gc)) {
           return handle(template_object, isolate);
@@ -100,7 +105,8 @@ Handle<JSArray> TemplateObjectDescription::GetTemplateObject(
   Handle<ArrayList> old_cached_templates;
   if (!maybe_cached_templates.ToHandle(&old_cached_templates) ||
       *old_cached_templates != *cached_templates) {
-    HeapObject maybe_template_weakmap = native_context->template_weakmap();
+    Tagged<HeapObject> maybe_template_weakmap =
+        native_context->template_weakmap();
     Handle<EphemeronHashTable> template_weakmap;
     if (IsUndefined(maybe_template_weakmap)) {
       template_weakmap = EphemeronHashTable::New(isolate, 1);
diff --git a/src/objects/templates-inl.h b/src/objects/templates-inl.h
index c05a9e81ad2..afa42afc4e6 100644
--- a/src/objects/templates-inl.h
+++ b/src/objects/templates-inl.h
@@ -129,33 +129,34 @@ bool FunctionTemplateInfo::instantiated() {
 }
 
 inline bool FunctionTemplateInfo::BreakAtEntry(Isolate* isolate) {
-  Object maybe_shared = shared_function_info();
+  Tagged<Object> maybe_shared = shared_function_info();
   if (IsSharedFunctionInfo(maybe_shared)) {
-    SharedFunctionInfo shared = SharedFunctionInfo::cast(maybe_shared);
+    Tagged<SharedFunctionInfo> shared = SharedFunctionInfo::cast(maybe_shared);
     return shared->BreakAtEntry(isolate);
   }
   return false;
 }
 
-FunctionTemplateInfo FunctionTemplateInfo::GetParent(Isolate* isolate) {
-  Object parent = GetParentTemplate();
-  return IsUndefined(parent, isolate) ? FunctionTemplateInfo()
+Tagged<FunctionTemplateInfo> FunctionTemplateInfo::GetParent(Isolate* isolate) {
+  Tagged<Object> parent = GetParentTemplate();
+  return IsUndefined(parent, isolate) ? Tagged<FunctionTemplateInfo>{}
                                       : FunctionTemplateInfo::cast(parent);
 }
 
-ObjectTemplateInfo ObjectTemplateInfo::GetParent(Isolate* isolate) {
-  Object maybe_ctor = constructor();
+Tagged<ObjectTemplateInfo> ObjectTemplateInfo::GetParent(Isolate* isolate) {
+  Tagged<Object> maybe_ctor = constructor();
   if (IsUndefined(maybe_ctor, isolate)) return ObjectTemplateInfo();
-  FunctionTemplateInfo constructor = FunctionTemplateInfo::cast(maybe_ctor);
+  Tagged<FunctionTemplateInfo> constructor =
+      FunctionTemplateInfo::cast(maybe_ctor);
   while (true) {
     constructor = constructor->GetParent(isolate);
     if (constructor.is_null()) return ObjectTemplateInfo();
-    Object maybe_obj = constructor->GetInstanceTemplate();
+    Tagged<Object> maybe_obj = constructor->GetInstanceTemplate();
     if (!IsUndefined(maybe_obj, isolate)) {
       return ObjectTemplateInfo::cast(maybe_obj);
     }
   }
-  return ObjectTemplateInfo();
+  return Tagged<ObjectTemplateInfo>();
 }
 
 int ObjectTemplateInfo::embedder_field_count() const {
@@ -183,7 +184,7 @@ void ObjectTemplateInfo::set_code_like(bool is_code_like) {
   return set_data(IsCodeKindBit::update(data(), is_code_like));
 }
 
-bool FunctionTemplateInfo::IsTemplateFor(JSObject object) {
+bool FunctionTemplateInfo::IsTemplateFor(Tagged<JSObject> object) const {
   return IsTemplateFor(object->map());
 }
 
diff --git a/src/objects/templates.cc b/src/objects/templates.cc
index c97272724f5..c197cde6d45 100644
--- a/src/objects/templates.cc
+++ b/src/objects/templates.cc
@@ -25,7 +25,7 @@ bool FunctionTemplateInfo::HasInstanceType() {
 Handle<SharedFunctionInfo> FunctionTemplateInfo::GetOrCreateSharedFunctionInfo(
     Isolate* isolate, Handle<FunctionTemplateInfo> info,
     MaybeHandle<Name> maybe_name) {
-  Object current_info = info->shared_function_info();
+  Tagged<Object> current_info = info->shared_function_info();
   if (IsSharedFunctionInfo(current_info)) {
     return handle(SharedFunctionInfo::cast(current_info), isolate);
   }
@@ -59,7 +59,7 @@ Handle<SharedFunctionInfo> FunctionTemplateInfo::GetOrCreateSharedFunctionInfo(
   return sfi;
 }
 
-bool FunctionTemplateInfo::IsTemplateFor(Map map) const {
+bool FunctionTemplateInfo::IsTemplateFor(Tagged<Map> map) const {
   RCS_SCOPE(
       LocalHeap::Current() == nullptr
           ? GetIsolateChecked()->counters()->runtime_call_stats()
@@ -80,10 +80,10 @@ bool FunctionTemplateInfo::IsTemplateFor(Map map) const {
   }
 
   // Fetch the constructor function of the object.
-  Object cons_obj = map->GetConstructor();
-  Object type;
+  Tagged<Object> cons_obj = map->GetConstructor();
+  Tagged<Object> type;
   if (IsJSFunction(cons_obj)) {
-    JSFunction fun = JSFunction::cast(cons_obj);
+    Tagged<JSFunction> fun = JSFunction::cast(cons_obj);
     type = fun->shared()->function_data(kAcquireLoad);
   } else if (IsFunctionTemplateInfo(cons_obj)) {
     type = FunctionTemplateInfo::cast(cons_obj);
@@ -100,7 +100,8 @@ bool FunctionTemplateInfo::IsTemplateFor(Map map) const {
   return false;
 }
 
-bool FunctionTemplateInfo::IsLeafTemplateForApiObject(Object object) const {
+bool FunctionTemplateInfo::IsLeafTemplateForApiObject(
+    Tagged<Object> object) const {
   i::DisallowGarbageCollection no_gc;
 
   if (!IsJSApiObject(object)) {
@@ -108,8 +109,8 @@ bool FunctionTemplateInfo::IsLeafTemplateForApiObject(Object object) const {
   }
 
   bool result = false;
-  Map map = HeapObject::cast(object)->map();
-  Object constructor_obj = map->GetConstructor();
+  Tagged<Map> map = HeapObject::cast(object)->map();
+  Tagged<Object> constructor_obj = map->GetConstructor();
   if (IsJSFunction(constructor_obj)) {
     JSFunction fun = JSFunction::cast(constructor_obj);
     result = (*this == fun->shared()->function_data(kAcquireLoad));
@@ -131,8 +132,8 @@ FunctionTemplateInfo::AllocateFunctionTemplateRareData(
   return *rare_data;
 }
 
-base::Optional<Name> FunctionTemplateInfo::TryGetCachedPropertyName(
-    Isolate* isolate, Object getter) {
+base::Optional<Tagged<Name>> FunctionTemplateInfo::TryGetCachedPropertyName(
+    Isolate* isolate, Tagged<Object> getter) {
   DisallowGarbageCollection no_gc;
   if (!IsFunctionTemplateInfo(getter)) {
     if (!IsJSFunction(getter)) return {};
@@ -141,7 +142,7 @@ base::Optional<Name> FunctionTemplateInfo::TryGetCachedPropertyName(
     getter = info->api_func_data();
   }
   // Check if the accessor uses a cached property.
-  Object maybe_name =
+  Tagged<Object> maybe_name =
       FunctionTemplateInfo::cast(getter)->cached_property_name();
   if (IsTheHole(maybe_name, isolate)) return {};
   return Name::cast(maybe_name);
diff --git a/src/objects/templates.h b/src/objects/templates.h
index 164aa7ed81d..1683a7cb0bc 100644
--- a/src/objects/templates.h
+++ b/src/objects/templates.h
@@ -153,22 +153,22 @@ class FunctionTemplateInfo
   }
 
   // Returns parent function template or a null FunctionTemplateInfo.
-  inline FunctionTemplateInfo GetParent(Isolate* isolate);
+  inline Tagged<FunctionTemplateInfo> GetParent(Isolate* isolate);
   // Returns true if |object| is an instance of this function template.
-  inline bool IsTemplateFor(JSObject object);
-  bool IsTemplateFor(Map map) const;
+  inline bool IsTemplateFor(Tagged<JSObject> object) const;
+  bool IsTemplateFor(Tagged<Map> map) const;
   // Returns true if |object| is an API object and is constructed by this
   // particular function template (skips walking up the chain of inheriting
   // functions that is done by IsTemplateFor).
-  bool IsLeafTemplateForApiObject(Object object) const;
+  bool IsLeafTemplateForApiObject(Tagged<Object> object) const;
   inline bool instantiated();
 
   bool BreakAtEntry(Isolate* isolate);
   bool HasInstanceType();
 
   // Helper function for cached accessors.
-  static base::Optional<Name> TryGetCachedPropertyName(Isolate* isolate,
-                                                       Object getter);
+  static base::Optional<Tagged<Name>> TryGetCachedPropertyName(
+      Isolate* isolate, Tagged<Object> getter);
   // Fast API overloads.
   int GetCFunctionsCount() const;
   Address GetCFunction(int index) const;
@@ -210,7 +210,7 @@ class ObjectTemplateInfo
 
   // Starting from given object template's constructor walk up the inheritance
   // chain till a function template that has an instance template is found.
-  inline ObjectTemplateInfo GetParent(Isolate* isolate);
+  inline Tagged<ObjectTemplateInfo> GetParent(Isolate* isolate);
 
   using BodyDescriptor = StructBodyDescriptor;
 
diff --git a/src/objects/transitions-inl.h b/src/objects/transitions-inl.h
index b9a2a646539..2ec11cc67f2 100644
--- a/src/objects/transitions-inl.h
+++ b/src/objects/transitions-inl.h
@@ -18,7 +18,7 @@ namespace v8 {
 namespace internal {
 
 // static
-TransitionArray TransitionsAccessor::GetTransitionArray(
+Tagged<TransitionArray> TransitionsAccessor::GetTransitionArray(
     Isolate* isolate, MaybeObject raw_transitions) {
   DCHECK_EQ(kFullTransitionArray, GetEncoding(isolate, raw_transitions));
   USE(isolate);
@@ -26,13 +26,13 @@ TransitionArray TransitionsAccessor::GetTransitionArray(
 }
 
 // static
-TransitionArray TransitionsAccessor::GetTransitionArray(Isolate* isolate,
-                                                        Handle<Map> map) {
+Tagged<TransitionArray> TransitionsAccessor::GetTransitionArray(
+    Isolate* isolate, Handle<Map> map) {
   MaybeObject raw_transitions = map->raw_transitions(isolate, kAcquireLoad);
   return GetTransitionArray(isolate, raw_transitions);
 }
 
-TransitionArray TransitionsAccessor::transitions() {
+Tagged<TransitionArray> TransitionsAccessor::transitions() {
   return GetTransitionArray(isolate_, raw_transitions_);
 }
 
@@ -44,9 +44,9 @@ bool TransitionArray::HasPrototypeTransitions() {
   return Get(kPrototypeTransitionsIndex) != MaybeObject::FromSmi(Smi::zero());
 }
 
-WeakFixedArray TransitionArray::GetPrototypeTransitions() {
+Tagged<WeakFixedArray> TransitionArray::GetPrototypeTransitions() {
   DCHECK(HasPrototypeTransitions());  // Callers must check first.
-  Object prototype_transitions =
+  Tagged<Object> prototype_transitions =
       Get(kPrototypeTransitionsIndex)->GetHeapObjectAssumeStrong();
   return WeakFixedArray::cast(prototype_transitions);
 }
@@ -56,31 +56,32 @@ HeapObjectSlot TransitionArray::GetKeySlot(int transition_number) {
   return HeapObjectSlot(RawFieldOfElementAt(ToKeyIndex(transition_number)));
 }
 
-void TransitionArray::SetPrototypeTransitions(WeakFixedArray transitions) {
+void TransitionArray::SetPrototypeTransitions(
+    Tagged<WeakFixedArray> transitions) {
   DCHECK(IsWeakFixedArray(transitions));
   WeakFixedArray::Set(kPrototypeTransitionsIndex,
                       HeapObjectReference::Strong(transitions));
 }
 
 int TransitionArray::NumberOfPrototypeTransitions(
-    WeakFixedArray proto_transitions) {
+    Tagged<WeakFixedArray> proto_transitions) {
   if (proto_transitions->length() == 0) return 0;
   MaybeObject raw =
       proto_transitions->Get(kProtoTransitionNumberOfEntriesOffset);
   return raw.ToSmi().value();
 }
 
-Name TransitionArray::GetKey(int transition_number) {
+Tagged<Name> TransitionArray::GetKey(int transition_number) {
   DCHECK(transition_number < number_of_transitions());
   return Name::cast(
       Get(ToKeyIndex(transition_number))->GetHeapObjectAssumeStrong());
 }
 
-Name TransitionArray::GetKey(InternalIndex index) {
+Tagged<Name> TransitionArray::GetKey(InternalIndex index) {
   return GetKey(index.as_int());
 }
 
-Name TransitionsAccessor::GetKey(int transition_number) {
+Tagged<Name> TransitionsAccessor::GetKey(int transition_number) {
   switch (encoding()) {
     case kPrototypeInfo:
     case kUninitialized:
@@ -88,7 +89,7 @@ Name TransitionsAccessor::GetKey(int transition_number) {
       UNREACHABLE();
       return Name();
     case kWeakRef: {
-      Map map = Map::cast(raw_transitions_->GetHeapObjectAssumeWeak());
+      Tagged<Map> map = Map::cast(raw_transitions_->GetHeapObjectAssumeWeak());
       return GetSimpleTransitionKey(map);
     }
     case kFullTransitionArray:
@@ -97,7 +98,7 @@ Name TransitionsAccessor::GetKey(int transition_number) {
   UNREACHABLE();
 }
 
-void TransitionArray::SetKey(int transition_number, Name key) {
+void TransitionArray::SetKey(int transition_number, Tagged<Name> key) {
   DCHECK(transition_number < number_of_transitions());
   WeakFixedArray::Set(ToKeyIndex(transition_number),
                       HeapObjectReference::Strong(key));
@@ -109,27 +110,31 @@ HeapObjectSlot TransitionArray::GetTargetSlot(int transition_number) {
 }
 
 // static
-PropertyDetails TransitionsAccessor::GetTargetDetails(Name name, Map target) {
+PropertyDetails TransitionsAccessor::GetTargetDetails(Tagged<Name> name,
+                                                      Tagged<Map> target) {
   DCHECK(!IsSpecialTransition(name->GetReadOnlyRoots(), name));
   InternalIndex descriptor = target->LastAdded();
-  DescriptorArray descriptors = target->instance_descriptors(kRelaxedLoad);
+  Tagged<DescriptorArray> descriptors =
+      target->instance_descriptors(kRelaxedLoad);
   // Transitions are allowed only for the last added property.
   DCHECK(descriptors->GetKey(descriptor)->Equals(name));
   return descriptors->GetDetails(descriptor);
 }
 
-PropertyDetails TransitionsAccessor::GetSimpleTargetDetails(Map transition) {
+PropertyDetails TransitionsAccessor::GetSimpleTargetDetails(
+    Tagged<Map> transition) {
   return transition->GetLastDescriptorDetails(isolate_);
 }
 
 // static
-Name TransitionsAccessor::GetSimpleTransitionKey(Map transition) {
+Tagged<Name> TransitionsAccessor::GetSimpleTransitionKey(
+    Tagged<Map> transition) {
   InternalIndex descriptor = transition->LastAdded();
   return transition->instance_descriptors()->GetKey(descriptor);
 }
 
 // static
-Map TransitionsAccessor::GetTargetFromRaw(MaybeObject raw) {
+Tagged<Map> TransitionsAccessor::GetTargetFromRaw(MaybeObject raw) {
   return Map::cast(raw->GetHeapObjectAssumeWeak());
 }
 
@@ -138,12 +143,12 @@ MaybeObject TransitionArray::GetRawTarget(int transition_number) {
   return Get(ToTargetIndex(transition_number));
 }
 
-Map TransitionArray::GetTarget(int transition_number) {
+Tagged<Map> TransitionArray::GetTarget(int transition_number) {
   MaybeObject raw = GetRawTarget(transition_number);
   return TransitionsAccessor::GetTargetFromRaw(raw);
 }
 
-Map TransitionsAccessor::GetTarget(int transition_number) {
+Tagged<Map> TransitionsAccessor::GetTarget(int transition_number) {
   switch (encoding()) {
     case kPrototypeInfo:
     case kUninitialized:
@@ -166,9 +171,9 @@ void TransitionArray::SetRawTarget(int transition_number, MaybeObject value) {
 }
 
 bool TransitionArray::GetTargetIfExists(int transition_number, Isolate* isolate,
-                                        Map* target) {
+                                        Tagged<Map>* target) {
   MaybeObject raw = GetRawTarget(transition_number);
-  HeapObject heap_object;
+  Tagged<HeapObject> heap_object;
   // If the raw target is a Smi, then this TransitionArray is in the process of
   // being deserialized, and doesn't yet have an initialized entry for this
   // transition.
@@ -185,28 +190,30 @@ bool TransitionArray::GetTargetIfExists(int transition_number, Isolate* isolate,
   return true;
 }
 
-int TransitionArray::SearchNameForTesting(Name name, int* out_insertion_index) {
+int TransitionArray::SearchNameForTesting(Tagged<Name> name,
+                                          int* out_insertion_index) {
   return SearchName(name, out_insertion_index);
 }
 
-Map TransitionArray::SearchAndGetTargetForTesting(
-    PropertyKind kind, Name name, PropertyAttributes attributes) {
+Tagged<Map> TransitionArray::SearchAndGetTargetForTesting(
+    PropertyKind kind, Tagged<Name> name, PropertyAttributes attributes) {
   return SearchAndGetTarget(kind, name, attributes);
 }
 
-int TransitionArray::SearchSpecial(Symbol symbol, bool concurrent_search,
+int TransitionArray::SearchSpecial(Tagged<Symbol> symbol,
+                                   bool concurrent_search,
                                    int* out_insertion_index) {
   return SearchName(symbol, concurrent_search, out_insertion_index);
 }
 
-int TransitionArray::SearchName(Name name, bool concurrent_search,
+int TransitionArray::SearchName(Tagged<Name> name, bool concurrent_search,
                                 int* out_insertion_index) {
   DCHECK(IsUniqueName(name));
   return internal::Search<ALL_ENTRIES>(this, name, number_of_entries(),
                                        out_insertion_index, concurrent_search);
 }
 
-TransitionsAccessor::TransitionsAccessor(Isolate* isolate, Map map,
+TransitionsAccessor::TransitionsAccessor(Isolate* isolate, Tagged<Map> map,
                                          bool concurrent_access)
     : isolate_(isolate),
       map_(map),
@@ -221,7 +228,7 @@ int TransitionsAccessor::Capacity() { return transitions()->Capacity(); }
 // static
 TransitionsAccessor::Encoding TransitionsAccessor::GetEncoding(
     Isolate* isolate, MaybeObject raw_transitions) {
-  HeapObject heap_object;
+  Tagged<HeapObject> heap_object;
   if (raw_transitions->IsSmi() || raw_transitions->IsCleared()) {
     return kUninitialized;
   } else if (raw_transitions->IsWeak()) {
@@ -242,7 +249,7 @@ TransitionsAccessor::Encoding TransitionsAccessor::GetEncoding(
 
 // static
 TransitionsAccessor::Encoding TransitionsAccessor::GetEncoding(
-    Isolate* isolate, TransitionArray array) {
+    Isolate* isolate, Tagged<TransitionArray> array) {
   return GetEncoding(isolate, MaybeObject::FromObject(array));
 }
 
@@ -255,10 +262,10 @@ TransitionsAccessor::Encoding TransitionsAccessor::GetEncoding(
 
 // static
 MaybeHandle<Map> TransitionsAccessor::SearchTransition(
-    Isolate* isolate, Handle<Map> map, Name name, PropertyKind kind,
+    Isolate* isolate, Handle<Map> map, Tagged<Name> name, PropertyKind kind,
     PropertyAttributes attributes) {
-  Map result = TransitionsAccessor(isolate, *map)
-                   .SearchTransition(name, kind, attributes);
+  Tagged<Map> result = TransitionsAccessor(isolate, *map)
+                           .SearchTransition(name, kind, attributes);
   if (result.is_null()) return MaybeHandle<Map>();
   return MaybeHandle<Map>(result, isolate);
 }
@@ -266,8 +273,8 @@ MaybeHandle<Map> TransitionsAccessor::SearchTransition(
 // static
 MaybeHandle<Map> TransitionsAccessor::SearchSpecial(Isolate* isolate,
                                                     Handle<Map> map,
-                                                    Symbol name) {
-  Map result = TransitionsAccessor(isolate, *map).SearchSpecial(name);
+                                                    Tagged<Symbol> name) {
+  Tagged<Map> result = TransitionsAccessor(isolate, *map).SearchSpecial(name);
   if (result.is_null()) return MaybeHandle<Map>();
   return MaybeHandle<Map>(result, isolate);
 }
@@ -277,9 +284,11 @@ int TransitionArray::number_of_transitions() const {
   return Get(kTransitionLengthIndex).ToSmi().value();
 }
 
-int TransitionArray::CompareKeys(Name key1, uint32_t hash1, PropertyKind kind1,
-                                 PropertyAttributes attributes1, Name key2,
-                                 uint32_t hash2, PropertyKind kind2,
+int TransitionArray::CompareKeys(Tagged<Name> key1, uint32_t hash1,
+                                 PropertyKind kind1,
+                                 PropertyAttributes attributes1,
+                                 Tagged<Name> key2, uint32_t hash2,
+                                 PropertyKind kind2,
                                  PropertyAttributes attributes2) {
   int cmp = CompareNames(key1, hash1, key2, hash2);
   if (cmp != 0) return cmp;
@@ -287,8 +296,8 @@ int TransitionArray::CompareKeys(Name key1, uint32_t hash1, PropertyKind kind1,
   return CompareDetails(kind1, attributes1, kind2, attributes2);
 }
 
-int TransitionArray::CompareNames(Name key1, uint32_t hash1, Name key2,
-                                  uint32_t hash2) {
+int TransitionArray::CompareNames(Tagged<Name> key1, uint32_t hash1,
+                                  Tagged<Name> key2, uint32_t hash2) {
   if (key1 != key2) {
     // In case of hash collisions key1 is always "less" than key2.
     return hash1 <= hash2 ? -1 : 1;
@@ -313,13 +322,14 @@ int TransitionArray::CompareDetails(PropertyKind kind1,
   return 0;
 }
 
-void TransitionArray::Set(int transition_number, Name key, MaybeObject target) {
+void TransitionArray::Set(int transition_number, Tagged<Name> key,
+                          MaybeObject target) {
   WeakFixedArray::Set(ToKeyIndex(transition_number),
                       MaybeObject::FromObject(key));
   WeakFixedArray::Set(ToTargetIndex(transition_number), target);
 }
 
-Name TransitionArray::GetSortedKey(int transition_number) {
+Tagged<Name> TransitionArray::GetSortedKey(int transition_number) {
   return GetKey(transition_number);
 }
 
@@ -348,13 +358,14 @@ Handle<String> TransitionsAccessor::ExpectedTransitionKey() {
     case kFullTransitionArray:
       return Handle<String>::null();
     case kWeakRef: {
-      Map target = Map::cast(raw_transitions_->GetHeapObjectAssumeWeak());
+      Tagged<Map> target =
+          Map::cast(raw_transitions_->GetHeapObjectAssumeWeak());
       PropertyDetails details = GetSimpleTargetDetails(target);
       if (details.location() != PropertyLocation::kField)
         return Handle<String>::null();
       DCHECK_EQ(PropertyKind::kData, details.kind());
       if (details.attributes() != NONE) return Handle<String>::null();
-      Name name = GetSimpleTransitionKey(target);
+      Tagged<Name> name = GetSimpleTransitionKey(target);
       if (!IsString(name)) return Handle<String>::null();
       return handle(String::cast(name), isolate_);
     }
diff --git a/src/objects/transitions.cc b/src/objects/transitions.cc
index 09fee58861d..0a7230463cc 100644
--- a/src/objects/transitions.cc
+++ b/src/objects/transitions.cc
@@ -13,18 +13,18 @@ namespace v8 {
 namespace internal {
 
 // static
-Map TransitionsAccessor::GetSimpleTransition(Isolate* isolate,
-                                             Handle<Map> map) {
+Tagged<Map> TransitionsAccessor::GetSimpleTransition(Isolate* isolate,
+                                                     Handle<Map> map) {
   MaybeObject raw_transitions = map->raw_transitions(isolate, kAcquireLoad);
   switch (GetEncoding(isolate, raw_transitions)) {
     case kWeakRef:
       return Map::cast(raw_transitions->GetHeapObjectAssumeWeak());
     default:
-      return Map();
+      return Tagged<Map>();
   }
 }
 
-bool TransitionsAccessor::HasSimpleTransitionTo(Map map) {
+bool TransitionsAccessor::HasSimpleTransitionTo(Tagged<Map> map) {
   switch (encoding()) {
     case kWeakRef:
       return raw_transitions_->GetHeapObjectAssumeWeak() == map;
@@ -61,11 +61,11 @@ void TransitionsAccessor::Insert(Isolate* isolate, Handle<Map> map,
   }
 
   if (encoding == kWeakRef) {
-    Map simple_transition = GetSimpleTransition(isolate, map);
+    Tagged<Map> simple_transition = GetSimpleTransition(isolate, map);
     DCHECK(!simple_transition.is_null());
 
     if (flag == SIMPLE_PROPERTY_TRANSITION) {
-      Name key = GetSimpleTransitionKey(simple_transition);
+      Tagged<Name> key = GetSimpleTransitionKey(simple_transition);
       PropertyDetails old_details =
           simple_transition->GetLastDescriptorDetails(isolate);
       PropertyDetails new_details = GetTargetDetails(*name, *target);
@@ -221,17 +221,17 @@ void TransitionsAccessor::Insert(Isolate* isolate, Handle<Map> map,
   ReplaceTransitions(isolate, map, result);
 }
 
-Map TransitionsAccessor::SearchTransition(Name name, PropertyKind kind,
-                                          PropertyAttributes attributes) {
+Tagged<Map> TransitionsAccessor::SearchTransition(
+    Tagged<Name> name, PropertyKind kind, PropertyAttributes attributes) {
   DCHECK(IsUniqueName(name));
   switch (encoding()) {
     case kPrototypeInfo:
     case kUninitialized:
     case kMigrationTarget:
-      return Map();
+      return Tagged<Map>();
     case kWeakRef: {
-      Map map = Map::cast(raw_transitions_->GetHeapObjectAssumeWeak());
-      if (!IsMatchingMap(map, name, kind, attributes)) return Map();
+      Tagged<Map> map = Map::cast(raw_transitions_->GetHeapObjectAssumeWeak());
+      if (!IsMatchingMap(map, name, kind, attributes)) return Tagged<Map>();
       return map;
     }
     case kFullTransitionArray: {
@@ -243,17 +243,18 @@ Map TransitionsAccessor::SearchTransition(Name name, PropertyKind kind,
   UNREACHABLE();
 }
 
-Map TransitionsAccessor::SearchSpecial(Symbol name) {
-  if (encoding() != kFullTransitionArray) return Map();
+Tagged<Map> TransitionsAccessor::SearchSpecial(Tagged<Symbol> name) {
+  if (encoding() != kFullTransitionArray) return Tagged<Map>();
   base::SharedMutexGuardIf<base::kShared> scope(
       isolate_->full_transition_array_access(), concurrent_access_);
   int transition = transitions()->SearchSpecial(name, concurrent_access_);
-  if (transition == kNotFound) return Map();
+  if (transition == kNotFound) return Tagged<Map>();
   return transitions()->GetTarget(transition);
 }
 
 // static
-bool TransitionsAccessor::IsSpecialTransition(ReadOnlyRoots roots, Name name) {
+bool TransitionsAccessor::IsSpecialTransition(ReadOnlyRoots roots,
+                                              Tagged<Name> name) {
   if (!IsSymbol(name)) return false;
   return name == roots.nonextensible_symbol() ||
          name == roots.sealed_symbol() || name == roots.frozen_symbol() ||
@@ -266,7 +267,7 @@ MaybeHandle<Map> TransitionsAccessor::FindTransitionToDataProperty(
   DCHECK(IsUniqueName(*name));
   DisallowGarbageCollection no_gc;
   PropertyAttributes attributes = name->IsPrivate() ? DONT_ENUM : NONE;
-  Map target = SearchTransition(*name, PropertyKind::kData, attributes);
+  Tagged<Map> target = SearchTransition(*name, PropertyKind::kData, attributes);
   if (target.is_null()) return MaybeHandle<Map>();
   PropertyDetails details = target->GetLastDescriptorDetails(isolate_);
   DCHECK_EQ(attributes, details.attributes());
@@ -279,7 +280,7 @@ MaybeHandle<Map> TransitionsAccessor::FindTransitionToDataProperty(
 }
 
 void TransitionsAccessor::ForEachTransitionTo(
-    Name name, const ForEachTransitionCallback& callback,
+    Tagged<Name> name, const ForEachTransitionCallback& callback,
     DisallowGarbageCollection* no_gc) {
   DCHECK(IsUniqueName(name));
   switch (encoding()) {
@@ -288,10 +289,11 @@ void TransitionsAccessor::ForEachTransitionTo(
     case kMigrationTarget:
       return;
     case kWeakRef: {
-      Map target = Map::cast(raw_transitions_->GetHeapObjectAssumeWeak());
+      Tagged<Map> target =
+          Map::cast(raw_transitions_->GetHeapObjectAssumeWeak());
       InternalIndex descriptor = target->LastAdded();
       DescriptorArray descriptors = target->instance_descriptors(kRelaxedLoad);
-      Name key = descriptors->GetKey(descriptor);
+      Tagged<Name> key = descriptors->GetKey(descriptor);
       if (key == name) {
         callback(target);
       }
@@ -319,20 +321,20 @@ bool TransitionsAccessor::CanHaveMoreTransitions(Isolate* isolate,
 }
 
 // static
-bool TransitionsAccessor::IsMatchingMap(Map target, Name name,
+bool TransitionsAccessor::IsMatchingMap(Tagged<Map> target, Tagged<Name> name,
                                         PropertyKind kind,
                                         PropertyAttributes attributes) {
   InternalIndex descriptor = target->LastAdded();
   DescriptorArray descriptors = target->instance_descriptors(kRelaxedLoad);
-  Name key = descriptors->GetKey(descriptor);
+  Tagged<Name> key = descriptors->GetKey(descriptor);
   if (key != name) return false;
   return descriptors->GetDetails(descriptor)
       .HasKindAndAttributes(kind, attributes);
 }
 
 // static
-bool TransitionArray::CompactPrototypeTransitionArray(Isolate* isolate,
-                                                      WeakFixedArray array) {
+bool TransitionArray::CompactPrototypeTransitionArray(
+    Isolate* isolate, Tagged<WeakFixedArray> array) {
   const int header = kProtoTransitionHeaderSize;
   int number_of_transitions = NumberOfPrototypeTransitions(array);
   if (number_of_transitions == 0) {
@@ -445,15 +447,15 @@ Handle<Map> TransitionsAccessor::GetPrototypeTransition(
     bool new_target_is_base) {
   DisallowGarbageCollection no_gc;
   Object prototype = *prototype_handle;
-  WeakFixedArray cache = GetPrototypeTransitions(isolate, map);
+  Tagged<WeakFixedArray> cache = GetPrototypeTransitions(isolate, map);
   int length = TransitionArray::NumberOfPrototypeTransitions(cache);
   for (int i = 0; i < length; i++) {
     MaybeObject target =
         cache->Get(TransitionArray::kProtoTransitionHeaderSize + i);
     DCHECK(target->IsWeakOrCleared());
-    HeapObject heap_object;
+    Tagged<HeapObject> heap_object;
     if (target->GetHeapObjectIfWeak(&heap_object)) {
-      Map target_map = Map::cast(heap_object);
+      Tagged<Map> target_map = Map::cast(heap_object);
       if (target_map->prototype() == prototype &&
           target_map->new_target_is_base() == new_target_is_base) {
         return handle(target_map, isolate);
@@ -464,8 +466,8 @@ Handle<Map> TransitionsAccessor::GetPrototypeTransition(
 }
 
 // static
-WeakFixedArray TransitionsAccessor::GetPrototypeTransitions(Isolate* isolate,
-                                                            Handle<Map> map) {
+Tagged<WeakFixedArray> TransitionsAccessor::GetPrototypeTransitions(
+    Isolate* isolate, Handle<Map> map) {
   MaybeObject raw_transitions = map->raw_transitions(isolate, kAcquireLoad);
   if (GetEncoding(isolate, raw_transitions) != kFullTransitionArray) {
     return ReadOnlyRoots(isolate).empty_weak_fixed_array();
@@ -480,7 +482,7 @@ WeakFixedArray TransitionsAccessor::GetPrototypeTransitions(Isolate* isolate,
 
 // static
 void TransitionArray::SetNumberOfPrototypeTransitions(
-    WeakFixedArray proto_transitions, int value) {
+    Tagged<WeakFixedArray> proto_transitions, int value) {
   DCHECK_NE(proto_transitions->length(), 0);
   proto_transitions->Set(kProtoTransitionNumberOfEntriesOffset,
                          MaybeObject::FromSmi(Smi::FromInt(value)));
@@ -502,7 +504,7 @@ int TransitionsAccessor::NumberOfTransitions() {
 
 // static
 void TransitionsAccessor::SetMigrationTarget(Isolate* isolate, Handle<Map> map,
-                                             Map migration_target) {
+                                             Tagged<Map> migration_target) {
   // We only cache the migration target for maps with empty transitions for GC's
   // sake.
   if (GetEncoding(isolate, map) != kUninitialized) return;
@@ -511,11 +513,11 @@ void TransitionsAccessor::SetMigrationTarget(Isolate* isolate, Handle<Map> map,
                            kReleaseStore);
 }
 
-Map TransitionsAccessor::GetMigrationTarget() {
+Tagged<Map> TransitionsAccessor::GetMigrationTarget() {
   if (encoding() == kMigrationTarget) {
     return map_->raw_transitions(kAcquireLoad)->cast<Map>();
   }
-  return Map();
+  return Tagged<Map>();
 }
 
 // static
@@ -566,8 +568,8 @@ void TransitionsAccessor::EnsureHasFullTransitionArray(Isolate* isolate,
       result->SetNumberOfTransitions(0);
     } else {
       // Otherwise populate the new array.
-      Map target = GetSimpleTransition(isolate, map);
-      Name key = GetSimpleTransitionKey(target);
+      Tagged<Map> target = GetSimpleTransition(isolate, map);
+      Tagged<Name> key = GetSimpleTransitionKey(target);
       result->Set(0, key, HeapObjectReference::Weak(target));
     }
   }
@@ -584,7 +586,7 @@ void TransitionsAccessor::TraverseTransitionTreeInternal(
 
   // Pre-order iterative depth-first-search.
   while (!stack.empty()) {
-    Map current_map = stack.back();
+    Tagged<Map> current_map = stack.back();
     stack.pop_back();
 
     callback(current_map);
@@ -607,13 +609,14 @@ void TransitionsAccessor::TraverseTransitionTreeInternal(
         TransitionArray transitions =
             TransitionArray::cast(raw_transitions->GetHeapObjectAssumeStrong());
         if (transitions->HasPrototypeTransitions()) {
-          WeakFixedArray proto_trans = transitions->GetPrototypeTransitions();
+          Tagged<WeakFixedArray> proto_trans =
+              transitions->GetPrototypeTransitions();
           int length =
               TransitionArray::NumberOfPrototypeTransitions(proto_trans);
           for (int i = 0; i < length; ++i) {
             int index = TransitionArray::kProtoTransitionHeaderSize + i;
             MaybeObject target = proto_trans->Get(index);
-            HeapObject heap_object;
+            Tagged<HeapObject> heap_object;
             if (target->GetHeapObjectIfWeak(&heap_object)) {
               stack.emplace_back(Map::cast(heap_object));
             } else {
@@ -632,18 +635,17 @@ void TransitionsAccessor::TraverseTransitionTreeInternal(
 
 #ifdef DEBUG
 // static
-void TransitionsAccessor::CheckNewTransitionsAreConsistent(Isolate* isolate,
-                                                           Handle<Map> map,
-                                                           Object transitions) {
+void TransitionsAccessor::CheckNewTransitionsAreConsistent(
+    Isolate* isolate, Handle<Map> map, Tagged<Object> transitions) {
   // This function only handles full transition arrays.
   TransitionArray old_transitions = GetTransitionArray(isolate, map);
   DCHECK_EQ(kFullTransitionArray, GetEncoding(isolate, old_transitions));
   TransitionArray new_transitions = TransitionArray::cast(transitions);
   for (int i = 0; i < old_transitions->number_of_transitions(); i++) {
-    Map target = old_transitions->GetTarget(i);
+    Tagged<Map> target = old_transitions->GetTarget(i);
     if (target->instance_descriptors(isolate) ==
         map->instance_descriptors(isolate)) {
-      Name key = old_transitions->GetKey(i);
+      Tagged<Name> key = old_transitions->GetKey(i);
       int new_target_index;
       if (IsSpecialTransition(ReadOnlyRoots(isolate), key)) {
         new_target_index = new_transitions->SearchSpecial(Symbol::cast(key));
@@ -666,10 +668,10 @@ int TransitionArray::SearchDetails(int transition, PropertyKind kind,
                                    int* out_insertion_index) {
   int nof_transitions = number_of_transitions();
   DCHECK(transition < nof_transitions);
-  Name key = GetKey(transition);
+  Tagged<Name> key = GetKey(transition);
   for (; transition < nof_transitions && GetKey(transition) == key;
        transition++) {
-    Map target = GetTarget(transition);
+    Tagged<Map> target = GetTarget(transition);
     PropertyDetails target_details =
         TransitionsAccessor::GetTargetDetails(key, target);
 
@@ -685,15 +687,14 @@ int TransitionArray::SearchDetails(int transition, PropertyKind kind,
   return kNotFound;
 }
 
-Map TransitionArray::SearchDetailsAndGetTarget(int transition,
-                                               PropertyKind kind,
-                                               PropertyAttributes attributes) {
+Tagged<Map> TransitionArray::SearchDetailsAndGetTarget(
+    int transition, PropertyKind kind, PropertyAttributes attributes) {
   int nof_transitions = number_of_transitions();
   DCHECK(transition < nof_transitions);
-  Name key = GetKey(transition);
+  Tagged<Name> key = GetKey(transition);
   for (; transition < nof_transitions && GetKey(transition) == key;
        transition++) {
-    Map target = GetTarget(transition);
+    Tagged<Map> target = GetTarget(transition);
     PropertyDetails target_details =
         TransitionsAccessor::GetTargetDetails(key, target);
 
@@ -705,10 +706,10 @@ Map TransitionArray::SearchDetailsAndGetTarget(int transition,
       break;
     }
   }
-  return Map();
+  return Tagged<Map>();
 }
 
-int TransitionArray::Search(PropertyKind kind, Name name,
+int TransitionArray::Search(PropertyKind kind, Tagged<Name> name,
                             PropertyAttributes attributes,
                             int* out_insertion_index) {
   int transition = SearchName(name, false, out_insertion_index);
@@ -716,26 +717,27 @@ int TransitionArray::Search(PropertyKind kind, Name name,
   return SearchDetails(transition, kind, attributes, out_insertion_index);
 }
 
-Map TransitionArray::SearchAndGetTarget(PropertyKind kind, Name name,
-                                        PropertyAttributes attributes) {
+Tagged<Map> TransitionArray::SearchAndGetTarget(PropertyKind kind,
+                                                Tagged<Name> name,
+                                                PropertyAttributes attributes) {
   int transition = SearchName(name);
   if (transition == kNotFound) {
-    return Map();
+    return Tagged<Map>();
   }
   return SearchDetailsAndGetTarget(transition, kind, attributes);
 }
 
 void TransitionArray::ForEachTransitionTo(
-    Name name, const ForEachTransitionCallback& callback) {
+    Tagged<Name> name, const ForEachTransitionCallback& callback) {
   int transition = SearchName(name);
   if (transition == kNotFound) return;
 
   int nof_transitions = number_of_transitions();
   DCHECK(transition < nof_transitions);
-  Name key = GetKey(transition);
+  Tagged<Name> key = GetKey(transition);
   for (; transition < nof_transitions && GetKey(transition) == key;
        transition++) {
-    Map target = GetTarget(transition);
+    Tagged<Map> target = GetTarget(transition);
     callback(target);
   }
 }
@@ -746,12 +748,12 @@ void TransitionArray::Sort() {
   int length = number_of_transitions();
   ReadOnlyRoots roots = GetReadOnlyRoots();
   for (int i = 1; i < length; i++) {
-    Name key = GetKey(i);
+    Tagged<Name> key = GetKey(i);
     MaybeObject target = GetRawTarget(i);
     PropertyKind kind = PropertyKind::kData;
     PropertyAttributes attributes = NONE;
     if (!TransitionsAccessor::IsSpecialTransition(roots, key)) {
-      Map target_map = TransitionsAccessor::GetTargetFromRaw(target);
+      Tagged<Map> target_map = TransitionsAccessor::GetTargetFromRaw(target);
       PropertyDetails details =
           TransitionsAccessor::GetTargetDetails(key, target_map);
       kind = details.kind();
@@ -759,12 +761,12 @@ void TransitionArray::Sort() {
     }
     int j;
     for (j = i - 1; j >= 0; j--) {
-      Name temp_key = GetKey(j);
+      Tagged<Name> temp_key = GetKey(j);
       MaybeObject temp_target = GetRawTarget(j);
       PropertyKind temp_kind = PropertyKind::kData;
       PropertyAttributes temp_attributes = NONE;
       if (!TransitionsAccessor::IsSpecialTransition(roots, temp_key)) {
-        Map temp_target_map =
+        Tagged<Map> temp_target_map =
             TransitionsAccessor::GetTargetFromRaw(temp_target);
         PropertyDetails details =
             TransitionsAccessor::GetTargetDetails(temp_key, temp_target_map);
@@ -788,7 +790,8 @@ void TransitionArray::Sort() {
 }
 
 bool TransitionsAccessor::HasIntegrityLevelTransitionTo(
-    Map to, Symbol* out_symbol, PropertyAttributes* out_integrity_level) {
+    Tagged<Map> to, Tagged<Symbol>* out_symbol,
+    PropertyAttributes* out_integrity_level) {
   ReadOnlyRoots roots(isolate_);
   if (SearchSpecial(roots.frozen_symbol()) == to) {
     if (out_integrity_level) *out_integrity_level = FROZEN;
diff --git a/src/objects/transitions.h b/src/objects/transitions.h
index 13c36cfabd0..66e80f1704b 100644
--- a/src/objects/transitions.h
+++ b/src/objects/transitions.h
@@ -25,7 +25,7 @@ class Impl;
 }
 
 // Find all transitions with given name and calls the callback.
-using ForEachTransitionCallback = std::function<void(Map)>;
+using ForEachTransitionCallback = std::function<void(Tagged<Map>)>;
 
 // TransitionsAccessor is a helper class to encapsulate access to the various
 // ways a Map can store transitions to other maps in its respective field at
@@ -49,7 +49,7 @@ class V8_EXPORT_PRIVATE TransitionsAccessor {
   // {concurrent_access} signals that the TransitionsAccessor will only be used
   // in background threads. It acquires a reader lock for critical paths, as
   // well as blocking the accessor from modifying the TransitionsArray.
-  inline TransitionsAccessor(Isolate* isolate, Map map,
+  inline TransitionsAccessor(Isolate* isolate, Tagged<Map> map,
                              bool concurrent_access = false);
 
   // Insert a new transition into |map|'s transition array, extending it
@@ -57,19 +57,20 @@ class V8_EXPORT_PRIVATE TransitionsAccessor {
   static void Insert(Isolate* isolate, Handle<Map> map, Handle<Name> name,
                      Handle<Map> target, SimpleTransitionFlag flag);
 
-  Map SearchTransition(Name name, PropertyKind kind,
-                       PropertyAttributes attributes);
+  Tagged<Map> SearchTransition(Tagged<Name> name, PropertyKind kind,
+                               PropertyAttributes attributes);
   static inline MaybeHandle<Map> SearchTransition(
-      Isolate* isolate, Handle<Map> map, Name name, PropertyKind kind,
+      Isolate* isolate, Handle<Map> map, Tagged<Name> name, PropertyKind kind,
       PropertyAttributes attributes);
 
-  Map SearchSpecial(Symbol name);
+  Tagged<Map> SearchSpecial(Tagged<Symbol> name);
   static inline MaybeHandle<Map> SearchSpecial(Isolate* isolate,
-                                               Handle<Map> map, Symbol name);
+                                               Handle<Map> map,
+                                               Tagged<Symbol> name);
 
   // Returns true for non-property transitions like elements kind, or
   // or frozen/sealed transitions.
-  static bool IsSpecialTransition(ReadOnlyRoots roots, Name name);
+  static bool IsSpecialTransition(ReadOnlyRoots roots, Tagged<Name> name);
 
   enum RequestedLocation { kAnyLocation, kFieldOnly };
   MaybeHandle<Map> FindTransitionToDataProperty(
@@ -84,7 +85,8 @@ class V8_EXPORT_PRIVATE TransitionsAccessor {
   // lock are allowed inside the callback.
   // If any of the GC- or lock-requiring processing is necessary, it has to be
   // done outside of the callback.
-  void ForEachTransitionTo(Name name, const ForEachTransitionCallback& callback,
+  void ForEachTransitionTo(Tagged<Name> name,
+                           const ForEachTransitionCallback& callback,
                            DisallowGarbageCollection* no_gc);
 
   inline Handle<String> ExpectedTransitionKey();
@@ -95,21 +97,22 @@ class V8_EXPORT_PRIVATE TransitionsAccessor {
   // object space. Otherwise ClearNonLiveReferences would leak memory while
   // applying in-place right trimming.
   static const int kMaxNumberOfTransitions = 1024 + 512;
-  inline Name GetKey(int transition_number);
-  inline Map GetTarget(int transition_number);
-  static inline PropertyDetails GetTargetDetails(Name name, Map target);
+  inline Tagged<Name> GetKey(int transition_number);
+  inline Tagged<Map> GetTarget(int transition_number);
+  static inline PropertyDetails GetTargetDetails(Tagged<Name> name,
+                                                 Tagged<Map> target);
 
   static bool CanHaveMoreTransitions(Isolate* isolate, Handle<Map> map);
 
-  static bool IsMatchingMap(Map target, Name name, PropertyKind kind,
-                            PropertyAttributes attributes);
+  static bool IsMatchingMap(Tagged<Map> target, Tagged<Name> name,
+                            PropertyKind kind, PropertyAttributes attributes);
 
   bool HasIntegrityLevelTransitionTo(
-      Map to, Symbol* out_symbol = nullptr,
+      Tagged<Map> to, Tagged<Symbol>* out_symbol = nullptr,
       PropertyAttributes* out_integrity_level = nullptr);
 
   // ===== ITERATION =====
-  using TraverseCallback = std::function<void(Map)>;
+  using TraverseCallback = std::function<void(Tagged<Map>)>;
 
   // Traverse the transition tree in preorder.
   void TraverseTransitionTree(const TraverseCallback& callback) {
@@ -142,12 +145,13 @@ class V8_EXPORT_PRIVATE TransitionsAccessor {
   // we will check this cache slot as a shortcut to get the migration target
   // map.
   static void SetMigrationTarget(Isolate* isolate, Handle<Map> map,
-                                 Map migration_target);
-  Map GetMigrationTarget();
+                                 Tagged<Map> migration_target);
+  Tagged<Map> GetMigrationTarget();
 
 #if DEBUG || OBJECT_PRINT
   void PrintTransitions(std::ostream& os);
-  static void PrintOneTransition(std::ostream& os, Name key, Map target);
+  static void PrintOneTransition(std::ostream& os, Tagged<Name> key,
+                                 Tagged<Map> target);
   void PrintTransitionTree();
   void PrintTransitionTree(std::ostream& os, int level,
                            DisallowGarbageCollection* no_gc);
@@ -155,7 +159,7 @@ class V8_EXPORT_PRIVATE TransitionsAccessor {
 #if DEBUG
   static void CheckNewTransitionsAreConsistent(Isolate* isolate,
                                                Handle<Map> map,
-                                               Object transitions);
+                                               Tagged<Object> transitions);
   bool IsConsistentWithBackPointers();
   bool IsSortedNoDuplicates();
 #endif
@@ -174,7 +178,7 @@ class V8_EXPORT_PRIVATE TransitionsAccessor {
 
   inline int Capacity();
 
-  inline TransitionArray transitions();
+  inline Tagged<TransitionArray> transitions();
 
   DISALLOW_GARBAGE_COLLECTION(no_gc_)
 
@@ -185,25 +189,27 @@ class V8_EXPORT_PRIVATE TransitionsAccessor {
 
   static inline Encoding GetEncoding(Isolate* isolate,
                                      MaybeObject raw_transitions);
-  static inline Encoding GetEncoding(Isolate* isolate, TransitionArray array);
+  static inline Encoding GetEncoding(Isolate* isolate,
+                                     Tagged<TransitionArray> array);
   static inline Encoding GetEncoding(Isolate* isolate, Handle<Map> map);
 
-  static inline TransitionArray GetTransitionArray(Isolate* isolate,
-                                                   MaybeObject raw_transitions);
-  static inline TransitionArray GetTransitionArray(Isolate* isolate,
-                                                   Handle<Map> map);
+  static inline Tagged<TransitionArray> GetTransitionArray(
+      Isolate* isolate, MaybeObject raw_transitions);
+  static inline Tagged<TransitionArray> GetTransitionArray(Isolate* isolate,
+                                                           Handle<Map> map);
 
-  static inline Map GetSimpleTransition(Isolate* isolate, Handle<Map> map);
-  static inline Name GetSimpleTransitionKey(Map transition);
-  inline PropertyDetails GetSimpleTargetDetails(Map transition);
+  static inline Tagged<Map> GetSimpleTransition(Isolate* isolate,
+                                                Handle<Map> map);
+  static inline Tagged<Name> GetSimpleTransitionKey(Tagged<Map> transition);
+  inline PropertyDetails GetSimpleTargetDetails(Tagged<Map> transition);
 
-  static inline Map GetTargetFromRaw(MaybeObject raw);
+  static inline Tagged<Map> GetTargetFromRaw(MaybeObject raw);
 
   static void EnsureHasFullTransitionArray(Isolate* isolate, Handle<Map> map);
   static void SetPrototypeTransitions(Isolate* isolate, Handle<Map> map,
                                       Handle<WeakFixedArray> proto_transitions);
-  static WeakFixedArray GetPrototypeTransitions(Isolate* isolate,
-                                                Handle<Map> map);
+  static Tagged<WeakFixedArray> GetPrototypeTransitions(Isolate* isolate,
+                                                        Handle<Map> map);
 
   static inline void ReplaceTransitions(Isolate* isolate, Handle<Map> map,
                                         MaybeObject new_transitions);
@@ -211,9 +217,9 @@ class V8_EXPORT_PRIVATE TransitionsAccessor {
       Isolate* isolate, Handle<Map> map,
       Handle<TransitionArray> new_transitions);
 
-  bool HasSimpleTransitionTo(Map map);
+  bool HasSimpleTransitionTo(Tagged<Map> map);
 
-  inline Map GetTargetMapFromWeakRef();
+  inline Tagged<Map> GetTargetMapFromWeakRef();
 
   void TraverseTransitionTreeInternal(const TraverseCallback& callback,
                                       DisallowGarbageCollection* no_gc);
@@ -243,26 +249,26 @@ class TransitionArray : public WeakFixedArray {
  public:
   DECL_CAST(TransitionArray)
 
-  inline WeakFixedArray GetPrototypeTransitions();
+  inline Tagged<WeakFixedArray> GetPrototypeTransitions();
   inline bool HasPrototypeTransitions();
 
   // Accessors for fetching instance transition at transition number.
-  inline void SetKey(int transition_number, Name value);
-  inline Name GetKey(int transition_number);
+  inline void SetKey(int transition_number, Tagged<Name> value);
+  inline Tagged<Name> GetKey(int transition_number);
   inline HeapObjectSlot GetKeySlot(int transition_number);
 
-  inline Map GetTarget(int transition_number);
+  inline Tagged<Map> GetTarget(int transition_number);
   inline void SetRawTarget(int transition_number, MaybeObject target);
   inline MaybeObject GetRawTarget(int transition_number);
   inline HeapObjectSlot GetTargetSlot(int transition_number);
   inline bool GetTargetIfExists(int transition_number, Isolate* isolate,
-                                Map* target);
+                                Tagged<Map>* target);
 
   // Required for templatized Search interface.
-  inline Name GetKey(InternalIndex index);
+  inline Tagged<Name> GetKey(InternalIndex index);
   static constexpr int kNotFound = -1;
 
-  inline Name GetSortedKey(int transition_number);
+  inline Tagged<Name> GetSortedKey(int transition_number);
   int GetSortedKeyIndex(int transition_number) { return transition_number; }
   inline int number_of_entries() const;
 #ifdef DEBUG
@@ -295,11 +301,11 @@ class TransitionArray : public WeakFixedArray {
     return kFirstIndex + (transition_number * kEntrySize) + kEntryTargetIndex;
   }
 
-  inline int SearchNameForTesting(Name name,
+  inline int SearchNameForTesting(Tagged<Name> name,
                                   int* out_insertion_index = nullptr);
 
-  inline Map SearchAndGetTargetForTesting(PropertyKind kind, Name name,
-                                          PropertyAttributes attributes);
+  inline Tagged<Map> SearchAndGetTargetForTesting(
+      PropertyKind kind, Tagged<Name> name, PropertyAttributes attributes);
 
  private:
   friend class Factory;
@@ -318,12 +324,13 @@ class TransitionArray : public WeakFixedArray {
   static const int kProtoTransitionHeaderSize = 1;
   static const int kMaxCachedPrototypeTransitions = 256;
 
-  inline void SetPrototypeTransitions(WeakFixedArray prototype_transitions);
+  inline void SetPrototypeTransitions(
+      Tagged<WeakFixedArray> prototype_transitions);
 
   static inline int NumberOfPrototypeTransitions(
-      WeakFixedArray proto_transitions);
-  static void SetNumberOfPrototypeTransitions(WeakFixedArray proto_transitions,
-                                              int value);
+      Tagged<WeakFixedArray> proto_transitions);
+  static void SetNumberOfPrototypeTransitions(
+      Tagged<WeakFixedArray> proto_transitions, int value);
 
   static const int kProtoTransitionNumberOfEntriesOffset = 0;
   static_assert(kProtoTransitionHeaderSize == 1);
@@ -335,47 +342,50 @@ class TransitionArray : public WeakFixedArray {
   }
 
   // Search a  transition for a given kind, property name and attributes.
-  int Search(PropertyKind kind, Name name, PropertyAttributes attributes,
-             int* out_insertion_index = nullptr);
+  int Search(PropertyKind kind, Tagged<Name> name,
+             PropertyAttributes attributes, int* out_insertion_index = nullptr);
 
-  V8_EXPORT_PRIVATE Map SearchAndGetTarget(PropertyKind kind, Name name,
-                                           PropertyAttributes attributes);
+  V8_EXPORT_PRIVATE Tagged<Map> SearchAndGetTarget(
+      PropertyKind kind, Tagged<Name> name, PropertyAttributes attributes);
 
   // Search a non-property transition (like elements kind, observe or frozen
   // transitions).
-  inline int SearchSpecial(Symbol symbol, bool concurrent_search = false,
+  inline int SearchSpecial(Tagged<Symbol> symbol,
+                           bool concurrent_search = false,
                            int* out_insertion_index = nullptr);
   // Search a first transition for a given property name.
-  inline int SearchName(Name name, bool concurrent_search = false,
+  inline int SearchName(Tagged<Name> name, bool concurrent_search = false,
                         int* out_insertion_index = nullptr);
   int SearchDetails(int transition, PropertyKind kind,
                     PropertyAttributes attributes, int* out_insertion_index);
-  Map SearchDetailsAndGetTarget(int transition, PropertyKind kind,
-                                PropertyAttributes attributes);
+  Tagged<Map> SearchDetailsAndGetTarget(int transition, PropertyKind kind,
+                                        PropertyAttributes attributes);
 
   // Find all transitions with given name and calls the callback.
-  void ForEachTransitionTo(Name name,
+  void ForEachTransitionTo(Tagged<Name> name,
                            const ForEachTransitionCallback& callback);
 
   inline int number_of_transitions() const;
 
   static bool CompactPrototypeTransitionArray(Isolate* isolate,
-                                              WeakFixedArray array);
+                                              Tagged<WeakFixedArray> array);
 
   static Handle<WeakFixedArray> GrowPrototypeTransitionArray(
       Handle<WeakFixedArray> array, int new_capacity, Isolate* isolate);
 
   // Compares two tuples <key, kind, attributes>, returns -1 if
   // tuple1 is "less" than tuple2, 0 if tuple1 equal to tuple2 and 1 otherwise.
-  static inline int CompareKeys(Name key1, uint32_t hash1, PropertyKind kind1,
-                                PropertyAttributes attributes1, Name key2,
-                                uint32_t hash2, PropertyKind kind2,
+  static inline int CompareKeys(Tagged<Name> key1, uint32_t hash1,
+                                PropertyKind kind1,
+                                PropertyAttributes attributes1,
+                                Tagged<Name> key2, uint32_t hash2,
+                                PropertyKind kind2,
                                 PropertyAttributes attributes2);
 
   // Compares keys, returns -1 if key1 is "less" than key2,
   // 0 if key1 equal to key2 and 1 otherwise.
-  static inline int CompareNames(Name key1, uint32_t hash1, Name key2,
-                                 uint32_t hash2);
+  static inline int CompareNames(Tagged<Name> key1, uint32_t hash1,
+                                 Tagged<Name> key2, uint32_t hash2);
 
   // Compares two details, returns -1 if details1 is "less" than details2,
   // 0 if details1 equal to details2 and 1 otherwise.
@@ -384,7 +394,7 @@ class TransitionArray : public WeakFixedArray {
                                    PropertyKind kind2,
                                    PropertyAttributes attributes2);
 
-  inline void Set(int transition_number, Name key, MaybeObject target);
+  inline void Set(int transition_number, Tagged<Name> key, MaybeObject target);
 
   OBJECT_CONSTRUCTORS(TransitionArray, WeakFixedArray);
 };
diff --git a/src/objects/value-serializer.cc b/src/objects/value-serializer.cc
index d6251bc8406..1058f9c8a62 100644
--- a/src/objects/value-serializer.cc
+++ b/src/objects/value-serializer.cc
@@ -350,7 +350,7 @@ void ValueSerializer::WriteTwoByteString(base::Vector<const base::uc16> chars) {
   WriteRawBytes(chars.begin(), chars.length() * sizeof(base::uc16));
 }
 
-void ValueSerializer::WriteBigIntContents(BigInt bigint) {
+void ValueSerializer::WriteBigIntContents(Tagged<BigInt> bigint) {
   uint32_t bitfield = bigint->GetBitfieldForSerialization();
   int bytelength = BigInt::DigitsByteLengthForBitfield(bitfield);
   WriteVarint<uint32_t>(bitfield);
@@ -488,7 +488,7 @@ Maybe<bool> ValueSerializer::WriteObject(Handle<Object> object) {
   }
 }
 
-void ValueSerializer::WriteOddball(Oddball oddball) {
+void ValueSerializer::WriteOddball(Tagged<Oddball> oddball) {
   SerializationTag tag = SerializationTag::kUndefined;
   switch (oddball->kind()) {
     case Oddball::kUndefined:
@@ -509,18 +509,18 @@ void ValueSerializer::WriteOddball(Oddball oddball) {
   WriteTag(tag);
 }
 
-void ValueSerializer::WriteSmi(Smi smi) {
+void ValueSerializer::WriteSmi(Tagged<Smi> smi) {
   static_assert(kSmiValueSize <= 32, "Expected SMI <= 32 bits.");
   WriteTag(SerializationTag::kInt32);
   WriteZigZag<int32_t>(smi.value());
 }
 
-void ValueSerializer::WriteHeapNumber(HeapNumber number) {
+void ValueSerializer::WriteHeapNumber(Tagged<HeapNumber> number) {
   WriteTag(SerializationTag::kDouble);
   WriteDouble(number->value());
 }
 
-void ValueSerializer::WriteBigInt(BigInt bigint) {
+void ValueSerializer::WriteBigInt(Tagged<BigInt> bigint) {
   WriteTag(SerializationTag::kBigInt);
   WriteBigIntContents(bigint);
 }
@@ -730,7 +730,7 @@ Maybe<bool> ValueSerializer::WriteJSArray(Handle<JSArray> array) {
     switch (array->GetElementsKind(cage_base)) {
       case PACKED_SMI_ELEMENTS: {
         DisallowGarbageCollection no_gc;
-        FixedArray elements = FixedArray::cast(array->elements());
+        Tagged<FixedArray> elements = FixedArray::cast(array->elements());
         for (i = 0; i < length; i++)
           WriteSmi(Smi::cast(elements->get(cage_base, i)));
         break;
@@ -740,7 +740,8 @@ Maybe<bool> ValueSerializer::WriteJSArray(Handle<JSArray> array) {
         // is empty. No elements to encode in this case anyhow.
         if (length == 0) break;
         DisallowGarbageCollection no_gc;
-        FixedDoubleArray elements = FixedDoubleArray::cast(array->elements());
+        Tagged<FixedDoubleArray> elements =
+            FixedDoubleArray::cast(array->elements());
         for (i = 0; i < length; i++) {
           WriteTag(SerializationTag::kDouble);
           WriteDouble(elements->get_scalar(i));
@@ -818,7 +819,7 @@ Maybe<bool> ValueSerializer::WriteJSArray(Handle<JSArray> array) {
   return ThrowIfOutOfMemory();
 }
 
-void ValueSerializer::WriteJSDate(JSDate date) {
+void ValueSerializer::WriteJSDate(Tagged<JSDate> date) {
   WriteTag(SerializationTag::kDate);
   WriteDouble(Object::Number(date->value()));
 }
@@ -828,7 +829,7 @@ Maybe<bool> ValueSerializer::WriteJSPrimitiveWrapper(
   PtrComprCageBase cage_base(isolate_);
   {
     DisallowGarbageCollection no_gc;
-    Object inner_value = value->value();
+    Tagged<Object> inner_value = value->value();
     if (IsTrue(inner_value, isolate_)) {
       WriteTag(SerializationTag::kTrueObject);
     } else if (IsFalse(inner_value, isolate_)) {
@@ -864,12 +865,12 @@ Maybe<bool> ValueSerializer::WriteJSMap(Handle<JSMap> js_map) {
   Handle<FixedArray> entries = isolate_->factory()->NewFixedArray(length);
   {
     DisallowGarbageCollection no_gc;
-    OrderedHashMap raw_table = *table;
-    FixedArray raw_entries = *entries;
-    Hole the_hole = ReadOnlyRoots(isolate_).the_hole_value();
+    Tagged<OrderedHashMap> raw_table = *table;
+    Tagged<FixedArray> raw_entries = *entries;
+    Tagged<Hole> the_hole = ReadOnlyRoots(isolate_).the_hole_value();
     int result_index = 0;
     for (InternalIndex entry : raw_table->IterateEntries()) {
-      Object key = raw_table->KeyAt(entry);
+      Tagged<Object> key = raw_table->KeyAt(entry);
       if (key == the_hole) continue;
       raw_entries->set(result_index++, key);
       raw_entries->set(result_index++, raw_table->ValueAt(entry));
@@ -896,12 +897,12 @@ Maybe<bool> ValueSerializer::WriteJSSet(Handle<JSSet> js_set) {
   Handle<FixedArray> entries = isolate_->factory()->NewFixedArray(length);
   {
     DisallowGarbageCollection no_gc;
-    OrderedHashSet raw_table = *table;
-    FixedArray raw_entries = *entries;
-    Hole the_hole = ReadOnlyRoots(isolate_).the_hole_value();
+    Tagged<OrderedHashSet> raw_table = *table;
+    Tagged<FixedArray> raw_entries = *entries;
+    Tagged<Hole> the_hole = ReadOnlyRoots(isolate_).the_hole_value();
     int result_index = 0;
     for (InternalIndex entry : raw_table->IterateEntries()) {
-      Object key = raw_table->KeyAt(entry);
+      Tagged<Object> key = raw_table->KeyAt(entry);
       if (key == the_hole) continue;
       raw_entries->set(result_index++, key);
     }
@@ -971,7 +972,8 @@ Maybe<bool> ValueSerializer::WriteJSArrayBuffer(
   return ThrowIfOutOfMemory();
 }
 
-Maybe<bool> ValueSerializer::WriteJSArrayBufferView(JSArrayBufferView view) {
+Maybe<bool> ValueSerializer::WriteJSArrayBufferView(
+    Tagged<JSArrayBufferView> view) {
   if (treat_array_buffer_views_as_host_objects_) {
     return WriteHostObject(handle(view, isolate_));
   }
@@ -2187,8 +2189,8 @@ MaybeHandle<JSArrayBufferView> ValueDeserializer::ReadJSArrayBufferView(
 }
 
 bool ValueDeserializer::ValidateJSArrayBufferViewFlags(
-    JSArrayBuffer buffer, uint32_t serialized_flags, bool& is_length_tracking,
-    bool& is_backed_by_rab) {
+    Tagged<JSArrayBuffer> buffer, uint32_t serialized_flags,
+    bool& is_length_tracking, bool& is_backed_by_rab) {
   is_length_tracking =
       JSArrayBufferViewIsLengthTracking::decode(serialized_flags);
   is_backed_by_rab = JSArrayBufferViewIsBackedByRab::decode(serialized_flags);
@@ -2423,7 +2425,7 @@ static void CommitProperties(Handle<JSObject> object, Handle<Map> map,
   DCHECK(!object->map()->is_dictionary_map());
 
   DisallowGarbageCollection no_gc;
-  DescriptorArray descriptors = object->map()->instance_descriptors();
+  Tagged<DescriptorArray> descriptors = object->map()->instance_descriptors();
   for (InternalIndex i : InternalIndex::Range(properties.size())) {
     // Initializing store.
     object->WriteToField(i, descriptors->GetDetails(i),
@@ -2431,7 +2433,7 @@ static void CommitProperties(Handle<JSObject> object, Handle<Map> map,
   }
 }
 
-static bool IsValidObjectKey(Object value, Isolate* isolate) {
+static bool IsValidObjectKey(Tagged<Object> value, Isolate* isolate) {
   if (IsSmi(value)) return true;
   auto instance_type = HeapObject::cast(value)->map(isolate)->instance_type();
   return InstanceTypeChecker::IsName(instance_type) ||
@@ -2514,7 +2516,7 @@ Maybe<uint32_t> ValueDeserializer::ReadJSObjectProperties(
             if (expected_representation.IsHeapObject() &&
                 !target->instance_descriptors(isolate_)
                      ->GetFieldType(descriptor)
-                     .NowContains(value)) {
+                     ->NowContains(value)) {
               Handle<FieldType> value_type = Object::OptimalType(
                   *value, isolate_, expected_representation);
               MapUpdater::GeneralizeField(isolate_, target, descriptor,
@@ -2523,7 +2525,7 @@ Maybe<uint32_t> ValueDeserializer::ReadJSObjectProperties(
             }
             DCHECK(target->instance_descriptors(isolate_)
                        ->GetFieldType(descriptor)
-                       .NowContains(value));
+                       ->NowContains(value));
             properties.push_back(value);
             map = target;
             continue;
@@ -2594,7 +2596,7 @@ MaybeHandle<JSReceiver> ValueDeserializer::GetObjectWithID(uint32_t id) {
   if (id >= static_cast<unsigned>(id_map_->length())) {
     return MaybeHandle<JSReceiver>();
   }
-  Object value = id_map_->get(id);
+  Tagged<Object> value = id_map_->get(id);
   if (IsTheHole(value, isolate_)) return MaybeHandle<JSReceiver>();
   DCHECK(IsJSReceiver(value));
   return Handle<JSReceiver>(JSReceiver::cast(value), isolate_);
diff --git a/src/objects/value-serializer.h b/src/objects/value-serializer.h
index f5ccdcbf0a5..9b2d97cf79a 100644
--- a/src/objects/value-serializer.h
+++ b/src/objects/value-serializer.h
@@ -109,21 +109,21 @@ class ValueSerializer {
   void WriteZigZag(T value);
   void WriteOneByteString(base::Vector<const uint8_t> chars);
   void WriteTwoByteString(base::Vector<const base::uc16> chars);
-  void WriteBigIntContents(BigInt bigint);
+  void WriteBigIntContents(Tagged<BigInt> bigint);
   Maybe<uint8_t*> ReserveRawBytes(size_t bytes);
 
   // Writing V8 objects of various kinds.
-  void WriteOddball(Oddball oddball);
-  void WriteSmi(Smi smi);
-  void WriteHeapNumber(HeapNumber number);
-  void WriteBigInt(BigInt bigint);
+  void WriteOddball(Tagged<Oddball> oddball);
+  void WriteSmi(Tagged<Smi> smi);
+  void WriteHeapNumber(Tagged<HeapNumber> number);
+  void WriteBigInt(Tagged<BigInt> bigint);
   void WriteString(Handle<String> string);
   Maybe<bool> WriteJSReceiver(Handle<JSReceiver> receiver)
       V8_WARN_UNUSED_RESULT;
   Maybe<bool> WriteJSObject(Handle<JSObject> object) V8_WARN_UNUSED_RESULT;
   Maybe<bool> WriteJSObjectSlow(Handle<JSObject> object) V8_WARN_UNUSED_RESULT;
   Maybe<bool> WriteJSArray(Handle<JSArray> array) V8_WARN_UNUSED_RESULT;
-  void WriteJSDate(JSDate date);
+  void WriteJSDate(Tagged<JSDate> date);
   Maybe<bool> WriteJSPrimitiveWrapper(Handle<JSPrimitiveWrapper> value)
       V8_WARN_UNUSED_RESULT;
   void WriteJSRegExp(Handle<JSRegExp> regexp);
@@ -131,7 +131,7 @@ class ValueSerializer {
   Maybe<bool> WriteJSSet(Handle<JSSet> map) V8_WARN_UNUSED_RESULT;
   Maybe<bool> WriteJSArrayBuffer(Handle<JSArrayBuffer> array_buffer)
       V8_WARN_UNUSED_RESULT;
-  Maybe<bool> WriteJSArrayBufferView(JSArrayBufferView array_buffer);
+  Maybe<bool> WriteJSArrayBufferView(Tagged<JSArrayBufferView> array_buffer);
   Maybe<bool> WriteJSError(Handle<JSObject> error) V8_WARN_UNUSED_RESULT;
   Maybe<bool> WriteJSSharedArray(Handle<JSSharedArray> shared_array)
       V8_WARN_UNUSED_RESULT;
@@ -303,8 +303,8 @@ class ValueDeserializer {
   MaybeHandle<JSArrayBufferView> ReadJSArrayBufferView(
       Handle<JSArrayBuffer> buffer) V8_WARN_UNUSED_RESULT;
   bool ValidateJSArrayBufferViewFlags(
-      JSArrayBuffer buffer, uint32_t serialized_flags, bool& is_length_tracking,
-      bool& is_backed_by_rab) V8_WARN_UNUSED_RESULT;
+      Tagged<JSArrayBuffer> buffer, uint32_t serialized_flags,
+      bool& is_length_tracking, bool& is_backed_by_rab) V8_WARN_UNUSED_RESULT;
   MaybeHandle<Object> ReadJSError() V8_WARN_UNUSED_RESULT;
 #if V8_ENABLE_WEBASSEMBLY
   MaybeHandle<JSObject> ReadWasmModuleTransfer() V8_WARN_UNUSED_RESULT;
diff --git a/src/objects/visitors-inl.h b/src/objects/visitors-inl.h
index d0b7f6e5204..954dde64636 100644
--- a/src/objects/visitors-inl.h
+++ b/src/objects/visitors-inl.h
@@ -44,23 +44,24 @@ inline void ClientRootVisitor<Visitor>::VisitRunningCode(
     FullObjectSlot code_slot, FullObjectSlot maybe_istream_slot) {
 #if DEBUG
   DCHECK(!HeapObject::cast(*code_slot).InWritableSharedSpace());
-  Object maybe_istream = *maybe_istream_slot;
+  Tagged<Object> maybe_istream = *maybe_istream_slot;
   DCHECK(maybe_istream == Smi::zero() ||
          !HeapObject::cast(maybe_istream).InWritableSharedSpace());
 #endif
 }
 
 template <typename Visitor>
-inline void ClientObjectVisitor<Visitor>::VisitMapPointer(HeapObject host) {
-  if (!IsSharedHeapObject(host.map(cage_base()))) return;
+inline void ClientObjectVisitor<Visitor>::VisitMapPointer(
+    Tagged<HeapObject> host) {
+  if (!IsSharedHeapObject(host->map(cage_base()))) return;
   actual_visitor_->VisitMapPointer(host);
 }
 
 template <typename Visitor>
 inline void ClientObjectVisitor<Visitor>::VisitCodeTarget(
-    InstructionStream host, RelocInfo* rinfo) {
+    Tagged<InstructionStream> host, RelocInfo* rinfo) {
 #if DEBUG
-  InstructionStream target =
+  Tagged<InstructionStream> target =
       InstructionStream::FromTargetAddress(rinfo->target_address());
   DCHECK(!target.InWritableSharedSpace());
 #endif
@@ -68,7 +69,7 @@ inline void ClientObjectVisitor<Visitor>::VisitCodeTarget(
 
 template <typename Visitor>
 inline void ClientObjectVisitor<Visitor>::VisitEmbeddedPointer(
-    InstructionStream host, RelocInfo* rinfo) {
+    Tagged<InstructionStream> host, RelocInfo* rinfo) {
   if (!IsSharedHeapObject(rinfo->target_object(cage_base()))) return;
   actual_visitor_->VisitEmbeddedPointer(host, rinfo);
 }
diff --git a/src/objects/visitors.cc b/src/objects/visitors.cc
index dc18868ed16..71f3299bc28 100644
--- a/src/objects/visitors.cc
+++ b/src/objects/visitors.cc
@@ -27,7 +27,8 @@ const char* RootVisitor::RootName(Root root) {
   UNREACHABLE();
 }
 
-void ObjectVisitor::VisitRelocInfo(InstructionStream host, RelocIterator* it) {
+void ObjectVisitor::VisitRelocInfo(Tagged<InstructionStream> host,
+                                   RelocIterator* it) {
   // RelocInfo iteration is only valid for fully-initialized InstructionStream
   // objects. Callers must ensure this.
   DCHECK_NE(host->raw_code(kAcquireLoad), Smi::zero());
diff --git a/src/objects/visitors.h b/src/objects/visitors.h
index 7810a9bd10d..ac7ef40241d 100644
--- a/src/objects/visitors.h
+++ b/src/objects/visitors.h
@@ -126,65 +126,70 @@ class ObjectVisitor {
 
   // Visits a contiguous arrays of pointers in the half-open range
   // [start, end). Any or all of the values may be modified on return.
-  virtual void VisitPointers(HeapObject host, ObjectSlot start,
+  virtual void VisitPointers(Tagged<HeapObject> host, ObjectSlot start,
                              ObjectSlot end) = 0;
-  virtual void VisitPointers(HeapObject host, MaybeObjectSlot start,
+  virtual void VisitPointers(Tagged<HeapObject> host, MaybeObjectSlot start,
                              MaybeObjectSlot end) = 0;
   // When V8_EXTERNAL_CODE_SPACE is enabled, visits a InstructionStream pointer
   // slot. The values may be modified on return. Not used when
   // V8_EXTERNAL_CODE_SPACE is not enabled (the InstructionStream pointer slots
   // are visited as a part of on-heap slot visitation - via VisitPointers()).
-  virtual void VisitInstructionStreamPointer(Code host,
+  virtual void VisitInstructionStreamPointer(Tagged<Code> host,
                                              InstructionStreamSlot slot) = 0;
 
   // Custom weak pointers must be ignored by the GC but not other
   // visitors. They're used for e.g., lists that are recreated after GC. The
   // default implementation treats them as strong pointers. Visitors who want to
   // ignore them must override this function with empty.
-  virtual void VisitCustomWeakPointers(HeapObject host, ObjectSlot start,
-                                       ObjectSlot end) {
+  virtual void VisitCustomWeakPointers(Tagged<HeapObject> host,
+                                       ObjectSlot start, ObjectSlot end) {
     VisitPointers(host, start, end);
   }
 
   // Handy shorthand for visiting a single pointer.
-  virtual void VisitPointer(HeapObject host, ObjectSlot p) {
+  virtual void VisitPointer(Tagged<HeapObject> host, ObjectSlot p) {
     VisitPointers(host, p, p + 1);
   }
-  virtual void VisitPointer(HeapObject host, MaybeObjectSlot p) {
+  virtual void VisitPointer(Tagged<HeapObject> host, MaybeObjectSlot p) {
     VisitPointers(host, p, p + 1);
   }
-  virtual void VisitCustomWeakPointer(HeapObject host, ObjectSlot p) {
+  virtual void VisitCustomWeakPointer(Tagged<HeapObject> host, ObjectSlot p) {
     VisitCustomWeakPointers(host, p, p + 1);
   }
 
-  virtual void VisitEphemeron(HeapObject host, int index, ObjectSlot key,
-                              ObjectSlot value) {
+  virtual void VisitEphemeron(Tagged<HeapObject> host, int index,
+                              ObjectSlot key, ObjectSlot value) {
     VisitPointer(host, key);
     VisitPointer(host, value);
   }
 
   // Visits the relocation info using the given iterator.
-  void VisitRelocInfo(InstructionStream host, RelocIterator* it);
+  void VisitRelocInfo(Tagged<InstructionStream> host, RelocIterator* it);
 
-  virtual void VisitCodeTarget(InstructionStream host, RelocInfo* rinfo) {}
-  virtual void VisitEmbeddedPointer(InstructionStream host, RelocInfo* rinfo) {}
-  virtual void VisitExternalReference(InstructionStream host,
+  virtual void VisitCodeTarget(Tagged<InstructionStream> host,
+                               RelocInfo* rinfo) {}
+  virtual void VisitEmbeddedPointer(Tagged<InstructionStream> host,
+                                    RelocInfo* rinfo) {}
+  virtual void VisitExternalReference(Tagged<InstructionStream> host,
                                       RelocInfo* rinfo) {}
-  virtual void VisitInternalReference(InstructionStream host,
+  virtual void VisitInternalReference(Tagged<InstructionStream> host,
                                       RelocInfo* rinfo) {}
   // TODO(ishell): rename to VisitBuiltinEntry.
-  virtual void VisitOffHeapTarget(InstructionStream host, RelocInfo* rinfo) {}
+  virtual void VisitOffHeapTarget(Tagged<InstructionStream> host,
+                                  RelocInfo* rinfo) {}
 
-  virtual void VisitExternalPointer(HeapObject host, ExternalPointerSlot slot,
+  virtual void VisitExternalPointer(Tagged<HeapObject> host,
+                                    ExternalPointerSlot slot,
                                     ExternalPointerTag tag) {}
 
-  virtual void VisitIndirectPointer(HeapObject host, IndirectPointerSlot slot,
+  virtual void VisitIndirectPointer(Tagged<HeapObject> host,
+                                    IndirectPointerSlot slot,
                                     IndirectPointerMode mode) {}
 
-  virtual void VisitIndirectPointerTableEntry(HeapObject host,
+  virtual void VisitIndirectPointerTableEntry(Tagged<HeapObject> host,
                                               IndirectPointerSlot slot) {}
 
-  virtual void VisitMapPointer(HeapObject host) { UNREACHABLE(); }
+  virtual void VisitMapPointer(Tagged<HeapObject> host) { UNREACHABLE(); }
 };
 
 // Helper version of ObjectVisitor that also takes care of caching base values
@@ -257,7 +262,7 @@ class ClientRootVisitor final : public RootVisitor {
   }
 
  private:
-  V8_INLINE static bool IsSharedHeapObject(Object object) {
+  V8_INLINE static bool IsSharedHeapObject(Tagged<Object> object) {
     return IsHeapObject(object) &&
            HeapObject::cast(object).InWritableSharedSpace();
   }
@@ -278,14 +283,15 @@ class ClientObjectVisitor final : public ObjectVisitorWithCageBases {
                                    actual_visitor->code_cage_base()),
         actual_visitor_(actual_visitor) {}
 
-  void VisitPointer(HeapObject host, ObjectSlot p) final {
+  void VisitPointer(Tagged<HeapObject> host, ObjectSlot p) final {
     if (!IsSharedHeapObject(p.load(cage_base()))) return;
     actual_visitor_->VisitPointer(host, p);
   }
 
-  inline void VisitMapPointer(HeapObject host) final;
+  inline void VisitMapPointer(Tagged<HeapObject> host) final;
 
-  void VisitPointers(HeapObject host, ObjectSlot start, ObjectSlot end) final {
+  void VisitPointers(Tagged<HeapObject> host, ObjectSlot start,
+                     ObjectSlot end) final {
     for (ObjectSlot p = start; p < end; ++p) {
       // The map slot should be handled in VisitMapPointer.
       DCHECK_NE(host->map_slot(), p);
@@ -294,30 +300,31 @@ class ClientObjectVisitor final : public ObjectVisitorWithCageBases {
     }
   }
 
-  void VisitInstructionStreamPointer(Code host,
+  void VisitInstructionStreamPointer(Tagged<Code> host,
                                      InstructionStreamSlot slot) final {
 #if DEBUG
-    Object istream_object = slot.load(code_cage_base());
-    InstructionStream istream;
+    Tagged<Object> istream_object = slot.load(code_cage_base());
+    Tagged<InstructionStream> istream;
     if (istream_object.GetHeapObject(&istream)) {
       DCHECK(!istream.InWritableSharedSpace());
     }
 #endif
   }
 
-  void VisitPointers(HeapObject host, MaybeObjectSlot start,
+  void VisitPointers(Tagged<HeapObject> host, MaybeObjectSlot start,
                      MaybeObjectSlot end) final {
     // At the moment, custom roots cannot contain weak pointers.
     UNREACHABLE();
   }
 
-  inline void VisitCodeTarget(InstructionStream host, RelocInfo* rinfo) final;
+  inline void VisitCodeTarget(Tagged<InstructionStream> host,
+                              RelocInfo* rinfo) final;
 
-  inline void VisitEmbeddedPointer(InstructionStream host,
+  inline void VisitEmbeddedPointer(Tagged<InstructionStream> host,
                                    RelocInfo* rinfo) final;
 
  private:
-  V8_INLINE static bool IsSharedHeapObject(Object object) {
+  V8_INLINE static bool IsSharedHeapObject(Tagged<Object> object) {
     return IsHeapObject(object) &&
            HeapObject::cast(object).InWritableSharedSpace();
   }
diff --git a/src/parsing/parse-info.cc b/src/parsing/parse-info.cc
index 4b288a06e1a..aba8e188b8a 100644
--- a/src/parsing/parse-info.cc
+++ b/src/parsing/parse-info.cc
@@ -44,13 +44,13 @@ UnoptimizedCompileFlags::UnoptimizedCompileFlags(Isolate* isolate,
 
 // static
 UnoptimizedCompileFlags UnoptimizedCompileFlags::ForFunctionCompile(
-    Isolate* isolate, SharedFunctionInfo shared) {
-  Script script = Script::cast(shared->script());
+    Isolate* isolate, Tagged<SharedFunctionInfo> shared) {
+  Tagged<Script> script = Script::cast(shared->script());
 
   UnoptimizedCompileFlags flags(isolate, script->id());
 
   flags.SetFlagsForFunctionFromScript(script);
-  flags.SetFlagsFromFunction(&shared);
+  flags.SetFlagsFromFunction(shared);
   flags.set_allow_lazy_parsing(true);
   flags.set_is_lazy_compile(true);
 
@@ -67,7 +67,7 @@ UnoptimizedCompileFlags UnoptimizedCompileFlags::ForFunctionCompile(
 
 // static
 UnoptimizedCompileFlags UnoptimizedCompileFlags::ForScriptCompile(
-    Isolate* isolate, Script script) {
+    Isolate* isolate, Tagged<Script> script) {
   UnoptimizedCompileFlags flags(isolate, script->id());
 
   flags.SetFlagsForFunctionFromScript(script);
@@ -147,13 +147,14 @@ void UnoptimizedCompileFlags::SetFlagsForToplevelCompile(
   set_block_coverage_enabled(block_coverage_enabled() && is_user_javascript);
 }
 
-void UnoptimizedCompileFlags::SetFlagsForFunctionFromScript(Script script) {
+void UnoptimizedCompileFlags::SetFlagsForFunctionFromScript(
+    Tagged<Script> script) {
   DCHECK_EQ(script_id(), script->id());
 
   set_is_eval(script->compilation_type() == Script::CompilationType::kEval);
   if (is_eval()) {
     DCHECK(script->has_eval_from_shared());
-    set_outer_language_mode(script.eval_from_shared()->language_mode());
+    set_outer_language_mode(script->eval_from_shared()->language_mode());
   }
   set_is_module(script->origin_options().IsModule());
   DCHECK_IMPLIES(is_eval(), !is_module());
@@ -310,7 +311,7 @@ void ParseInfo::set_character_stream(
   character_stream_.swap(character_stream);
 }
 
-void ParseInfo::CheckFlagsForToplevelCompileFromScript(Script script) {
+void ParseInfo::CheckFlagsForToplevelCompileFromScript(Tagged<Script> script) {
   CheckFlagsForFunctionFromScript(script);
   DCHECK(flags().is_toplevel());
   DCHECK_EQ(flags().is_repl_mode(), script->is_repl_mode());
@@ -320,7 +321,7 @@ void ParseInfo::CheckFlagsForToplevelCompileFromScript(Script script) {
   }
 }
 
-void ParseInfo::CheckFlagsForFunctionFromScript(Script script) {
+void ParseInfo::CheckFlagsForFunctionFromScript(Tagged<Script> script) {
   DCHECK_EQ(flags().script_id(), script->id());
   // We set "is_eval" for wrapped scripts to get an outer declaration scope.
   // This is a bit hacky, but ok since we can't be both eval and wrapped.
diff --git a/src/parsing/parse-info.h b/src/parsing/parse-info.h
index 940acbcb641..59de7059190 100644
--- a/src/parsing/parse-info.h
+++ b/src/parsing/parse-info.h
@@ -77,12 +77,12 @@ class V8_EXPORT_PRIVATE UnoptimizedCompileFlags {
 
   // Set-up flags for a compiling a particular function (either a lazy compile
   // or a recompile).
-  static UnoptimizedCompileFlags ForFunctionCompile(Isolate* isolate,
-                                                    SharedFunctionInfo shared);
+  static UnoptimizedCompileFlags ForFunctionCompile(
+      Isolate* isolate, Tagged<SharedFunctionInfo> shared);
 
   // Set-up flags for a full compilation of a given script.
   static UnoptimizedCompileFlags ForScriptCompile(Isolate* isolate,
-                                                  Script script);
+                                                  Tagged<Script> script);
 
   // Set-up flags for a parallel toplevel function compilation, based on the
   // flags of an existing toplevel compilation.
@@ -146,7 +146,7 @@ class V8_EXPORT_PRIVATE UnoptimizedCompileFlags {
                                   LanguageMode language_mode,
                                   REPLMode repl_mode, ScriptType type,
                                   bool lazy);
-  void SetFlagsForFunctionFromScript(Script script);
+  void SetFlagsForFunctionFromScript(Tagged<Script> script);
 
   uint32_t flags_;
   int script_id_;
@@ -339,7 +339,7 @@ class V8_EXPORT_PRIVATE ParseInfo {
     source_range_map_ = source_range_map;
   }
 
-  void CheckFlagsForFunctionFromScript(Script script);
+  void CheckFlagsForFunctionFromScript(Tagged<Script> script);
 
   bool is_background_compilation() const { return is_background_compilation_; }
 
@@ -369,7 +369,7 @@ class V8_EXPORT_PRIVATE ParseInfo {
             ReusableUnoptimizedCompileState* reusable_state,
             uintptr_t stack_limit, RuntimeCallStats* runtime_call_stats);
 
-  void CheckFlagsForToplevelCompileFromScript(Script script);
+  void CheckFlagsForToplevelCompileFromScript(Tagged<Script> script);
 
   //------------- Inputs to parsing and scope analysis -----------------------
   const UnoptimizedCompileFlags flags_;
diff --git a/src/parsing/preparse-data-impl.h b/src/parsing/preparse-data-impl.h
index 3e271844a5f..733c5aea2f9 100644
--- a/src/parsing/preparse-data-impl.h
+++ b/src/parsing/preparse-data-impl.h
@@ -17,18 +17,28 @@ namespace internal {
 // a header for tests.
 
 // Wraps a ZoneVector<uint8_t> to have with functions named the same as
-// PodArray<uint8_t>.
+// Tagged<PodArray<uint8_t>>.
 class ZoneVectorWrapper {
  public:
-  ZoneVectorWrapper() = default;
-  explicit ZoneVectorWrapper(ZoneVector<uint8_t>* data) : data_(data) {}
+  class Inner {
+   public:
+    Inner() = default;
+    explicit Inner(ZoneVector<uint8_t>* data) : data_(data) {}
 
-  int data_length() const { return static_cast<int>(data_->size()); }
+    int data_length() const { return static_cast<int>(data_->size()); }
+    uint8_t get(int index) const { return data_->at(index); }
+
+   private:
+    ZoneVector<uint8_t>* data_ = nullptr;
+  };
+
+  ZoneVectorWrapper() = default;
+  explicit ZoneVectorWrapper(ZoneVector<uint8_t>* data) : inner_(data) {}
 
-  uint8_t get(int index) const { return data_->at(index); }
+  const Inner* operator->() const { return &inner_; }
 
  private:
-  ZoneVector<uint8_t>* data_ = nullptr;
+  Inner inner_;
 };
 
 template <class Data>
@@ -62,29 +72,29 @@ class BaseConsumedPreparseData : public ConsumedPreparseData {
     };
 
     void SetPosition(int position) {
-      DCHECK_LE(position, data_.data_length());
+      DCHECK_LE(position, data_->data_length());
       index_ = position;
     }
 
     size_t RemainingBytes() const {
       DCHECK(has_data_);
-      DCHECK_LE(index_, data_.data_length());
-      return data_.data_length() - index_;
+      DCHECK_LE(index_, data_->data_length());
+      return data_->data_length() - index_;
     }
 
     bool HasRemainingBytes(size_t bytes) const {
       DCHECK(has_data_);
-      return index_ <= data_.data_length() && bytes <= RemainingBytes();
+      return index_ <= data_->data_length() && bytes <= RemainingBytes();
     }
 
     int32_t ReadUint32() {
       DCHECK(has_data_);
       DCHECK(HasRemainingBytes(kUint32Size));
       // Check that there indeed is an integer following.
-      DCHECK_EQ(data_.get(index_++), kUint32Size);
-      int32_t result = data_.get(index_) + (data_.get(index_ + 1) << 8) +
-                       (data_.get(index_ + 2) << 16) +
-                       (data_.get(index_ + 3) << 24);
+      DCHECK_EQ(data_->get(index_++), kUint32Size);
+      int32_t result = data_->get(index_) + (data_->get(index_ + 1) << 8) +
+                       (data_->get(index_ + 2) << 16) +
+                       (data_->get(index_ + 3) << 24);
       index_ += 4;
       stored_quarters_ = 0;
       return result;
@@ -92,17 +102,17 @@ class BaseConsumedPreparseData : public ConsumedPreparseData {
 
     int32_t ReadVarint32() {
       DCHECK(HasRemainingBytes(kVarint32MinSize));
-      DCHECK_EQ(data_.get(index_++), kVarint32MinSize);
+      DCHECK_EQ(data_->get(index_++), kVarint32MinSize);
       int32_t value = 0;
       bool has_another_byte;
       unsigned shift = 0;
       do {
-        uint8_t byte = data_.get(index_++);
+        uint8_t byte = data_->get(index_++);
         value |= static_cast<int32_t>(byte & 0x7F) << shift;
         shift += 7;
         has_another_byte = byte & 0x80;
       } while (has_another_byte);
-      DCHECK_EQ(data_.get(index_++), kVarint32EndMarker);
+      DCHECK_EQ(data_->get(index_++), kVarint32EndMarker);
       stored_quarters_ = 0;
       return value;
     }
@@ -111,9 +121,9 @@ class BaseConsumedPreparseData : public ConsumedPreparseData {
       DCHECK(has_data_);
       DCHECK(HasRemainingBytes(kUint8Size));
       // Check that there indeed is a byte following.
-      DCHECK_EQ(data_.get(index_++), kUint8Size);
+      DCHECK_EQ(data_->get(index_++), kUint8Size);
       stored_quarters_ = 0;
-      return data_.get(index_++);
+      return data_->get(index_++);
     }
 
     uint8_t ReadQuarter() {
@@ -121,8 +131,8 @@ class BaseConsumedPreparseData : public ConsumedPreparseData {
       if (stored_quarters_ == 0) {
         DCHECK(HasRemainingBytes(kUint8Size));
         // Check that there indeed are quarters following.
-        DCHECK_EQ(data_.get(index_++), kQuarterMarker);
-        stored_byte_ = data_.get(index_++);
+        DCHECK_EQ(data_->get(index_++), kQuarterMarker);
+        stored_byte_ = data_->get(index_++);
         stored_quarters_ = 4;
       }
       // Read the first 2 bits from stored_byte_.
@@ -180,11 +190,11 @@ class BaseConsumedPreparseData : public ConsumedPreparseData {
 
 // Implementation of ConsumedPreparseData for on-heap data.
 class OnHeapConsumedPreparseData final
-    : public BaseConsumedPreparseData<PreparseData> {
+    : public BaseConsumedPreparseData<Tagged<PreparseData>> {
  public:
   OnHeapConsumedPreparseData(LocalIsolate* isolate, Handle<PreparseData> data);
 
-  PreparseData GetScopeData() final;
+  Tagged<PreparseData> GetScopeData() final;
   ProducedPreparseData* GetChildData(Zone* zone, int child_index) final;
 
  private:
diff --git a/src/parsing/preparse-data.cc b/src/parsing/preparse-data.cc
index a6318fde04c..acc520be0e4 100644
--- a/src/parsing/preparse-data.cc
+++ b/src/parsing/preparse-data.cc
@@ -762,7 +762,9 @@ bool BaseConsumedPreparseData<Data>::VerifyDataStart() {
 }
 #endif
 
-PreparseData OnHeapConsumedPreparseData::GetScopeData() { return *data_; }
+Tagged<PreparseData> OnHeapConsumedPreparseData::GetScopeData() {
+  return *data_;
+}
 
 ProducedPreparseData* OnHeapConsumedPreparseData::GetChildData(Zone* zone,
                                                                int index) {
@@ -773,7 +775,9 @@ ProducedPreparseData* OnHeapConsumedPreparseData::GetChildData(Zone* zone,
 
 OnHeapConsumedPreparseData::OnHeapConsumedPreparseData(
     LocalIsolate* isolate, Handle<PreparseData> data)
-    : BaseConsumedPreparseData<PreparseData>(), isolate_(isolate), data_(data) {
+    : BaseConsumedPreparseData<Tagged<PreparseData>>(),
+      isolate_(isolate),
+      data_(data) {
   DCHECK_NOT_NULL(isolate);
   DCHECK(IsPreparseData(*data));
   DCHECK(VerifyDataStart());
diff --git a/src/parsing/scanner-character-streams.cc b/src/parsing/scanner-character-streams.cc
index 4c81cca1418..245d7684875 100644
--- a/src/parsing/scanner-character-streams.cc
+++ b/src/parsing/scanner-character-streams.cc
@@ -23,7 +23,7 @@ namespace internal {
 
 class V8_NODISCARD ScopedExternalStringLock {
  public:
-  explicit ScopedExternalStringLock(ExternalString string) {
+  explicit ScopedExternalStringLock(Tagged<ExternalString> string) {
     DCHECK(!string.is_null());
     if (IsExternalOneByteString(string)) {
       resource_ = ExternalOneByteString::cast(string)->resource();
@@ -866,9 +866,9 @@ Utf16CharacterStream* ScannerStream::For(Isolate* isolate, Handle<String> data,
   DCHECK_LE(end_pos, data->length());
   size_t start_offset = 0;
   if (IsSlicedString(*data)) {
-    SlicedString string = SlicedString::cast(*data);
+    Tagged<SlicedString> string = SlicedString::cast(*data);
     start_offset = string->offset();
-    String parent = string->parent();
+    Tagged<String> parent = string->parent();
     if (IsThinString(parent)) parent = ThinString::cast(parent)->actual();
     data = handle(parent, isolate);
   } else {
diff --git a/src/profiler/allocation-tracker.cc b/src/profiler/allocation-tracker.cc
index a51c20284d8..eac8c082aee 100644
--- a/src/profiler/allocation-tracker.cc
+++ b/src/profiler/allocation-tracker.cc
@@ -211,7 +211,7 @@ void AllocationTracker::AllocationEvent(Address addr, int size) {
   JavaScriptStackFrameIterator it(isolate);
   while (!it.done() && length < kMaxAllocationTraceLength) {
     JavaScriptFrame* frame = it.frame();
-    SharedFunctionInfo shared = frame->function()->shared();
+    Tagged<SharedFunctionInfo> shared = frame->function()->shared();
     SnapshotObjectId id =
         ids_->FindOrAddEntry(shared.address(), shared->Size(),
                              HeapObjectsMap::MarkEntryAccessed::kNo);
@@ -236,7 +236,7 @@ static uint32_t SnapshotObjectIdHash(SnapshotObjectId id) {
   return ComputeUnseededHash(static_cast<uint32_t>(id));
 }
 
-unsigned AllocationTracker::AddFunctionInfo(SharedFunctionInfo shared,
+unsigned AllocationTracker::AddFunctionInfo(Tagged<SharedFunctionInfo> shared,
                                             SnapshotObjectId id) {
   base::HashMap::Entry* entry = id_to_function_info_index_.LookupOrInsert(
       reinterpret_cast<void*>(id), SnapshotObjectIdHash(id));
@@ -245,9 +245,9 @@ unsigned AllocationTracker::AddFunctionInfo(SharedFunctionInfo shared,
     info->name = names_->GetCopy(shared->DebugNameCStr().get());
     info->function_id = id;
     if (IsScript(shared->script())) {
-      Script script = Script::cast(shared->script());
+      Tagged<Script> script = Script::cast(shared->script());
       if (IsName(script->name())) {
-        Name name = Name::cast(script->name());
+        Tagged<Name> name = Name::cast(script->name());
         info->script_name = names_->GetName(name);
       }
       info->script_id = script->id();
@@ -274,7 +274,7 @@ unsigned AllocationTracker::functionInfoIndexForVMState(StateTag state) {
   return info_index_for_other_state_;
 }
 
-AllocationTracker::UnresolvedLocation::UnresolvedLocation(Script script,
+AllocationTracker::UnresolvedLocation::UnresolvedLocation(Tagged<Script> script,
                                                           int start,
                                                           FunctionInfo* info)
     : start_position_(start), info_(info) {
diff --git a/src/profiler/allocation-tracker.h b/src/profiler/allocation-tracker.h
index 3f199c91fbc..bb59c92a780 100644
--- a/src/profiler/allocation-tracker.h
+++ b/src/profiler/allocation-tracker.h
@@ -123,12 +123,13 @@ class AllocationTracker {
   AddressToTraceMap* address_to_trace() { return &address_to_trace_; }
 
  private:
-  unsigned AddFunctionInfo(SharedFunctionInfo info, SnapshotObjectId id);
+  unsigned AddFunctionInfo(Tagged<SharedFunctionInfo> info,
+                           SnapshotObjectId id);
   unsigned functionInfoIndexForVMState(StateTag state);
 
   class UnresolvedLocation {
    public:
-    UnresolvedLocation(Script script, int start, FunctionInfo* info);
+    UnresolvedLocation(Tagged<Script> script, int start, FunctionInfo* info);
     ~UnresolvedLocation();
     void Resolve();
 
diff --git a/src/profiler/cpu-profiler.cc b/src/profiler/cpu-profiler.cc
index ab00945de4b..6bcba08f5bf 100644
--- a/src/profiler/cpu-profiler.cc
+++ b/src/profiler/cpu-profiler.cc
@@ -423,7 +423,7 @@ void ProfilerCodeObserver::LogBuiltins() {
        ++builtin) {
     CodeEventsContainer evt_rec(CodeEventRecord::Type::kReportBuiltin);
     ReportBuiltinEventRecord* rec = &evt_rec.ReportBuiltinEventRecord_;
-    Code code = builtins->code(builtin);
+    Tagged<Code> code = builtins->code(builtin);
     rec->instruction_start = code->instruction_start();
     rec->instruction_size = code->instruction_size();
     rec->builtin = builtin;
@@ -635,7 +635,7 @@ CpuProfilingResult CpuProfiler::StartProfiling(
 }
 
 CpuProfilingResult CpuProfiler::StartProfiling(
-    String title, CpuProfilingOptions options,
+    Tagged<String> title, CpuProfilingOptions options,
     std::unique_ptr<DiscardedSamplesDelegate> delegate) {
   return StartProfiling(profiles_->GetName(title), std::move(options),
                         std::move(delegate));
@@ -693,7 +693,7 @@ CpuProfile* CpuProfiler::StopProfiling(ProfilerId id) {
   return profile;
 }
 
-CpuProfile* CpuProfiler::StopProfiling(String title) {
+CpuProfile* CpuProfiler::StopProfiling(Tagged<String> title) {
   return StopProfiling(profiles_->GetName(title));
 }
 
diff --git a/src/profiler/cpu-profiler.h b/src/profiler/cpu-profiler.h
index 6cd1c4b0188..9a3ad6759af 100644
--- a/src/profiler/cpu-profiler.h
+++ b/src/profiler/cpu-profiler.h
@@ -360,11 +360,11 @@ class V8_EXPORT_PRIVATE CpuProfiler {
       const char* title, CpuProfilingOptions options = {},
       std::unique_ptr<DiscardedSamplesDelegate> delegate = nullptr);
   CpuProfilingResult StartProfiling(
-      String title, CpuProfilingOptions options = {},
+      Tagged<String> title, CpuProfilingOptions options = {},
       std::unique_ptr<DiscardedSamplesDelegate> delegate = nullptr);
 
   CpuProfile* StopProfiling(const char* title);
-  CpuProfile* StopProfiling(String title);
+  CpuProfile* StopProfiling(Tagged<String> title);
   CpuProfile* StopProfiling(ProfilerId id);
 
   int GetProfilesCount();
diff --git a/src/profiler/heap-profiler.cc b/src/profiler/heap-profiler.cc
index 01d1953542c..a23e6d94d17 100644
--- a/src/profiler/heap-profiler.cc
+++ b/src/profiler/heap-profiler.cc
@@ -127,13 +127,11 @@ bool HeapProfiler::StartSamplingHeapProfiler(
   return true;
 }
 
-
 void HeapProfiler::StopSamplingHeapProfiler() {
   sampling_heap_profiler_.reset();
   MaybeClearStringsStorage();
 }
 
-
 v8::AllocationProfile* HeapProfiler::GetAllocationProfile() {
   if (sampling_heap_profiler_.get()) {
     return sampling_heap_profiler_->GetAllocationProfile();
@@ -142,7 +140,6 @@ v8::AllocationProfile* HeapProfiler::GetAllocationProfile() {
   }
 }
 
-
 void HeapProfiler::StartHeapObjectsTracking(bool track_allocations) {
   ids_->UpdateHeapObjectsMap();
   if (native_move_listener_) {
@@ -218,7 +215,6 @@ void HeapProfiler::AllocationEvent(Address addr, int size) {
   }
 }
 
-
 void HeapProfiler::UpdateObjectSizeEvent(Address addr, int size) {
   ids_->UpdateObjectSize(addr, size);
 }
@@ -227,7 +223,7 @@ Handle<HeapObject> HeapProfiler::FindHeapObjectById(SnapshotObjectId id) {
   CombinedHeapObjectIterator iterator(heap(),
                                       HeapObjectIterator::kFilterUnreachable);
   // Make sure that the object with the given id is still reachable.
-  for (HeapObject obj = iterator.Next(); !obj.is_null();
+  for (Tagged<HeapObject> obj = iterator.Next(); !obj.is_null();
        obj = iterator.Next()) {
     if (ids_->FindEntry(obj.address()) == id)
       return Handle<HeapObject>(obj, isolate());
@@ -235,7 +231,6 @@ Handle<HeapObject> HeapProfiler::FindHeapObjectById(SnapshotObjectId id) {
   return Handle<HeapObject>();
 }
 
-
 void HeapProfiler::ClearHeapObjectMap() {
   ids_.reset(new HeapObjectsMap(heap()));
   if (!allocation_tracker_) {
@@ -247,7 +242,6 @@ void HeapProfiler::ClearHeapObjectMap() {
   }
 }
 
-
 Heap* HeapProfiler::heap() const { return ids_->heap(); }
 
 Isolate* HeapProfiler::isolate() const { return heap()->isolate(); }
@@ -264,8 +258,8 @@ void HeapProfiler::QueryObjects(Handle<Context> context,
       std::vector<Handle<JSTypedArray>> on_heap_typed_arrays;
       CombinedHeapObjectIterator heap_iterator(
           heap(), HeapObjectIterator::kFilterUnreachable);
-      for (HeapObject heap_obj = heap_iterator.Next(); !heap_obj.is_null();
-           heap_obj = heap_iterator.Next()) {
+      for (Tagged<HeapObject> heap_obj = heap_iterator.Next();
+           !heap_obj.is_null(); heap_obj = heap_iterator.Next()) {
         if (IsFeedbackVector(heap_obj)) {
           FeedbackVector::cast(heap_obj)->ClearSlots(isolate());
         } else if (IsJSTypedArray(heap_obj) &&
@@ -288,8 +282,8 @@ void HeapProfiler::QueryObjects(Handle<Context> context,
     CombinedHeapObjectIterator heap_iterator(
         heap(), HeapObjectIterator::kFilterUnreachable);
     PtrComprCageBase cage_base(isolate());
-    for (HeapObject heap_obj = heap_iterator.Next(); !heap_obj.is_null();
-         heap_obj = heap_iterator.Next()) {
+    for (Tagged<HeapObject> heap_obj = heap_iterator.Next();
+         !heap_obj.is_null(); heap_obj = heap_iterator.Next()) {
       if (!IsJSObject(heap_obj, cage_base) ||
           IsJSExternalObject(heap_obj, cage_base))
         continue;
diff --git a/src/profiler/heap-snapshot-generator.cc b/src/profiler/heap-snapshot-generator.cc
index e47538177d1..ba780ac0d24 100644
--- a/src/profiler/heap-snapshot-generator.cc
+++ b/src/profiler/heap-snapshot-generator.cc
@@ -55,7 +55,7 @@ namespace internal {
 #ifdef V8_ENABLE_HEAP_SNAPSHOT_VERIFY
 class HeapEntryVerifier {
  public:
-  HeapEntryVerifier(HeapSnapshotGenerator* generator, HeapObject obj)
+  HeapEntryVerifier(HeapSnapshotGenerator* generator, Tagged<HeapObject> obj)
       : generator_(generator),
         primary_object_(obj),
         reference_summary_(
@@ -70,7 +70,8 @@ class HeapEntryVerifier {
   // Checks that `host` retains `target`, according to the marking visitor. This
   // allows us to verify, when adding edges to the snapshot, that they
   // correspond to real retaining relationships.
-  void CheckStrongReference(HeapObject host, HeapObject target) {
+  void CheckStrongReference(Tagged<HeapObject> host,
+                            Tagged<HeapObject> target) {
     // All references should be from the current primary object.
     CHECK_EQ(host, primary_object_);
 
@@ -100,7 +101,7 @@ class HeapEntryVerifier {
 
   // Checks that `host` has a weak reference to `target`, according to the
   // marking visitor.
-  void CheckWeakReference(HeapObject host, HeapObject target) {
+  void CheckWeakReference(Tagged<HeapObject> host, Tagged<HeapObject> target) {
     // All references should be from the current primary object.
     CHECK_EQ(host, primary_object_);
 
@@ -113,7 +114,8 @@ class HeapEntryVerifier {
   // marking visitor found no such relationship. This is necessary for
   // ephemerons, where a pair of objects is required to retain the target.
   // Use this function with care, since it bypasses verification.
-  void MarkReferenceCheckedWithoutChecking(HeapObject host, HeapObject target) {
+  void MarkReferenceCheckedWithoutChecking(Tagged<HeapObject> host,
+                                           Tagged<HeapObject> target) {
     if (host == primary_object_) {
       checked_objects_.insert(target);
     }
@@ -129,12 +131,12 @@ class HeapEntryVerifier {
     // snapshot deliberately omits many of those (see IsEssentialObject).
     // Read-only objects can't ever retain normal read-write objects, so these
     // are fine to skip.
-    for (HeapObject obj : reference_summary_.strong_references()) {
+    for (Tagged<HeapObject> obj : reference_summary_.strong_references()) {
       if (!BasicMemoryChunk::FromHeapObject(obj)->InReadOnlySpace()) {
         CHECK_NE(checked_objects_.find(obj), checked_objects_.end());
       }
     }
-    for (HeapObject obj : reference_summary_.weak_references()) {
+    for (Tagged<HeapObject> obj : reference_summary_.weak_references()) {
       if (!BasicMemoryChunk::FromHeapObject(obj)->InReadOnlySpace()) {
         CHECK_NE(checked_objects_.find(obj), checked_objects_.end());
       }
@@ -154,7 +156,7 @@ class HeapEntryVerifier {
       const UnorderedHeapObjectSet& previous =
           level == 0 ? reference_summary_.strong_references()
                      : indirect_strong_references_[level - 1];
-      for (HeapObject obj : previous) {
+      for (Tagged<HeapObject> obj : previous) {
         if (BasicMemoryChunk::FromHeapObject(obj)->InReadOnlySpace()) {
           // Marking visitors don't expect to visit objects in read-only space,
           // and will fail DCHECKs if they are used on those objects. Read-only
@@ -206,11 +208,8 @@ HeapGraphEdge::HeapGraphEdge(Type type, const char* name, HeapEntry* from,
                  FromIndexField::encode(from->index())),
       to_entry_(to),
       name_(name) {
-  DCHECK(type == kContextVariable
-      || type == kProperty
-      || type == kInternal
-      || type == kShortcut
-      || type == kWeak);
+  DCHECK(type == kContextVariable || type == kProperty || type == kInternal ||
+         type == kShortcut || type == kWeak);
 }
 
 HeapGraphEdge::HeapGraphEdge(Type type, int index, HeapEntry* from,
@@ -262,8 +261,8 @@ void HeapEntry::VerifyReference(HeapGraphEdge::Type type, HeapEntry* entry,
     // Verification is not possible.
     return;
   }
-  HeapObject from_obj = HeapObject::cast(Object(from_address));
-  HeapObject to_obj = HeapObject::cast(Object(to_address));
+  Tagged<HeapObject> from_obj = HeapObject::cast(Object(from_address));
+  Tagged<HeapObject> to_obj = HeapObject::cast(Object(to_address));
   if (BasicMemoryChunk::FromHeapObject(to_obj)->InReadOnlySpace()) {
     // We can't verify pointers into read-only space, because marking visitors
     // might not mark those. For example, every Map has a pointer to the
@@ -377,24 +376,38 @@ void HeapEntry::Print(const char* prefix, const char* edge_name, int max_depth,
 
 const char* HeapEntry::TypeAsString() const {
   switch (type()) {
-    case kHidden: return "/hidden/";
-    case kObject: return "/object/";
-    case kClosure: return "/closure/";
-    case kString: return "/string/";
-    case kCode: return "/code/";
-    case kArray: return "/array/";
-    case kRegExp: return "/regexp/";
-    case kHeapNumber: return "/number/";
-    case kNative: return "/native/";
-    case kSynthetic: return "/synthetic/";
-    case kConsString: return "/concatenated string/";
-    case kSlicedString: return "/sliced string/";
-    case kSymbol: return "/symbol/";
+    case kHidden:
+      return "/hidden/";
+    case kObject:
+      return "/object/";
+    case kClosure:
+      return "/closure/";
+    case kString:
+      return "/string/";
+    case kCode:
+      return "/code/";
+    case kArray:
+      return "/array/";
+    case kRegExp:
+      return "/regexp/";
+    case kHeapNumber:
+      return "/number/";
+    case kNative:
+      return "/native/";
+    case kSynthetic:
+      return "/synthetic/";
+    case kConsString:
+      return "/concatenated string/";
+    case kSlicedString:
+      return "/sliced string/";
+    case kSymbol:
+      return "/symbol/";
     case kBigInt:
       return "/bigint/";
     case kObjectShape:
       return "/object shape/";
-    default: return "???";
+    default:
+      return "???";
   }
 }
 
@@ -417,9 +430,7 @@ HeapSnapshot::HeapSnapshot(HeapProfiler* profiler,
   memset(&gc_subroot_entries_, 0, sizeof(gc_subroot_entries_));
 }
 
-void HeapSnapshot::Delete() {
-  profiler_->RemoveSnapshot(this);
-}
+void HeapSnapshot::Delete() { profiler_->RemoveSnapshot(this); }
 
 void HeapSnapshot::RememberLastJSObjectId() {
   max_snapshot_js_object_id_ = profiler_->heap_object_map()->last_assigned_id();
@@ -462,10 +473,8 @@ void HeapSnapshot::AddLocation(HeapEntry* entry, int scriptId, int line,
   locations_.emplace_back(entry->index(), scriptId, line, col);
 }
 
-HeapEntry* HeapSnapshot::AddEntry(HeapEntry::Type type,
-                                  const char* name,
-                                  SnapshotObjectId id,
-                                  size_t size,
+HeapEntry* HeapSnapshot::AddEntry(HeapEntry::Type type, const char* name,
+                                  SnapshotObjectId id, size_t size,
                                   unsigned trace_node_id) {
   DCHECK(!is_complete());
   entries_.emplace_back(this, static_cast<int>(entries_.size()), type, name, id,
@@ -498,9 +507,7 @@ HeapEntry* HeapSnapshot::GetEntryById(SnapshotObjectId id) {
   return it != entries_by_id_cache_.end() ? it->second : nullptr;
 }
 
-void HeapSnapshot::Print(int max_depth) {
-  root()->Print("", "", max_depth, 0);
-}
+void HeapSnapshot::Print(int max_depth) { root()->Print("", "", max_depth, 0); }
 
 // We split IDs on evens for embedder objects (see
 // HeapObjectsMap::GenerateId) and odds for native objects.
@@ -572,12 +579,10 @@ bool HeapObjectsMap::MoveObject(Address from, Address to, int object_size) {
   return from_value != nullptr;
 }
 
-
 void HeapObjectsMap::UpdateObjectSize(Address addr, int size) {
   FindOrAddEntry(addr, size, MarkEntryAccessed::kNo);
 }
 
-
 SnapshotObjectId HeapObjectsMap::FindEntry(Address addr) {
   base::HashMap::Entry* entry = entries_map_.Lookup(
       reinterpret_cast<void*>(addr), ComputeAddressHash(addr));
@@ -647,7 +652,7 @@ void HeapObjectsMap::UpdateHeapObjectsMap() {
                                   GarbageCollectionReason::kHeapProfiler);
   PtrComprCageBase cage_base(heap_->isolate());
   CombinedHeapObjectIterator iterator(heap_);
-  for (HeapObject obj = iterator.Next(); !obj.is_null();
+  for (Tagged<HeapObject> obj = iterator.Next(); !obj.is_null();
        obj = iterator.Next()) {
     int object_size = obj->Size(cage_base);
     FindOrAddEntry(obj.address(), object_size);
@@ -713,7 +718,6 @@ SnapshotObjectId HeapObjectsMap::PushHeapObjectsStats(OutputStream* stream,
   return last_assigned_id();
 }
 
-
 void HeapObjectsMap::RemoveDeadEntries() {
   DCHECK(entries_.size() > 0 && entries_.at(0).id == 0 &&
          entries_.at(0).addr == kNullAddress);
@@ -778,7 +782,7 @@ HeapEntry* V8HeapExplorer::AllocateEntry(HeapThing ptr) {
   return AddEntry(HeapObject::cast(Object(reinterpret_cast<Address>(ptr))));
 }
 
-HeapEntry* V8HeapExplorer::AllocateEntry(Smi smi) {
+HeapEntry* V8HeapExplorer::AllocateEntry(Tagged<Smi> smi) {
   SnapshotObjectId id = heap_object_map_->get_next_id();
   HeapEntry* entry =
       snapshot_->AddEntry(HeapEntry::kHeapNumber, "smi number", id, 0, 0);
@@ -788,17 +792,19 @@ HeapEntry* V8HeapExplorer::AllocateEntry(Smi smi) {
   return entry;
 }
 
-JSFunction V8HeapExplorer::GetLocationFunction(HeapObject object) {
+Tagged<JSFunction> V8HeapExplorer::GetLocationFunction(
+    Tagged<HeapObject> object) {
   DisallowHeapAllocation no_gc;
 
   if (IsJSFunction(object)) {
     return JSFunction::cast(object);
   } else if (IsJSGeneratorObject(object)) {
-    JSGeneratorObject gen = JSGeneratorObject::cast(object);
+    Tagged<JSGeneratorObject> gen = JSGeneratorObject::cast(object);
     return gen->function();
   } else if (IsJSObject(object)) {
-    JSObject obj = JSObject::cast(object);
-    JSFunction maybe_constructor = GetConstructor(heap_->isolate(), obj);
+    Tagged<JSObject> obj = JSObject::cast(object);
+    Tagged<JSFunction> maybe_constructor =
+        GetConstructor(heap_->isolate(), obj);
 
     return maybe_constructor;
   }
@@ -806,18 +812,19 @@ JSFunction V8HeapExplorer::GetLocationFunction(HeapObject object) {
   return JSFunction();
 }
 
-void V8HeapExplorer::ExtractLocation(HeapEntry* entry, HeapObject object) {
+void V8HeapExplorer::ExtractLocation(HeapEntry* entry,
+                                     Tagged<HeapObject> object) {
   DisallowHeapAllocation no_gc;
-  JSFunction func = GetLocationFunction(object);
+  Tagged<JSFunction> func = GetLocationFunction(object);
   if (!func.is_null()) {
     ExtractLocationForJSFunction(entry, func);
   }
 }
 
 void V8HeapExplorer::ExtractLocationForJSFunction(HeapEntry* entry,
-                                                  JSFunction func) {
+                                                  Tagged<JSFunction> func) {
   if (!IsScript(func->shared()->script())) return;
-  Script script = Script::cast(func->shared()->script());
+  Tagged<Script> script = Script::cast(func->shared()->script());
   int scriptId = script->id();
   int start = func->shared()->StartPosition();
   DCHECK(script->has_line_ends());
@@ -826,13 +833,13 @@ void V8HeapExplorer::ExtractLocationForJSFunction(HeapEntry* entry,
   snapshot_->AddLocation(entry, scriptId, info.line, info.column);
 }
 
-HeapEntry* V8HeapExplorer::AddEntry(HeapObject object) {
+HeapEntry* V8HeapExplorer::AddEntry(Tagged<HeapObject> object) {
   PtrComprCageBase cage_base(isolate());
   InstanceType instance_type = object->map(cage_base)->instance_type();
   if (InstanceTypeChecker::IsJSObject(instance_type)) {
     if (InstanceTypeChecker::IsJSFunction(instance_type)) {
-      JSFunction func = JSFunction::cast(object);
-      SharedFunctionInfo shared = func->shared();
+      Tagged<JSFunction> func = JSFunction::cast(object);
+      Tagged<SharedFunctionInfo> shared = func->shared();
       const char* name = names_->GetName(shared->Name());
       return AddEntry(object, HeapEntry::kClosure, name);
 
@@ -840,7 +847,7 @@ HeapEntry* V8HeapExplorer::AddEntry(HeapObject object) {
       return AddEntry(object, HeapEntry::kClosure, "native_bind");
     }
     if (InstanceTypeChecker::IsJSRegExp(instance_type)) {
-      JSRegExp re = JSRegExp::cast(object);
+      Tagged<JSRegExp> re = JSRegExp::cast(object);
       return AddEntry(object, HeapEntry::kRegExp,
                       names_->GetName(re->source()));
     }
@@ -857,7 +864,7 @@ HeapEntry* V8HeapExplorer::AddEntry(HeapObject object) {
     return AddEntry(object, HeapEntry::kObject, name);
 
   } else if (InstanceTypeChecker::IsString(instance_type)) {
-    String string = String::cast(object);
+    Tagged<String> string = String::cast(object);
     if (IsConsString(string, cage_base)) {
       return AddEntry(object, HeapEntry::kConsString, "(concatenated string)");
     } else if (IsSlicedString(string, cage_base)) {
@@ -880,11 +887,11 @@ HeapEntry* V8HeapExplorer::AddEntry(HeapObject object) {
     return AddEntry(object, HeapEntry::kCode, "");
 
   } else if (InstanceTypeChecker::IsSharedFunctionInfo(instance_type)) {
-    String name = SharedFunctionInfo::cast(object)->Name();
+    Tagged<String> name = SharedFunctionInfo::cast(object)->Name();
     return AddEntry(object, HeapEntry::kCode, names_->GetName(name));
 
   } else if (InstanceTypeChecker::IsScript(instance_type)) {
-    Object name = Script::cast(object)->name();
+    Tagged<Object> name = Script::cast(object)->name();
     return AddEntry(object, HeapEntry::kCode,
                     IsString(name) ? names_->GetName(String::cast(name)) : "");
 
@@ -899,7 +906,7 @@ HeapEntry* V8HeapExplorer::AddEntry(HeapObject object) {
   }
 #if V8_ENABLE_WEBASSEMBLY
   if (InstanceTypeChecker::IsWasmObject(instance_type)) {
-    WasmTypeInfo info = object->map()->wasm_type_info();
+    Tagged<WasmTypeInfo> info = object->map()->wasm_type_info();
     // The cast is safe; structs and arrays always have their instance defined.
     wasm::NamesProvider* names = WasmInstanceObject::cast(info->instance())
                                      ->module_object()
@@ -916,8 +923,8 @@ HeapEntry* V8HeapExplorer::AddEntry(HeapObject object) {
                   GetSystemEntryName(object));
 }
 
-HeapEntry* V8HeapExplorer::AddEntry(HeapObject object, HeapEntry::Type type,
-                                    const char* name) {
+HeapEntry* V8HeapExplorer::AddEntry(Tagged<HeapObject> object,
+                                    HeapEntry::Type type, const char* name) {
   if (v8_flags.heap_profiler_show_hidden_objects &&
       type == HeapEntry::kHidden) {
     type = HeapEntry::kNative;
@@ -926,29 +933,29 @@ HeapEntry* V8HeapExplorer::AddEntry(HeapObject object, HeapEntry::Type type,
   return AddEntry(object.address(), type, name, object->Size(cage_base));
 }
 
-HeapEntry* V8HeapExplorer::AddEntry(Address address,
-                                    HeapEntry::Type type,
-                                    const char* name,
-                                    size_t size) {
+HeapEntry* V8HeapExplorer::AddEntry(Address address, HeapEntry::Type type,
+                                    const char* name, size_t size) {
   SnapshotObjectId object_id = heap_object_map_->FindOrAddEntry(
       address, static_cast<unsigned int>(size));
   unsigned trace_node_id = 0;
   if (AllocationTracker* allocation_tracker =
-      snapshot_->profiler()->allocation_tracker()) {
+          snapshot_->profiler()->allocation_tracker()) {
     trace_node_id =
         allocation_tracker->address_to_trace()->GetTraceNodeId(address);
   }
   return snapshot_->AddEntry(type, name, object_id, size, trace_node_id);
 }
 
-const char* V8HeapExplorer::GetSystemEntryName(HeapObject object) {
+const char* V8HeapExplorer::GetSystemEntryName(Tagged<HeapObject> object) {
   if (IsMap(object)) {
     switch (Map::cast(object)->instance_type()) {
 #define MAKE_STRING_MAP_CASE(instance_type, size, name, Name) \
-        case instance_type: return "system / Map (" #Name ")";
+  case instance_type:                                         \
+    return "system / Map (" #Name ")";
       STRING_TYPE_LIST(MAKE_STRING_MAP_CASE)
 #undef MAKE_STRING_MAP_CASE
-        default: return "system / Map";
+      default:
+        return "system / Map";
     }
   }
 
@@ -984,7 +991,7 @@ const char* V8HeapExplorer::GetSystemEntryName(HeapObject object) {
   }
 }
 
-HeapEntry::Type V8HeapExplorer::GetSystemEntryType(HeapObject object) {
+HeapEntry::Type V8HeapExplorer::GetSystemEntryType(Tagged<HeapObject> object) {
   InstanceType type = object->map()->instance_type();
   if (InstanceTypeChecker::IsAllocationSite(type) ||
       InstanceTypeChecker::IsArrayBoilerplateDescription(type) ||
@@ -1036,11 +1043,11 @@ void V8HeapExplorer::PopulateLineEnds() {
 
   {
     Script::Iterator iterator(isolate());
-    for (Script script = iterator.Next(); !script.is_null();
+    for (Tagged<Script> script = iterator.Next(); !script.is_null();
          script = iterator.Next()) {
-        if (!script->has_line_ends()) {
+      if (!script->has_line_ends()) {
         scripts.push_back(handle(script, isolate()));
-        }
+      }
     }
   }
 
@@ -1082,8 +1089,8 @@ int AdjustEmbedderFieldIndex(HeapObject heap_obj, int field_index) {
 #endif  // V8_TARGET_BIG_ENDIAN
 class IndexedReferencesExtractor : public ObjectVisitorWithCageBases {
  public:
-  IndexedReferencesExtractor(V8HeapExplorer* generator, HeapObject parent_obj,
-                             HeapEntry* parent)
+  IndexedReferencesExtractor(V8HeapExplorer* generator,
+                             Tagged<HeapObject> parent_obj, HeapEntry* parent)
       : ObjectVisitorWithCageBases(generator->isolate()),
         generator_(generator),
         parent_obj_(parent_obj),
@@ -1092,14 +1099,14 @@ class IndexedReferencesExtractor : public ObjectVisitorWithCageBases {
             parent_obj_->RawMaybeWeakField(parent_obj_->Size(cage_base()))),
         parent_(parent),
         next_index_(0) {}
-  void VisitPointers(HeapObject host, ObjectSlot start,
+  void VisitPointers(Tagged<HeapObject> host, ObjectSlot start,
                      ObjectSlot end) override {
     VisitPointers(host, MaybeObjectSlot(start), MaybeObjectSlot(end));
   }
-  void VisitMapPointer(HeapObject object) override {
+  void VisitMapPointer(Tagged<HeapObject> object) override {
     VisitSlotImpl(cage_base(), object->map_slot());
   }
-  void VisitPointers(HeapObject host, MaybeObjectSlot start,
+  void VisitPointers(Tagged<HeapObject> host, MaybeObjectSlot start,
                      MaybeObjectSlot end) override {
     // [start,end) must be a sub-region of [parent_start_, parent_end), i.e.
     // all the slots must point inside the object.
@@ -1110,20 +1117,22 @@ class IndexedReferencesExtractor : public ObjectVisitorWithCageBases {
     }
   }
 
-  void VisitInstructionStreamPointer(Code host,
+  void VisitInstructionStreamPointer(Tagged<Code> host,
                                      InstructionStreamSlot slot) override {
     VisitSlotImpl(code_cage_base(), slot);
   }
 
-  void VisitCodeTarget(InstructionStream host, RelocInfo* rinfo) override {
-    InstructionStream target =
+  void VisitCodeTarget(Tagged<InstructionStream> host,
+                       RelocInfo* rinfo) override {
+    Tagged<InstructionStream> target =
         InstructionStream::FromTargetAddress(rinfo->target_address());
     VisitHeapObjectImpl(target, -1);
   }
 
-  void VisitEmbeddedPointer(InstructionStream host, RelocInfo* rinfo) override {
-    HeapObject object = rinfo->target_object(cage_base());
-    Code code = Code::unchecked_cast(host->raw_code(kAcquireLoad));
+  void VisitEmbeddedPointer(Tagged<InstructionStream> host,
+                            RelocInfo* rinfo) override {
+    Tagged<HeapObject> object = rinfo->target_object(cage_base());
+    Tagged<Code> code = Code::unchecked_cast(host->raw_code(kAcquireLoad));
     if (code->IsWeakObject(object)) {
       generator_->SetWeakReference(parent_, next_index_++, object, {});
     } else {
@@ -1131,7 +1140,7 @@ class IndexedReferencesExtractor : public ObjectVisitorWithCageBases {
     }
   }
 
-  void VisitIndirectPointer(HeapObject host, IndirectPointerSlot slot,
+  void VisitIndirectPointer(Tagged<HeapObject> host, IndirectPointerSlot slot,
                             IndirectPointerMode mode) override {
     // The JSFunction::Code field is handled separately in
     // ExtractJSObjectReferences but here we have to mark it as visited.
@@ -1154,7 +1163,7 @@ class IndexedReferencesExtractor : public ObjectVisitorWithCageBases {
     if (generator_->visited_fields_[field_index]) {
       generator_->visited_fields_[field_index] = false;
     } else {
-      HeapObject heap_object;
+      Tagged<HeapObject> heap_object;
       auto loaded_value = slot.load(cage_base);
       if (loaded_value.GetHeapObjectIfStrong(&heap_object)) {
         VisitHeapObjectImpl(heap_object, field_index);
@@ -1164,7 +1173,8 @@ class IndexedReferencesExtractor : public ObjectVisitorWithCageBases {
     }
   }
 
-  V8_INLINE void VisitHeapObjectImpl(HeapObject heap_object, int field_index) {
+  V8_INLINE void VisitHeapObjectImpl(Tagged<HeapObject> heap_object,
+                                     int field_index) {
     DCHECK_LE(-1, field_index);
     // The last parameter {field_offset} is only used to check some well-known
     // skipped references, so passing -1 * kTaggedSize for objects embedded
@@ -1181,7 +1191,8 @@ class IndexedReferencesExtractor : public ObjectVisitorWithCageBases {
   int next_index_;
 };
 
-void V8HeapExplorer::ExtractReferences(HeapEntry* entry, HeapObject obj) {
+void V8HeapExplorer::ExtractReferences(HeapEntry* entry,
+                                       Tagged<HeapObject> obj) {
   if (IsJSGlobalProxy(obj)) {
     ExtractJSGlobalProxyReferences(entry, JSGlobalProxy::cast(obj));
   } else if (IsJSArrayBuffer(obj)) {
@@ -1280,15 +1291,15 @@ void V8HeapExplorer::ExtractReferences(HeapEntry* entry, HeapObject obj) {
   }
 }
 
-void V8HeapExplorer::ExtractJSGlobalProxyReferences(HeapEntry* entry,
-                                                    JSGlobalProxy proxy) {
+void V8HeapExplorer::ExtractJSGlobalProxyReferences(
+    HeapEntry* entry, Tagged<JSGlobalProxy> proxy) {
   SetInternalReference(entry, "native_context", proxy->native_context(),
                        JSGlobalProxy::kNativeContextOffset);
 }
 
 void V8HeapExplorer::ExtractJSObjectReferences(HeapEntry* entry,
-                                               JSObject js_obj) {
-  HeapObject obj = js_obj;
+                                               Tagged<JSObject> js_obj) {
+  Tagged<HeapObject> obj = js_obj;
   ExtractPropertyReferences(js_obj, entry);
   ExtractElementReferences(js_obj, entry);
   ExtractInternalReferences(js_obj, entry);
@@ -1297,7 +1308,7 @@ void V8HeapExplorer::ExtractJSObjectReferences(HeapEntry* entry,
   ReadOnlyRoots roots(isolate);
   SetPropertyReference(entry, roots.proto_string(), iter.GetCurrent());
   if (IsJSBoundFunction(obj)) {
-    JSBoundFunction js_fun = JSBoundFunction::cast(obj);
+    Tagged<JSBoundFunction> js_fun = JSBoundFunction::cast(obj);
     TagObject(js_fun->bound_arguments(), "(bound arguments)");
     SetInternalReference(entry, "bindings", js_fun->bound_arguments(),
                          JSBoundFunction::kBoundArgumentsOffset);
@@ -1306,15 +1317,16 @@ void V8HeapExplorer::ExtractJSObjectReferences(HeapEntry* entry,
     SetInternalReference(entry, "bound_function",
                          js_fun->bound_target_function(),
                          JSBoundFunction::kBoundTargetFunctionOffset);
-    FixedArray bindings = js_fun->bound_arguments();
+    Tagged<FixedArray> bindings = js_fun->bound_arguments();
     for (int i = 0; i < bindings->length(); i++) {
       const char* reference_name = names_->GetFormatted("bound_argument_%d", i);
       SetNativeBindReference(entry, reference_name, bindings->get(i));
     }
   } else if (IsJSFunction(obj)) {
-    JSFunction js_fun = JSFunction::cast(js_obj);
+    Tagged<JSFunction> js_fun = JSFunction::cast(js_obj);
     if (js_fun->has_prototype_slot()) {
-      Object proto_or_map = js_fun->prototype_or_initial_map(kAcquireLoad);
+      Tagged<Object> proto_or_map =
+          js_fun->prototype_or_initial_map(kAcquireLoad);
       if (!IsTheHole(proto_or_map, isolate)) {
         if (!IsMap(proto_or_map)) {
           SetPropertyReference(entry, roots.prototype_string(), proto_or_map,
@@ -1328,7 +1340,7 @@ void V8HeapExplorer::ExtractJSObjectReferences(HeapEntry* entry,
         }
       }
     }
-    SharedFunctionInfo shared_info = js_fun->shared();
+    Tagged<SharedFunctionInfo> shared_info = js_fun->shared();
     TagObject(js_fun->raw_feedback_cell(), "(function feedback cell)");
     SetInternalReference(entry, "feedback_cell", js_fun->raw_feedback_cell(),
                          JSFunction::kFeedbackCellOffset);
@@ -1341,7 +1353,7 @@ void V8HeapExplorer::ExtractJSObjectReferences(HeapEntry* entry,
     SetInternalReference(entry, "code", js_fun->code(),
                          JSFunction::kCodeOffset);
   } else if (IsJSGlobalObject(obj)) {
-    JSGlobalObject global_obj = JSGlobalObject::cast(obj);
+    Tagged<JSGlobalObject> global_obj = JSGlobalObject::cast(obj);
     SetInternalReference(entry, "native_context", global_obj->native_context(),
                          JSGlobalObject::kNativeContextOffset);
     SetInternalReference(entry, "global_proxy", global_obj->global_proxy(),
@@ -1349,7 +1361,7 @@ void V8HeapExplorer::ExtractJSObjectReferences(HeapEntry* entry,
     static_assert(JSGlobalObject::kHeaderSize - JSObject::kHeaderSize ==
                   2 * kTaggedSize);
   } else if (IsJSArrayBufferView(obj)) {
-    JSArrayBufferView view = JSArrayBufferView::cast(obj);
+    Tagged<JSArrayBufferView> view = JSArrayBufferView::cast(obj);
     SetInternalReference(entry, "buffer", view->buffer(),
                          JSArrayBufferView::kBufferOffset);
   }
@@ -1363,51 +1375,54 @@ void V8HeapExplorer::ExtractJSObjectReferences(HeapEntry* entry,
                        JSObject::kElementsOffset);
 }
 
-void V8HeapExplorer::ExtractStringReferences(HeapEntry* entry, String string) {
+void V8HeapExplorer::ExtractStringReferences(HeapEntry* entry,
+                                             Tagged<String> string) {
   if (IsConsString(string)) {
-    ConsString cs = ConsString::cast(string);
+    Tagged<ConsString> cs = ConsString::cast(string);
     SetInternalReference(entry, "first", cs->first(), ConsString::kFirstOffset);
     SetInternalReference(entry, "second", cs->second(),
                          ConsString::kSecondOffset);
   } else if (IsSlicedString(string)) {
-    SlicedString ss = SlicedString::cast(string);
+    Tagged<SlicedString> ss = SlicedString::cast(string);
     SetInternalReference(entry, "parent", ss->parent(),
                          SlicedString::kParentOffset);
   } else if (IsThinString(string)) {
-    ThinString ts = ThinString::cast(string);
+    Tagged<ThinString> ts = ThinString::cast(string);
     SetInternalReference(entry, "actual", ts->actual(),
                          ThinString::kActualOffset);
   }
 }
 
-void V8HeapExplorer::ExtractSymbolReferences(HeapEntry* entry, Symbol symbol) {
+void V8HeapExplorer::ExtractSymbolReferences(HeapEntry* entry,
+                                             Tagged<Symbol> symbol) {
   SetInternalReference(entry, "name", symbol->description(),
                        Symbol::kDescriptionOffset);
 }
 
-void V8HeapExplorer::ExtractJSCollectionReferences(HeapEntry* entry,
-                                                   JSCollection collection) {
+void V8HeapExplorer::ExtractJSCollectionReferences(
+    HeapEntry* entry, Tagged<JSCollection> collection) {
   SetInternalReference(entry, "table", collection->table(),
                        JSCollection::kTableOffset);
 }
 
-void V8HeapExplorer::ExtractJSWeakCollectionReferences(HeapEntry* entry,
-                                                       JSWeakCollection obj) {
+void V8HeapExplorer::ExtractJSWeakCollectionReferences(
+    HeapEntry* entry, Tagged<JSWeakCollection> obj) {
   SetInternalReference(entry, "table", obj->table(),
                        JSWeakCollection::kTableOffset);
 }
 
 void V8HeapExplorer::ExtractEphemeronHashTableReferences(
-    HeapEntry* entry, EphemeronHashTable table) {
+    HeapEntry* entry, Tagged<EphemeronHashTable> table) {
   for (InternalIndex i : table->IterateEntries()) {
     int key_index = EphemeronHashTable::EntryToIndex(i) +
                     EphemeronHashTable::kEntryKeyIndex;
     int value_index = EphemeronHashTable::EntryToValueIndex(i);
-    Object key = table->get(key_index);
-    Object value = table->get(value_index);
-    SetWeakReference(entry, key_index, key, table.OffsetOfElementAt(key_index));
+    Tagged<Object> key = table->get(key_index);
+    Tagged<Object> value = table->get(value_index);
+    SetWeakReference(entry, key_index, key,
+                     table->OffsetOfElementAt(key_index));
     SetWeakReference(entry, value_index, value,
-                     table.OffsetOfElementAt(value_index));
+                     table->OffsetOfElementAt(value_index));
     HeapEntry* key_entry = GetEntry(key);
     HeapEntry* value_entry = GetEntry(value);
     HeapEntry* table_entry = GetEntry(table);
@@ -1439,18 +1454,18 @@ static const struct {
 };
 
 void V8HeapExplorer::ExtractContextReferences(HeapEntry* entry,
-                                              Context context) {
+                                              Tagged<Context> context) {
   DisallowGarbageCollection no_gc;
   if (!IsNativeContext(context) && context->is_declaration_context()) {
-    ScopeInfo scope_info = context->scope_info();
+    Tagged<ScopeInfo> scope_info = context->scope_info();
     // Add context allocated locals.
-    for (auto it : ScopeInfo::IterateLocalNames(&scope_info, no_gc)) {
+    for (auto it : ScopeInfo::IterateLocalNames(scope_info, no_gc)) {
       int idx = scope_info->ContextHeaderLength() + it->index();
       SetContextReference(entry, it->name(), context->get(idx),
                           Context::OffsetOfElementAt(idx));
     }
     if (scope_info->HasContextAllocatedFunctionName()) {
-      String name = String::cast(scope_info->FunctionName());
+      Tagged<String> name = String::cast(scope_info->FunctionName());
       int idx = scope_info->FunctionContextSlotIndex(name);
       if (idx >= 0) {
         SetContextReference(entry, name, context->get(idx),
@@ -1486,9 +1501,9 @@ void V8HeapExplorer::ExtractContextReferences(HeapEntry* entry,
   }
 }
 
-void V8HeapExplorer::ExtractMapReferences(HeapEntry* entry, Map map) {
+void V8HeapExplorer::ExtractMapReferences(HeapEntry* entry, Tagged<Map> map) {
   MaybeObject maybe_raw_transitions_or_prototype_info = map->raw_transitions();
-  HeapObject raw_transitions_or_prototype_info;
+  Tagged<HeapObject> raw_transitions_or_prototype_info;
   if (maybe_raw_transitions_or_prototype_info->GetHeapObjectIfWeak(
           &raw_transitions_or_prototype_info)) {
     DCHECK(IsMap(raw_transitions_or_prototype_info));
@@ -1497,7 +1512,7 @@ void V8HeapExplorer::ExtractMapReferences(HeapEntry* entry, Map map) {
   } else if (maybe_raw_transitions_or_prototype_info->GetHeapObjectIfStrong(
                  &raw_transitions_or_prototype_info)) {
     if (IsTransitionArray(raw_transitions_or_prototype_info)) {
-      TransitionArray transitions =
+      Tagged<TransitionArray> transitions =
           TransitionArray::cast(raw_transitions_or_prototype_info);
       if (map->CanTransition() && transitions->HasPrototypeTransitions()) {
         TagObject(transitions->GetPrototypeTransitions(),
@@ -1518,19 +1533,20 @@ void V8HeapExplorer::ExtractMapReferences(HeapEntry* entry, Map map) {
                            Map::kTransitionsOrPrototypeInfoOffset);
     }
   }
-  DescriptorArray descriptors = map->instance_descriptors();
+  Tagged<DescriptorArray> descriptors = map->instance_descriptors();
   TagObject(descriptors, "(map descriptors)");
   SetInternalReference(entry, "descriptors", descriptors,
                        Map::kInstanceDescriptorsOffset);
   SetInternalReference(entry, "prototype", map->prototype(),
                        Map::kPrototypeOffset);
   if (IsContextMap(map)) {
-    Object native_context = map->native_context();
+    Tagged<Object> native_context = map->native_context();
     TagObject(native_context, "(native context)");
     SetInternalReference(entry, "native_context", native_context,
                          Map::kConstructorOrBackPointerOrNativeContextOffset);
   } else {
-    Object constructor_or_back_pointer = map->constructor_or_back_pointer();
+    Tagged<Object> constructor_or_back_pointer =
+        map->constructor_or_back_pointer();
     if (IsMap(constructor_or_back_pointer)) {
       TagObject(constructor_or_back_pointer, "(back pointer)");
       SetInternalReference(entry, "back_pointer", constructor_or_back_pointer,
@@ -1553,11 +1569,11 @@ void V8HeapExplorer::ExtractMapReferences(HeapEntry* entry, Map map) {
 }
 
 void V8HeapExplorer::ExtractSharedFunctionInfoReferences(
-    HeapEntry* entry, SharedFunctionInfo shared) {
+    HeapEntry* entry, Tagged<SharedFunctionInfo> shared) {
   TagObject(shared, "(shared function info)");
   {
     std::unique_ptr<char[]> name = shared->DebugNameCStr();
-    Code code = shared->GetCode(isolate());
+    Tagged<Code> code = shared->GetCode(isolate());
     TagObject(code, name[0] != '\0'
                         ? names_->GetFormatted("(code for %s)", name.get())
                         : names_->GetFormatted("(%s code)",
@@ -1572,7 +1588,7 @@ void V8HeapExplorer::ExtractSharedFunctionInfoReferences(
     }
   }
 
-  Object name_or_scope_info = shared->name_or_scope_info(kAcquireLoad);
+  Tagged<Object> name_or_scope_info = shared->name_or_scope_info(kAcquireLoad);
   if (IsScopeInfo(name_or_scope_info)) {
     TagObject(name_or_scope_info, "(function scope info)");
   }
@@ -1589,7 +1605,8 @@ void V8HeapExplorer::ExtractSharedFunctionInfoReferences(
       SharedFunctionInfo::kOuterScopeInfoOrFeedbackMetadataOffset);
 }
 
-void V8HeapExplorer::ExtractScriptReferences(HeapEntry* entry, Script script) {
+void V8HeapExplorer::ExtractScriptReferences(HeapEntry* entry,
+                                             Tagged<Script> script) {
   SetInternalReference(entry, "source", script->source(),
                        Script::kSourceOffset);
   SetInternalReference(entry, "name", script->name(), Script::kNameOffset);
@@ -1604,16 +1621,16 @@ void V8HeapExplorer::ExtractScriptReferences(HeapEntry* entry, Script script) {
             HeapEntry::kCode);
 }
 
-void V8HeapExplorer::ExtractAccessorInfoReferences(HeapEntry* entry,
-                                                   AccessorInfo accessor_info) {
+void V8HeapExplorer::ExtractAccessorInfoReferences(
+    HeapEntry* entry, Tagged<AccessorInfo> accessor_info) {
   SetInternalReference(entry, "name", accessor_info->name(),
                        AccessorInfo::kNameOffset);
   SetInternalReference(entry, "data", accessor_info->data(),
                        AccessorInfo::kDataOffset);
 }
 
-void V8HeapExplorer::ExtractAccessorPairReferences(HeapEntry* entry,
-                                                   AccessorPair accessors) {
+void V8HeapExplorer::ExtractAccessorPairReferences(
+    HeapEntry* entry, Tagged<AccessorPair> accessors) {
   SetInternalReference(entry, "getter", accessors->getter(),
                        AccessorPair::kGetterOffset);
   SetInternalReference(entry, "setter", accessors->setter(),
@@ -1621,20 +1638,20 @@ void V8HeapExplorer::ExtractAccessorPairReferences(HeapEntry* entry,
 }
 
 void V8HeapExplorer::ExtractJSWeakRefReferences(HeapEntry* entry,
-                                                JSWeakRef js_weak_ref) {
+                                                Tagged<JSWeakRef> js_weak_ref) {
   SetWeakReference(entry, "target", js_weak_ref->target(),
                    JSWeakRef::kTargetOffset);
 }
 
 void V8HeapExplorer::ExtractWeakCellReferences(HeapEntry* entry,
-                                               WeakCell weak_cell) {
+                                               Tagged<WeakCell> weak_cell) {
   SetWeakReference(entry, "target", weak_cell->target(),
                    WeakCell::kTargetOffset);
   SetWeakReference(entry, "unregister_token", weak_cell->unregister_token(),
                    WeakCell::kUnregisterTokenOffset);
 }
 
-void V8HeapExplorer::TagBuiltinCodeObject(Code code, const char* name) {
+void V8HeapExplorer::TagBuiltinCodeObject(Tagged<Code> code, const char* name) {
   TagObject(code, names_->GetFormatted("(%s builtin code)", name));
   if (code->has_instruction_stream()) {
     TagObject(code->instruction_stream(),
@@ -1642,7 +1659,8 @@ void V8HeapExplorer::TagBuiltinCodeObject(Code code, const char* name) {
   }
 }
 
-void V8HeapExplorer::ExtractCodeReferences(HeapEntry* entry, Code code) {
+void V8HeapExplorer::ExtractCodeReferences(HeapEntry* entry,
+                                           Tagged<Code> code) {
   if (!code->has_instruction_stream()) return;
 
   SetInternalReference(entry, "instruction_stream", code->instruction_stream(),
@@ -1659,7 +1677,7 @@ void V8HeapExplorer::ExtractCodeReferences(HeapEntry* entry, Code code) {
                          code->bytecode_offset_table(),
                          Code::kPositionTableOffset);
   } else {
-    DeoptimizationData deoptimization_data =
+    Tagged<DeoptimizationData> deoptimization_data =
         DeoptimizationData::cast(code->deoptimization_data());
     TagObject(deoptimization_data, "(code deopt data)", HeapEntry::kCode);
     SetInternalReference(entry, "deoptimization_data", deoptimization_data,
@@ -1681,8 +1699,8 @@ void V8HeapExplorer::ExtractCodeReferences(HeapEntry* entry, Code code) {
 }
 
 void V8HeapExplorer::ExtractInstructionStreamReferences(
-    HeapEntry* entry, InstructionStream istream) {
-  Code code;
+    HeapEntry* entry, Tagged<InstructionStream> istream) {
+  Tagged<Code> code;
   if (!istream->TryGetCode(&code, kAcquireLoad))
     return;  // Not yet initialized.
   TagObject(code, "(code)", HeapEntry::kCode);
@@ -1694,19 +1712,20 @@ void V8HeapExplorer::ExtractInstructionStreamReferences(
                        InstructionStream::kRelocationInfoOffset);
 }
 
-void V8HeapExplorer::ExtractCellReferences(HeapEntry* entry, Cell cell) {
+void V8HeapExplorer::ExtractCellReferences(HeapEntry* entry,
+                                           Tagged<Cell> cell) {
   SetInternalReference(entry, "value", cell->value(), Cell::kValueOffset);
 }
 
-void V8HeapExplorer::ExtractFeedbackCellReferences(HeapEntry* entry,
-                                                   FeedbackCell feedback_cell) {
+void V8HeapExplorer::ExtractFeedbackCellReferences(
+    HeapEntry* entry, Tagged<FeedbackCell> feedback_cell) {
   TagObject(feedback_cell, "(feedback cell)");
   SetInternalReference(entry, "value", feedback_cell->value(),
                        FeedbackCell::kValueOffset);
 }
 
 void V8HeapExplorer::ExtractPropertyCellReferences(HeapEntry* entry,
-                                                   PropertyCell cell) {
+                                                   Tagged<PropertyCell> cell) {
   SetInternalReference(entry, "value", cell->value(),
                        PropertyCell::kValueOffset);
   TagObject(cell->dependent_code(), "(dependent code)");
@@ -1714,16 +1733,16 @@ void V8HeapExplorer::ExtractPropertyCellReferences(HeapEntry* entry,
                        PropertyCell::kDependentCodeOffset);
 }
 
-void V8HeapExplorer::ExtractPrototypeInfoReferences(HeapEntry* entry,
-                                                    PrototypeInfo info) {
+void V8HeapExplorer::ExtractPrototypeInfoReferences(
+    HeapEntry* entry, Tagged<PrototypeInfo> info) {
   TagObject(info->prototype_chain_enum_cache(), "(prototype chain enum cache)",
             HeapEntry::kObjectShape);
   TagObject(info->prototype_users(), "(prototype users)",
             HeapEntry::kObjectShape);
 }
 
-void V8HeapExplorer::ExtractAllocationSiteReferences(HeapEntry* entry,
-                                                     AllocationSite site) {
+void V8HeapExplorer::ExtractAllocationSiteReferences(
+    HeapEntry* entry, Tagged<AllocationSite> site) {
   SetInternalReference(entry, "transition_info",
                        site->transition_info_or_boilerplate(),
                        AllocationSite::kTransitionInfoOrBoilerplateOffset);
@@ -1735,30 +1754,28 @@ void V8HeapExplorer::ExtractAllocationSiteReferences(HeapEntry* entry,
 }
 
 void V8HeapExplorer::ExtractArrayBoilerplateDescriptionReferences(
-    HeapEntry* entry, ArrayBoilerplateDescription value) {
-  FixedArrayBase constant_elements = value->constant_elements();
+    HeapEntry* entry, Tagged<ArrayBoilerplateDescription> value) {
+  Tagged<FixedArrayBase> constant_elements = value->constant_elements();
   SetInternalReference(entry, "constant_elements", constant_elements,
                        ArrayBoilerplateDescription::kConstantElementsOffset);
   TagObject(constant_elements, "(constant elements)", HeapEntry::kCode);
 }
 
 void V8HeapExplorer::ExtractRegExpBoilerplateDescriptionReferences(
-    HeapEntry* entry, RegExpBoilerplateDescription value) {
+    HeapEntry* entry, Tagged<RegExpBoilerplateDescription> value) {
   TagObject(value->data(), "(RegExp data)", HeapEntry::kCode);
 }
 
 class JSArrayBufferDataEntryAllocator : public HeapEntriesAllocator {
  public:
   JSArrayBufferDataEntryAllocator(size_t size, V8HeapExplorer* explorer)
-      : size_(size)
-      , explorer_(explorer) {
-  }
+      : size_(size), explorer_(explorer) {}
   HeapEntry* AllocateEntry(HeapThing ptr) override {
     return explorer_->AddEntry(reinterpret_cast<Address>(ptr),
                                HeapEntry::kNative, "system / JSArrayBufferData",
                                size_);
   }
-  HeapEntry* AllocateEntry(Smi smi) override {
+  HeapEntry* AllocateEntry(Tagged<Smi> smi) override {
     DCHECK(false);
     return nullptr;
   }
@@ -1768,8 +1785,8 @@ class JSArrayBufferDataEntryAllocator : public HeapEntriesAllocator {
   V8HeapExplorer* explorer_;
 };
 
-void V8HeapExplorer::ExtractJSArrayBufferReferences(HeapEntry* entry,
-                                                    JSArrayBuffer buffer) {
+void V8HeapExplorer::ExtractJSArrayBufferReferences(
+    HeapEntry* entry, Tagged<JSArrayBuffer> buffer) {
   // Setup a reference to a native memory backing_store object.
   if (!buffer->backing_store()) return;
   size_t data_size = buffer->byte_length();
@@ -1781,14 +1798,14 @@ void V8HeapExplorer::ExtractJSArrayBufferReferences(HeapEntry* entry,
 }
 
 void V8HeapExplorer::ExtractJSPromiseReferences(HeapEntry* entry,
-                                                JSPromise promise) {
+                                                Tagged<JSPromise> promise) {
   SetInternalReference(entry, "reactions_or_result",
                        promise->reactions_or_result(),
                        JSPromise::kReactionsOrResultOffset);
 }
 
 void V8HeapExplorer::ExtractJSGeneratorObjectReferences(
-    HeapEntry* entry, JSGeneratorObject generator) {
+    HeapEntry* entry, Tagged<JSGeneratorObject> generator) {
   SetInternalReference(entry, "function", generator->function(),
                        JSGeneratorObject::kFunctionOffset);
   SetInternalReference(entry, "context", generator->context(),
@@ -1801,14 +1818,15 @@ void V8HeapExplorer::ExtractJSGeneratorObjectReferences(
 }
 
 void V8HeapExplorer::ExtractFixedArrayReferences(HeapEntry* entry,
-                                                 FixedArray array) {
+                                                 Tagged<FixedArray> array) {
   for (int i = 0, l = array->length(); i < l; ++i) {
     DCHECK(!HasWeakHeapObjectTag(array->get(i)));
-    SetInternalReference(entry, i, array->get(i), array.OffsetOfElementAt(i));
+    SetInternalReference(entry, i, array->get(i), array->OffsetOfElementAt(i));
   }
 }
 
-void V8HeapExplorer::ExtractNumberReference(HeapEntry* entry, Object number) {
+void V8HeapExplorer::ExtractNumberReference(HeapEntry* entry,
+                                            Tagged<Object> number) {
   DCHECK(IsNumber(number));
 
   // Must be large enough to fit any double, int, or size_t.
@@ -1833,8 +1851,8 @@ void V8HeapExplorer::ExtractNumberReference(HeapEntry* entry, Object number) {
                            generator_);
 }
 
-void V8HeapExplorer::ExtractBytecodeArrayReferences(HeapEntry* entry,
-                                                    BytecodeArray bytecode) {
+void V8HeapExplorer::ExtractBytecodeArrayReferences(
+    HeapEntry* entry, Tagged<BytecodeArray> bytecode) {
   RecursivelyTagConstantPool(bytecode->constant_pool(), "(constant pool)",
                              HeapEntry::kCode, 3);
   TagObject(bytecode->handler_table(), "(handler table)", HeapEntry::kCode);
@@ -1843,7 +1861,7 @@ void V8HeapExplorer::ExtractBytecodeArrayReferences(HeapEntry* entry,
 }
 
 void V8HeapExplorer::ExtractScopeInfoReferences(HeapEntry* entry,
-                                                ScopeInfo info) {
+                                                Tagged<ScopeInfo> info) {
   if (!info->HasInlinedLocalNames()) {
     TagObject(info->context_local_names_hashtable(), "(context local names)",
               HeapEntry::kCode);
@@ -1851,16 +1869,16 @@ void V8HeapExplorer::ExtractScopeInfoReferences(HeapEntry* entry,
 }
 
 void V8HeapExplorer::ExtractFeedbackVectorReferences(
-    HeapEntry* entry, FeedbackVector feedback_vector) {
+    HeapEntry* entry, Tagged<FeedbackVector> feedback_vector) {
   MaybeObject code = feedback_vector->maybe_optimized_code();
-  HeapObject code_heap_object;
+  Tagged<HeapObject> code_heap_object;
   if (code->GetHeapObjectIfWeak(&code_heap_object)) {
     SetWeakReference(entry, "optimized code", code_heap_object,
                      FeedbackVector::kMaybeOptimizedCodeOffset);
   }
   for (int i = 0; i < feedback_vector->length(); ++i) {
     MaybeObject maybe_entry = *(feedback_vector->slots_start() + i);
-    HeapObject entry;
+    Tagged<HeapObject> entry;
     if (maybe_entry.GetHeapObjectIfStrong(&entry) &&
         (entry->map(isolate())->instance_type() == WEAK_FIXED_ARRAY_TYPE ||
          IsFixedArrayExact(entry))) {
@@ -1869,8 +1887,8 @@ void V8HeapExplorer::ExtractFeedbackVectorReferences(
   }
 }
 
-void V8HeapExplorer::ExtractDescriptorArrayReferences(HeapEntry* entry,
-                                                      DescriptorArray array) {
+void V8HeapExplorer::ExtractDescriptorArrayReferences(
+    HeapEntry* entry, Tagged<DescriptorArray> array) {
   SetInternalReference(entry, "enum_cache", array->enum_cache(),
                        DescriptorArray::kEnumCacheOffset);
   MaybeObjectSlot start = MaybeObjectSlot(array->GetDescriptorSlot(0));
@@ -1880,7 +1898,7 @@ void V8HeapExplorer::ExtractDescriptorArrayReferences(HeapEntry* entry,
     MaybeObjectSlot slot = start + i;
     int offset = static_cast<int>(slot.address() - array.address());
     MaybeObject object = *slot;
-    HeapObject heap_object;
+    Tagged<HeapObject> heap_object;
     if (object->GetHeapObjectIfWeak(&heap_object)) {
       SetWeakReference(entry, i, heap_object, offset);
     } else if (object->GetHeapObjectIfStrong(&heap_object)) {
@@ -1890,13 +1908,13 @@ void V8HeapExplorer::ExtractDescriptorArrayReferences(HeapEntry* entry,
 }
 
 void V8HeapExplorer::ExtractEnumCacheReferences(HeapEntry* entry,
-                                                EnumCache cache) {
+                                                Tagged<EnumCache> cache) {
   TagObject(cache->keys(), "(enum cache)", HeapEntry::kObjectShape);
   TagObject(cache->indices(), "(enum cache)", HeapEntry::kObjectShape);
 }
 
 void V8HeapExplorer::ExtractTransitionArrayReferences(
-    HeapEntry* entry, TransitionArray transitions) {
+    HeapEntry* entry, Tagged<TransitionArray> transitions) {
   if (transitions->HasPrototypeTransitions()) {
     TagObject(transitions->GetPrototypeTransitions(), "(prototype transitions)",
               HeapEntry::kObjectShape);
@@ -1905,10 +1923,11 @@ void V8HeapExplorer::ExtractTransitionArrayReferences(
 
 template <typename T>
 void V8HeapExplorer::ExtractWeakArrayReferences(int header_size,
-                                                HeapEntry* entry, T array) {
-  for (int i = 0; i < array.length(); ++i) {
-    MaybeObject object = array.Get(i);
-    HeapObject heap_object;
+                                                HeapEntry* entry,
+                                                Tagged<T> array) {
+  for (int i = 0; i < array->length(); ++i) {
+    MaybeObject object = array->Get(i);
+    Tagged<HeapObject> heap_object;
     if (object->GetHeapObjectIfWeak(&heap_object)) {
       SetWeakReference(entry, i, heap_object, header_size + i * kTaggedSize);
     } else if (object->GetHeapObjectIfStrong(&heap_object)) {
@@ -1918,11 +1937,12 @@ void V8HeapExplorer::ExtractWeakArrayReferences(int header_size,
   }
 }
 
-void V8HeapExplorer::ExtractPropertyReferences(JSObject js_obj,
+void V8HeapExplorer::ExtractPropertyReferences(Tagged<JSObject> js_obj,
                                                HeapEntry* entry) {
   Isolate* isolate = js_obj->GetIsolate();
   if (js_obj->HasFastProperties()) {
-    DescriptorArray descs = js_obj->map()->instance_descriptors(isolate);
+    Tagged<DescriptorArray> descs =
+        js_obj->map()->instance_descriptors(isolate);
     for (InternalIndex i : js_obj->map()->IterateOwnDescriptors()) {
       PropertyDetails details = descs->GetDetails(i);
       switch (details.location()) {
@@ -1932,10 +1952,10 @@ void V8HeapExplorer::ExtractPropertyReferences(JSObject js_obj,
             if (r.IsSmi() || r.IsDouble()) break;
           }
 
-          Name k = descs->GetKey(i);
+          Tagged<Name> k = descs->GetKey(i);
           FieldIndex field_index =
               FieldIndex::ForDetails(js_obj->map(), details);
-          Object value = js_obj->RawFastPropertyAt(field_index);
+          Tagged<Object> value = js_obj->RawFastPropertyAt(field_index);
           int field_offset =
               field_index.is_inobject() ? field_index.offset() : -1;
 
@@ -1952,14 +1972,14 @@ void V8HeapExplorer::ExtractPropertyReferences(JSObject js_obj,
     }
   } else if (IsJSGlobalObject(js_obj)) {
     // We assume that global objects can only have slow properties.
-    GlobalDictionary dictionary =
+    Tagged<GlobalDictionary> dictionary =
         JSGlobalObject::cast(js_obj)->global_dictionary(kAcquireLoad);
     ReadOnlyRoots roots(isolate);
     for (InternalIndex i : dictionary->IterateEntries()) {
-      if (!dictionary.IsKey(roots, dictionary->KeyAt(i))) continue;
-      PropertyCell cell = dictionary->CellAt(i);
-      Name name = cell->name();
-      Object value = cell->value();
+      if (!dictionary->IsKey(roots, dictionary->KeyAt(i))) continue;
+      Tagged<PropertyCell> cell = dictionary->CellAt(i);
+      Tagged<Name> name = cell->name();
+      Tagged<Object> value = cell->value();
       PropertyDetails details = cell->property_details();
       SetDataOrAccessorPropertyReference(details.kind(), entry, name, value);
     }
@@ -1968,23 +1988,24 @@ void V8HeapExplorer::ExtractPropertyReferences(JSObject js_obj,
     // leak out of here.
     HandleScope scope(isolate);
 
-    SwissNameDictionary dictionary = js_obj->property_dictionary_swiss();
+    Tagged<SwissNameDictionary> dictionary =
+        js_obj->property_dictionary_swiss();
     ReadOnlyRoots roots(isolate);
     for (InternalIndex i : dictionary->IterateEntries()) {
-      Object k = dictionary->KeyAt(i);
-      if (!dictionary.IsKey(roots, k)) continue;
-      Object value = dictionary->ValueAt(i);
+      Tagged<Object> k = dictionary->KeyAt(i);
+      if (!dictionary->IsKey(roots, k)) continue;
+      Tagged<Object> value = dictionary->ValueAt(i);
       PropertyDetails details = dictionary->DetailsAt(i);
       SetDataOrAccessorPropertyReference(details.kind(), entry, Name::cast(k),
                                          value);
     }
   } else {
-    NameDictionary dictionary = js_obj->property_dictionary();
+    Tagged<NameDictionary> dictionary = js_obj->property_dictionary();
     ReadOnlyRoots roots(isolate);
     for (InternalIndex i : dictionary->IterateEntries()) {
-      Object k = dictionary->KeyAt(i);
-      if (!dictionary.IsKey(roots, k)) continue;
-      Object value = dictionary->ValueAt(i);
+      Tagged<Object> k = dictionary->KeyAt(i);
+      if (!dictionary->IsKey(roots, k)) continue;
+      Tagged<Object> value = dictionary->ValueAt(i);
       PropertyDetails details = dictionary->DetailsAt(i);
       SetDataOrAccessorPropertyReference(details.kind(), entry, Name::cast(k),
                                          value);
@@ -1992,27 +2013,28 @@ void V8HeapExplorer::ExtractPropertyReferences(JSObject js_obj,
   }
 }
 
-void V8HeapExplorer::ExtractAccessorPairProperty(HeapEntry* entry, Name key,
-                                                 Object callback_obj,
+void V8HeapExplorer::ExtractAccessorPairProperty(HeapEntry* entry,
+                                                 Tagged<Name> key,
+                                                 Tagged<Object> callback_obj,
                                                  int field_offset) {
   if (!IsAccessorPair(callback_obj)) return;
-  AccessorPair accessors = AccessorPair::cast(callback_obj);
+  Tagged<AccessorPair> accessors = AccessorPair::cast(callback_obj);
   SetPropertyReference(entry, key, accessors, nullptr, field_offset);
-  Object getter = accessors->getter();
+  Tagged<Object> getter = accessors->getter();
   if (!IsOddball(getter)) {
     SetPropertyReference(entry, key, getter, "get %s");
   }
-  Object setter = accessors->setter();
+  Tagged<Object> setter = accessors->setter();
   if (!IsOddball(setter)) {
     SetPropertyReference(entry, key, setter, "set %s");
   }
 }
 
-void V8HeapExplorer::ExtractElementReferences(JSObject js_obj,
+void V8HeapExplorer::ExtractElementReferences(Tagged<JSObject> js_obj,
                                               HeapEntry* entry) {
   ReadOnlyRoots roots = js_obj->GetReadOnlyRoots();
   if (js_obj->HasObjectElements()) {
-    FixedArray elements = FixedArray::cast(js_obj->elements());
+    Tagged<FixedArray> elements = FixedArray::cast(js_obj->elements());
     int length = IsJSArray(js_obj) ? Smi::ToInt(JSArray::cast(js_obj)->length())
                                    : elements->length();
     for (int i = 0; i < length; ++i) {
@@ -2021,10 +2043,10 @@ void V8HeapExplorer::ExtractElementReferences(JSObject js_obj,
       }
     }
   } else if (js_obj->HasDictionaryElements()) {
-    NumberDictionary dictionary = js_obj->element_dictionary();
+    Tagged<NumberDictionary> dictionary = js_obj->element_dictionary();
     for (InternalIndex i : dictionary->IterateEntries()) {
-      Object k = dictionary->KeyAt(i);
-      if (!dictionary.IsKey(roots, k)) continue;
+      Tagged<Object> k = dictionary->KeyAt(i);
+      if (!dictionary->IsKey(roots, k)) continue;
       DCHECK(IsNumber(k));
       uint32_t index = static_cast<uint32_t>(Object::Number(k));
       SetElementReference(entry, index, dictionary->ValueAt(i));
@@ -2032,21 +2054,21 @@ void V8HeapExplorer::ExtractElementReferences(JSObject js_obj,
   }
 }
 
-void V8HeapExplorer::ExtractInternalReferences(JSObject js_obj,
+void V8HeapExplorer::ExtractInternalReferences(Tagged<JSObject> js_obj,
                                                HeapEntry* entry) {
   int length = js_obj->GetEmbedderFieldCount();
   for (int i = 0; i < length; ++i) {
-    Object o = js_obj->GetEmbedderField(i);
+    Tagged<Object> o = js_obj->GetEmbedderField(i);
     SetInternalReference(entry, i, o, js_obj->GetEmbedderFieldOffset(i));
   }
 }
 
 #if V8_ENABLE_WEBASSEMBLY
 
-void V8HeapExplorer::ExtractWasmStructReferences(WasmStruct obj,
+void V8HeapExplorer::ExtractWasmStructReferences(Tagged<WasmStruct> obj,
                                                  HeapEntry* entry) {
   wasm::StructType* type = obj->type();
-  WasmTypeInfo info = obj->map()->wasm_type_info();
+  Tagged<WasmTypeInfo> info = obj->map()->wasm_type_info();
   // The cast is safe; structs always have their instance defined.
   wasm::NamesProvider* names = WasmInstanceObject::cast(info->instance())
                                    ->module_object()
@@ -2059,7 +2081,7 @@ void V8HeapExplorer::ExtractWasmStructReferences(WasmStruct obj,
     sb << '\0';
     const char* field_name = names_->GetCopy(sb.start());
     int field_offset = type->field_offset(i);
-    Object value = obj->RawField(field_offset).load(entry->isolate());
+    Tagged<Object> value = obj->RawField(field_offset).load(entry->isolate());
     HeapEntry* value_entry = GetEntry(value);
     entry->SetNamedReference(HeapGraphEdge::kProperty, field_name, value_entry,
                              generator_);
@@ -2067,7 +2089,7 @@ void V8HeapExplorer::ExtractWasmStructReferences(WasmStruct obj,
   }
 }
 
-void V8HeapExplorer::ExtractWasmArrayReferences(WasmArray obj,
+void V8HeapExplorer::ExtractWasmArrayReferences(Tagged<WasmArray> obj,
                                                 HeapEntry* entry) {
   if (!obj->type()->element_type().is_reference()) return;
   for (uint32_t i = 0; i < obj->length(); i++) {
@@ -2076,8 +2098,8 @@ void V8HeapExplorer::ExtractWasmArrayReferences(WasmArray obj,
   }
 }
 
-void V8HeapExplorer::ExtractWasmInstanceObjectReference(WasmInstanceObject obj,
-                                                        HeapEntry* entry) {
+void V8HeapExplorer::ExtractWasmInstanceObjectReference(
+    Tagged<WasmInstanceObject> obj, HeapEntry* entry) {
   PtrComprCageBase cage_base(heap_->isolate());
   for (size_t i = 0; i < WasmInstanceObject::kTaggedFieldOffsets.size(); i++) {
     const uint16_t offset = WasmInstanceObject::kTaggedFieldOffsets[i];
@@ -2089,8 +2111,8 @@ void V8HeapExplorer::ExtractWasmInstanceObjectReference(WasmInstanceObject obj,
 
 #endif  // V8_ENABLE_WEBASSEMBLY
 
-JSFunction V8HeapExplorer::GetConstructor(Isolate* isolate,
-                                          JSReceiver receiver) {
+Tagged<JSFunction> V8HeapExplorer::GetConstructor(Isolate* isolate,
+                                                  Tagged<JSReceiver> receiver) {
   DisallowGarbageCollection no_gc;
   HandleScope scope(isolate);
   MaybeHandle<JSFunction> maybe_constructor =
@@ -2101,14 +2123,15 @@ JSFunction V8HeapExplorer::GetConstructor(Isolate* isolate,
   return *maybe_constructor.ToHandleChecked();
 }
 
-String V8HeapExplorer::GetConstructorName(Isolate* isolate, JSObject object) {
+Tagged<String> V8HeapExplorer::GetConstructorName(Isolate* isolate,
+                                                  Tagged<JSObject> object) {
   if (IsJSFunction(object)) return ReadOnlyRoots(isolate).closure_string();
   DisallowGarbageCollection no_gc;
   HandleScope scope(isolate);
   return *JSReceiver::GetConstructorName(isolate, handle(object, isolate));
 }
 
-HeapEntry* V8HeapExplorer::GetEntry(Object obj) {
+HeapEntry* V8HeapExplorer::GetEntry(Tagged<Object> obj) {
   if (IsHeapObject(obj)) {
     return generator_->FindOrAddEntry(reinterpret_cast<void*>(obj.ptr()), this);
   }
@@ -2159,9 +2182,9 @@ class RootsReferencesExtractor : public RootVisitor {
   // MarkCompactCollector::RootMarkingVisitor::VisitRunningCode.
   void VisitRunningCode(FullObjectSlot code_slot,
                         FullObjectSlot istream_or_smi_zero_slot) final {
-    Object istream_or_smi_zero = *istream_or_smi_zero_slot;
+    Tagged<Object> istream_or_smi_zero = *istream_or_smi_zero_slot;
     if (istream_or_smi_zero != Smi::zero()) {
-      Code code = Code::cast(*code_slot);
+      Tagged<Code> code = Code::cast(*code_slot);
       code->IterateDeoptimizationLiterals(this);
       VisitRootPointer(Root::kStackRoots, nullptr, istream_or_smi_zero_slot);
     }
@@ -2204,7 +2227,7 @@ bool V8HeapExplorer::IterateAndExtractReferences(
   PtrComprCageBase cage_base(heap_->isolate());
   // Heap iteration need not be finished but progress reporting may depend on
   // it being finished.
-  for (HeapObject obj = iterator.Next(); !obj.is_null();
+  for (Tagged<HeapObject> obj = iterator.Next(); !obj.is_null();
        obj = iterator.Next(), progress_->ProgressStep()) {
     if (interrupted) continue;
 
@@ -2252,7 +2275,7 @@ bool V8HeapExplorer::IterateAndExtractReferences(
   return interrupted ? false : progress_->ProgressReport(true);
 }
 
-bool V8HeapExplorer::IsEssentialObject(Object object) {
+bool V8HeapExplorer::IsEssentialObject(Tagged<Object> object) {
   if (!IsHeapObject(object)) return false;
   // Avoid comparing InstructionStream objects with non-InstructionStream
   // objects below.
@@ -2274,7 +2297,7 @@ bool V8HeapExplorer::IsEssentialObject(Object object) {
          object != roots.two_pointer_filler_map();
 }
 
-bool V8HeapExplorer::IsEssentialHiddenReference(Object parent,
+bool V8HeapExplorer::IsEssentialHiddenReference(Tagged<Object> parent,
                                                 int field_offset) {
   if (IsAllocationSite(parent) &&
       field_offset == AllocationSite::kWeakNextOffset)
@@ -2289,8 +2312,9 @@ bool V8HeapExplorer::IsEssentialHiddenReference(Object parent,
 }
 
 void V8HeapExplorer::SetContextReference(HeapEntry* parent_entry,
-                                         String reference_name,
-                                         Object child_obj, int field_offset) {
+                                         Tagged<String> reference_name,
+                                         Tagged<Object> child_obj,
+                                         int field_offset) {
   HeapEntry* child_entry = GetEntry(child_obj);
   if (child_entry == nullptr) return;
   parent_entry->SetNamedReference(HeapGraphEdge::kContextVariable,
@@ -2308,7 +2332,7 @@ void V8HeapExplorer::MarkVisitedField(int offset) {
 
 void V8HeapExplorer::SetNativeBindReference(HeapEntry* parent_entry,
                                             const char* reference_name,
-                                            Object child_obj) {
+                                            Tagged<Object> child_obj) {
   HeapEntry* child_entry = GetEntry(child_obj);
   if (child_entry == nullptr) return;
   parent_entry->SetNamedReference(HeapGraphEdge::kShortcut, reference_name,
@@ -2316,7 +2340,7 @@ void V8HeapExplorer::SetNativeBindReference(HeapEntry* parent_entry,
 }
 
 void V8HeapExplorer::SetElementReference(HeapEntry* parent_entry, int index,
-                                         Object child_obj) {
+                                         Tagged<Object> child_obj) {
   HeapEntry* child_entry = GetEntry(child_obj);
   if (child_entry == nullptr) return;
   parent_entry->SetIndexedReference(HeapGraphEdge::kElement, index, child_entry,
@@ -2325,7 +2349,8 @@ void V8HeapExplorer::SetElementReference(HeapEntry* parent_entry, int index,
 
 void V8HeapExplorer::SetInternalReference(HeapEntry* parent_entry,
                                           const char* reference_name,
-                                          Object child_obj, int field_offset) {
+                                          Tagged<Object> child_obj,
+                                          int field_offset) {
   if (!IsEssentialObject(child_obj)) {
     return;
   }
@@ -2337,7 +2362,8 @@ void V8HeapExplorer::SetInternalReference(HeapEntry* parent_entry,
 }
 
 void V8HeapExplorer::SetInternalReference(HeapEntry* parent_entry, int index,
-                                          Object child_obj, int field_offset) {
+                                          Tagged<Object> child_obj,
+                                          int field_offset) {
   if (!IsEssentialObject(child_obj)) {
     return;
   }
@@ -2349,9 +2375,10 @@ void V8HeapExplorer::SetInternalReference(HeapEntry* parent_entry, int index,
   MarkVisitedField(field_offset);
 }
 
-void V8HeapExplorer::SetHiddenReference(HeapObject parent_obj,
+void V8HeapExplorer::SetHiddenReference(Tagged<HeapObject> parent_obj,
                                         HeapEntry* parent_entry, int index,
-                                        Object child_obj, int field_offset) {
+                                        Tagged<Object> child_obj,
+                                        int field_offset) {
   DCHECK_EQ(parent_entry, GetEntry(parent_obj));
   DCHECK(!MapWord::IsPacked(child_obj.ptr()));
   if (!IsEssentialObject(child_obj)) {
@@ -2366,8 +2393,9 @@ void V8HeapExplorer::SetHiddenReference(HeapObject parent_obj,
 }
 
 void V8HeapExplorer::SetWeakReference(
-    HeapEntry* parent_entry, const char* reference_name, Object child_obj,
-    int field_offset, HeapEntry::ReferenceVerification verification) {
+    HeapEntry* parent_entry, const char* reference_name,
+    Tagged<Object> child_obj, int field_offset,
+    HeapEntry::ReferenceVerification verification) {
   if (!IsEssentialObject(child_obj)) {
     return;
   }
@@ -2379,7 +2407,7 @@ void V8HeapExplorer::SetWeakReference(
 }
 
 void V8HeapExplorer::SetWeakReference(HeapEntry* parent_entry, int index,
-                                      Object child_obj,
+                                      Tagged<Object> child_obj,
                                       base::Optional<int> field_offset) {
   if (!IsEssentialObject(child_obj)) {
     return;
@@ -2395,8 +2423,9 @@ void V8HeapExplorer::SetWeakReference(HeapEntry* parent_entry, int index,
 }
 
 void V8HeapExplorer::SetDataOrAccessorPropertyReference(
-    PropertyKind kind, HeapEntry* parent_entry, Name reference_name,
-    Object child_obj, const char* name_format_string, int field_offset) {
+    PropertyKind kind, HeapEntry* parent_entry, Tagged<Name> reference_name,
+    Tagged<Object> child_obj, const char* name_format_string,
+    int field_offset) {
   if (kind == PropertyKind::kAccessor) {
     ExtractAccessorPairProperty(parent_entry, reference_name, child_obj,
                                 field_offset);
@@ -2407,7 +2436,8 @@ void V8HeapExplorer::SetDataOrAccessorPropertyReference(
 }
 
 void V8HeapExplorer::SetPropertyReference(HeapEntry* parent_entry,
-                                          Name reference_name, Object child_obj,
+                                          Tagged<Name> reference_name,
+                                          Tagged<Object> child_obj,
                                           const char* name_format_string,
                                           int field_offset) {
   HeapEntry* child_entry = GetEntry(child_obj);
@@ -2434,7 +2464,7 @@ void V8HeapExplorer::SetRootGcRootsReference() {
       HeapGraphEdge::kElement, snapshot_->gc_roots(), generator_);
 }
 
-void V8HeapExplorer::SetUserGlobalReference(Object child_obj) {
+void V8HeapExplorer::SetUserGlobalReference(Tagged<Object> child_obj) {
   HeapEntry* child_entry = GetEntry(child_obj);
   DCHECK_NOT_NULL(child_entry);
   snapshot_->root()->SetNamedAutoIndexReference(
@@ -2447,7 +2477,8 @@ void V8HeapExplorer::SetGcRootsReference(Root root) {
 }
 
 void V8HeapExplorer::SetGcSubrootReference(Root root, const char* description,
-                                           bool is_weak, Object child_obj) {
+                                           bool is_weak,
+                                           Tagged<Object> child_obj) {
   if (IsSmi(child_obj)) {
     // TODO(arenevier): if we handle smis here, the snapshot gets 2 to 3 times
     // slower on large heaps. According to perf, The bulk of the extra works
@@ -2478,7 +2509,8 @@ void V8HeapExplorer::SetGcSubrootReference(Root root, const char* description,
   // also used as starting points in distance calculations.
   if (is_weak || !IsNativeContext(child_heap_obj)) return;
 
-  JSGlobalObject global = Context::cast(child_heap_obj)->global_object();
+  Tagged<JSGlobalObject> global =
+      Context::cast(child_heap_obj)->global_object();
   if (!IsJSGlobalObject(global)) return;
 
   if (!user_roots_.insert(global).second) return;
@@ -2486,13 +2518,13 @@ void V8HeapExplorer::SetGcSubrootReference(Root root, const char* description,
   SetUserGlobalReference(global);
 }
 
-const char* V8HeapExplorer::GetStrongGcSubrootName(HeapObject object) {
+const char* V8HeapExplorer::GetStrongGcSubrootName(Tagged<HeapObject> object) {
   if (strong_gc_subroot_names_.empty()) {
     Isolate* isolate = Isolate::FromHeap(heap_);
     for (RootIndex root_index = RootIndex::kFirstStrongOrReadOnlyRoot;
          root_index <= RootIndex::kLastStrongOrReadOnlyRoot; ++root_index) {
       const char* name = RootsTable::name(root_index);
-      Object root = isolate->root(root_index);
+      Tagged<Object> root = isolate->root(root_index);
       CHECK(!IsSmi(root));
       strong_gc_subroot_names_.emplace(HeapObject::cast(root), name);
     }
@@ -2502,7 +2534,7 @@ const char* V8HeapExplorer::GetStrongGcSubrootName(HeapObject object) {
   return it != strong_gc_subroot_names_.end() ? it->second : nullptr;
 }
 
-void V8HeapExplorer::TagObject(Object obj, const char* tag,
+void V8HeapExplorer::TagObject(Tagged<Object> obj, const char* tag,
                                base::Optional<HeapEntry::Type> type) {
   if (IsEssentialObject(obj)) {
     HeapEntry* entry = GetEntry(obj);
@@ -2515,12 +2547,13 @@ void V8HeapExplorer::TagObject(Object obj, const char* tag,
   }
 }
 
-void V8HeapExplorer::RecursivelyTagConstantPool(Object obj, const char* tag,
+void V8HeapExplorer::RecursivelyTagConstantPool(Tagged<Object> obj,
+                                                const char* tag,
                                                 HeapEntry::Type type,
                                                 int recursion_limit) {
   --recursion_limit;
   if (IsFixedArrayExact(obj, isolate())) {
-    FixedArray arr = FixedArray::cast(obj);
+    Tagged<FixedArray> arr = FixedArray::cast(obj);
     TagObject(arr, tag, type);
     if (recursion_limit <= 0) return;
     for (int i = 0; i < arr->length(); ++i) {
@@ -2555,11 +2588,11 @@ class GlobalObjectsEnumerator : public RootVisitor {
                              TSlot end) {
     for (TSlot p = start; p < end; ++p) {
       DCHECK(!MapWord::IsPacked(p.Relaxed_Load(isolate_).ptr()));
-      Object o = p.load(isolate_);
+      Tagged<Object> o = p.load(isolate_);
       if (!IsNativeContext(o, isolate_)) continue;
-      JSObject proxy = Context::cast(o)->global_proxy();
+      Tagged<JSObject> proxy = Context::cast(o)->global_proxy();
       if (!IsJSGlobalProxy(proxy, isolate_)) continue;
-      Object global = proxy->map(isolate_)->prototype(isolate_);
+      Tagged<Object> global = proxy->map(isolate_)->prototype(isolate_);
       if (!IsJSGlobalObject(global, isolate_)) continue;
       handler_(handle(JSGlobalObject::cast(global), isolate_));
     }
@@ -2616,8 +2649,8 @@ class EmbedderGraphImpl : public EmbedderGraph {
 
   class V8NodeImpl : public Node {
    public:
-    explicit V8NodeImpl(Object object) : object_(object) {}
-    Object GetObject() { return object_; }
+    explicit V8NodeImpl(Tagged<Object> object) : object_(object) {}
+    Tagged<Object> GetObject() { return object_; }
 
     // Node overrides.
     bool IsEmbedderNode() override { return false; }
@@ -2665,7 +2698,7 @@ class EmbedderGraphEntriesAllocator : public HeapEntriesAllocator {
         names_(snapshot_->profiler()->names()),
         heap_object_map_(snapshot_->profiler()->heap_object_map()) {}
   HeapEntry* AllocateEntry(HeapThing ptr) override;
-  HeapEntry* AllocateEntry(Smi smi) override;
+  HeapEntry* AllocateEntry(Tagged<Smi> smi) override;
 
  private:
   HeapSnapshot* snapshot_;
@@ -2733,7 +2766,7 @@ HeapEntry* EmbedderGraphEntriesAllocator::AllocateEntry(HeapThing ptr) {
   return heap_entry;
 }
 
-HeapEntry* EmbedderGraphEntriesAllocator::AllocateEntry(Smi smi) {
+HeapEntry* EmbedderGraphEntriesAllocator::AllocateEntry(Tagged<Smi> smi) {
   DCHECK(false);
   return nullptr;
 }
@@ -2757,10 +2790,10 @@ void NativeObjectsExplorer::MergeNodeIntoEntry(
     // For V8 nodes only we can add a lookup.
     EmbedderGraphImpl::V8NodeImpl* v8_node =
         static_cast<EmbedderGraphImpl::V8NodeImpl*>(wrapper_node);
-    Object object = v8_node->GetObject();
+    Tagged<Object> object = v8_node->GetObject();
     DCHECK(!IsSmi(object));
     if (original_node->GetNativeObject()) {
-      HeapObject heap_object = HeapObject::cast(object);
+      Tagged<HeapObject> heap_object = HeapObject::cast(object);
       heap_object_map_->AddMergedNativeEntry(original_node->GetNativeObject(),
                                              heap_object.address());
       DCHECK_EQ(entry->id(), heap_object_map_->FindMergedNativeEntry(
@@ -2788,7 +2821,7 @@ HeapEntry* NativeObjectsExplorer::EntryForEmbedderGraphNode(
                                       embedder_graph_entries_allocator_.get());
   }
   // Node is V8NodeImpl.
-  Object object =
+  Tagged<Object> object =
       static_cast<EmbedderGraphImpl::V8NodeImpl*>(node)->GetObject();
   if (IsSmi(object)) return nullptr;
   auto* entry = generator_->FindEntry(
@@ -2958,7 +2991,7 @@ void HeapSnapshotJSONSerializer::Serialize(v8::OutputStream* stream) {
   v8::base::ElapsedTimer timer;
   timer.Start();
   if (AllocationTracker* allocation_tracker =
-      snapshot_->profiler()->allocation_tracker()) {
+          snapshot_->profiler()->allocation_tracker()) {
     allocation_tracker->PrepareForSerialization();
   }
   DCHECK_NULL(writer_);
@@ -2974,7 +3007,6 @@ void HeapSnapshotJSONSerializer::Serialize(v8::OutputStream* stream) {
   timer.Stop();
 }
 
-
 void HeapSnapshotJSONSerializer::SerializeImpl() {
   DCHECK_EQ(0, snapshot_->root()->index());
   writer_->AddCharacter('{');
@@ -3018,7 +3050,6 @@ void HeapSnapshotJSONSerializer::SerializeImpl() {
   writer_->Finalize();
 }
 
-
 int HeapSnapshotJSONSerializer::GetStringId(const char* s) {
   base::HashMap::Entry* cache_entry =
       strings_.LookupOrInsert(const_cast<char*>(s), StringHash(s));
@@ -3028,21 +3059,23 @@ int HeapSnapshotJSONSerializer::GetStringId(const char* s) {
   return static_cast<int>(reinterpret_cast<intptr_t>(cache_entry->value));
 }
 
-
 namespace {
 
-template<size_t size> struct ToUnsigned;
+template <size_t size>
+struct ToUnsigned;
 
 template <>
 struct ToUnsigned<1> {
   using Type = uint8_t;
 };
 
-template<> struct ToUnsigned<4> {
+template <>
+struct ToUnsigned<4> {
   using Type = uint32_t;
 };
 
-template<> struct ToUnsigned<8> {
+template <>
+struct ToUnsigned<8> {
   using Type = uint64_t;
 };
 
@@ -3080,9 +3113,10 @@ void HeapSnapshotJSONSerializer::SerializeEdge(HeapGraphEdge* edge,
   static const int kBufferSize =
       MaxDecimalDigitsIn<sizeof(unsigned)>::kUnsigned * 3 + 3 + 2;
   base::EmbeddedVector<char, kBufferSize> buffer;
-  int edge_name_or_index = edge->type() == HeapGraphEdge::kElement
-      || edge->type() == HeapGraphEdge::kHidden
-      ? edge->index() : GetStringId(edge->name());
+  int edge_name_or_index = edge->type() == HeapGraphEdge::kElement ||
+                                   edge->type() == HeapGraphEdge::kHidden
+                               ? edge->index()
+                               : GetStringId(edge->name());
   int buffer_pos = 0;
   if (!first_edge) {
     buffer[buffer_pos++] = ',';
@@ -3150,7 +3184,7 @@ void HeapSnapshotJSONSerializer::SerializeSnapshot() {
   // The object describing node serialization layout.
   // We use a set of macros to improve readability.
 
-// clang-format off
+  // clang-format off
 #define JSON_A(s) "[" s "]"
 #define JSON_O(s) "{" s "}"
 #define JSON_S(s) "\"" s "\""
@@ -3239,7 +3273,6 @@ void HeapSnapshotJSONSerializer::SerializeSnapshot() {
   writer_->AddNumber(count);
 }
 
-
 static void WriteUChar(OutputStreamWriter* w, unibrow::uchar u) {
   static const char hex_chars[] = "0123456789ABCDEF";
   w->AddString("\\u");
@@ -3249,7 +3282,6 @@ static void WriteUChar(OutputStreamWriter* w, unibrow::uchar u) {
   w->AddCharacter(hex_chars[u & 0xF]);
 }
 
-
 void HeapSnapshotJSONSerializer::SerializeTraceTree() {
   AllocationTracker* tracker = snapshot_->profiler()->allocation_tracker();
   if (!tracker) return;
@@ -3257,7 +3289,6 @@ void HeapSnapshotJSONSerializer::SerializeTraceTree() {
   SerializeTraceNode(traces->root());
 }
 
-
 void HeapSnapshotJSONSerializer::SerializeTraceNode(AllocationTraceNode* node) {
   // The buffer needs space for 4 unsigned ints, 4 commas, [ and \0
   const int kBufferSize =
@@ -3286,7 +3317,6 @@ void HeapSnapshotJSONSerializer::SerializeTraceNode(AllocationTraceNode* node) {
   writer_->AddCharacter(']');
 }
 
-
 // 0-based position is converted to 1-based during the serialization.
 static int SerializePosition(int position, base::Vector<char> buffer,
                              int buffer_pos) {
@@ -3319,8 +3349,8 @@ void HeapSnapshotJSONSerializer::SerializeTraceNodeInfos() {
     buffer_pos = utoa(GetStringId(info->script_name), buffer, buffer_pos);
     buffer[buffer_pos++] = ',';
     // The cast is safe because script id is a non-negative Smi.
-    buffer_pos = utoa(static_cast<unsigned>(info->script_id), buffer,
-        buffer_pos);
+    buffer_pos =
+        utoa(static_cast<unsigned>(info->script_id), buffer, buffer_pos);
     buffer[buffer_pos++] = ',';
     buffer_pos = SerializePosition(info->line, buffer, buffer_pos);
     buffer[buffer_pos++] = ',';
@@ -3331,7 +3361,6 @@ void HeapSnapshotJSONSerializer::SerializeTraceNodeInfos() {
   }
 }
 
-
 void HeapSnapshotJSONSerializer::SerializeSamples() {
   const std::vector<HeapObjectsMap::TimeInterval>& samples =
       snapshot_->profiler()->heap_object_map()->samples();
@@ -3359,11 +3388,10 @@ void HeapSnapshotJSONSerializer::SerializeSamples() {
   }
 }
 
-
 void HeapSnapshotJSONSerializer::SerializeString(const unsigned char* s) {
   writer_->AddCharacter('\n');
   writer_->AddCharacter('\"');
-  for ( ; *s != '\0'; ++s) {
+  for (; *s != '\0'; ++s) {
     switch (*s) {
       case '\b':
         writer_->AddString("\\b");
@@ -3394,7 +3422,8 @@ void HeapSnapshotJSONSerializer::SerializeString(const unsigned char* s) {
         } else {
           // Convert UTF-8 into \u UTF-16 literal.
           size_t length = 1, cursor = 0;
-          for ( ; length <= 4 && *(s + length) != '\0'; ++length) { }
+          for (; length <= 4 && *(s + length) != '\0'; ++length) {
+          }
           unibrow::uchar c = unibrow::Utf8::CalculateValue(s, length, &cursor);
           if (c != unibrow::Utf8::kBadChar) {
             WriteUChar(writer_, c);
@@ -3409,7 +3438,6 @@ void HeapSnapshotJSONSerializer::SerializeString(const unsigned char* s) {
   writer_->AddCharacter('\"');
 }
 
-
 void HeapSnapshotJSONSerializer::SerializeStrings() {
   base::ScopedVector<const unsigned char*> sorted_strings(strings_.occupancy() +
                                                           1);
diff --git a/src/profiler/heap-snapshot-generator.h b/src/profiler/heap-snapshot-generator.h
index d001c929c8f..ef5241d0fb1 100644
--- a/src/profiler/heap-snapshot-generator.h
+++ b/src/profiler/heap-snapshot-generator.h
@@ -380,7 +380,7 @@ class HeapEntriesAllocator {
  public:
   virtual ~HeapEntriesAllocator() = default;
   virtual HeapEntry* AllocateEntry(HeapThing ptr) = 0;
-  virtual HeapEntry* AllocateEntry(Smi smi) = 0;
+  virtual HeapEntry* AllocateEntry(Tagged<Smi> smi) = 0;
 };
 
 class SnapshottingProgressReportingInterface {
@@ -403,7 +403,7 @@ class V8_EXPORT_PRIVATE V8HeapExplorer : public HeapEntriesAllocator {
   V8_INLINE Isolate* isolate() { return Isolate::FromHeap(heap_); }
 
   HeapEntry* AllocateEntry(HeapThing ptr) override;
-  HeapEntry* AllocateEntry(Smi smi) override;
+  HeapEntry* AllocateEntry(Tagged<Smi> smi) override;
   uint32_t EstimateObjectsCount();
   void PopulateLineEnds();
   bool IterateAndExtractReferences(HeapSnapshotGenerator* generator);
@@ -418,132 +418,149 @@ class V8_EXPORT_PRIVATE V8HeapExplorer : public HeapEntriesAllocator {
   // that can be used throughout snapshot generation.
   void MakeGlobalObjectTagMap(TemporaryGlobalObjectTags&&);
 
-  void TagBuiltinCodeObject(Code code, const char* name);
+  void TagBuiltinCodeObject(Tagged<Code> code, const char* name);
   HeapEntry* AddEntry(Address address,
                       HeapEntry::Type type,
                       const char* name,
                       size_t size);
 
-  static JSFunction GetConstructor(Isolate* isolate, JSReceiver receiver);
-  static String GetConstructorName(Isolate* isolate, JSObject object);
+  static Tagged<JSFunction> GetConstructor(Isolate* isolate,
+                                           Tagged<JSReceiver> receiver);
+  static Tagged<String> GetConstructorName(Isolate* isolate,
+                                           Tagged<JSObject> object);
 
  private:
   void MarkVisitedField(int offset);
 
-  HeapEntry* AddEntry(HeapObject object);
-  HeapEntry* AddEntry(HeapObject object, HeapEntry::Type type,
+  HeapEntry* AddEntry(Tagged<HeapObject> object);
+  HeapEntry* AddEntry(Tagged<HeapObject> object, HeapEntry::Type type,
                       const char* name);
 
-  const char* GetSystemEntryName(HeapObject object);
-  HeapEntry::Type GetSystemEntryType(HeapObject object);
-
-  JSFunction GetLocationFunction(HeapObject object);
-  void ExtractLocation(HeapEntry* entry, HeapObject object);
-  void ExtractLocationForJSFunction(HeapEntry* entry, JSFunction func);
-  void ExtractReferences(HeapEntry* entry, HeapObject obj);
-  void ExtractJSGlobalProxyReferences(HeapEntry* entry, JSGlobalProxy proxy);
-  void ExtractJSObjectReferences(HeapEntry* entry, JSObject js_obj);
-  void ExtractStringReferences(HeapEntry* entry, String obj);
-  void ExtractSymbolReferences(HeapEntry* entry, Symbol symbol);
-  void ExtractJSCollectionReferences(HeapEntry* entry, JSCollection collection);
+  const char* GetSystemEntryName(Tagged<HeapObject> object);
+  HeapEntry::Type GetSystemEntryType(Tagged<HeapObject> object);
+
+  Tagged<JSFunction> GetLocationFunction(Tagged<HeapObject> object);
+  void ExtractLocation(HeapEntry* entry, Tagged<HeapObject> object);
+  void ExtractLocationForJSFunction(HeapEntry* entry, Tagged<JSFunction> func);
+  void ExtractReferences(HeapEntry* entry, Tagged<HeapObject> obj);
+  void ExtractJSGlobalProxyReferences(HeapEntry* entry,
+                                      Tagged<JSGlobalProxy> proxy);
+  void ExtractJSObjectReferences(HeapEntry* entry, Tagged<JSObject> js_obj);
+  void ExtractStringReferences(HeapEntry* entry, Tagged<String> obj);
+  void ExtractSymbolReferences(HeapEntry* entry, Tagged<Symbol> symbol);
+  void ExtractJSCollectionReferences(HeapEntry* entry,
+                                     Tagged<JSCollection> collection);
   void ExtractJSWeakCollectionReferences(HeapEntry* entry,
-                                         JSWeakCollection collection);
+                                         Tagged<JSWeakCollection> collection);
   void ExtractEphemeronHashTableReferences(HeapEntry* entry,
-                                           EphemeronHashTable table);
-  void ExtractContextReferences(HeapEntry* entry, Context context);
-  void ExtractMapReferences(HeapEntry* entry, Map map);
+                                           Tagged<EphemeronHashTable> table);
+  void ExtractContextReferences(HeapEntry* entry, Tagged<Context> context);
+  void ExtractMapReferences(HeapEntry* entry, Tagged<Map> map);
   void ExtractSharedFunctionInfoReferences(HeapEntry* entry,
-                                           SharedFunctionInfo shared);
-  void ExtractScriptReferences(HeapEntry* entry, Script script);
+                                           Tagged<SharedFunctionInfo> shared);
+  void ExtractScriptReferences(HeapEntry* entry, Tagged<Script> script);
   void ExtractAccessorInfoReferences(HeapEntry* entry,
-                                     AccessorInfo accessor_info);
-  void ExtractAccessorPairReferences(HeapEntry* entry, AccessorPair accessors);
-  void ExtractCodeReferences(HeapEntry* entry, Code code);
+                                     Tagged<AccessorInfo> accessor_info);
+  void ExtractAccessorPairReferences(HeapEntry* entry,
+                                     Tagged<AccessorPair> accessors);
+  void ExtractCodeReferences(HeapEntry* entry, Tagged<Code> code);
   void ExtractInstructionStreamReferences(HeapEntry* entry,
-                                          InstructionStream code);
-  void ExtractCellReferences(HeapEntry* entry, Cell cell);
-  void ExtractJSWeakRefReferences(HeapEntry* entry, JSWeakRef js_weak_ref);
-  void ExtractWeakCellReferences(HeapEntry* entry, WeakCell weak_cell);
+                                          Tagged<InstructionStream> code);
+  void ExtractCellReferences(HeapEntry* entry, Tagged<Cell> cell);
+  void ExtractJSWeakRefReferences(HeapEntry* entry,
+                                  Tagged<JSWeakRef> js_weak_ref);
+  void ExtractWeakCellReferences(HeapEntry* entry, Tagged<WeakCell> weak_cell);
   void ExtractFeedbackCellReferences(HeapEntry* entry,
-                                     FeedbackCell feedback_cell);
-  void ExtractPropertyCellReferences(HeapEntry* entry, PropertyCell cell);
-  void ExtractPrototypeInfoReferences(HeapEntry* entry, PrototypeInfo info);
-  void ExtractAllocationSiteReferences(HeapEntry* entry, AllocationSite site);
+                                     Tagged<FeedbackCell> feedback_cell);
+  void ExtractPropertyCellReferences(HeapEntry* entry,
+                                     Tagged<PropertyCell> cell);
+  void ExtractPrototypeInfoReferences(HeapEntry* entry,
+                                      Tagged<PrototypeInfo> info);
+  void ExtractAllocationSiteReferences(HeapEntry* entry,
+                                       Tagged<AllocationSite> site);
   void ExtractArrayBoilerplateDescriptionReferences(
-      HeapEntry* entry, ArrayBoilerplateDescription value);
+      HeapEntry* entry, Tagged<ArrayBoilerplateDescription> value);
   void ExtractRegExpBoilerplateDescriptionReferences(
-      HeapEntry* entry, RegExpBoilerplateDescription value);
-  void ExtractJSArrayBufferReferences(HeapEntry* entry, JSArrayBuffer buffer);
-  void ExtractJSPromiseReferences(HeapEntry* entry, JSPromise promise);
+      HeapEntry* entry, Tagged<RegExpBoilerplateDescription> value);
+  void ExtractJSArrayBufferReferences(HeapEntry* entry,
+                                      Tagged<JSArrayBuffer> buffer);
+  void ExtractJSPromiseReferences(HeapEntry* entry, Tagged<JSPromise> promise);
   void ExtractJSGeneratorObjectReferences(HeapEntry* entry,
-                                          JSGeneratorObject generator);
-  void ExtractFixedArrayReferences(HeapEntry* entry, FixedArray array);
-  void ExtractNumberReference(HeapEntry* entry, Object number);
-  void ExtractBytecodeArrayReferences(HeapEntry* entry, BytecodeArray bytecode);
-  void ExtractScopeInfoReferences(HeapEntry* entry, ScopeInfo info);
+                                          Tagged<JSGeneratorObject> generator);
+  void ExtractFixedArrayReferences(HeapEntry* entry, Tagged<FixedArray> array);
+  void ExtractNumberReference(HeapEntry* entry, Tagged<Object> number);
+  void ExtractBytecodeArrayReferences(HeapEntry* entry,
+                                      Tagged<BytecodeArray> bytecode);
+  void ExtractScopeInfoReferences(HeapEntry* entry, Tagged<ScopeInfo> info);
   void ExtractFeedbackVectorReferences(HeapEntry* entry,
-                                       FeedbackVector feedback_vector);
+                                       Tagged<FeedbackVector> feedback_vector);
   void ExtractDescriptorArrayReferences(HeapEntry* entry,
-                                        DescriptorArray array);
-  void ExtractEnumCacheReferences(HeapEntry* entry, EnumCache cache);
+                                        Tagged<DescriptorArray> array);
+  void ExtractEnumCacheReferences(HeapEntry* entry, Tagged<EnumCache> cache);
   void ExtractTransitionArrayReferences(HeapEntry* entry,
-                                        TransitionArray transitions);
+                                        Tagged<TransitionArray> transitions);
   template <typename T>
-  void ExtractWeakArrayReferences(int header_size, HeapEntry* entry, T array);
-  void ExtractPropertyReferences(JSObject js_obj, HeapEntry* entry);
-  void ExtractAccessorPairProperty(HeapEntry* entry, Name key,
-                                   Object callback_obj, int field_offset = -1);
-  void ExtractElementReferences(JSObject js_obj, HeapEntry* entry);
-  void ExtractInternalReferences(JSObject js_obj, HeapEntry* entry);
+  void ExtractWeakArrayReferences(int header_size, HeapEntry* entry,
+                                  Tagged<T> array);
+  void ExtractPropertyReferences(Tagged<JSObject> js_obj, HeapEntry* entry);
+  void ExtractAccessorPairProperty(HeapEntry* entry, Tagged<Name> key,
+                                   Tagged<Object> callback_obj,
+                                   int field_offset = -1);
+  void ExtractElementReferences(Tagged<JSObject> js_obj, HeapEntry* entry);
+  void ExtractInternalReferences(Tagged<JSObject> js_obj, HeapEntry* entry);
 
 #if V8_ENABLE_WEBASSEMBLY
-  void ExtractWasmStructReferences(WasmStruct obj, HeapEntry* entry);
-  void ExtractWasmArrayReferences(WasmArray obj, HeapEntry* entry);
-  void ExtractWasmInstanceObjectReference(WasmInstanceObject obj,
+  void ExtractWasmStructReferences(Tagged<WasmStruct> obj, HeapEntry* entry);
+  void ExtractWasmArrayReferences(Tagged<WasmArray> obj, HeapEntry* entry);
+  void ExtractWasmInstanceObjectReference(Tagged<WasmInstanceObject> obj,
                                           HeapEntry* entry);
 #endif  // V8_ENABLE_WEBASSEMBLY
 
-  bool IsEssentialObject(Object object);
-  bool IsEssentialHiddenReference(Object parent, int field_offset);
+  bool IsEssentialObject(Tagged<Object> object);
+  bool IsEssentialHiddenReference(Tagged<Object> parent, int field_offset);
 
-  void SetContextReference(HeapEntry* parent_entry, String reference_name,
-                           Object child, int field_offset);
+  void SetContextReference(HeapEntry* parent_entry,
+                           Tagged<String> reference_name, Tagged<Object> child,
+                           int field_offset);
   void SetNativeBindReference(HeapEntry* parent_entry,
-                              const char* reference_name, Object child);
-  void SetElementReference(HeapEntry* parent_entry, int index, Object child);
+                              const char* reference_name, Tagged<Object> child);
+  void SetElementReference(HeapEntry* parent_entry, int index,
+                           Tagged<Object> child);
   void SetInternalReference(HeapEntry* parent_entry, const char* reference_name,
-                            Object child, int field_offset = -1);
-  void SetInternalReference(HeapEntry* parent_entry, int index, Object child,
-                            int field_offset = -1);
-  void SetHiddenReference(HeapObject parent_obj, HeapEntry* parent_entry,
-                          int index, Object child, int field_offset);
+                            Tagged<Object> child, int field_offset = -1);
+  void SetInternalReference(HeapEntry* parent_entry, int index,
+                            Tagged<Object> child, int field_offset = -1);
+  void SetHiddenReference(Tagged<HeapObject> parent_obj,
+                          HeapEntry* parent_entry, int index,
+                          Tagged<Object> child, int field_offset);
   void SetWeakReference(
-      HeapEntry* parent_entry, const char* reference_name, Object child_obj,
-      int field_offset,
+      HeapEntry* parent_entry, const char* reference_name,
+      Tagged<Object> child_obj, int field_offset,
       HeapEntry::ReferenceVerification verification = HeapEntry::kVerify);
-  void SetWeakReference(HeapEntry* parent_entry, int index, Object child_obj,
+  void SetWeakReference(HeapEntry* parent_entry, int index,
+                        Tagged<Object> child_obj,
                         base::Optional<int> field_offset);
-  void SetPropertyReference(HeapEntry* parent_entry, Name reference_name,
-                            Object child,
+  void SetPropertyReference(HeapEntry* parent_entry,
+                            Tagged<Name> reference_name, Tagged<Object> child,
                             const char* name_format_string = nullptr,
                             int field_offset = -1);
   void SetDataOrAccessorPropertyReference(
-      PropertyKind kind, HeapEntry* parent_entry, Name reference_name,
-      Object child, const char* name_format_string = nullptr,
+      PropertyKind kind, HeapEntry* parent_entry, Tagged<Name> reference_name,
+      Tagged<Object> child, const char* name_format_string = nullptr,
       int field_offset = -1);
 
-  void SetUserGlobalReference(Object user_global);
+  void SetUserGlobalReference(Tagged<Object> user_global);
   void SetRootGcRootsReference();
   void SetGcRootsReference(Root root);
   void SetGcSubrootReference(Root root, const char* description, bool is_weak,
-                             Object child);
-  const char* GetStrongGcSubrootName(HeapObject object);
-  void TagObject(Object obj, const char* tag,
+                             Tagged<Object> child);
+  const char* GetStrongGcSubrootName(Tagged<HeapObject> object);
+  void TagObject(Tagged<Object> obj, const char* tag,
                  base::Optional<HeapEntry::Type> type = {});
-  void RecursivelyTagConstantPool(Object obj, const char* tag,
+  void RecursivelyTagConstantPool(Tagged<Object> obj, const char* tag,
                                   HeapEntry::Type type, int recursion_limit);
 
-  HeapEntry* GetEntry(Object obj);
+  HeapEntry* GetEntry(Tagged<Object> obj);
 
   Heap* heap_;
   HeapSnapshot* snapshot_;
@@ -615,7 +632,7 @@ class HeapSnapshotGenerator : public SnapshottingProgressReportingInterface {
     return it != entries_map_.end() ? it->second : nullptr;
   }
 
-  HeapEntry* FindEntry(Smi smi) {
+  HeapEntry* FindEntry(Tagged<Smi> smi) {
     auto it = smis_map_.find(smi.value());
     return it != smis_map_.end() ? it->second : nullptr;
   }
@@ -648,7 +665,7 @@ class HeapSnapshotGenerator : public SnapshottingProgressReportingInterface {
   }
 #endif
 
-  HeapEntry* AddEntry(Smi smi, HeapEntriesAllocator* allocator) {
+  HeapEntry* AddEntry(Tagged<Smi> smi, HeapEntriesAllocator* allocator) {
     return smis_map_.emplace(smi.value(), allocator->AllocateEntry(smi))
         .first->second;
   }
@@ -658,7 +675,7 @@ class HeapSnapshotGenerator : public SnapshottingProgressReportingInterface {
     return entry != nullptr ? entry : AddEntry(ptr, allocator);
   }
 
-  HeapEntry* FindOrAddEntry(Smi smi, HeapEntriesAllocator* allocator) {
+  HeapEntry* FindOrAddEntry(Tagged<Smi> smi, HeapEntriesAllocator* allocator) {
     HeapEntry* entry = FindEntry(smi);
     return entry != nullptr ? entry : AddEntry(smi, allocator);
   }
diff --git a/src/profiler/profile-generator.cc b/src/profiler/profile-generator.cc
index 70aa925f8c2..498cabf4f9d 100644
--- a/src/profiler/profile-generator.cc
+++ b/src/profiler/profile-generator.cc
@@ -205,9 +205,9 @@ void CodeEntry::set_deopt_info(
   rare_data->deopt_inlined_frames_ = std::move(inlined_frames);
 }
 
-void CodeEntry::FillFunctionInfo(SharedFunctionInfo shared) {
+void CodeEntry::FillFunctionInfo(Tagged<SharedFunctionInfo> shared) {
   if (!IsScript(shared->script())) return;
-  Script script = Script::cast(shared->script());
+  Tagged<Script> script = Script::cast(shared->script());
   set_script_id(script->id());
   set_position(shared->StartPosition());
   if (shared->optimization_disabled()) {
diff --git a/src/profiler/profile-generator.h b/src/profiler/profile-generator.h
index bf2e95b88c5..f23de46ea96 100644
--- a/src/profiler/profile-generator.h
+++ b/src/profiler/profile-generator.h
@@ -131,7 +131,7 @@ class CodeEntry {
 
   Address** heap_object_location_address() { return &heap_object_location_; }
 
-  void FillFunctionInfo(SharedFunctionInfo shared);
+  void FillFunctionInfo(Tagged<SharedFunctionInfo> shared);
 
   void SetBuiltinId(Builtin id);
   Builtin builtin() const { return BuiltinField::decode(bit_field_); }
@@ -559,7 +559,9 @@ class V8_EXPORT_PRIVATE CpuProfilesCollection {
   std::vector<std::unique_ptr<CpuProfile>>* profiles() {
     return &finished_profiles_;
   }
-  const char* GetName(Name name) { return resource_names_.GetName(name); }
+  const char* GetName(Tagged<Name> name) {
+    return resource_names_.GetName(name);
+  }
   void RemoveProfile(CpuProfile* profile);
 
   // Finds a common sampling interval dividing each CpuProfile's interval,
diff --git a/src/profiler/profiler-listener.cc b/src/profiler/profiler-listener.cc
index ad5c4b2d756..adc620557af 100644
--- a/src/profiler/profiler-listener.cc
+++ b/src/profiler/profiler-listener.cc
@@ -294,8 +294,8 @@ void ProfilerListener::RegExpCodeCreateEvent(Handle<AbstractCode> code,
   DispatchCodeEvent(evt_rec);
 }
 
-void ProfilerListener::CodeMoveEvent(InstructionStream from,
-                                     InstructionStream to) {
+void ProfilerListener::CodeMoveEvent(Tagged<InstructionStream> from,
+                                     Tagged<InstructionStream> to) {
   DisallowGarbageCollection no_gc;
   CodeEventsContainer evt_rec(CodeEventRecord::Type::kCodeMove);
   CodeMoveEventRecord* rec = &evt_rec.CodeMoveEventRecord_;
@@ -304,7 +304,8 @@ void ProfilerListener::CodeMoveEvent(InstructionStream from,
   DispatchCodeEvent(evt_rec);
 }
 
-void ProfilerListener::BytecodeMoveEvent(BytecodeArray from, BytecodeArray to) {
+void ProfilerListener::BytecodeMoveEvent(Tagged<BytecodeArray> from,
+                                         Tagged<BytecodeArray> to) {
   DisallowGarbageCollection no_gc;
   CodeEventsContainer evt_rec(CodeEventRecord::Type::kCodeMove);
   CodeMoveEventRecord* rec = &evt_rec.CodeMoveEventRecord_;
@@ -367,14 +368,16 @@ const char* ProfilerListener::GetName(base::Vector<const char> name) {
   return GetName(null_terminated.begin());
 }
 
-Name ProfilerListener::InferScriptName(Name name, SharedFunctionInfo info) {
+Tagged<Name> ProfilerListener::InferScriptName(
+    Tagged<Name> name, Tagged<SharedFunctionInfo> info) {
   if (IsString(name) && String::cast(name)->length()) return name;
   if (!IsScript(info->script())) return name;
-  Object source_url = Script::cast(info->script())->source_url();
+  Tagged<Object> source_url = Script::cast(info->script())->source_url();
   return IsName(source_url) ? Name::cast(source_url) : name;
 }
 
-const char* ProfilerListener::GetFunctionName(SharedFunctionInfo shared) {
+const char* ProfilerListener::GetFunctionName(
+    Tagged<SharedFunctionInfo> shared) {
   switch (naming_mode_) {
     case kDebugNaming:
       return GetName(shared->DebugNameCStr().get());
diff --git a/src/profiler/profiler-listener.h b/src/profiler/profiler-listener.h
index eec7b08bfa8..a3dab7ea929 100644
--- a/src/profiler/profiler-listener.h
+++ b/src/profiler/profiler-listener.h
@@ -56,8 +56,10 @@ class V8_EXPORT_PRIVATE ProfilerListener : public LogEventListener,
   void SetterCallbackEvent(Handle<Name> name, Address entry_point) override;
   void RegExpCodeCreateEvent(Handle<AbstractCode> code,
                              Handle<String> source) override;
-  void CodeMoveEvent(InstructionStream from, InstructionStream to) override;
-  void BytecodeMoveEvent(BytecodeArray from, BytecodeArray to) override;
+  void CodeMoveEvent(Tagged<InstructionStream> from,
+                     Tagged<InstructionStream> to) override;
+  void BytecodeMoveEvent(Tagged<BytecodeArray> from,
+                         Tagged<BytecodeArray> to) override;
   void SharedFunctionInfoMoveEvent(Address from, Address to) override {}
   void NativeContextMoveEvent(Address from, Address to) override;
   void CodeMovingGCEvent() override {}
@@ -92,10 +94,11 @@ class V8_EXPORT_PRIVATE ProfilerListener : public LogEventListener,
   void set_observer(CodeEventObserver* observer) { observer_ = observer; }
 
  private:
-  const char* GetFunctionName(SharedFunctionInfo);
+  const char* GetFunctionName(Tagged<SharedFunctionInfo>);
 
   void AttachDeoptInlinedFrames(Handle<Code> code, CodeDeoptEventRecord* rec);
-  Name InferScriptName(Name name, SharedFunctionInfo info);
+  Tagged<Name> InferScriptName(Tagged<Name> name,
+                               Tagged<SharedFunctionInfo> info);
   V8_INLINE void DispatchCodeEvent(const CodeEventsContainer& evt_rec) {
     observer_->CodeEventHandler(evt_rec);
   }
diff --git a/src/profiler/sampling-heap-profiler.cc b/src/profiler/sampling-heap-profiler.cc
index eb89495f3bb..9da56aa3ea5 100644
--- a/src/profiler/sampling-heap-profiler.cc
+++ b/src/profiler/sampling-heap-profiler.cc
@@ -78,7 +78,7 @@ void SamplingHeapProfiler::SampleObject(Address soon_object, size_t size) {
   DCHECK(IsMap(HeapObject::FromAddress(soon_object)->map(isolate_), isolate_));
 
   HandleScope scope(isolate_);
-  HeapObject heap_object = HeapObject::FromAddress(soon_object);
+  Tagged<HeapObject> heap_object = HeapObject::FromAddress(soon_object);
   Handle<Object> obj(heap_object, isolate_);
 
   // Since soon_object can be in code space we can't use v8::Utils::ToLocal.
@@ -161,7 +161,7 @@ SamplingHeapProfiler::AllocationNode* SamplingHeapProfiler::AddStack() {
     // in the top frames of the stack). The allocations made in this
     // sensitive moment belong to the formerly optimized frame anyway.
     if (IsJSFunction(frame->unchecked_function())) {
-      SharedFunctionInfo shared = frame->function()->shared();
+      Tagged<SharedFunctionInfo> shared = frame->function()->shared();
       stack.push_back(shared);
       frames_captured++;
     } else {
@@ -207,11 +207,11 @@ SamplingHeapProfiler::AllocationNode* SamplingHeapProfiler::AddStack() {
   // We need to process the stack in reverse order as the top of the stack is
   // the first element in the list.
   for (auto it = stack.rbegin(); it != stack.rend(); ++it) {
-    SharedFunctionInfo shared = *it;
+    Tagged<SharedFunctionInfo> shared = *it;
     const char* name = this->names()->GetCopy(shared->DebugNameCStr().get());
     int script_id = v8::UnboundScript::kNoScriptId;
     if (IsScript(shared->script())) {
-      Script script = Script::cast(shared->script());
+      Tagged<Script> script = Script::cast(shared->script());
       script_id = script->id();
     }
     node = FindOrAddChildNode(node, name, script_id, shared->StartPosition());
@@ -242,7 +242,7 @@ v8::AllocationProfile::Node* SamplingHeapProfiler::TranslateAllocationNode(
     if (script_iterator != scripts.end()) {
       Handle<Script> script = script_iterator->second;
       if (IsName(script->name())) {
-        Name name = Name::cast(script->name());
+        Tagged<Name> name = Name::cast(script->name());
         script_name = ToApiHandle<v8::String>(
             isolate_->factory()->InternalizeUtf8String(names_->GetName(name)));
       }
@@ -284,7 +284,7 @@ v8::AllocationProfile* SamplingHeapProfiler::GetAllocationProfile() {
   std::map<int, Handle<Script>> scripts;
   {
     Script::Iterator iterator(isolate_);
-    for (Script script = iterator.Next(); !script.is_null();
+    for (Tagged<Script> script = iterator.Next(); !script.is_null();
          script = iterator.Next()) {
       scripts[script->id()] = handle(script, isolate_);
     }
diff --git a/src/profiler/strings-storage.cc b/src/profiler/strings-storage.cc
index 259e86b9ba0..c9b4e44874c 100644
--- a/src/profiler/strings-storage.cc
+++ b/src/profiler/strings-storage.cc
@@ -77,11 +77,11 @@ const char* StringsStorage::GetVFormatted(const char* format, va_list args) {
   return AddOrDisposeString(str.begin(), len);
 }
 
-const char* StringsStorage::GetSymbol(Symbol sym) {
+const char* StringsStorage::GetSymbol(Tagged<Symbol> sym) {
   if (!IsString(sym->description())) {
     return "<symbol>";
   }
-  String description = String::cast(sym->description());
+  Tagged<String> description = String::cast(sym->description());
   int length = std::min(v8_flags.heap_snapshot_string_limit.value(),
                         description->length());
   auto data = description->ToCString(DISALLOW_NULLS, ROBUST_STRING_TRAVERSAL, 0,
@@ -95,9 +95,9 @@ const char* StringsStorage::GetSymbol(Symbol sym) {
   return AddOrDisposeString(str_result, str_length - 1);
 }
 
-const char* StringsStorage::GetName(Name name) {
+const char* StringsStorage::GetName(Tagged<Name> name) {
   if (IsString(name)) {
-    String str = String::cast(name);
+    Tagged<String> str = String::cast(name);
     int length =
         std::min(v8_flags.heap_snapshot_string_limit.value(), str->length());
     int actual_length = 0;
@@ -114,9 +114,9 @@ const char* StringsStorage::GetName(int index) {
   return GetFormatted("%d", index);
 }
 
-const char* StringsStorage::GetConsName(const char* prefix, Name name) {
+const char* StringsStorage::GetConsName(const char* prefix, Tagged<Name> name) {
   if (IsString(name)) {
-    String str = String::cast(name);
+    Tagged<String> str = String::cast(name);
     int length =
         std::min(v8_flags.heap_snapshot_string_limit.value(), str->length());
     int actual_length = 0;
diff --git a/src/profiler/strings-storage.h b/src/profiler/strings-storage.h
index 1d4c2e44d2a..e5f7749aa25 100644
--- a/src/profiler/strings-storage.h
+++ b/src/profiler/strings-storage.h
@@ -33,12 +33,12 @@ class V8_EXPORT_PRIVATE StringsStorage {
   // Returns a formatted string, de-duplicated via the storage.
   PRINTF_FORMAT(2, 3) const char* GetFormatted(const char* format, ...);
   // Returns a stored string resulting from name, or "<symbol>" for a symbol.
-  const char* GetName(Name name);
+  const char* GetName(Tagged<Name> name);
   // Returns the string representation of the int from the store.
   const char* GetName(int index);
   // Appends string resulting from name to prefix, then returns the stored
   // result.
-  const char* GetConsName(const char* prefix, Name name);
+  const char* GetConsName(const char* prefix, Tagged<Name> name);
   // Reduces the refcount of the given string, freeing it if no other
   // references are made to it. Returns true if the string was successfully
   // unref'd, or false if the string was not present in the table.
@@ -61,7 +61,7 @@ class V8_EXPORT_PRIVATE StringsStorage {
   base::CustomMatcherHashMap::Entry* GetEntry(const char* str, int len);
   PRINTF_FORMAT(2, 0)
   const char* GetVFormatted(const char* format, va_list args);
-  const char* GetSymbol(Symbol sym);
+  const char* GetSymbol(Tagged<Symbol> sym);
 
   base::CustomMatcherHashMap names_;
   base::Mutex mutex_;
diff --git a/src/profiler/tick-sample.cc b/src/profiler/tick-sample.cc
index bea3d549de5..85546e215e5 100644
--- a/src/profiler/tick-sample.cc
+++ b/src/profiler/tick-sample.cc
@@ -237,10 +237,10 @@ bool TickSample::GetStackSample(Isolate* v8_isolate, RegisterState* regs,
     sample_info->embedder_state = embedder_state->GetState();
   }
 
-  Context top_context = isolate->context();
+  Tagged<Context> top_context = isolate->context();
   if (top_context.ptr() != i::Context::kNoContext &&
       top_context.ptr() != i::Context::kInvalidContext) {
-    NativeContext top_native_context = top_context->native_context();
+    Tagged<NativeContext> top_native_context = top_context->native_context();
     sample_info->context = reinterpret_cast<void*>(top_native_context.ptr());
   }
 
diff --git a/src/regexp/arm/regexp-macro-assembler-arm.cc b/src/regexp/arm/regexp-macro-assembler-arm.cc
index 3a671a3f249..fa2e345c53e 100644
--- a/src/regexp/arm/regexp-macro-assembler-arm.cc
+++ b/src/regexp/arm/regexp-macro-assembler-arm.cc
@@ -1210,7 +1210,7 @@ static T* frame_entry_address(Address re_frame, int frame_offset) {
 int RegExpMacroAssemblerARM::CheckStackGuardState(Address* return_address,
                                                   Address raw_code,
                                                   Address re_frame) {
-  InstructionStream re_code = InstructionStream::cast(Object(raw_code));
+  Tagged<InstructionStream> re_code = InstructionStream::cast(Object(raw_code));
   return NativeRegExpMacroAssembler::CheckStackGuardState(
       frame_entry<Isolate*>(re_frame, kIsolateOffset),
       frame_entry<int>(re_frame, kStartIndexOffset),
diff --git a/src/regexp/arm64/regexp-macro-assembler-arm64.cc b/src/regexp/arm64/regexp-macro-assembler-arm64.cc
index 9772a7e26d4..566b3f0ba7b 100644
--- a/src/regexp/arm64/regexp-macro-assembler-arm64.cc
+++ b/src/regexp/arm64/regexp-macro-assembler-arm64.cc
@@ -1433,7 +1433,7 @@ static T* frame_entry_address(Address re_frame, int frame_offset) {
 int RegExpMacroAssemblerARM64::CheckStackGuardState(
     Address* return_address, Address raw_code, Address re_frame,
     int start_index, const uint8_t** input_start, const uint8_t** input_end) {
-  InstructionStream re_code = InstructionStream::cast(Object(raw_code));
+  Tagged<InstructionStream> re_code = InstructionStream::cast(Object(raw_code));
   return NativeRegExpMacroAssembler::CheckStackGuardState(
       frame_entry<Isolate*>(re_frame, kIsolateOffset), start_index,
       static_cast<RegExp::CallOrigin>(
diff --git a/src/regexp/experimental/experimental-interpreter.cc b/src/regexp/experimental/experimental-interpreter.cc
index 47e4fedef58..f200b78f194 100644
--- a/src/regexp/experimental/experimental-interpreter.cc
+++ b/src/regexp/experimental/experimental-interpreter.cc
@@ -56,7 +56,7 @@ bool SatisfiesAssertion(RegExpAssertion::Type type,
 }
 
 base::Vector<RegExpInstruction> ToInstructionVector(
-    ByteArray raw_bytes, const DisallowGarbageCollection& no_gc) {
+    Tagged<ByteArray> raw_bytes, const DisallowGarbageCollection& no_gc) {
   RegExpInstruction* inst_begin =
       reinterpret_cast<RegExpInstruction*>(raw_bytes->GetDataStartAddress());
   int inst_num = raw_bytes->length() / sizeof(RegExpInstruction);
@@ -66,11 +66,11 @@ base::Vector<RegExpInstruction> ToInstructionVector(
 
 template <class Character>
 base::Vector<const Character> ToCharacterVector(
-    String str, const DisallowGarbageCollection& no_gc);
+    Tagged<String> str, const DisallowGarbageCollection& no_gc);
 
 template <>
 base::Vector<const uint8_t> ToCharacterVector<uint8_t>(
-    String str, const DisallowGarbageCollection& no_gc) {
+    Tagged<String> str, const DisallowGarbageCollection& no_gc) {
   DCHECK(str->IsFlat());
   String::FlatContent content = str->GetFlatContent(no_gc);
   DCHECK(content.IsOneByte());
@@ -79,7 +79,7 @@ base::Vector<const uint8_t> ToCharacterVector<uint8_t>(
 
 template <>
 base::Vector<const base::uc16> ToCharacterVector<base::uc16>(
-    String str, const DisallowGarbageCollection& no_gc) {
+    Tagged<String> str, const DisallowGarbageCollection& no_gc) {
   DCHECK(str->IsFlat());
   String::FlatContent content = str->GetFlatContent(no_gc);
   DCHECK(content.IsTwoByte());
@@ -137,8 +137,8 @@ class NfaInterpreter {
   // ACCEPTing thread with highest priority.
  public:
   NfaInterpreter(Isolate* isolate, RegExp::CallOrigin call_origin,
-                 ByteArray bytecode, int register_count_per_match, String input,
-                 int32_t input_index, Zone* zone)
+                 Tagged<ByteArray> bytecode, int register_count_per_match,
+                 Tagged<String> input, int32_t input_index, Zone* zone)
       : isolate_(isolate),
         call_origin_(call_origin),
         bytecode_object_(bytecode),
@@ -256,7 +256,7 @@ class NfaInterpreter {
         const bool was_one_byte =
             String::IsOneByteRepresentationUnderneath(input_object_);
 
-        Object result;
+        Tagged<Object> result;
         {
           AllowGarbageCollection yes_gc;
           result = isolate_->stack_guard()->HandleInterrupts();
@@ -557,9 +557,10 @@ class NfaInterpreter {
 }  // namespace
 
 int ExperimentalRegExpInterpreter::FindMatches(
-    Isolate* isolate, RegExp::CallOrigin call_origin, ByteArray bytecode,
-    int register_count_per_match, String input, int start_index,
-    int32_t* output_registers, int output_register_count, Zone* zone) {
+    Isolate* isolate, RegExp::CallOrigin call_origin,
+    Tagged<ByteArray> bytecode, int register_count_per_match,
+    Tagged<String> input, int start_index, int32_t* output_registers,
+    int output_register_count, Zone* zone) {
   DCHECK(input->IsFlat());
   DisallowGarbageCollection no_gc;
 
diff --git a/src/regexp/experimental/experimental-interpreter.h b/src/regexp/experimental/experimental-interpreter.h
index a21b01639a4..6fa4f04334a 100644
--- a/src/regexp/experimental/experimental-interpreter.h
+++ b/src/regexp/experimental/experimental-interpreter.h
@@ -24,9 +24,10 @@ class ExperimentalRegExpInterpreter final : public AllStatic {
   // are written to `matches_out`.  Provided in variants for one-byte and
   // two-byte strings.
   static int FindMatches(Isolate* isolate, RegExp::CallOrigin call_origin,
-                         ByteArray bytecode, int capture_count, String input,
-                         int start_index, int32_t* output_registers,
-                         int output_register_count, Zone* zone);
+                         Tagged<ByteArray> bytecode, int capture_count,
+                         Tagged<String> input, int start_index,
+                         int32_t* output_registers, int output_register_count,
+                         Zone* zone);
 };
 
 }  // namespace internal
diff --git a/src/regexp/experimental/experimental.cc b/src/regexp/experimental/experimental.cc
index 07c9b0518c5..83b64b62840 100644
--- a/src/regexp/experimental/experimental.cc
+++ b/src/regexp/experimental/experimental.cc
@@ -123,7 +123,8 @@ bool ExperimentalRegExp::Compile(Isolate* isolate, Handle<JSRegExp> re) {
   return true;
 }
 
-base::Vector<RegExpInstruction> AsInstructionSequence(ByteArray raw_bytes) {
+base::Vector<RegExpInstruction> AsInstructionSequence(
+    Tagged<ByteArray> raw_bytes) {
   RegExpInstruction* inst_begin =
       reinterpret_cast<RegExpInstruction*>(raw_bytes->GetDataStartAddress());
   int inst_num = raw_bytes->length() / sizeof(RegExpInstruction);
@@ -134,9 +135,9 @@ base::Vector<RegExpInstruction> AsInstructionSequence(ByteArray raw_bytes) {
 namespace {
 
 int32_t ExecRawImpl(Isolate* isolate, RegExp::CallOrigin call_origin,
-                    ByteArray bytecode, String subject, int capture_count,
-                    int32_t* output_registers, int32_t output_register_count,
-                    int32_t subject_index) {
+                    Tagged<ByteArray> bytecode, Tagged<String> subject,
+                    int capture_count, int32_t* output_registers,
+                    int32_t output_register_count, int32_t subject_index) {
   DisallowGarbageCollection no_gc;
   // TODO(cbruni): remove once gcmole is fixed.
   DisableGCMole no_gc_mole;
@@ -156,12 +157,10 @@ int32_t ExecRawImpl(Isolate* isolate, RegExp::CallOrigin call_origin,
 }  // namespace
 
 // Returns the number of matches.
-int32_t ExperimentalRegExp::ExecRaw(Isolate* isolate,
-                                    RegExp::CallOrigin call_origin,
-                                    JSRegExp regexp, String subject,
-                                    int32_t* output_registers,
-                                    int32_t output_register_count,
-                                    int32_t subject_index) {
+int32_t ExperimentalRegExp::ExecRaw(
+    Isolate* isolate, RegExp::CallOrigin call_origin, Tagged<JSRegExp> regexp,
+    Tagged<String> subject, int32_t* output_registers,
+    int32_t output_register_count, int32_t subject_index) {
   DCHECK(v8_flags.enable_experimental_regexp_engine);
   DisallowGarbageCollection no_gc;
 
@@ -171,7 +170,7 @@ int32_t ExperimentalRegExp::ExecRaw(Isolate* isolate,
   }
 
   static constexpr bool kIsLatin1 = true;
-  ByteArray bytecode = ByteArray::cast(regexp->bytecode(kIsLatin1));
+  Tagged<ByteArray> bytecode = ByteArray::cast(regexp->bytecode(kIsLatin1));
 
   return ExecRawImpl(isolate, call_origin, bytecode, subject,
                      regexp->capture_count(), output_registers,
@@ -192,9 +191,9 @@ int32_t ExperimentalRegExp::MatchForCallFromJs(
   DisallowHandleAllocation no_handles;
   DisallowHandleDereference no_deref;
 
-  String subject_string = String::cast(Object(subject));
+  Tagged<String> subject_string = String::cast(Object(subject));
 
-  JSRegExp regexp_obj = JSRegExp::cast(Object(regexp));
+  Tagged<JSRegExp> regexp_obj = JSRegExp::cast(Object(regexp));
 
   return ExecRaw(isolate, RegExp::kFromJs, regexp_obj, subject_string,
                  output_registers, output_register_count, start_position);
diff --git a/src/regexp/experimental/experimental.h b/src/regexp/experimental/experimental.h
index cdc683e97e9..ede9e6b2a25 100644
--- a/src/regexp/experimental/experimental.h
+++ b/src/regexp/experimental/experimental.h
@@ -41,7 +41,7 @@ class ExperimentalRegExp final : public AllStatic {
       int index, Handle<RegExpMatchInfo> last_match_info,
       RegExp::ExecQuirks exec_quirks = RegExp::ExecQuirks::kNone);
   static int32_t ExecRaw(Isolate* isolate, RegExp::CallOrigin call_origin,
-                         JSRegExp regexp, String subject,
+                         Tagged<JSRegExp> regexp, Tagged<String> subject,
                          int32_t* output_registers,
                          int32_t output_register_count, int32_t subject_index);
 
diff --git a/src/regexp/ia32/regexp-macro-assembler-ia32.cc b/src/regexp/ia32/regexp-macro-assembler-ia32.cc
index 2f2075d5590..6ca07a8ebff 100644
--- a/src/regexp/ia32/regexp-macro-assembler-ia32.cc
+++ b/src/regexp/ia32/regexp-macro-assembler-ia32.cc
@@ -1247,7 +1247,7 @@ static T* frame_entry_address(Address re_frame, int frame_offset) {
 int RegExpMacroAssemblerIA32::CheckStackGuardState(Address* return_address,
                                                    Address raw_code,
                                                    Address re_frame) {
-  InstructionStream re_code = InstructionStream::cast(Object(raw_code));
+  Tagged<InstructionStream> re_code = InstructionStream::cast(Object(raw_code));
   return NativeRegExpMacroAssembler::CheckStackGuardState(
       frame_entry<Isolate*>(re_frame, kIsolateOffset),
       frame_entry<int>(re_frame, kStartIndexOffset),
diff --git a/src/regexp/regexp-interpreter.cc b/src/regexp/regexp-interpreter.cc
index c3bb996b3da..33a75b7e76d 100644
--- a/src/regexp/regexp-interpreter.cc
+++ b/src/regexp/regexp-interpreter.cc
@@ -230,9 +230,9 @@ IrregexpInterpreter::Result MaybeThrowStackOverflow(
 template <typename Char>
 void UpdateCodeAndSubjectReferences(
     Isolate* isolate, Handle<ByteArray> code_array,
-    Handle<String> subject_string, ByteArray* code_array_out,
+    Handle<String> subject_string, Tagged<ByteArray>* code_array_out,
     const uint8_t** code_base_out, const uint8_t** pc_out,
-    String* subject_string_out,
+    Tagged<String>* subject_string_out,
     base::Vector<const Char>* subject_string_vector_out) {
   DisallowGarbageCollection no_gc;
 
@@ -253,8 +253,9 @@ void UpdateCodeAndSubjectReferences(
 // necessary.
 template <typename Char>
 IrregexpInterpreter::Result HandleInterrupts(
-    Isolate* isolate, RegExp::CallOrigin call_origin, ByteArray* code_array_out,
-    String* subject_string_out, const uint8_t** code_base_out,
+    Isolate* isolate, RegExp::CallOrigin call_origin,
+    Tagged<ByteArray>* code_array_out, Tagged<String>* subject_string_out,
+    const uint8_t** code_base_out,
     base::Vector<const Char>* subject_string_vector_out,
     const uint8_t** pc_out) {
   DisallowGarbageCollection no_gc;
@@ -384,10 +385,10 @@ bool IndexIsInBounds(int index, int length) {
 
 template <typename Char>
 IrregexpInterpreter::Result RawMatch(
-    Isolate* isolate, ByteArray code_array, String subject_string,
-    base::Vector<const Char> subject, int* output_registers,
-    int output_register_count, int total_register_count, int current,
-    uint32_t current_char, RegExp::CallOrigin call_origin,
+    Isolate* isolate, Tagged<ByteArray> code_array,
+    Tagged<String> subject_string, base::Vector<const Char> subject,
+    int* output_registers, int output_register_count, int total_register_count,
+    int current, uint32_t current_char, RegExp::CallOrigin call_origin,
     const uint32_t backtrack_limit) {
   DisallowGarbageCollection no_gc;
 
@@ -1056,13 +1057,13 @@ IrregexpInterpreter::Result RawMatch(
 
 // static
 IrregexpInterpreter::Result IrregexpInterpreter::Match(
-    Isolate* isolate, JSRegExp regexp, String subject_string,
+    Isolate* isolate, Tagged<JSRegExp> regexp, Tagged<String> subject_string,
     int* output_registers, int output_register_count, int start_position,
     RegExp::CallOrigin call_origin) {
   if (v8_flags.regexp_tier_up) regexp->TierUpTick();
 
   bool is_one_byte = String::IsOneByteRepresentationUnderneath(subject_string);
-  ByteArray code_array = ByteArray::cast(regexp->bytecode(is_one_byte));
+  Tagged<ByteArray> code_array = ByteArray::cast(regexp->bytecode(is_one_byte));
   int total_register_count = regexp->max_register_count();
 
   return MatchInternal(isolate, code_array, subject_string, output_registers,
@@ -1071,10 +1072,10 @@ IrregexpInterpreter::Result IrregexpInterpreter::Match(
 }
 
 IrregexpInterpreter::Result IrregexpInterpreter::MatchInternal(
-    Isolate* isolate, ByteArray code_array, String subject_string,
-    int* output_registers, int output_register_count, int total_register_count,
-    int start_position, RegExp::CallOrigin call_origin,
-    uint32_t backtrack_limit) {
+    Isolate* isolate, Tagged<ByteArray> code_array,
+    Tagged<String> subject_string, int* output_registers,
+    int output_register_count, int total_register_count, int start_position,
+    RegExp::CallOrigin call_origin, uint32_t backtrack_limit) {
   DCHECK(subject_string->IsFlat());
 
   // TODO(chromium:1262676): Remove this CHECK once fixed.
@@ -1131,8 +1132,8 @@ IrregexpInterpreter::Result IrregexpInterpreter::MatchForCallFromJs(
   DisallowHandleAllocation no_handles;
   DisallowHandleDereference no_deref;
 
-  String subject_string = String::cast(Object(subject));
-  JSRegExp regexp_obj = JSRegExp::cast(Object(regexp));
+  Tagged<String> subject_string = String::cast(Object(subject));
+  Tagged<JSRegExp> regexp_obj = JSRegExp::cast(Object(regexp));
 
   if (regexp_obj->MarkedForTierUp()) {
     // Returning RETRY will re-enter through runtime, where actual recompilation
diff --git a/src/regexp/regexp-interpreter.h b/src/regexp/regexp-interpreter.h
index e9dedd781b5..a1fe895a1b0 100644
--- a/src/regexp/regexp-interpreter.h
+++ b/src/regexp/regexp-interpreter.h
@@ -49,17 +49,18 @@ class V8_EXPORT_PRIVATE IrregexpInterpreter : public AllStatic {
                                    RegExp::CallOrigin call_origin,
                                    Isolate* isolate, Address regexp);
 
-  static Result MatchInternal(Isolate* isolate, ByteArray code_array,
-                              String subject_string, int* output_registers,
-                              int output_register_count,
+  static Result MatchInternal(Isolate* isolate, Tagged<ByteArray> code_array,
+                              Tagged<String> subject_string,
+                              int* output_registers, int output_register_count,
                               int total_register_count, int start_position,
                               RegExp::CallOrigin call_origin,
                               uint32_t backtrack_limit);
 
  private:
-  static Result Match(Isolate* isolate, JSRegExp regexp, String subject_string,
-                      int* output_registers, int output_register_count,
-                      int start_position, RegExp::CallOrigin call_origin);
+  static Result Match(Isolate* isolate, Tagged<JSRegExp> regexp,
+                      Tagged<String> subject_string, int* output_registers,
+                      int output_register_count, int start_position,
+                      RegExp::CallOrigin call_origin);
 };
 
 }  // namespace internal
diff --git a/src/regexp/regexp-macro-assembler.cc b/src/regexp/regexp-macro-assembler.cc
index ff68ae80f79..dea1568829c 100644
--- a/src/regexp/regexp-macro-assembler.cc
+++ b/src/regexp/regexp-macro-assembler.cc
@@ -283,8 +283,8 @@ bool NativeRegExpMacroAssembler::CanReadUnaligned() const {
 // static
 int NativeRegExpMacroAssembler::CheckStackGuardState(
     Isolate* isolate, int start_index, RegExp::CallOrigin call_origin,
-    Address* return_address, InstructionStream re_code, Address* subject,
-    const uint8_t** input_start, const uint8_t** input_end) {
+    Address* return_address, Tagged<InstructionStream> re_code,
+    Address* subject, const uint8_t** input_start, const uint8_t** input_end) {
   DisallowGarbageCollection no_gc;
   Address old_pc = PointerAuthentication::AuthenticatePC(return_address, 0);
   DCHECK_LE(re_code->instruction_start(), old_pc);
@@ -328,7 +328,7 @@ int NativeRegExpMacroAssembler::CheckStackGuardState(
       return_value = EXCEPTION;
     } else if (check.InterruptRequested()) {
       AllowGarbageCollection yes_gc;
-      Object result = isolate->stack_guard()->HandleInterrupts();
+      Tagged<Object> result = isolate->stack_guard()->HandleInterrupts();
       if (IsException(result, isolate)) return_value = EXCEPTION;
     }
 
@@ -377,7 +377,7 @@ int NativeRegExpMacroAssembler::Match(Handle<JSRegExp> regexp,
   // DisallowGarbageCollection, since regexps might be preempted, and another
   // thread might do allocation anyway.
 
-  String subject_ptr = *subject;
+  Tagged<String> subject_ptr = *subject;
   // Character offsets into string.
   int start_offset = previous_index;
   int char_length = subject_ptr->length() - start_offset;
@@ -389,7 +389,7 @@ int NativeRegExpMacroAssembler::Match(Handle<JSRegExp> regexp,
     DCHECK_EQ(0, ConsString::cast(subject_ptr)->second()->length());
     subject_ptr = ConsString::cast(subject_ptr)->first();
   } else if (StringShape(subject_ptr).IsSliced()) {
-    SlicedString slice = SlicedString::cast(subject_ptr);
+    Tagged<SlicedString> slice = SlicedString::cast(subject_ptr);
     subject_ptr = slice->parent();
     slice_offset = slice->offset();
   }
@@ -413,9 +413,9 @@ int NativeRegExpMacroAssembler::Match(Handle<JSRegExp> regexp,
 
 // static
 int NativeRegExpMacroAssembler::ExecuteForTesting(
-    String input, int start_offset, const uint8_t* input_start,
+    Tagged<String> input, int start_offset, const uint8_t* input_start,
     const uint8_t* input_end, int* output, int output_size, Isolate* isolate,
-    JSRegExp regexp) {
+    Tagged<JSRegExp> regexp) {
   return Execute(input, start_offset, input_start, input_end, output,
                  output_size, isolate, regexp);
 }
@@ -425,13 +425,14 @@ int NativeRegExpMacroAssembler::ExecuteForTesting(
 // the signature of the interpreter. We should get rid of JS objects passed to
 // internal methods.
 int NativeRegExpMacroAssembler::Execute(
-    String input,  // This needs to be the unpacked (sliced, cons) string.
+    Tagged<String>
+        input,  // This needs to be the unpacked (sliced, cons) string.
     int start_offset, const uint8_t* input_start, const uint8_t* input_end,
-    int* output, int output_size, Isolate* isolate, JSRegExp regexp) {
+    int* output, int output_size, Isolate* isolate, Tagged<JSRegExp> regexp) {
   RegExpStackScope stack_scope(isolate);
 
   bool is_one_byte = String::IsOneByteRepresentationUnderneath(input);
-  Code code = Code::cast(regexp->code(is_one_byte));
+  Tagged<Code> code = Code::cast(regexp->code(is_one_byte));
   RegExp::CallOrigin call_origin = RegExp::CallOrigin::kFromRuntime;
 
   using RegexpMatcherSig =
diff --git a/src/regexp/regexp-macro-assembler.h b/src/regexp/regexp-macro-assembler.h
index 4d69a776bac..932c90a7e11 100644
--- a/src/regexp/regexp-macro-assembler.h
+++ b/src/regexp/regexp-macro-assembler.h
@@ -303,12 +303,10 @@ class NativeRegExpMacroAssembler: public RegExpMacroAssembler {
                    int* offsets_vector, int offsets_vector_length,
                    int previous_index, Isolate* isolate);
 
-  V8_EXPORT_PRIVATE static int ExecuteForTesting(String input, int start_offset,
-                                                 const uint8_t* input_start,
-                                                 const uint8_t* input_end,
-                                                 int* output, int output_size,
-                                                 Isolate* isolate,
-                                                 JSRegExp regexp);
+  V8_EXPORT_PRIVATE static int ExecuteForTesting(
+      Tagged<String> input, int start_offset, const uint8_t* input_start,
+      const uint8_t* input_end, int* output, int output_size, Isolate* isolate,
+      Tagged<JSRegExp> regexp);
 
   bool CanReadUnaligned() const override;
 
@@ -332,8 +330,8 @@ class NativeRegExpMacroAssembler: public RegExpMacroAssembler {
   static int CheckStackGuardState(Isolate* isolate, int start_index,
                                   RegExp::CallOrigin call_origin,
                                   Address* return_address,
-                                  InstructionStream re_code, Address* subject,
-                                  const uint8_t** input_start,
+                                  Tagged<InstructionStream> re_code,
+                                  Address* subject, const uint8_t** input_start,
                                   const uint8_t** input_end);
 
   static Address word_character_map_address() {
@@ -350,9 +348,10 @@ class NativeRegExpMacroAssembler: public RegExpMacroAssembler {
 
  private:
   // Returns a {Result} sentinel, or the number of successful matches.
-  static int Execute(String input, int start_offset, const uint8_t* input_start,
-                     const uint8_t* input_end, int* output, int output_size,
-                     Isolate* isolate, JSRegExp regexp);
+  static int Execute(Tagged<String> input, int start_offset,
+                     const uint8_t* input_start, const uint8_t* input_end,
+                     int* output, int output_size, Isolate* isolate,
+                     Tagged<JSRegExp> regexp);
 
   ZoneUnorderedMap<uint32_t, Handle<FixedUInt16Array>> range_array_cache_;
 };
diff --git a/src/regexp/regexp.cc b/src/regexp/regexp.cc
index 1bb2b615e3d..b3610922b49 100644
--- a/src/regexp/regexp.cc
+++ b/src/regexp/regexp.cc
@@ -95,11 +95,13 @@ class RegExpImpl final : public AllStatic {
                       uint32_t& backtrack_limit);
 
   // For acting on the JSRegExp data FixedArray.
-  static int IrregexpMaxRegisterCount(FixedArray re);
-  static void SetIrregexpMaxRegisterCount(FixedArray re, int value);
-  static int IrregexpNumberOfCaptures(FixedArray re);
-  static ByteArray IrregexpByteCode(FixedArray re, bool is_one_byte);
-  static Code IrregexpNativeCode(FixedArray re, bool is_one_byte);
+  static int IrregexpMaxRegisterCount(Tagged<FixedArray> re);
+  static void SetIrregexpMaxRegisterCount(Tagged<FixedArray> re, int value);
+  static int IrregexpNumberOfCaptures(Tagged<FixedArray> re);
+  static Tagged<ByteArray> IrregexpByteCode(Tagged<FixedArray> re,
+                                            bool is_one_byte);
+  static Tagged<Code> IrregexpNativeCode(Tagged<FixedArray> re,
+                                         bool is_one_byte);
 };
 
 // static
@@ -348,8 +350,8 @@ void RegExpImpl::AtomCompile(Isolate* isolate, Handle<JSRegExp> re,
 namespace {
 
 void SetAtomLastCapture(Isolate* isolate,
-                        Handle<RegExpMatchInfo> last_match_info, String subject,
-                        int from, int to) {
+                        Handle<RegExpMatchInfo> last_match_info,
+                        Tagged<String> subject, int from, int to) {
   SealHandleScope shs(isolate);
   last_match_info->SetNumberOfCaptureRegisters(2);
   last_match_info->SetLastSubject(subject);
@@ -369,7 +371,7 @@ int RegExpImpl::AtomExecRaw(Isolate* isolate, Handle<JSRegExp> regexp,
   subject = String::Flatten(isolate, subject);
   DisallowGarbageCollection no_gc;  // ensure vectors stay valid
 
-  String needle = regexp->atom_pattern();
+  Tagged<String> needle = regexp->atom_pattern();
   int needle_len = needle->length();
   DCHECK(needle->IsFlat());
   DCHECK_LT(0, needle_len);
@@ -437,8 +439,8 @@ Handle<Object> RegExpImpl::AtomExec(Isolate* isolate, Handle<JSRegExp> re,
 bool RegExpImpl::EnsureCompiledIrregexp(Isolate* isolate, Handle<JSRegExp> re,
                                         Handle<String> sample_subject,
                                         bool is_one_byte) {
-  Object compiled_code = re->code(is_one_byte);
-  Object bytecode = re->bytecode(is_one_byte);
+  Tagged<Object> compiled_code = re->code(is_one_byte);
+  Tagged<Object> bytecode = re->bytecode(is_one_byte);
   bool needs_initial_compilation =
       compiled_code == Smi::FromInt(JSRegExp::kUninitializedValue);
   // Recompile is needed when we're dealing with the first execution of the
@@ -467,8 +469,8 @@ namespace {
 
 #ifdef DEBUG
 bool RegExpCodeIsValidForPreCompilation(Handle<JSRegExp> re, bool is_one_byte) {
-  Object entry = re->code(is_one_byte);
-  Object bytecode = re->bytecode(is_one_byte);
+  Tagged<Object> entry = re->code(is_one_byte);
+  Tagged<Object> bytecode = re->bytecode(is_one_byte);
   // If we're not using the tier-up strategy, entry can only be a smi
   // representing an uncompiled regexp here. If we're using the tier-up
   // strategy, entry can still be a smi representing an uncompiled regexp, when
@@ -579,7 +581,7 @@ bool RegExpImpl::CompileIrregexp(Isolate* isolate, Handle<JSRegExp> re,
   Handle<FixedArray> data =
       Handle<FixedArray>(FixedArray::cast(re->data()), isolate);
   if (compile_data.compilation_target == RegExpCompilationTarget::kNative) {
-    Code code = Code::cast(*compile_data.code);
+    Tagged<Code> code = Code::cast(*compile_data.code);
     data->set(JSRegExp::code_index(is_one_byte), code);
 
     // Reset bytecode to uninitialized. In case we use tier-up we know that
@@ -617,23 +619,25 @@ bool RegExpImpl::CompileIrregexp(Isolate* isolate, Handle<JSRegExp> re,
   return true;
 }
 
-int RegExpImpl::IrregexpMaxRegisterCount(FixedArray re) {
+int RegExpImpl::IrregexpMaxRegisterCount(Tagged<FixedArray> re) {
   return Smi::ToInt(re->get(JSRegExp::kIrregexpMaxRegisterCountIndex));
 }
 
-void RegExpImpl::SetIrregexpMaxRegisterCount(FixedArray re, int value) {
+void RegExpImpl::SetIrregexpMaxRegisterCount(Tagged<FixedArray> re, int value) {
   re->set(JSRegExp::kIrregexpMaxRegisterCountIndex, Smi::FromInt(value));
 }
 
-int RegExpImpl::IrregexpNumberOfCaptures(FixedArray re) {
+int RegExpImpl::IrregexpNumberOfCaptures(Tagged<FixedArray> re) {
   return Smi::ToInt(re->get(JSRegExp::kIrregexpCaptureCountIndex));
 }
 
-ByteArray RegExpImpl::IrregexpByteCode(FixedArray re, bool is_one_byte) {
+Tagged<ByteArray> RegExpImpl::IrregexpByteCode(Tagged<FixedArray> re,
+                                               bool is_one_byte) {
   return ByteArray::cast(re->get(JSRegExp::bytecode_index(is_one_byte)));
 }
 
-Code RegExpImpl::IrregexpNativeCode(FixedArray re, bool is_one_byte) {
+Tagged<Code> RegExpImpl::IrregexpNativeCode(Tagged<FixedArray> re,
+                                            bool is_one_byte) {
   return Code::cast(re->get(JSRegExp::code_index(is_one_byte)));
 }
 
@@ -1217,11 +1221,11 @@ int32_t* RegExpGlobalCache::LastSuccessfulMatch() {
   return &register_array_[index];
 }
 
-Object RegExpResultsCache::Lookup(Heap* heap, String key_string,
-                                  Object key_pattern,
-                                  FixedArray* last_match_cache,
-                                  ResultsCacheType type) {
-  FixedArray cache;
+Tagged<Object> RegExpResultsCache::Lookup(Heap* heap, Tagged<String> key_string,
+                                          Tagged<Object> key_pattern,
+                                          FixedArray* last_match_cache,
+                                          ResultsCacheType type) {
+  Tagged<FixedArray> cache;
   if (!IsInternalizedString(key_string)) return Smi::zero();
   if (type == STRING_SPLIT_SUBSTRINGS) {
     DCHECK(IsString(key_pattern));
@@ -1309,7 +1313,7 @@ void RegExpResultsCache::Enter(Isolate* isolate, Handle<String> key_string,
       ReadOnlyRoots(isolate).fixed_cow_array_map());
 }
 
-void RegExpResultsCache::Clear(FixedArray cache) {
+void RegExpResultsCache::Clear(Tagged<FixedArray> cache) {
   for (int i = 0; i < kRegExpResultsCacheSize; i++) {
     cache->set(i, Smi::zero());
   }
diff --git a/src/regexp/regexp.h b/src/regexp/regexp.h
index 2d9435d7051..b82b1b6fe80 100644
--- a/src/regexp/regexp.h
+++ b/src/regexp/regexp.h
@@ -214,14 +214,16 @@ class RegExpResultsCache final : public AllStatic {
 
   // Attempt to retrieve a cached result.  On failure, 0 is returned as a Smi.
   // On success, the returned result is guaranteed to be a COW-array.
-  static Object Lookup(Heap* heap, String key_string, Object key_pattern,
-                       FixedArray* last_match_out, ResultsCacheType type);
+  static Tagged<Object> Lookup(Heap* heap, Tagged<String> key_string,
+                               Tagged<Object> key_pattern,
+                               FixedArray* last_match_out,
+                               ResultsCacheType type);
   // Attempt to add value_array to the cache specified by type.  On success,
   // value_array is turned into a COW-array.
   static void Enter(Isolate* isolate, Handle<String> key_string,
                     Handle<Object> key_pattern, Handle<FixedArray> value_array,
                     Handle<FixedArray> last_match_cache, ResultsCacheType type);
-  static void Clear(FixedArray cache);
+  static void Clear(Tagged<FixedArray> cache);
 
   static constexpr int kRegExpResultsCacheSize = 0x100;
 
diff --git a/src/regexp/x64/regexp-macro-assembler-x64.cc b/src/regexp/x64/regexp-macro-assembler-x64.cc
index de474716f02..7c881f72b25 100644
--- a/src/regexp/x64/regexp-macro-assembler-x64.cc
+++ b/src/regexp/x64/regexp-macro-assembler-x64.cc
@@ -1310,7 +1310,7 @@ static T* frame_entry_address(Address re_frame, int frame_offset) {
 int RegExpMacroAssemblerX64::CheckStackGuardState(Address* return_address,
                                                   Address raw_code,
                                                   Address re_frame) {
-  InstructionStream re_code = InstructionStream::cast(Object(raw_code));
+  Tagged<InstructionStream> re_code = InstructionStream::cast(Object(raw_code));
   return NativeRegExpMacroAssembler::CheckStackGuardState(
       frame_entry<Isolate*>(re_frame, kIsolateOffset),
       frame_entry<int>(re_frame, kStartIndexOffset),
diff --git a/src/roots/roots-inl.h b/src/roots/roots-inl.h
index f4c033f4955..11fb3392107 100644
--- a/src/roots/roots-inl.h
+++ b/src/roots/roots-inl.h
@@ -125,7 +125,7 @@ Address ReadOnlyRoots::last_name_for_protector() const {
   return address_at(RootIndex::kLastNameForProtector);
 }
 
-bool ReadOnlyRoots::IsNameForProtector(HeapObject object) const {
+bool ReadOnlyRoots::IsNameForProtector(Tagged<HeapObject> object) const {
   return base::IsInRange(object.ptr(), first_name_for_protector(),
                          last_name_for_protector());
 }
diff --git a/src/roots/roots.cc b/src/roots/roots.cc
index adbc5022c5d..c5df21c3fae 100644
--- a/src/roots/roots.cc
+++ b/src/roots/roots.cc
@@ -48,10 +48,10 @@ void ReadOnlyRoots::Iterate(RootVisitor* visitor) {
 #ifdef DEBUG
 void ReadOnlyRoots::VerifyNameForProtectors() {
   DisallowGarbageCollection no_gc;
-  Name prev;
+  Tagged<Name> prev;
   for (RootIndex root_index = RootIndex::kFirstNameForProtector;
        root_index <= RootIndex::kLastNameForProtector; ++root_index) {
-    Name current = Name::cast(object_at(root_index));
+    Tagged<Name> current = Name::cast(object_at(root_index));
     DCHECK(IsNameForProtector(current));
     if (root_index != RootIndex::kFirstNameForProtector) {
       // Make sure the objects are adjacent in memory.
diff --git a/src/roots/roots.h b/src/roots/roots.h
index 322dbb8cc68..c3fbd291879 100644
--- a/src/roots/roots.h
+++ b/src/roots/roots.h
@@ -649,7 +649,7 @@ class ReadOnlyRoots {
   READ_ONLY_ROOT_LIST(ROOT_ACCESSOR)
 #undef ROOT_ACCESSOR
 
-  V8_INLINE bool IsNameForProtector(HeapObject object) const;
+  V8_INLINE bool IsNameForProtector(Tagged<HeapObject> object) const;
   V8_INLINE void VerifyNameForProtectorsPages() const;
 #ifdef DEBUG
   void VerifyNameForProtectors();
diff --git a/src/runtime/runtime-array.cc b/src/runtime/runtime-array.cc
index 2f6271f8300..6f9dde75bba 100644
--- a/src/runtime/runtime-array.cc
+++ b/src/runtime/runtime-array.cc
@@ -207,7 +207,7 @@ RUNTIME_FUNCTION(Runtime_ArrayIsArray) {
 RUNTIME_FUNCTION(Runtime_IsArray) {
   SealHandleScope shs(isolate);
   DCHECK_EQ(1, args.length());
-  Object obj = args[0];
+  Tagged<Object> obj = args[0];
   return isolate->heap()->ToBoolean(IsJSArray(obj));
 }
 
diff --git a/src/runtime/runtime-compiler.cc b/src/runtime/runtime-compiler.cc
index 60d32a27471..f2d3fb50582 100644
--- a/src/runtime/runtime-compiler.cc
+++ b/src/runtime/runtime-compiler.cc
@@ -85,7 +85,7 @@ RUNTIME_FUNCTION(Runtime_InstallBaselineCode) {
                                             &is_compiled_scope);
   {
     DisallowGarbageCollection no_gc;
-    Code baseline_code = sfi->baseline_code(kAcquireLoad);
+    Tagged<Code> baseline_code = sfi->baseline_code(kAcquireLoad);
     function->set_code(baseline_code);
     if V8_LIKELY (!v8_flags.log_function_events) return baseline_code;
   }
@@ -219,10 +219,10 @@ RUNTIME_FUNCTION(Runtime_InstantiateAsmJs) {
 
 namespace {
 
-bool TryGetOptimizedOsrCode(Isolate* isolate, FeedbackVector vector,
+bool TryGetOptimizedOsrCode(Isolate* isolate, Tagged<FeedbackVector> vector,
                             const interpreter::BytecodeArrayIterator& it,
-                            Code* code_out) {
-  base::Optional<Code> maybe_code =
+                            Tagged<Code>* code_out) {
+  base::Optional<Tagged<Code>> maybe_code =
       vector->GetOptimizedOsrCode(isolate, it.GetSlotOperand(2));
   if (maybe_code.has_value()) {
     *code_out = maybe_code.value();
@@ -242,7 +242,8 @@ bool TryGetOptimizedOsrCode(Isolate* isolate, FeedbackVector vector,
 //      }  // Type b: deopt exit < loop start < OSR backedge
 //    } // Type c: loop start < deopt exit < OSR backedge
 //  }  // The outermost loop
-void DeoptAllOsrLoopsContainingDeoptExit(Isolate* isolate, JSFunction function,
+void DeoptAllOsrLoopsContainingDeoptExit(Isolate* isolate,
+                                         Tagged<JSFunction> function,
                                          BytecodeOffset deopt_exit_offset) {
   DisallowGarbageCollection no_gc;
   DCHECK(!deopt_exit_offset.IsNone());
@@ -259,8 +260,8 @@ void DeoptAllOsrLoopsContainingDeoptExit(Isolate* isolate, JSFunction function,
   interpreter::BytecodeArrayIterator it(bytecode_array,
                                         deopt_exit_offset.ToInt());
 
-  FeedbackVector vector = function->feedback_vector();
-  Code code;
+  Tagged<FeedbackVector> vector = function->feedback_vector();
+  Tagged<Code> code;
   base::SmallVector<Code, 8> osr_codes;
   // Visit before the first loop-with-deopt is found
   for (; !it.done(); it.Advance()) {
@@ -443,8 +444,10 @@ void GetOsrOffsetAndFunctionForOSR(Isolate* isolate, BytecodeOffset* osr_offset,
   DCHECK((*function)->shared()->HasBytecodeArray());
 }
 
-Object CompileOptimizedOSR(Isolate* isolate, Handle<JSFunction> function,
-                           CodeKind min_opt_level, BytecodeOffset osr_offset) {
+Tagged<Object> CompileOptimizedOSR(Isolate* isolate,
+                                   Handle<JSFunction> function,
+                                   CodeKind min_opt_level,
+                                   BytecodeOffset osr_offset) {
   const ConcurrencyMode mode =
       V8_LIKELY(isolate->concurrent_recompilation_enabled() &&
                 v8_flags.concurrent_osr)
@@ -467,7 +470,7 @@ Object CompileOptimizedOSR(Isolate* isolate, Handle<JSFunction> function,
       function->set_code(function->shared()->GetCode(isolate));
     }
 
-    return {};
+    return Smi::zero();
   }
 
   DCHECK(!result.is_null());
@@ -475,7 +478,7 @@ Object CompileOptimizedOSR(Isolate* isolate, Handle<JSFunction> function,
   DCHECK(CodeKindIsOptimizedJSFunction(result->kind()));
 
 #ifdef DEBUG
-  DeoptimizationData data =
+  Tagged<DeoptimizationData> data =
       DeoptimizationData::cast(result->deoptimization_data());
   DCHECK_EQ(BytecodeOffset(data->OsrBytecodeOffset().value()), osr_offset);
   DCHECK_GE(data->OsrPcOffset().value(), 0);
@@ -501,9 +504,9 @@ RUNTIME_FUNCTION(Runtime_CompileOptimizedOSR) {
 
 namespace {
 
-Object CompileOptimizedOSRFromMaglev(Isolate* isolate,
-                                     Handle<JSFunction> function,
-                                     BytecodeOffset osr_offset) {
+Tagged<Object> CompileOptimizedOSRFromMaglev(Isolate* isolate,
+                                             Handle<JSFunction> function,
+                                             BytecodeOffset osr_offset) {
   // This path is only relevant for tests (all production configurations enable
   // concurrent OSR). It's quite subtle, if interested read on:
   if (V8_UNLIKELY(!isolate->concurrent_recompilation_enabled() ||
@@ -596,11 +599,12 @@ RUNTIME_FUNCTION(Runtime_LogOrTraceOptimizedOSREntry) {
   return ReadOnlyRoots(isolate).undefined_value();
 }
 
-static Object CompileGlobalEval(Isolate* isolate,
-                                Handle<i::Object> source_object,
-                                Handle<SharedFunctionInfo> outer_info,
-                                LanguageMode language_mode,
-                                int eval_scope_position, int eval_position) {
+static Tagged<Object> CompileGlobalEval(Isolate* isolate,
+                                        Handle<i::Object> source_object,
+                                        Handle<SharedFunctionInfo> outer_info,
+                                        LanguageMode language_mode,
+                                        int eval_scope_position,
+                                        int eval_position) {
   Handle<NativeContext> native_context = isolate->native_context();
 
   // Check if native context allows code generation from
diff --git a/src/runtime/runtime-debug.cc b/src/runtime/runtime-debug.cc
index 9af131065e2..029f447fc4e 100644
--- a/src/runtime/runtime-debug.cc
+++ b/src/runtime/runtime-debug.cc
@@ -57,7 +57,7 @@ RUNTIME_FUNCTION_RETURN_PAIR(Runtime_DebugBreakOnBytecode) {
   // If the user requested to restart a frame, there is no need
   // to get the return value or check the bytecode for side-effects.
   if (isolate->debug()->IsRestartFrameScheduled()) {
-    Object exception = isolate->TerminateExecution();
+    Tagged<Object> exception = isolate->TerminateExecution();
     return MakePair(exception,
                     Smi::FromInt(static_cast<uint8_t>(Bytecode::kIllegal)));
   }
@@ -75,8 +75,8 @@ RUNTIME_FUNCTION_RETURN_PAIR(Runtime_DebugBreakOnBytecode) {
 
   // Make sure to only access these objects after the side effect check, as the
   // check can allocate on failure.
-  SharedFunctionInfo shared = interpreted_frame->function()->shared();
-  BytecodeArray bytecode_array = shared->GetBytecodeArray(isolate);
+  Tagged<SharedFunctionInfo> shared = interpreted_frame->function()->shared();
+  Tagged<BytecodeArray> bytecode_array = shared->GetBytecodeArray(isolate);
   int bytecode_offset = interpreted_frame->GetBytecodeOffset();
   Bytecode bytecode = Bytecodes::FromByte(bytecode_array->get(bytecode_offset));
 
@@ -100,7 +100,7 @@ RUNTIME_FUNCTION_RETURN_PAIR(Runtime_DebugBreakOnBytecode) {
     return MakePair(ReadOnlyRoots(isolate).exception(),
                     Smi::FromInt(static_cast<uint8_t>(bytecode)));
   }
-  Object interrupt_object = isolate->stack_guard()->HandleInterrupts();
+  Tagged<Object> interrupt_object = isolate->stack_guard()->HandleInterrupts();
   if (IsException(interrupt_object, isolate)) {
     return MakePair(interrupt_object,
                     Smi::FromInt(static_cast<uint8_t>(bytecode)));
@@ -537,7 +537,7 @@ RUNTIME_FUNCTION(Runtime_FunctionGetInferredName) {
   SealHandleScope shs(isolate);
   DCHECK_EQ(1, args.length());
 
-  Object f = args[0];
+  Tagged<Object> f = args[0];
   if (IsJSFunction(f)) {
     return JSFunction::cast(f)->shared()->inferred_name();
   }
@@ -569,7 +569,7 @@ int ScriptLinePosition(Handle<Script> script, int line) {
 
   Script::InitLineEnds(script->GetIsolate(), script);
 
-  FixedArray line_ends_array = FixedArray::cast(script->line_ends());
+  Tagged<FixedArray> line_ends_array = FixedArray::cast(script->line_ends());
   const int line_count = line_ends_array->length();
   DCHECK_LT(0, line_count);
 
@@ -662,7 +662,7 @@ Handle<Object> ScriptLocationFromLine(Isolate* isolate, Handle<Script> script,
 // Slow traversal over all scripts on the heap.
 bool GetScriptById(Isolate* isolate, int needle, Handle<Script>* result) {
   Script::Iterator iterator(isolate);
-  for (Script script = iterator.Next(); !script.is_null();
+  for (Tagged<Script> script = iterator.Next(); !script.is_null();
        script = iterator.Next()) {
     if (script->id() == needle) {
       *result = handle(script, isolate);
diff --git a/src/runtime/runtime-function.cc b/src/runtime/runtime-function.cc
index 126cbee1214..2ea763ed1da 100644
--- a/src/runtime/runtime-function.cc
+++ b/src/runtime/runtime-function.cc
@@ -88,7 +88,7 @@ RUNTIME_FUNCTION(Runtime_Call) {
 RUNTIME_FUNCTION(Runtime_IsFunction) {
   SealHandleScope shs(isolate);
   DCHECK_EQ(1, args.length());
-  Object object = args[0];
+  Tagged<Object> object = args[0];
   return isolate->heap()->ToBoolean(IsFunction(object));
 }
 
diff --git a/src/runtime/runtime-internal.cc b/src/runtime/runtime-internal.cc
index 1ad36b46f3f..abf04ae6699 100644
--- a/src/runtime/runtime-internal.cc
+++ b/src/runtime/runtime-internal.cc
@@ -393,9 +393,9 @@ RUNTIME_FUNCTION(Runtime_StackGuardWithGap) {
 
 namespace {
 
-Object BytecodeBudgetInterruptWithStackCheck(Isolate* isolate,
-                                             RuntimeArguments& args,
-                                             CodeKind code_kind) {
+Tagged<Object> BytecodeBudgetInterruptWithStackCheck(Isolate* isolate,
+                                                     RuntimeArguments& args,
+                                                     CodeKind code_kind) {
   HandleScope scope(isolate);
   DCHECK_EQ(1, args.length());
   Handle<JSFunction> function = args.at<JSFunction>(0);
@@ -410,7 +410,7 @@ Object BytecodeBudgetInterruptWithStackCheck(Isolate* isolate,
     // the runtime function call being what overflows the stack.
     return isolate->StackOverflow();
   } else if (check.InterruptRequested()) {
-    Object return_value = isolate->stack_guard()->HandleInterrupts();
+    Tagged<Object> return_value = isolate->stack_guard()->HandleInterrupts();
     if (!IsUndefined(return_value, isolate)) {
       return return_value;
     }
@@ -420,8 +420,8 @@ Object BytecodeBudgetInterruptWithStackCheck(Isolate* isolate,
   return ReadOnlyRoots(isolate).undefined_value();
 }
 
-Object BytecodeBudgetInterrupt(Isolate* isolate, RuntimeArguments& args,
-                               CodeKind code_kind) {
+Tagged<Object> BytecodeBudgetInterrupt(Isolate* isolate, RuntimeArguments& args,
+                                       CodeKind code_kind) {
   HandleScope scope(isolate);
   DCHECK_EQ(1, args.length());
   Handle<JSFunction> function = args.at<JSFunction>(0);
diff --git a/src/runtime/runtime-literals.cc b/src/runtime/runtime-literals.cc
index ba056a9e22b..962e5e3f7ab 100644
--- a/src/runtime/runtime-literals.cc
+++ b/src/runtime/runtime-literals.cc
@@ -18,7 +18,7 @@ namespace internal {
 
 namespace {
 
-bool IsUninitializedLiteralSite(Object literal_site) {
+bool IsUninitializedLiteralSite(Tagged<Object> literal_site) {
   return literal_site == Smi::zero();
 }
 
@@ -112,7 +112,7 @@ MaybeHandle<JSObject> JSObjectWalkVisitor<ContextObject>::StructureWalk(
         FieldIndex index = FieldIndex::ForPropertyIndex(
             copy->map(isolate), details.field_index(),
             details.representation());
-        Object raw = copy->RawFastPropertyAt(isolate, index);
+        Tagged<Object> raw = copy->RawFastPropertyAt(isolate, index);
         if (IsJSObject(raw, isolate)) {
           Handle<JSObject> value(JSObject::cast(raw), isolate);
           ASSIGN_RETURN_ON_EXCEPTION(
@@ -130,7 +130,7 @@ MaybeHandle<JSObject> JSObjectWalkVisitor<ContextObject>::StructureWalk(
         Handle<SwissNameDictionary> dict(
             copy->property_dictionary_swiss(isolate), isolate);
         for (InternalIndex i : dict->IterateEntries()) {
-          Object raw = dict->ValueAt(i);
+          Tagged<Object> raw = dict->ValueAt(i);
           if (!IsJSObject(raw, isolate)) continue;
           DCHECK(IsName(dict->KeyAt(i)));
           Handle<JSObject> value(JSObject::cast(raw), isolate);
@@ -142,7 +142,7 @@ MaybeHandle<JSObject> JSObjectWalkVisitor<ContextObject>::StructureWalk(
         Handle<NameDictionary> dict(copy->property_dictionary(isolate),
                                     isolate);
         for (InternalIndex i : dict->IterateEntries()) {
-          Object raw = dict->ValueAt(isolate, i);
+          Tagged<Object> raw = dict->ValueAt(isolate, i);
           if (!IsJSObject(raw, isolate)) continue;
           DCHECK(IsName(dict->KeyAt(isolate, i)));
           Handle<JSObject> value(JSObject::cast(raw), isolate);
@@ -179,7 +179,7 @@ MaybeHandle<JSObject> JSObjectWalkVisitor<ContextObject>::StructureWalk(
 #endif
       } else {
         for (int i = 0; i < elements->length(); i++) {
-          Object raw = elements->get(isolate, i);
+          Tagged<Object> raw = elements->get(isolate, i);
           if (!IsJSObject(raw, isolate)) continue;
           Handle<JSObject> value(JSObject::cast(raw), isolate);
           ASSIGN_RETURN_ON_EXCEPTION(
@@ -193,7 +193,7 @@ MaybeHandle<JSObject> JSObjectWalkVisitor<ContextObject>::StructureWalk(
       Handle<NumberDictionary> element_dictionary(
           copy->element_dictionary(isolate), isolate);
       for (InternalIndex i : element_dictionary->IterateEntries()) {
-        Object raw = element_dictionary->ValueAt(isolate, i);
+        Tagged<Object> raw = element_dictionary->ValueAt(isolate, i);
         if (!IsJSObject(raw, isolate)) continue;
         Handle<JSObject> value(JSObject::cast(raw), isolate);
         ASSIGN_RETURN_ON_EXCEPTION(
@@ -482,8 +482,8 @@ Handle<JSObject> CreateArrayLiteral(
           isolate->factory()->CopyFixedArray(fixed_array_values);
       copied_elements_values = fixed_array_values_copy;
       for (int i = 0; i < fixed_array_values->length(); i++) {
-        Object value = fixed_array_values_copy->get(isolate, i);
-        HeapObject value_heap_object;
+        Tagged<Object> value = fixed_array_values_copy->get(isolate, i);
+        Tagged<HeapObject> value_heap_object;
         if (value.GetHeapObject(isolate, &value_heap_object)) {
           if (IsArrayBoilerplateDescription(value_heap_object, isolate)) {
             HandleScope sub_scope(isolate);
diff --git a/src/runtime/runtime-promise.cc b/src/runtime/runtime-promise.cc
index 61b313c938b..c8e030d9bc1 100644
--- a/src/runtime/runtime-promise.cc
+++ b/src/runtime/runtime-promise.cc
@@ -89,8 +89,8 @@ RUNTIME_FUNCTION(Runtime_PerformMicrotaskCheckpoint) {
 RUNTIME_FUNCTION(Runtime_RunMicrotaskCallback) {
   HandleScope scope(isolate);
   DCHECK_EQ(2, args.length());
-  Object microtask_callback = args[0];
-  Object microtask_data = args[1];
+  Tagged<Object> microtask_callback = args[0];
+  Tagged<Object> microtask_data = args[1];
   MicrotaskCallback callback = ToCData<MicrotaskCallback>(microtask_callback);
   void* data = ToCData<void*>(microtask_data);
   callback(data);
diff --git a/src/runtime/runtime-proxy.cc b/src/runtime/runtime-proxy.cc
index a27740d6d42..627f405f28d 100644
--- a/src/runtime/runtime-proxy.cc
+++ b/src/runtime/runtime-proxy.cc
@@ -14,7 +14,7 @@ namespace internal {
 RUNTIME_FUNCTION(Runtime_IsJSProxy) {
   SealHandleScope shs(isolate);
   DCHECK_EQ(1, args.length());
-  Object obj = args[0];
+  Tagged<Object> obj = args[0];
   return isolate->heap()->ToBoolean(IsJSProxy(obj));
 }
 
diff --git a/src/runtime/runtime-test-wasm.cc b/src/runtime/runtime-test-wasm.cc
index c3a11055f44..90a6656d988 100644
--- a/src/runtime/runtime-test-wasm.cc
+++ b/src/runtime/runtime-test-wasm.cc
@@ -149,7 +149,7 @@ RUNTIME_FUNCTION(Runtime_CountUnoptimizedWasmToJSWrapper) {
   Handle<WasmInstanceObject> instance = args.at<WasmInstanceObject>(0);
   Address wrapper_start = isolate->builtins()
                               ->code(Builtin::kWasmToJsWrapperAsm)
-                              .instruction_start();
+                              ->instruction_start();
   int result = 0;
   int import_count = instance->imported_function_targets()->length();
   for (int i = 0; i < import_count; ++i) {
@@ -194,12 +194,13 @@ RUNTIME_FUNCTION(Runtime_HasUnoptimizedWasmToJSWrapper) {
     internal = wasm_js_function->shared()->wasm_js_function_data()->internal();
   }
 
-  Code wrapper = isolate->builtins()->code(Builtin::kWasmToJsWrapperAsm);
+  Tagged<Code> wrapper =
+      isolate->builtins()->code(Builtin::kWasmToJsWrapperAsm);
   if (!internal->call_target()) {
     return isolate->heap()->ToBoolean(internal->code() == wrapper);
   }
   return isolate->heap()->ToBoolean(internal->call_target() ==
-                                    wrapper.instruction_start());
+                                    wrapper->instruction_start());
 }
 
 RUNTIME_FUNCTION(Runtime_WasmTraceEnter) {
@@ -239,7 +240,7 @@ RUNTIME_FUNCTION(Runtime_WasmTraceEnter) {
 RUNTIME_FUNCTION(Runtime_WasmTraceExit) {
   HandleScope shs(isolate);
   DCHECK_EQ(1, args.length());
-  Smi return_addr_smi = Smi::cast(args[0]);
+  Tagged<Smi> return_addr_smi = Smi::cast(args[0]);
 
   PrintIndentation(WasmStackSize(isolate));
   PrintF("}");
@@ -332,7 +333,7 @@ RUNTIME_FUNCTION(Runtime_IsWasmCode) {
   SealHandleScope shs(isolate);
   DCHECK_EQ(1, args.length());
   auto function = JSFunction::cast(args[0]);
-  Code code = function->code();
+  Tagged<Code> code = function->code();
   bool is_js_to_wasm = code->kind() == CodeKind::JS_TO_WASM_FUNCTION ||
                        (code->builtin_id() == Builtin::kJSToWasmWrapper);
   return isolate->heap()->ToBoolean(is_js_to_wasm);
@@ -457,7 +458,7 @@ RUNTIME_FUNCTION(Runtime_WasmGetNumberOfInstances) {
   DCHECK_EQ(1, args.length());
   Handle<WasmModuleObject> module_obj = args.at<WasmModuleObject>(0);
   int instance_count = 0;
-  WeakArrayList weak_instance_list =
+  Tagged<WeakArrayList> weak_instance_list =
       module_obj->script()->wasm_weak_instance_list();
   for (int i = 0; i < weak_instance_list->length(); ++i) {
     if (weak_instance_list->Get(i)->IsWeak()) instance_count++;
@@ -517,7 +518,7 @@ RUNTIME_FUNCTION(Runtime_WasmTierUpFunction) {
   CHECK(WasmExportedFunction::IsWasmExportedFunction(*function));
   Handle<WasmExportedFunction> exp_fun =
       Handle<WasmExportedFunction>::cast(function);
-  WasmInstanceObject instance = exp_fun->instance();
+  Tagged<WasmInstanceObject> instance = exp_fun->instance();
   int func_index = exp_fun->function_index();
   wasm::TierUpNowForTesting(isolate, instance, func_index);
   return ReadOnlyRoots(isolate).undefined_value();
diff --git a/src/runtime/runtime-test.cc b/src/runtime/runtime-test.cc
index 0cc3944f3f0..9f1b57dbceb 100644
--- a/src/runtime/runtime-test.cc
+++ b/src/runtime/runtime-test.cc
@@ -55,7 +55,7 @@ namespace v8 {
 namespace internal {
 
 namespace {
-V8_WARN_UNUSED_RESULT Object CrashUnlessFuzzing(Isolate* isolate) {
+V8_WARN_UNUSED_RESULT Tagged<Object> CrashUnlessFuzzing(Isolate* isolate) {
   CHECK(v8_flags.fuzzing);
   return ReadOnlyRoots(isolate).undefined_value();
 }
@@ -67,7 +67,8 @@ V8_WARN_UNUSED_RESULT bool CrashUnlessFuzzingReturnFalse(Isolate* isolate) {
 
 // Returns |value| unless correctness-fuzzer-supressions is enabled,
 // otherwise returns undefined_value.
-V8_WARN_UNUSED_RESULT Object ReturnFuzzSafe(Object value, Isolate* isolate) {
+V8_WARN_UNUSED_RESULT Tagged<Object> ReturnFuzzSafe(Tagged<Object> value,
+                                                    Isolate* isolate) {
   return v8_flags.correctness_fuzzer_suppressions
              ? ReadOnlyRoots(isolate).undefined_value()
              : value;
@@ -88,7 +89,7 @@ V8_WARN_UNUSED_RESULT Object ReturnFuzzSafe(Object value, Isolate* isolate) {
   if (!IsBoolean(args[index])) return CrashUnlessFuzzing(isolate); \
   bool name = IsTrue(args[index], isolate);
 
-bool IsAsmWasmFunction(Isolate* isolate, JSFunction function) {
+bool IsAsmWasmFunction(Isolate* isolate, Tagged<JSFunction> function) {
   DisallowGarbageCollection no_gc;
 #if V8_ENABLE_WEBASSEMBLY
   // For simplicity we include invalid asm.js functions whose code hasn't yet
@@ -339,8 +340,9 @@ bool CanOptimizeFunction(CodeKind target_kind, Handle<JSFunction> function,
   return true;
 }
 
-Object OptimizeFunctionOnNextCall(RuntimeArguments& args, Isolate* isolate,
-                                  CodeKind target_kind) {
+Tagged<Object> OptimizeFunctionOnNextCall(RuntimeArguments& args,
+                                          Isolate* isolate,
+                                          CodeKind target_kind) {
   if (args.length() != 1 && args.length() != 2) {
     return CrashUnlessFuzzing(isolate);
   }
@@ -371,7 +373,7 @@ Object OptimizeFunctionOnNextCall(RuntimeArguments& args, Isolate* isolate,
   // function has.
   if (!function->is_compiled()) {
     DCHECK(function->shared()->HasBytecodeArray());
-    Code code = *BUILTIN_CODE(isolate, InterpreterEntryTrampoline);
+    Tagged<Code> code = *BUILTIN_CODE(isolate, InterpreterEntryTrampoline);
     if (function->shared()->HasBaselineCode()) {
       code = function->shared()->baseline_code(kAcquireLoad);
     }
@@ -902,7 +904,7 @@ RUNTIME_FUNCTION(Runtime_GetOptimizationStatus) {
   }
 
   if (function->HasAttachedOptimizedCode()) {
-    Code code = function->code();
+    Tagged<Code> code = function->code();
     if (code->marked_for_deoptimization()) {
       status |= static_cast<int>(OptimizationStatus::kMarkedForDeoptimization);
     } else {
@@ -1221,7 +1223,7 @@ static void DebugPrintImpl(MaybeObject maybe_object, std::ostream& os) {
   if (maybe_object->IsCleared()) {
     os << "[weak cleared]";
   } else {
-    Object object = maybe_object.GetHeapObjectOrSmi();
+    Tagged<Object> object = maybe_object.GetHeapObjectOrSmi();
     bool weak = maybe_object.IsWeak();
 
 #ifdef OBJECT_PRINT
@@ -1269,7 +1271,7 @@ RUNTIME_FUNCTION(Runtime_DebugPrintPtr) {
 
   MaybeObject maybe_object(*args.address_of_arg_at(0));
   if (!maybe_object.IsCleared()) {
-    Object object = maybe_object.GetHeapObjectOrSmi();
+    Tagged<Object> object = maybe_object.GetHeapObjectOrSmi();
     size_t pointer;
     if (Object::ToIntegerIndex(object, &pointer)) {
       MaybeObject from_pointer(static_cast<Address>(pointer));
@@ -1439,7 +1441,7 @@ RUNTIME_FUNCTION(Runtime_SetForceSlowPath) {
   if (args.length() != 1) {
     return CrashUnlessFuzzing(isolate);
   }
-  Object arg = args[0];
+  Tagged<Object> arg = args[0];
   if (IsTrue(arg, isolate)) {
     isolate->set_force_slow_path(true);
   } else {
@@ -1548,7 +1550,7 @@ RUNTIME_FUNCTION(Runtime_TraceExit) {
   if (args.length() != 1) {
     return CrashUnlessFuzzing(isolate);
   }
-  Object obj = args[0];
+  Tagged<Object> obj = args[0];
   PrintIndentation(StackSize(isolate));
   PrintF("} -> ");
   ShortPrint(obj);
@@ -1584,7 +1586,7 @@ RUNTIME_FUNCTION(Runtime_HasElementsInALargeObjectSpace) {
     return CrashUnlessFuzzing(isolate);
   }
   auto array = JSArray::cast(args[0]);
-  FixedArrayBase elements = array->elements();
+  Tagged<FixedArrayBase> elements = array->elements();
   return isolate->heap()->ToBoolean(
       isolate->heap()->new_lo_space()->Contains(elements) ||
       isolate->heap()->lo_space()->Contains(elements));
@@ -1595,7 +1597,7 @@ RUNTIME_FUNCTION(Runtime_InYoungGeneration) {
   if (args.length() != 1) {
     return CrashUnlessFuzzing(isolate);
   }
-  Object obj = args[0];
+  Tagged<Object> obj = args[0];
   return isolate->heap()->ToBoolean(ObjectInYoungGeneration(obj));
 }
 
@@ -1604,9 +1606,9 @@ RUNTIME_FUNCTION(Runtime_PretenureAllocationSite) {
   DisallowGarbageCollection no_gc;
 
   if (args.length() != 1) return CrashUnlessFuzzing(isolate);
-  Object arg = args[0];
+  Tagged<Object> arg = args[0];
   if (!IsJSObject(arg)) return CrashUnlessFuzzing(isolate);
-  JSObject object = JSObject::cast(arg);
+  Tagged<JSObject> object = JSObject::cast(arg);
 
   Heap* heap = object->GetHeap();
   if (!heap->InYoungGeneration(object)) {
@@ -1615,13 +1617,13 @@ RUNTIME_FUNCTION(Runtime_PretenureAllocationSite) {
   }
 
   PretenuringHandler* pretenuring_handler = heap->pretenuring_handler();
-  AllocationMemento memento =
+  Tagged<AllocationMemento> memento =
       pretenuring_handler
           ->FindAllocationMemento<PretenuringHandler::kForRuntime>(
               object->map(), object);
   if (memento.is_null())
     return ReturnFuzzSafe(ReadOnlyRoots(isolate).false_value(), isolate);
-  AllocationSite site = memento->GetAllocationSite();
+  Tagged<AllocationSite> site = memento->GetAllocationSite();
   pretenuring_handler->PretenureAllocationSiteOnNextCollection(site);
   return ReturnFuzzSafe(ReadOnlyRoots(isolate).true_value(), isolate);
 }
@@ -1938,8 +1940,10 @@ RUNTIME_FUNCTION(Runtime_EnableCodeLoggingForTesting) {
     void SetterCallbackEvent(Handle<Name> name, Address entry_point) final {}
     void RegExpCodeCreateEvent(Handle<AbstractCode> code,
                                Handle<String> source) final {}
-    void CodeMoveEvent(InstructionStream from, InstructionStream to) final {}
-    void BytecodeMoveEvent(BytecodeArray from, BytecodeArray to) final {}
+    void CodeMoveEvent(Tagged<InstructionStream> from,
+                       Tagged<InstructionStream> to) final {}
+    void BytecodeMoveEvent(Tagged<BytecodeArray> from,
+                           Tagged<BytecodeArray> to) final {}
     void SharedFunctionInfoMoveEvent(Address from, Address to) final {}
     void NativeContextMoveEvent(Address from, Address to) final {}
     void CodeMovingGCEvent() final {}
diff --git a/src/runtime/runtime-trace.cc b/src/runtime/runtime-trace.cc
index aa7168e0b3e..bd92269fd7d 100644
--- a/src/runtime/runtime-trace.cc
+++ b/src/runtime/runtime-trace.cc
@@ -44,7 +44,7 @@ void PrintRegisterRange(UnoptimizedFrame* frame, std::ostream& os,
                         interpreter::Register first_reg, int range) {
   for (int reg_index = first_reg.index(); reg_index < first_reg.index() + range;
        reg_index++) {
-    Object reg_object = frame->ReadInterpreterRegister(reg_index);
+    Tagged<Object> reg_object = frame->ReadInterpreterRegister(reg_index);
     os << "      [ " << std::setw(reg_field_width)
        << interpreter::Register(reg_index).ToString() << arrow_direction;
     ShortPrint(reg_object, os);
diff --git a/src/runtime/runtime-utils.h b/src/runtime/runtime-utils.h
index 1b8526c6904..345e326eed8 100644
--- a/src/runtime/runtime-utils.h
+++ b/src/runtime/runtime-utils.h
@@ -24,7 +24,7 @@ struct ObjectPair {
   Address y;
 };
 
-static inline ObjectPair MakePair(Object x, Object y) {
+static inline ObjectPair MakePair(Tagged<Object> x, Tagged<Object> y) {
   ObjectPair result = {x.ptr(), y.ptr()};
   // Pointers x and y returned in rax and rdx, in AMD-x64-abi.
   // In Win64 they are assigned to a hidden first argument.
@@ -32,7 +32,7 @@ static inline ObjectPair MakePair(Object x, Object y) {
 }
 #else
 using ObjectPair = uint64_t;
-static inline ObjectPair MakePair(Object x, Object y) {
+static inline ObjectPair MakePair(Tagged<Object> x, Tagged<Object> y) {
 #if defined(V8_TARGET_LITTLE_ENDIAN)
   return x.ptr() | (static_cast<ObjectPair>(y.ptr()) << 32);
 #elif defined(V8_TARGET_BIG_ENDIAN)
diff --git a/src/runtime/runtime-wasm.cc b/src/runtime/runtime-wasm.cc
index 660a32fca6b..06047444667 100644
--- a/src/runtime/runtime-wasm.cc
+++ b/src/runtime/runtime-wasm.cc
@@ -365,7 +365,7 @@ void ReplaceWrapper(Isolate* isolate, Handle<WasmInstanceObject> instance,
   Handle<JSFunction> exported_function =
       WasmInternalFunction::GetOrCreateExternal(internal);
   exported_function->set_code(*wrapper_code);
-  WasmExportedFunctionData function_data =
+  Tagged<WasmExportedFunctionData> function_data =
       exported_function->shared()->wasm_exported_function_data();
   function_data->set_wrapper_code(*wrapper_code);
 }
diff --git a/src/sandbox/code-pointer-inl.h b/src/sandbox/code-pointer-inl.h
index efa90450364..56a5b302375 100644
--- a/src/sandbox/code-pointer-inl.h
+++ b/src/sandbox/code-pointer-inl.h
@@ -16,7 +16,7 @@ namespace internal {
 
 V8_INLINE void InitCodePointerTableEntryField(Address field_address,
                                               Isolate* isolate,
-                                              HeapObject owning_code,
+                                              Tagged<HeapObject> owning_code,
                                               Address entrypoint) {
 #ifdef V8_CODE_POINTER_SANDBOXING
   CodePointerTable::Space* space =
diff --git a/src/sandbox/code-pointer.h b/src/sandbox/code-pointer.h
index 043f097be36..5eff0f3981a 100644
--- a/src/sandbox/code-pointer.h
+++ b/src/sandbox/code-pointer.h
@@ -15,7 +15,7 @@ namespace internal {
 // owning code object as well as to the entrypoint.
 V8_INLINE void InitCodePointerTableEntryField(Address field_address,
                                               Isolate* isolate,
-                                              HeapObject owning_code,
+                                              Tagged<HeapObject> owning_code,
                                               Address entrypoint);
 
 // If the sandbox is enabled: reads the CodePointerHandle from the field and
diff --git a/src/sandbox/indirect-pointer-inl.h b/src/sandbox/indirect-pointer-inl.h
index 26f425e84fc..736ae48ba7a 100644
--- a/src/sandbox/indirect-pointer-inl.h
+++ b/src/sandbox/indirect-pointer-inl.h
@@ -14,7 +14,7 @@
 namespace v8 {
 namespace internal {
 
-V8_INLINE Object ReadIndirectPointerField(Address field_address) {
+V8_INLINE Tagged<Object> ReadIndirectPointerField(Address field_address) {
 #ifdef V8_CODE_POINTER_SANDBOXING
   // Here we assume that the load from the table cannot be reordered before the
   // load of the code object pointer due to the data dependency between the two
diff --git a/src/sandbox/testing.cc b/src/sandbox/testing.cc
index 723037347b7..49d4ddf870d 100644
--- a/src/sandbox/testing.cc
+++ b/src/sandbox/testing.cc
@@ -173,7 +173,7 @@ void SandboxIsValidObjectAt(const v8::FunctionCallbackInfo<v8::Value>& info) {
   }
 
   Heap* heap = reinterpret_cast<Isolate*>(isolate)->heap();
-  auto IsLocatedInMappedMemory = [&](HeapObject obj) {
+  auto IsLocatedInMappedMemory = [&](Tagged<HeapObject> obj) {
     // Note that IsOutsideAllocatedSpace is imprecise and may return false for
     // some addresses outside the allocated space. However, it's probably good
     // enough for our purposes.
@@ -182,7 +182,7 @@ void SandboxIsValidObjectAt(const v8::FunctionCallbackInfo<v8::Value>& info) {
 
   bool is_valid = false;
   if (IsLocatedInMappedMemory(obj)) {
-    Map map = obj->map();
+    Tagged<Map> map = obj->map();
     if (IsLocatedInMappedMemory(map)) {
       is_valid = IsMap(map);
     }
diff --git a/src/snapshot/code-serializer.cc b/src/snapshot/code-serializer.cc
index e603c83d1bf..12f79c6ff35 100644
--- a/src/snapshot/code-serializer.cc
+++ b/src/snapshot/code-serializer.cc
@@ -113,7 +113,7 @@ void CodeSerializer::SerializeObjectImpl(Handle<HeapObject> obj,
   InstanceType instance_type;
   {
     DisallowGarbageCollection no_gc;
-    HeapObject raw = *obj;
+    Tagged<HeapObject> raw = *obj;
     if (SerializeHotObject(raw)) return;
     if (SerializeRoot(raw)) return;
     if (SerializeBackReference(raw)) return;
@@ -128,13 +128,13 @@ void CodeSerializer::SerializeObjectImpl(Handle<HeapObject> obj,
     Handle<Object> context_data;
     {
       DisallowGarbageCollection no_gc;
-      Script script_obj = Script::cast(*obj);
+      Tagged<Script> script_obj = Script::cast(*obj);
       DCHECK_NE(script_obj->compilation_type(), Script::CompilationType::kEval);
       // We want to differentiate between undefined and uninitialized_symbol for
       // context_data for now. It is hack to allow debugging for scripts that
       // are included as a part of custom snapshot. (see
       // debug::Script::IsEmbedded())
-      Object raw_context_data = script_obj->context_data();
+      Tagged<Object> raw_context_data = script_obj->context_data();
       if (raw_context_data != roots.undefined_value() &&
           raw_context_data != roots.uninitialized_symbol()) {
         script_obj->set_context_data(roots.undefined_value());
@@ -148,7 +148,7 @@ void CodeSerializer::SerializeObjectImpl(Handle<HeapObject> obj,
     SerializeGeneric(obj, slot_type);
     {
       DisallowGarbageCollection no_gc;
-      Script script_obj = Script::cast(*obj);
+      Tagged<Script> script_obj = Script::cast(*obj);
       script_obj->set_host_defined_options(*host_options);
       script_obj->set_context_data(*context_data);
     }
@@ -158,7 +158,7 @@ void CodeSerializer::SerializeObjectImpl(Handle<HeapObject> obj,
     bool restore_bytecode = false;
     {
       DisallowGarbageCollection no_gc;
-      SharedFunctionInfo sfi = SharedFunctionInfo::cast(*obj);
+      Tagged<SharedFunctionInfo> sfi = SharedFunctionInfo::cast(*obj);
       DCHECK(!sfi->IsApiFunction());
 #if V8_ENABLE_WEBASSEMBLY
       // TODO(7110): Enable serializing of Asm modules once the AsmWasmData
@@ -178,7 +178,7 @@ void CodeSerializer::SerializeObjectImpl(Handle<HeapObject> obj,
     SerializeGeneric(obj, slot_type);
     if (restore_bytecode) {
       DisallowGarbageCollection no_gc;
-      SharedFunctionInfo sfi = SharedFunctionInfo::cast(*obj);
+      Tagged<SharedFunctionInfo> sfi = SharedFunctionInfo::cast(*obj);
       sfi->SetActiveBytecodeArray(debug_info->DebugBytecodeArray());
     }
     return;
@@ -248,13 +248,13 @@ void CreateInterpreterDataForDeserializedCode(
   Handle<Script> script(Script::cast(result_sfi->script()), isolate);
   if (log_code_creation) Script::InitLineEnds(isolate, script);
 
-  String name = ReadOnlyRoots(isolate).empty_string();
+  Tagged<String> name = ReadOnlyRoots(isolate).empty_string();
   if (IsString(script->name())) name = String::cast(script->name());
   Handle<String> name_handle(name, isolate);
 
   SharedFunctionInfo::ScriptIterator iter(isolate, *script);
-  for (SharedFunctionInfo shared_info = iter.Next(); !shared_info.is_null();
-       shared_info = iter.Next()) {
+  for (Tagged<SharedFunctionInfo> shared_info = iter.Next();
+       !shared_info.is_null(); shared_info = iter.Next()) {
     IsCompiledScope is_compiled(shared_info, isolate);
     if (!is_compiled.is_compiled()) continue;
     DCHECK(shared_info->HasBytecodeArray());
@@ -357,7 +357,7 @@ void FinalizeDeserialization(Isolate* isolate,
   }
 
   SharedFunctionInfo::ScriptIterator iter(isolate, *script);
-  for (SharedFunctionInfo info = iter.Next(); !info.is_null();
+  for (Tagged<SharedFunctionInfo> info = iter.Next(); !info.is_null();
        info = iter.Next()) {
     if (!info->is_compiled()) continue;
     Handle<SharedFunctionInfo> shared_info(info, isolate);
@@ -377,13 +377,14 @@ void FinalizeDeserialization(Isolate* isolate,
   }
 }
 
-void BaselineBatchCompileIfSparkplugCompiled(Isolate* isolate, Script script) {
+void BaselineBatchCompileIfSparkplugCompiled(Isolate* isolate,
+                                             Tagged<Script> script) {
   // Here is main thread, we trigger early baseline compilation only in
   // concurrent sparkplug and baseline batch compilation mode which consumes
   // little main thread execution time.
   if (v8_flags.concurrent_sparkplug && v8_flags.baseline_batch_compilation) {
     SharedFunctionInfo::ScriptIterator iter(isolate, script);
-    for (SharedFunctionInfo info = iter.Next(); !info.is_null();
+    for (Tagged<SharedFunctionInfo> info = iter.Next(); !info.is_null();
          info = iter.Next()) {
       if (info->sparkplug_compiled() && CanCompileWithBaseline(isolate, info)) {
         isolate->baseline_batch_compiler()->EnqueueSFI(info);
diff --git a/src/snapshot/context-serializer.cc b/src/snapshot/context-serializer.cc
index 3c3f2104398..384a30c208e 100644
--- a/src/snapshot/context-serializer.cc
+++ b/src/snapshot/context-serializer.cc
@@ -22,7 +22,8 @@ namespace {
 // serialization, the original state is restored.
 class V8_NODISCARD SanitizeNativeContextScope final {
  public:
-  SanitizeNativeContextScope(Isolate* isolate, NativeContext native_context,
+  SanitizeNativeContextScope(Isolate* isolate,
+                             Tagged<NativeContext> native_context,
                              bool allow_active_isolate_for_testing,
                              const DisallowGarbageCollection& no_gc)
       : native_context_(native_context), no_gc_(no_gc) {
@@ -128,7 +129,7 @@ void ContextSerializer::SerializeObjectImpl(Handle<HeapObject> obj,
 
   {
     DisallowGarbageCollection no_gc;
-    HeapObject raw = *obj;
+    Tagged<HeapObject> raw = *obj;
     if (SerializeHotObject(raw)) return;
     if (SerializeRoot(raw)) return;
     if (SerializeBackReference(raw)) return;
@@ -166,7 +167,7 @@ void ContextSerializer::SerializeObjectImpl(Handle<HeapObject> obj,
       DisallowGarbageCollection no_gc;
       // Unconditionally reset the JSFunction to its SFI's code, since we can't
       // serialize optimized code anyway.
-      JSFunction closure = JSFunction::cast(*obj);
+      Tagged<JSFunction> closure = JSFunction::cast(*obj);
       if (closure->shared()->HasBytecodeArray()) {
         closure->SetInterruptBudget(isolate());
       }
@@ -187,7 +188,7 @@ void ContextSerializer::SerializeObjectImpl(Handle<HeapObject> obj,
   serializer.Serialize(slot_type);
 }
 
-bool ContextSerializer::ShouldBeInTheStartupObjectCache(HeapObject o) {
+bool ContextSerializer::ShouldBeInTheStartupObjectCache(Tagged<HeapObject> o) {
   // We can't allow scripts to be part of the context snapshot because they
   // contain a unique ID, and deserializing several context snapshots containing
   // script would cause dupes.
@@ -198,7 +199,7 @@ bool ContextSerializer::ShouldBeInTheStartupObjectCache(HeapObject o) {
          o->map() == ReadOnlyRoots(isolate()).fixed_cow_array_map();
 }
 
-bool ContextSerializer::ShouldBeInTheSharedObjectCache(HeapObject o) {
+bool ContextSerializer::ShouldBeInTheSharedObjectCache(Tagged<HeapObject> o) {
   // v8_flags.shared_string_table may be true during deserialization, so put
   // internalized strings into the shared object snapshot.
   return IsInternalizedString(o);
@@ -211,7 +212,7 @@ bool DataIsEmpty(const StartupData& data) { return data.raw_size == 0; }
 bool ContextSerializer::SerializeJSObjectWithEmbedderFields(
     Handle<JSObject> obj) {
   DisallowGarbageCollection no_gc;
-  JSObject js_obj = *obj;
+  Tagged<JSObject> js_obj = *obj;
   int embedder_fields_count = js_obj->GetEmbedderFieldCount();
   if (embedder_fields_count == 0) return false;
   CHECK_GT(embedder_fields_count, 0);
@@ -233,7 +234,7 @@ bool ContextSerializer::SerializeJSObjectWithEmbedderFields(
     EmbedderDataSlot embedder_data_slot(js_obj, i);
     original_embedder_values.emplace_back(
         embedder_data_slot.load_raw(isolate(), no_gc));
-    Object object = embedder_data_slot.load_tagged();
+    Tagged<Object> object = embedder_data_slot.load_tagged();
     if (IsHeapObject(object)) {
       DCHECK(IsValidHeapObject(isolate()->heap(), HeapObject::cast(object)));
       serialized_data.push_back({nullptr, 0});
@@ -303,7 +304,7 @@ bool ContextSerializer::SerializeJSObjectWithEmbedderFields(
   return true;
 }
 
-void ContextSerializer::CheckRehashability(HeapObject obj) {
+void ContextSerializer::CheckRehashability(Tagged<HeapObject> obj) {
   if (!can_be_rehashed_) return;
   if (!obj->NeedsRehashing(cage_base())) return;
   if (obj->CanBeRehashed(cage_base())) return;
diff --git a/src/snapshot/context-serializer.h b/src/snapshot/context-serializer.h
index 98d138315f2..2d4214a7ac0 100644
--- a/src/snapshot/context-serializer.h
+++ b/src/snapshot/context-serializer.h
@@ -30,10 +30,10 @@ class V8_EXPORT_PRIVATE ContextSerializer : public Serializer {
 
  private:
   void SerializeObjectImpl(Handle<HeapObject> o, SlotType slot_type) override;
-  bool ShouldBeInTheStartupObjectCache(HeapObject o);
-  bool ShouldBeInTheSharedObjectCache(HeapObject o);
+  bool ShouldBeInTheStartupObjectCache(Tagged<HeapObject> o);
+  bool ShouldBeInTheSharedObjectCache(Tagged<HeapObject> o);
   bool SerializeJSObjectWithEmbedderFields(Handle<JSObject> obj);
-  void CheckRehashability(HeapObject obj);
+  void CheckRehashability(Tagged<HeapObject> obj);
 
   StartupSerializer* startup_serializer_;
   v8::SerializeEmbedderFieldsCallback serialize_embedder_fields_;
diff --git a/src/snapshot/deserializer.cc b/src/snapshot/deserializer.cc
index 5c755083f14..ffb1d35d8ae 100644
--- a/src/snapshot/deserializer.cc
+++ b/src/snapshot/deserializer.cc
@@ -66,7 +66,7 @@ class SlotAccessorForHeapObject {
     CombinedWriteBarrier(*object_, current_slot, value, UPDATE_WRITE_BARRIER);
     return 1;
   }
-  int Write(HeapObject value, HeapObjectReferenceType ref_type,
+  int Write(Tagged<HeapObject> value, HeapObjectReferenceType ref_type,
             int slot_offset = 0) {
     return Write(HeapObjectReference::From(value, ref_type), slot_offset);
   }
@@ -75,7 +75,7 @@ class SlotAccessorForHeapObject {
     return Write(*value, ref_type, slot_offset);
   }
 
-  int WriteIndirect(HeapObject value) {
+  int WriteIndirect(Tagged<HeapObject> value) {
     // TODO(saelo): currently, only Code objects can be referenced indirectly
     // through a pointer table, and so here we directly downcast to Code
     // because we need to know the offset at which the object has its pointer
@@ -87,7 +87,7 @@ class SlotAccessorForHeapObject {
     // and simply copy the handle into the slot. For that we don't even need to
     // know which table the object uses.
     CHECK(IsCode(value));
-    Code code = Code::cast(value);
+    Tagged<Code> code = Code::cast(value);
     IndirectPointerSlot dest = object_->RawIndirectPointerField(offset_);
     dest.store(code);
     IndirectPointerWriteBarrier(*object_, dest, value, UPDATE_WRITE_BARRIER);
@@ -119,7 +119,7 @@ class SlotAccessorForRootSlots {
     current_slot.Relaxed_Store(value);
     return 1;
   }
-  int Write(HeapObject value, HeapObjectReferenceType ref_type,
+  int Write(Tagged<HeapObject> value, HeapObjectReferenceType ref_type,
             int slot_offset = 0) {
     return Write(HeapObjectReference::From(value, ref_type), slot_offset);
   }
@@ -127,7 +127,7 @@ class SlotAccessorForRootSlots {
             int slot_offset = 0) {
     return Write(*value, ref_type, slot_offset);
   }
-  int WriteIndirect(HeapObject value) { UNREACHABLE(); }
+  int WriteIndirect(Tagged<HeapObject> value) { UNREACHABLE(); }
 
  private:
   const FullMaybeObjectSlot slot_;
@@ -147,7 +147,7 @@ class SlotAccessorForHandle {
   int offset() const { UNREACHABLE(); }
 
   int Write(MaybeObject value, int slot_offset = 0) { UNREACHABLE(); }
-  int Write(HeapObject value, HeapObjectReferenceType ref_type,
+  int Write(Tagged<HeapObject> value, HeapObjectReferenceType ref_type,
             int slot_offset = 0) {
     DCHECK_EQ(slot_offset, 0);
     DCHECK_EQ(ref_type, HeapObjectReferenceType::STRONG);
@@ -161,7 +161,7 @@ class SlotAccessorForHandle {
     *handle_ = value;
     return 1;
   }
-  int WriteIndirect(HeapObject value) { UNREACHABLE(); }
+  int WriteIndirect(Tagged<HeapObject> value) { UNREACHABLE(); }
 
  private:
   Handle<HeapObject>* handle_;
@@ -171,7 +171,7 @@ class SlotAccessorForHandle {
 template <typename IsolateT>
 template <typename SlotAccessor>
 int Deserializer<IsolateT>::WriteHeapPointer(SlotAccessor slot_accessor,
-                                             HeapObject heap_object,
+                                             Tagged<HeapObject> heap_object,
                                              ReferenceDescriptor descr) {
   if (descr.is_indirect_pointer) {
     return slot_accessor.WriteIndirect(heap_object);
@@ -315,7 +315,7 @@ void Deserializer<IsolateT>::WeakenDescriptorArrays() {
 }
 
 template <typename IsolateT>
-void Deserializer<IsolateT>::LogScriptEvents(Script script) {
+void Deserializer<IsolateT>::LogScriptEvents(Tagged<Script> script) {
   DisallowGarbageCollection no_gc;
   LOG(isolate(), ScriptEvent(ScriptEventType::kDeserialize, script->id()));
   LOG(isolate(), ScriptDetails(script));
@@ -323,7 +323,7 @@ void Deserializer<IsolateT>::LogScriptEvents(Script script) {
 
 namespace {
 template <typename IsolateT>
-uint32_t ComputeRawHashField(IsolateT* isolate, String string) {
+uint32_t ComputeRawHashField(IsolateT* isolate, Tagged<String> string) {
   // Make sure raw_hash_field() is computed.
   string->EnsureHash(SharedStringAccessGuardIfNeeded(isolate));
   return string->raw_hash_field();
@@ -353,13 +353,15 @@ StringTableInsertionKey::StringTableInsertionKey(
 }
 
 template <typename IsolateT>
-bool StringTableInsertionKey::IsMatch(IsolateT* isolate, String string) {
+bool StringTableInsertionKey::IsMatch(IsolateT* isolate,
+                                      Tagged<String> string) {
   // We want to compare the content of two strings here.
   return string_->SlowEquals(string, SharedStringAccessGuardIfNeeded(isolate));
 }
-template bool StringTableInsertionKey::IsMatch(Isolate* isolate, String string);
+template bool StringTableInsertionKey::IsMatch(Isolate* isolate,
+                                               Tagged<String> string);
 template bool StringTableInsertionKey::IsMatch(LocalIsolate* isolate,
-                                               String string);
+                                               Tagged<String> string);
 
 namespace {
 
@@ -371,7 +373,8 @@ void NoExternalReferencesCallback() {
   FATAL("No external references provided via API");
 }
 
-void PostProcessExternalString(ExternalString string, Isolate* isolate) {
+void PostProcessExternalString(Tagged<ExternalString> string,
+                               Isolate* isolate) {
   DisallowGarbageCollection no_gc;
   uint32_t index = string->GetResourceRefForDeserialization();
   Address address =
@@ -387,7 +390,7 @@ void PostProcessExternalString(ExternalString string, Isolate* isolate) {
 
 // Should be called only on the main thread (not thread safe).
 template <>
-void Deserializer<Isolate>::PostProcessNewJSReceiver(Map map,
+void Deserializer<Isolate>::PostProcessNewJSReceiver(Tagged<Map> map,
                                                      Handle<JSReceiver> obj,
                                                      InstanceType instance_type,
                                                      SnapshotSpace space) {
@@ -450,7 +453,7 @@ void Deserializer<Isolate>::PostProcessNewJSReceiver(Map map,
 
 template <>
 void Deserializer<LocalIsolate>::PostProcessNewJSReceiver(
-    Map map, Handle<JSReceiver> obj, InstanceType instance_type,
+    Tagged<Map> map, Handle<JSReceiver> obj, InstanceType instance_type,
     SnapshotSpace space) {
   UNREACHABLE();
 }
@@ -460,15 +463,15 @@ void Deserializer<IsolateT>::PostProcessNewObject(Handle<Map> map,
                                                   Handle<HeapObject> obj,
                                                   SnapshotSpace space) {
   DisallowGarbageCollection no_gc;
-  Map raw_map = *map;
+  Tagged<Map> raw_map = *map;
   DCHECK_EQ(raw_map, obj->map(isolate_));
   InstanceType instance_type = raw_map->instance_type();
-  HeapObject raw_obj = *obj;
+  Tagged<HeapObject> raw_obj = *obj;
   DCHECK_IMPLIES(deserializing_user_code(), should_rehash());
   if (should_rehash()) {
     if (InstanceTypeChecker::IsString(instance_type)) {
       // Uninitialize hash field as we need to recompute the hash.
-      String string = String::cast(raw_obj);
+      Tagged<String> string = String::cast(raw_obj);
       string->set_raw_hash_field(String::kEmptyHashField);
       // Rehash strings before read-only space is sealed. Strings outside
       // read-only space are rehashed lazily. (e.g. when rehashing dictionaries)
@@ -492,10 +495,11 @@ void Deserializer<IsolateT>::PostProcessNewObject(Handle<Map> map,
         StringTableInsertionKey key(
             isolate(), string,
             DeserializingUserCodeOption::kIsDeserializingUserCode);
-        String result = *isolate()->string_table()->LookupKey(isolate(), &key);
+        Tagged<String> result =
+            *isolate()->string_table()->LookupKey(isolate(), &key);
 
         if (result != raw_obj) {
-          String::cast(raw_obj).MakeThin(isolate(), result);
+          String::cast(raw_obj)->MakeThin(isolate(), result);
           // Mutate the given object handle so that the backreference entry is
           // also updated.
           obj.PatchValue(result);
@@ -526,20 +530,20 @@ void Deserializer<IsolateT>::PostProcessNewObject(Handle<Map> map,
       new_code_objects_.push_back(Handle<InstructionStream>::cast(obj));
     }
   } else if (InstanceTypeChecker::IsCode(instance_type)) {
-    Code code = Code::cast(raw_obj);
-    code.init_instruction_start(main_thread_isolate(), kNullAddress);
+    Tagged<Code> code = Code::cast(raw_obj);
+    code->init_instruction_start(main_thread_isolate(), kNullAddress);
     if (!code->has_instruction_stream()) {
-      code.SetInstructionStartForOffHeapBuiltin(
+      code->SetInstructionStartForOffHeapBuiltin(
           main_thread_isolate(), EmbeddedData::FromBlob(main_thread_isolate())
                                      .InstructionStartOf(code->builtin_id()));
     } else {
-      code.UpdateInstructionStart(main_thread_isolate(),
-                                  code->instruction_stream());
+      code->UpdateInstructionStart(main_thread_isolate(),
+                                   code->instruction_stream());
     }
   } else if (InstanceTypeChecker::IsSharedFunctionInfo(instance_type)) {
-    SharedFunctionInfo sfi = SharedFunctionInfo::cast(raw_obj);
+    Tagged<SharedFunctionInfo> sfi = SharedFunctionInfo::cast(raw_obj);
     // Reset the id to avoid collisions - it must be unique in this isolate.
-    sfi.set_unique_id(isolate()->GetAndIncNextUniqueSfiId());
+    sfi->set_unique_id(isolate()->GetAndIncNextUniqueSfiId());
   } else if (InstanceTypeChecker::IsMap(instance_type)) {
     if (v8_flags.log_maps) {
       // Keep track of all seen Maps to log them later since they might be only
@@ -669,12 +673,12 @@ Handle<HeapObject> Deserializer<IsolateT>::ReadObject(SnapshotSpace space) {
   //     before fields with objects.
   //     - We ensure this is the case by DCHECKing on object allocation that the
   //       previously allocated object has a valid size (see `Allocate`).
-  HeapObject raw_obj =
+  Tagged<HeapObject> raw_obj =
       Allocate(allocation, size_in_bytes, HeapObject::RequiredAlignment(*map));
   raw_obj->set_map_after_allocation(*map);
   MemsetTagged(raw_obj->RawField(kTaggedSize),
                Smi::uninitialized_deserialization_value(), size_in_tagged - 1);
-  DCHECK(raw_obj.CheckRequiredAlignment(isolate()));
+  DCHECK(raw_obj->CheckRequiredAlignment(isolate()));
 
   // Make sure BytecodeArrays have a valid age, so that the marker doesn't
   // break when making them older.
@@ -684,10 +688,10 @@ Handle<HeapObject> Deserializer<IsolateT>::ReadObject(SnapshotSpace space) {
     // Make sure EphemeronHashTables have valid HeapObject keys, so that the
     // marker does not break when marking EphemeronHashTable, see
     // MarkingVisitorBase::VisitEphemeronHashTable.
-    EphemeronHashTable table = EphemeronHashTable::cast(raw_obj);
-    MemsetTagged(table->RawField(table.kElementsStartOffset),
+    Tagged<EphemeronHashTable> table = EphemeronHashTable::cast(raw_obj);
+    MemsetTagged(table->RawField(table->kElementsStartOffset),
                  ReadOnlyRoots(isolate()).undefined_value(),
-                 (size_in_bytes - table.kElementsStartOffset) / kTaggedSize);
+                 (size_in_bytes - table->kElementsStartOffset) / kTaggedSize);
   }
 
 #ifdef DEBUG
@@ -695,7 +699,7 @@ Handle<HeapObject> Deserializer<IsolateT>::ReadObject(SnapshotSpace space) {
   // We want to make sure that all embedder pointers are initialized to null.
   if (IsJSObject(raw_obj, cage_base) &&
       JSObject::cast(raw_obj)->MayHaveEmbedderFields()) {
-    JSObject js_obj = JSObject::cast(raw_obj);
+    Tagged<JSObject> js_obj = JSObject::cast(raw_obj);
     for (int i = 0; i < js_obj->GetEmbedderFieldCount(); ++i) {
       void* pointer;
       CHECK(EmbedderDataSlot(js_obj, i).ToAlignedPointer(main_thread_isolate(),
@@ -703,7 +707,7 @@ Handle<HeapObject> Deserializer<IsolateT>::ReadObject(SnapshotSpace space) {
       CHECK_NULL(pointer);
     }
   } else if (IsEmbedderDataArray(raw_obj, cage_base)) {
-    EmbedderDataArray array = EmbedderDataArray::cast(raw_obj);
+    Tagged<EmbedderDataArray> array = EmbedderDataArray::cast(raw_obj);
     EmbedderDataSlot start(array, 0);
     EmbedderDataSlot end(array, array->length());
     for (EmbedderDataSlot slot = start; slot < end; ++slot) {
@@ -738,12 +742,12 @@ Handle<HeapObject> Deserializer<IsolateT>::ReadMetaMap() {
   const int size_in_bytes = Map::kSize;
   const int size_in_tagged = size_in_bytes / kTaggedSize;
 
-  HeapObject raw_obj =
+  Tagged<HeapObject> raw_obj =
       Allocate(SpaceToAllocation(space), size_in_bytes, kTaggedAligned);
   raw_obj->set_map_after_allocation(Map::unchecked_cast(raw_obj));
   MemsetTagged(raw_obj->RawField(kTaggedSize),
                Smi::uninitialized_deserialization_value(), size_in_tagged - 1);
-  DCHECK(raw_obj.CheckRequiredAlignment(isolate()));
+  DCHECK(raw_obj->CheckRequiredAlignment(isolate()));
 
   Handle<HeapObject> obj = handle(raw_obj, isolate());
   back_refs_.push_back(obj);
@@ -941,7 +945,7 @@ int Deserializer<IsolateT>::ReadReadOnlyHeapRef(uint8_t data,
   ReadOnlySpace* read_only_space = isolate()->heap()->read_only_space();
   ReadOnlyPage* page = read_only_space->pages()[chunk_index];
   Address address = page->OffsetToAddress(chunk_offset);
-  HeapObject heap_object = HeapObject::FromAddress(address);
+  Tagged<HeapObject> heap_object = HeapObject::FromAddress(address);
 
   return WriteHeapPointer(slot_accessor, heap_object,
                           GetAndResetNextReferenceDescriptor());
@@ -971,7 +975,7 @@ int Deserializer<IsolateT>::ReadStartupObjectCache(uint8_t data,
   int cache_index = source_.GetUint30();
   // TODO(leszeks): Could we use the address of the startup_object_cache
   // entry as a Handle backing?
-  HeapObject heap_object = HeapObject::cast(
+  Tagged<HeapObject> heap_object = HeapObject::cast(
       main_thread_isolate()->startup_object_cache()->at(cache_index));
   return WriteHeapPointer(slot_accessor, heap_object,
                           GetAndResetNextReferenceDescriptor());
@@ -986,7 +990,7 @@ int Deserializer<IsolateT>::ReadSharedHeapObjectCache(
   int cache_index = source_.GetUint30();
   // TODO(leszeks): Could we use the address of the
   // shared_heap_object_cache entry as a Handle backing?
-  HeapObject heap_object = HeapObject::cast(
+  Tagged<HeapObject> heap_object = HeapObject::cast(
       main_thread_isolate()->shared_heap_object_cache()->at(cache_index));
   DCHECK(SharedHeapSerializer::ShouldBeInSharedHeapObjectCache(heap_object));
   return WriteHeapPointer(slot_accessor, heap_object,
@@ -1271,8 +1275,8 @@ ExternalPointerTag Deserializer<IsolateT>::ReadExternalPointerTag() {
 }
 
 template <typename IsolateT>
-HeapObject Deserializer<IsolateT>::Allocate(AllocationType allocation, int size,
-                                            AllocationAlignment alignment) {
+Tagged<HeapObject> Deserializer<IsolateT>::Allocate(
+    AllocationType allocation, int size, AllocationAlignment alignment) {
 #ifdef DEBUG
   if (!previous_allocation_obj_.is_null()) {
     // Make sure that the previous object is initialized sufficiently to
@@ -1282,8 +1286,9 @@ HeapObject Deserializer<IsolateT>::Allocate(AllocationType allocation, int size,
   }
 #endif
 
-  HeapObject obj = HeapObject::FromAddress(isolate()->heap()->AllocateRawOrFail(
-      size, allocation, AllocationOrigin::kRuntime, alignment));
+  Tagged<HeapObject> obj =
+      HeapObject::FromAddress(isolate()->heap()->AllocateRawOrFail(
+          size, allocation, AllocationOrigin::kRuntime, alignment));
 
 #ifdef DEBUG
   previous_allocation_obj_ = handle(obj, isolate());
diff --git a/src/snapshot/deserializer.h b/src/snapshot/deserializer.h
index 58fc2289cec..b298b042970 100644
--- a/src/snapshot/deserializer.h
+++ b/src/snapshot/deserializer.h
@@ -57,7 +57,7 @@ class Deserializer : public SerializerDeserializer {
 
   // Create Log events for newly deserialized objects.
   void LogNewObjectEvents();
-  void LogScriptEvents(Script script);
+  void LogScriptEvents(Tagged<Script> script);
   void LogNewMapEvents();
 
   // Descriptor arrays are deserialized as "strong", so that there is no risk of
@@ -153,7 +153,8 @@ class Deserializer : public SerializerDeserializer {
   void Synchronize(VisitorSynchronization::SyncTag tag) override;
 
   template <typename SlotAccessor>
-  int WriteHeapPointer(SlotAccessor slot_accessor, HeapObject heap_object,
+  int WriteHeapPointer(SlotAccessor slot_accessor,
+                       Tagged<HeapObject> heap_object,
                        ReferenceDescriptor descr);
   template <typename SlotAccessor>
   int WriteHeapPointer(SlotAccessor slot_accessor,
@@ -162,7 +163,8 @@ class Deserializer : public SerializerDeserializer {
 
   inline int WriteExternalPointer(ExternalPointerSlot dest, Address value,
                                   ExternalPointerTag tag);
-  inline int WriteIndirectPointer(IndirectPointerSlot dest, HeapObject value);
+  inline int WriteIndirectPointer(IndirectPointerSlot dest,
+                                  Tagged<HeapObject> value);
 
   // Fills in a heap object's data from start to end (exclusive). Start and end
   // are slot indices within the object.
@@ -243,12 +245,12 @@ class Deserializer : public SerializerDeserializer {
   // Special handling for serialized code like hooking up internalized strings.
   void PostProcessNewObject(Handle<Map> map, Handle<HeapObject> obj,
                             SnapshotSpace space);
-  void PostProcessNewJSReceiver(Map map, Handle<JSReceiver> obj,
+  void PostProcessNewJSReceiver(Tagged<Map> map, Handle<JSReceiver> obj,
                                 InstanceType instance_type,
                                 SnapshotSpace space);
 
-  HeapObject Allocate(AllocationType allocation, int size,
-                      AllocationAlignment alignment);
+  Tagged<HeapObject> Allocate(AllocationType allocation, int size,
+                              AllocationAlignment alignment);
 
   // Cached current isolate.
   IsolateT* isolate_;
@@ -342,7 +344,7 @@ class StringTableInsertionKey final : public StringTableKey {
       DeserializingUserCodeOption deserializing_user_code);
 
   template <typename IsolateT>
-  bool IsMatch(IsolateT* isolate, String string);
+  bool IsMatch(IsolateT* isolate, Tagged<String> string);
 
   void PrepareForInsertion(Isolate* isolate) {
     // When sharing the string table, all string table lookups during snapshot
diff --git a/src/snapshot/embedded/embedded-data.cc b/src/snapshot/embedded/embedded-data.cc
index 02f81bbb5b9..fb3642c9595 100644
--- a/src/snapshot/embedded/embedded-data.cc
+++ b/src/snapshot/embedded/embedded-data.cc
@@ -178,7 +178,7 @@ void FinalizeEmbeddedCodeTargets(Isolate* isolate, EmbeddedData* blob) {
   static_assert(Builtins::kAllBuiltinsAreIsolateIndependent);
   for (Builtin builtin = Builtins::kFirst; builtin <= Builtins::kLast;
        ++builtin) {
-    Code code = isolate->builtins()->code(builtin);
+    Tagged<Code> code = isolate->builtins()->code(builtin);
     RelocIterator on_heap_it(code, kRelocMask);
     RelocIterator off_heap_it(blob, code, kRelocMask);
 
@@ -195,7 +195,8 @@ void FinalizeEmbeddedCodeTargets(Isolate* isolate, EmbeddedData* blob) {
 
       RelocInfo* rinfo = on_heap_it.rinfo();
       DCHECK_EQ(rinfo->rmode(), off_heap_it.rinfo()->rmode());
-      Code target_code = Code::FromTargetAddress(rinfo->target_address());
+      Tagged<Code> target_code =
+          Code::FromTargetAddress(rinfo->target_address());
       CHECK(Builtins::IsIsolateIndependentBuiltin(target_code));
 
       // Do not emit write-barrier for off-heap writes.
@@ -216,7 +217,7 @@ void FinalizeEmbeddedCodeTargets(Isolate* isolate, EmbeddedData* blob) {
   }
 }
 
-void EnsureRelocatable(Code code) {
+void EnsureRelocatable(Tagged<Code> code) {
   if (code->relocation_size() == 0) return;
 
   // On some architectures (arm) the builtin might have a non-empty reloc
@@ -253,7 +254,7 @@ EmbeddedData EmbeddedData::NewFromIsolate(Isolate* isolate) {
     BuiltinsSorter sorter;
     std::vector<uint32_t> builtin_sizes;
     for (Builtin i = Builtins::kFirst; i <= Builtins::kLast; ++i) {
-      Code code = builtins->code(i);
+      Tagged<Code> code = builtins->code(i);
       uint32_t instruction_size =
           static_cast<uint32_t>(code->instruction_size());
       uint32_t padding_size = PadAndAlignCode(instruction_size);
@@ -272,7 +273,7 @@ EmbeddedData EmbeddedData::NewFromIsolate(Isolate* isolate) {
     } else {
       builtin = reordered_builtins[embedded_index];
     }
-    Code code = builtins->code(builtin);
+    Tagged<Code> code = builtins->code(builtin);
 
     // Sanity-check that the given builtin is isolate-independent.
     if (!code->IsIsolateIndependent(isolate)) {
@@ -348,7 +349,7 @@ EmbeddedData EmbeddedData::NewFromIsolate(Isolate* isolate) {
   static_assert(Builtins::kAllBuiltinsAreIsolateIndependent);
   for (Builtin builtin = Builtins::kFirst; builtin <= Builtins::kLast;
        ++builtin) {
-    Code code = builtins->code(builtin);
+    Tagged<Code> code = builtins->code(builtin);
     uint32_t offset =
         layout_descriptions[static_cast<int>(builtin)].metadata_offset;
     uint8_t* dst = raw_metadata_start + offset;
@@ -366,7 +367,7 @@ EmbeddedData EmbeddedData::NewFromIsolate(Isolate* isolate) {
   static_assert(Builtins::kAllBuiltinsAreIsolateIndependent);
   for (Builtin builtin = Builtins::kFirst; builtin <= Builtins::kLast;
        ++builtin) {
-    Code code = builtins->code(builtin);
+    Tagged<Code> code = builtins->code(builtin);
     uint32_t offset =
         layout_descriptions[static_cast<int>(builtin)].instruction_offset;
     uint8_t* dst = raw_code_start + offset;
@@ -402,7 +403,7 @@ EmbeddedData EmbeddedData::NewFromIsolate(Isolate* isolate) {
   if (DEBUG_BOOL) {
     for (Builtin builtin = Builtins::kFirst; builtin <= Builtins::kLast;
          ++builtin) {
-      Code code = builtins->code(builtin);
+      Tagged<Code> code = builtins->code(builtin);
       CHECK_EQ(d.InstructionSizeOf(builtin), code->instruction_size());
     }
   }
diff --git a/src/snapshot/embedded/embedded-file-writer.cc b/src/snapshot/embedded/embedded-file-writer.cc
index f64a44ce101..b397434b1fe 100644
--- a/src/snapshot/embedded/embedded-file-writer.cc
+++ b/src/snapshot/embedded/embedded-file-writer.cc
@@ -271,8 +271,8 @@ void EmbeddedFileWriter::PrepareBuiltinSourcePositionMap(Builtins* builtins) {
   for (Builtin builtin = Builtins::kFirst; builtin <= Builtins::kLast;
        ++builtin) {
     // Retrieve the SourcePositionTable and copy it.
-    Code code = builtins->code(builtin);
-    ByteArray source_position_table = code->source_position_table();
+    Tagged<Code> code = builtins->code(builtin);
+    Tagged<ByteArray> source_position_table = code->source_position_table();
     std::vector<unsigned char> data(
         source_position_table->GetDataStartAddress(),
         source_position_table->GetDataEndAddress());
diff --git a/src/snapshot/read-only-serializer.cc b/src/snapshot/read-only-serializer.cc
index 5e4ab58ced4..ecf2a217835 100644
--- a/src/snapshot/read-only-serializer.cc
+++ b/src/snapshot/read-only-serializer.cc
@@ -26,8 +26,8 @@ class ObjectPreProcessor final {
   V(CallHandlerInfo)             \
   V(Code)
 
-  void PreProcessIfNeeded(HeapObject o) {
-    const InstanceType itype = o.map(isolate_)->instance_type();
+  void PreProcessIfNeeded(Tagged<HeapObject> o) {
+    const InstanceType itype = o->map(isolate_)->instance_type();
 #define V(TYPE)                               \
   if (InstanceTypeChecker::Is##TYPE(itype)) { \
     return PreProcess##TYPE(TYPE::cast(o));   \
@@ -62,23 +62,23 @@ class ObjectPreProcessor final {
     DisallowGarbageCollection no_gc;
     slot.ReplaceContentWithIndexForSerialization(no_gc, encoded.ToUint32());
   }
-  void PreProcessAccessorInfo(AccessorInfo o) {
+  void PreProcessAccessorInfo(Tagged<AccessorInfo> o) {
     EncodeExternalPointerSlot(
-        o.RawExternalPointerField(AccessorInfo::kMaybeRedirectedGetterOffset),
-        o.getter(isolate_),  // Pass the non-redirected value.
+        o->RawExternalPointerField(AccessorInfo::kMaybeRedirectedGetterOffset),
+        o->getter(isolate_),  // Pass the non-redirected value.
         kAccessorInfoGetterTag);
     EncodeExternalPointerSlot(
-        o.RawExternalPointerField(AccessorInfo::kSetterOffset),
+        o->RawExternalPointerField(AccessorInfo::kSetterOffset),
         kAccessorInfoSetterTag);
   }
-  void PreProcessCallHandlerInfo(CallHandlerInfo o) {
+  void PreProcessCallHandlerInfo(Tagged<CallHandlerInfo> o) {
     EncodeExternalPointerSlot(
-        o.RawExternalPointerField(
+        o->RawExternalPointerField(
             CallHandlerInfo::kMaybeRedirectedCallbackOffset),
-        o.callback(isolate_),  // Pass the non-redirected value.
+        o->callback(isolate_),  // Pass the non-redirected value.
         kCallHandlerInfoCallbackTag);
   }
-  void PreProcessCode(Code o) {
+  void PreProcessCode(Tagged<Code> o) {
     o->ClearInstructionStartForSerialization(isolate_);
   }
 
@@ -113,7 +113,7 @@ struct ReadOnlySegmentForSerialization {
     DCHECK_GE(segment_start, page->area_start());
     const Address segment_end = segment_start + segment_size;
     ReadOnlyPageObjectIterator it(page, segment_start);
-    for (HeapObject o = it.Next(); !o.is_null(); o = it.Next()) {
+    for (Tagged<HeapObject> o = it.Next(); !o.is_null(); o = it.Next()) {
       if (o.address() >= segment_end) break;
       size_t o_offset = o.ptr() - segment_start;
       Address o_dst = reinterpret_cast<Address>(contents.get()) + o_offset;
@@ -135,7 +135,7 @@ struct ReadOnlySegmentForSerialization {
   friend class EncodeRelocationsVisitor;
 };
 
-ro::EncodedTagged Encode(Isolate* isolate, HeapObject o) {
+ro::EncodedTagged Encode(Isolate* isolate, Tagged<HeapObject> o) {
   Address o_address = o.address();
   BasicMemoryChunk* chunk = BasicMemoryChunk::FromAddress(o_address);
 
@@ -164,44 +164,44 @@ class EncodeRelocationsVisitor final : public ObjectVisitor {
     DCHECK(!V8_STATIC_ROOTS_BOOL);
   }
 
-  void VisitPointers(HeapObject host, ObjectSlot start,
+  void VisitPointers(Tagged<HeapObject> host, ObjectSlot start,
                      ObjectSlot end) override {
     VisitPointers(host, MaybeObjectSlot(start), MaybeObjectSlot(end));
   }
 
-  void VisitPointers(HeapObject host, MaybeObjectSlot start,
+  void VisitPointers(Tagged<HeapObject> host, MaybeObjectSlot start,
                      MaybeObjectSlot end) override {
     for (MaybeObjectSlot slot = start; slot < end; slot++) {
       ProcessSlot(slot);
     }
   }
 
-  void VisitMapPointer(HeapObject host) override {
+  void VisitMapPointer(Tagged<HeapObject> host) override {
     ProcessSlot(host->RawMaybeWeakField(HeapObject::kMapOffset));
   }
 
   // Sanity-checks:
-  void VisitInstructionStreamPointer(Code host,
+  void VisitInstructionStreamPointer(Tagged<Code> host,
                                      InstructionStreamSlot slot) override {
     // RO space contains only builtin Code objects.
     DCHECK(!host->has_instruction_stream());
   }
-  void VisitCodeTarget(InstructionStream, RelocInfo*) override {
+  void VisitCodeTarget(Tagged<InstructionStream>, RelocInfo*) override {
     UNREACHABLE();
   }
-  void VisitEmbeddedPointer(InstructionStream, RelocInfo*) override {
+  void VisitEmbeddedPointer(Tagged<InstructionStream>, RelocInfo*) override {
     UNREACHABLE();
   }
-  void VisitExternalReference(InstructionStream, RelocInfo*) override {
+  void VisitExternalReference(Tagged<InstructionStream>, RelocInfo*) override {
     UNREACHABLE();
   }
-  void VisitInternalReference(InstructionStream, RelocInfo*) override {
+  void VisitInternalReference(Tagged<InstructionStream>, RelocInfo*) override {
     UNREACHABLE();
   }
-  void VisitOffHeapTarget(InstructionStream, RelocInfo*) override {
+  void VisitOffHeapTarget(Tagged<InstructionStream>, RelocInfo*) override {
     UNREACHABLE();
   }
-  void VisitExternalPointer(HeapObject, ExternalPointerSlot slot,
+  void VisitExternalPointer(Tagged<HeapObject>, ExternalPointerSlot slot,
                             ExternalPointerTag tag) override {
     // This slot was encoded in a previous pass, see EncodeExternalPointerSlot.
 #ifdef DEBUG
@@ -266,7 +266,7 @@ void ReadOnlySegmentForSerialization::EncodeTaggedSlots(Isolate* isolate) {
   const Address segment_end = segment_start + segment_size;
   ReadOnlyPageObjectIterator it(page, segment_start,
                                 SkipFreeSpaceOrFiller::kNo);
-  for (HeapObject o = it.Next(); !o.is_null(); o = it.Next()) {
+  for (Tagged<HeapObject> o = it.Next(); !o.is_null(); o = it.Next()) {
     if (o.address() >= segment_end) break;
     o->Iterate(cage_base, &v);
   }
@@ -380,7 +380,7 @@ class ReadOnlyHeapImageSerializer {
       ReadOnlyRoots roots(isolate_);
       for (size_t i = 0; i < ReadOnlyRoots::kEntriesCount; i++) {
         RootIndex rudi = static_cast<RootIndex>(i);
-        HeapObject rudolf = HeapObject::cast(roots.object_at(rudi));
+        Tagged<HeapObject> rudolf = HeapObject::cast(roots.object_at(rudi));
         ro::EncodedTagged encoded = Encode(isolate_, rudolf);
         sink_->PutUint32(encoded.ToUint32(), "read only roots entry");
       }
@@ -400,8 +400,8 @@ std::vector<ReadOnlyHeapImageSerializer::MemoryRegion> GetUnmappedRegions(
   // snapshot size and accessing uninitialized and/or unmapped memory, the
   // serializer skips the padding bytes and the payload.
   ReadOnlyRoots ro_roots(isolate);
-  WasmNull wasm_null = ro_roots.wasm_null();
-  HeapObject wasm_null_padding = ro_roots.wasm_null_padding();
+  Tagged<WasmNull> wasm_null = ro_roots.wasm_null();
+  Tagged<HeapObject> wasm_null_padding = ro_roots.wasm_null_padding();
   CHECK(IsFreeSpace(wasm_null_padding));
   Address wasm_null_padding_start =
       wasm_null_padding.address() + FreeSpace::kHeaderSize;
@@ -433,7 +433,7 @@ void ReadOnlySerializer::Serialize() {
                                          GetUnmappedRegions(isolate()));
 
   ReadOnlyHeapObjectIterator it(isolate()->read_only_heap());
-  for (HeapObject o = it.Next(); !o.is_null(); o = it.Next()) {
+  for (Tagged<HeapObject> o = it.Next(); !o.is_null(); o = it.Next()) {
     CheckRehashability(o);
     if (v8_flags.serialization_statistics) {
       CountAllocation(o->map(), o->Size(), SnapshotSpace::kReadOnlyHeap);
diff --git a/src/snapshot/references.h b/src/snapshot/references.h
index 02dacf09053..37247b9a8c7 100644
--- a/src/snapshot/references.h
+++ b/src/snapshot/references.h
@@ -104,7 +104,7 @@ class SerializerReferenceMap {
   explicit SerializerReferenceMap(Isolate* isolate)
       : map_(isolate->heap()), attached_reference_index_(0) {}
 
-  const SerializerReference* LookupReference(HeapObject object) const {
+  const SerializerReference* LookupReference(Tagged<HeapObject> object) const {
     return map_.Find(object);
   }
 
@@ -118,7 +118,7 @@ class SerializerReferenceMap {
     return &it->second;
   }
 
-  void Add(HeapObject object, SerializerReference reference) {
+  void Add(Tagged<HeapObject> object, SerializerReference reference) {
     DCHECK_NULL(LookupReference(object));
     map_.Insert(object, reference);
   }
@@ -128,7 +128,7 @@ class SerializerReferenceMap {
     backing_store_map_.emplace(backing_store, reference);
   }
 
-  SerializerReference AddAttachedReference(HeapObject object) {
+  SerializerReference AddAttachedReference(Tagged<HeapObject> object) {
     SerializerReference reference =
         SerializerReference::AttachedReference(attached_reference_index_++);
     map_.Insert(object, reference);
diff --git a/src/snapshot/roots-serializer.cc b/src/snapshot/roots-serializer.cc
index 4b6872f065e..3c42cfbebc5 100644
--- a/src/snapshot/roots-serializer.cc
+++ b/src/snapshot/roots-serializer.cc
@@ -57,7 +57,7 @@ void RootsSerializer::VisitRootPointers(Root root, const char* description,
   }
 }
 
-void RootsSerializer::CheckRehashability(HeapObject obj) {
+void RootsSerializer::CheckRehashability(Tagged<HeapObject> obj) {
   if (!can_be_rehashed_) return;
   if (!obj->NeedsRehashing(cage_base())) return;
   if (obj->CanBeRehashed(cage_base())) return;
diff --git a/src/snapshot/roots-serializer.h b/src/snapshot/roots-serializer.h
index 739d8df9036..5d3c5658f74 100644
--- a/src/snapshot/roots-serializer.h
+++ b/src/snapshot/roots-serializer.h
@@ -34,14 +34,14 @@ class RootsSerializer : public Serializer {
     return root_has_been_serialized_.test(static_cast<size_t>(root_index));
   }
 
-  bool IsRootAndHasBeenSerialized(HeapObject obj) const {
+  bool IsRootAndHasBeenSerialized(Tagged<HeapObject> obj) const {
     RootIndex root_index;
     return root_index_map()->Lookup(obj, &root_index) &&
            root_has_been_serialized(root_index);
   }
 
  protected:
-  void CheckRehashability(HeapObject obj);
+  void CheckRehashability(Tagged<HeapObject> obj);
 
   // Serializes |object| if not previously seen and returns its cache index.
   int SerializeInObjectCache(Handle<HeapObject> object);
diff --git a/src/snapshot/serializer-deserializer.cc b/src/snapshot/serializer-deserializer.cc
index 712fb846711..eb49a49b365 100644
--- a/src/snapshot/serializer-deserializer.cc
+++ b/src/snapshot/serializer-deserializer.cc
@@ -41,7 +41,8 @@ void SerializerDeserializer::IterateSharedHeapObjectCache(
                      Root::kSharedHeapObjectCache, visitor);
 }
 
-bool SerializerDeserializer::CanBeDeferred(HeapObject o, SlotType slot_type) {
+bool SerializerDeserializer::CanBeDeferred(Tagged<HeapObject> o,
+                                           SlotType slot_type) {
   // HeapObjects' map slots cannot be deferred as objects are expected to have a
   // valid map immediately.
   if (slot_type == SlotType::kMapSlot) {
@@ -65,13 +66,13 @@ bool SerializerDeserializer::CanBeDeferred(HeapObject o, SlotType slot_type) {
 }
 
 void SerializerDeserializer::RestoreExternalReferenceRedirector(
-    Isolate* isolate, AccessorInfo accessor_info) {
+    Isolate* isolate, Tagged<AccessorInfo> accessor_info) {
   DisallowGarbageCollection no_gc;
   accessor_info->init_getter_redirection(isolate);
 }
 
 void SerializerDeserializer::RestoreExternalReferenceRedirector(
-    Isolate* isolate, CallHandlerInfo call_handler_info) {
+    Isolate* isolate, Tagged<CallHandlerInfo> call_handler_info) {
   DisallowGarbageCollection no_gc;
   call_handler_info->init_callback_redirection(isolate);
 }
diff --git a/src/snapshot/serializer-deserializer.h b/src/snapshot/serializer-deserializer.h
index 50158c0db64..968af391146 100644
--- a/src/snapshot/serializer-deserializer.h
+++ b/src/snapshot/serializer-deserializer.h
@@ -29,14 +29,14 @@ class SerializerDeserializer : public RootVisitor {
     kAnySlot,
     kMapSlot,
   };
-  static bool CanBeDeferred(HeapObject o, SlotType slot_type);
+  static bool CanBeDeferred(Tagged<HeapObject> o, SlotType slot_type);
 
   void RestoreExternalReferenceRedirector(Isolate* isolate,
-                                          AccessorInfo accessor_info);
-  void RestoreExternalReferenceRedirector(Isolate* isolate,
-                                          CallHandlerInfo call_handler_info);
+                                          Tagged<AccessorInfo> accessor_info);
+  void RestoreExternalReferenceRedirector(
+      Isolate* isolate, Tagged<CallHandlerInfo> call_handler_info);
 
-// clang-format off
+  // clang-format off
 #define UNUSED_SERIALIZER_BYTE_CODES(V)                           \
   /* Free range 0x10..0x1f */                                     \
   V(0x1b) V(0x1c) V(0x1d) V(0x1e) V(0x1f)                         \
diff --git a/src/snapshot/serializer-inl.h b/src/snapshot/serializer-inl.h
index 3767f192e5d..487efe35a99 100644
--- a/src/snapshot/serializer-inl.h
+++ b/src/snapshot/serializer-inl.h
@@ -11,7 +11,7 @@
 namespace v8 {
 namespace internal {
 
-bool Serializer::IsNotMappedSymbol(HeapObject obj) const {
+bool Serializer::IsNotMappedSymbol(Tagged<HeapObject> obj) const {
   Object not_mapped_symbol = ReadOnlyRoots(isolate()).not_mapped_symbol();
   if (V8_EXTERNAL_CODE_SPACE_BOOL) {
     // It's possible that a InstructionStream object might have the same
diff --git a/src/snapshot/serializer.cc b/src/snapshot/serializer.cc
index ae914f75153..2d3fb4b077d 100644
--- a/src/snapshot/serializer.cc
+++ b/src/snapshot/serializer.cc
@@ -58,7 +58,8 @@ Serializer::Serializer(Isolate* isolate, Snapshot::SerializerFlags flags)
 void Serializer::PopStack() { stack_.Pop(); }
 #endif
 
-void Serializer::CountAllocation(Map map, int size, SnapshotSpace space) {
+void Serializer::CountAllocation(Tagged<Map> map, int size,
+                                 SnapshotSpace space) {
   DCHECK(v8_flags.serialization_statistics);
 
   const int space_number = static_cast<int>(space);
@@ -166,7 +167,7 @@ void Serializer::SerializeObject(Handle<HeapObject> obj, SlotType slot_type) {
   SerializeObjectImpl(obj, slot_type);
 }
 
-bool Serializer::MustBeDeferred(HeapObject object) { return false; }
+bool Serializer::MustBeDeferred(Tagged<HeapObject> object) { return false; }
 
 void Serializer::VisitRootPointers(Root root, const char* description,
                                    FullObjectSlot start, FullObjectSlot end) {
@@ -195,7 +196,7 @@ void Serializer::PrintStack(std::ostream& out) {
 }
 #endif  // DEBUG
 
-bool Serializer::SerializeRoot(HeapObject obj) {
+bool Serializer::SerializeRoot(Tagged<HeapObject> obj) {
   RootIndex root_index;
   // Derived serializers are responsible for determining if the root has
   // actually been serialized before calling this.
@@ -206,7 +207,7 @@ bool Serializer::SerializeRoot(HeapObject obj) {
   return false;
 }
 
-bool Serializer::SerializeHotObject(HeapObject obj) {
+bool Serializer::SerializeHotObject(Tagged<HeapObject> obj) {
   DisallowGarbageCollection no_gc;
   // Encode a reference to a hot object by its index in the working set.
   int index = hot_objects_.Find(obj);
@@ -221,7 +222,7 @@ bool Serializer::SerializeHotObject(HeapObject obj) {
   return true;
 }
 
-bool Serializer::SerializeBackReference(HeapObject obj) {
+bool Serializer::SerializeBackReference(Tagged<HeapObject> obj) {
   DisallowGarbageCollection no_gc;
   const SerializerReference* reference = reference_map_.LookupReference(obj);
   if (reference == nullptr) return false;
@@ -249,7 +250,7 @@ bool Serializer::SerializeBackReference(HeapObject obj) {
   return true;
 }
 
-bool Serializer::SerializePendingObject(HeapObject obj) {
+bool Serializer::SerializePendingObject(Tagged<HeapObject> obj) {
   PendingObjectReferences* refs_to_object =
       forward_refs_per_pending_object_.Find(obj);
   if (refs_to_object == nullptr) {
@@ -259,7 +260,7 @@ bool Serializer::SerializePendingObject(HeapObject obj) {
   return true;
 }
 
-bool Serializer::ObjectIsBytecodeHandler(HeapObject obj) const {
+bool Serializer::ObjectIsBytecodeHandler(Tagged<HeapObject> obj) const {
   if (!IsCode(obj)) return false;
   return (Code::cast(obj)->kind() == CodeKind::BYTECODE_HANDLER);
 }
@@ -267,7 +268,7 @@ bool Serializer::ObjectIsBytecodeHandler(HeapObject obj) const {
 void Serializer::PutRoot(RootIndex root) {
   DisallowGarbageCollection no_gc;
   int root_index = static_cast<int>(root);
-  HeapObject object = HeapObject::cast(isolate()->root(root));
+  Tagged<HeapObject> object = HeapObject::cast(isolate()->root(root));
   if (v8_flags.trace_serializer) {
     PrintF(" Encoding root %d:", root_index);
     ShortPrint(object);
@@ -306,7 +307,7 @@ void Serializer::PutSmiRoot(FullObjectSlot slot) {
   sink_.PutRaw(raw_value_as_bytes, bytes_to_output, "Bytes");
 }
 
-void Serializer::PutBackReference(HeapObject object,
+void Serializer::PutBackReference(Tagged<HeapObject> object,
                                   SerializerReference reference) {
   DCHECK_EQ(object, *back_refs_[reference.back_ref_index()]);
   sink_.PutUint30(reference.back_ref_index(), "BackRefIndex");
@@ -372,7 +373,7 @@ ExternalReferenceEncoder::Value Serializer::EncodeExternalReference(
   return result.FromJust();
 }
 
-void Serializer::RegisterObjectIsPending(HeapObject obj) {
+void Serializer::RegisterObjectIsPending(Tagged<HeapObject> obj) {
   DisallowGarbageCollection no_gc;
   if (IsNotMappedSymbol(obj)) return;
 
@@ -389,7 +390,7 @@ void Serializer::RegisterObjectIsPending(HeapObject obj) {
                  CanBeDeferred(obj, SlotType::kAnySlot));
 }
 
-void Serializer::ResolvePendingObject(HeapObject obj) {
+void Serializer::ResolvePendingObject(Tagged<HeapObject> obj) {
   DisallowGarbageCollection no_gc;
   if (IsNotMappedSymbol(obj)) return;
 
@@ -422,7 +423,8 @@ void Serializer::InitializeCodeAddressMap() {
   code_address_map_ = std::make_unique<CodeAddressMap>(isolate_);
 }
 
-InstructionStream Serializer::CopyCode(InstructionStream istream) {
+Tagged<InstructionStream> Serializer::CopyCode(
+    Tagged<InstructionStream> istream) {
   code_buffer_.clear();  // Clear buffer without deleting backing store.
   // Add InstructionStream padding which is usually added by the allocator.
   // While this doesn't guarantee the exact same alignment, it's enough to
@@ -440,7 +442,8 @@ InstructionStream Serializer::CopyCode(InstructionStream istream) {
 }
 
 void Serializer::ObjectSerializer::SerializePrologue(SnapshotSpace space,
-                                                     int size, Map map) {
+                                                     int size,
+                                                     Tagged<Map> map) {
   if (serializer_->code_address_map_) {
     const char* code_name =
         serializer_->code_address_map_->Lookup(object_->address());
@@ -661,7 +664,7 @@ void Serializer::ObjectSerializer::SerializeExternalStringAsSequentialString() {
   DCHECK(IsExternalString(*object_, cage_base));
   Handle<ExternalString> string = Handle<ExternalString>::cast(object_);
   int length = string->length();
-  Map map;
+  Tagged<Map> map;
   int content_size;
   int allocation_size;
   const uint8_t* resource;
@@ -717,7 +720,7 @@ void Serializer::ObjectSerializer::SerializeExternalStringAsSequentialString() {
 // TODO(all): replace this with proper iteration of weak slots in serializer.
 class V8_NODISCARD UnlinkWeakNextScope {
  public:
-  explicit UnlinkWeakNextScope(Heap* heap, HeapObject object) {
+  explicit UnlinkWeakNextScope(Heap* heap, Tagged<HeapObject> object) {
     Isolate* isolate = heap->isolate();
     if (IsAllocationSite(object, isolate) &&
         AllocationSite::cast(object)->HasWeakNext()) {
@@ -734,7 +737,7 @@ class V8_NODISCARD UnlinkWeakNextScope {
   }
 
  private:
-  HeapObject object_;
+  Tagged<HeapObject> object_;
   Object next_ = Smi::zero();
   DISALLOW_GARBAGE_COLLECTION(no_gc_)
 };
@@ -744,7 +747,7 @@ void Serializer::ObjectSerializer::Serialize(SlotType slot_type) {
 
   {
     DisallowGarbageCollection no_gc;
-    HeapObject raw = *object_;
+    Tagged<HeapObject> raw = *object_;
     // Defer objects as "pending" if they cannot be serialized now, or if we
     // exceed a certain recursion depth. Some objects cannot be deferred.
     bool should_defer =
@@ -813,7 +816,7 @@ void Serializer::ObjectSerializer::Serialize(SlotType slot_type) {
 }
 
 namespace {
-SnapshotSpace GetSnapshotSpace(HeapObject object) {
+SnapshotSpace GetSnapshotSpace(Tagged<HeapObject> object) {
   if (V8_ENABLE_THIRD_PARTY_HEAP_BOOL) {
     if (IsInstructionStream(object)) {
       return SnapshotSpace::kCode;
@@ -856,7 +859,7 @@ SnapshotSpace GetSnapshotSpace(HeapObject object) {
 }  // namespace
 
 void Serializer::ObjectSerializer::SerializeObject() {
-  Map map = object_->map(serializer_->cage_base());
+  Tagged<Map> map = object_->map(serializer_->cage_base());
   int size = object_->SizeFromMap(map);
 
   // Descriptor arrays have complex element weakness, that is dependent on the
@@ -899,8 +902,8 @@ void Serializer::ObjectSerializer::SerializeDeferred() {
   Serialize(SlotType::kAnySlot);
 }
 
-void Serializer::ObjectSerializer::SerializeContent(Map map, int size) {
-  HeapObject raw = *object_;
+void Serializer::ObjectSerializer::SerializeContent(Tagged<Map> map, int size) {
+  Tagged<HeapObject> raw = *object_;
   UnlinkWeakNextScope unlink_weak_next(isolate()->heap(), raw);
   // Iterate references first.
   raw->IterateBody(map, size, this);
@@ -908,13 +911,13 @@ void Serializer::ObjectSerializer::SerializeContent(Map map, int size) {
   OutputRawData(raw.address() + size);
 }
 
-void Serializer::ObjectSerializer::VisitPointers(HeapObject host,
+void Serializer::ObjectSerializer::VisitPointers(Tagged<HeapObject> host,
                                                  ObjectSlot start,
                                                  ObjectSlot end) {
   VisitPointers(host, MaybeObjectSlot(start), MaybeObjectSlot(end));
 }
 
-void Serializer::ObjectSerializer::VisitPointers(HeapObject host,
+void Serializer::ObjectSerializer::VisitPointers(Tagged<HeapObject> host,
                                                  MaybeObjectSlot start,
                                                  MaybeObjectSlot end) {
   HandleScope scope(isolate());
@@ -931,12 +934,12 @@ void Serializer::ObjectSerializer::VisitPointers(HeapObject host,
     }
     // TODO(ishell): Revisit this change once we stick to 32-bit compressed
     // tagged values.
-    while (current < end && current.load(cage_base).IsCleared()) {
+    while (current < end && current.load(cage_base)->IsCleared()) {
       sink_->Put(kClearedWeakReference, "ClearedWeakReference");
       bytes_processed_so_far_ += kTaggedSize;
       ++current;
     }
-    HeapObject current_contents;
+    Tagged<HeapObject> current_contents;
     HeapObjectReferenceType reference_type;
     while (current < end && current.load(cage_base)->GetHeapObject(
                                 &current_contents, &reference_type)) {
@@ -983,34 +986,34 @@ void Serializer::ObjectSerializer::VisitPointers(HeapObject host,
 }
 
 void Serializer::ObjectSerializer::VisitInstructionStreamPointer(
-    Code host, InstructionStreamSlot slot) {
+    Tagged<Code> host, InstructionStreamSlot slot) {
   DCHECK(!host->has_instruction_stream());
 }
 
 // All of these visitor functions are unreachable since we don't serialize
 // InstructionStream objects anymore.
-void Serializer::ObjectSerializer::VisitEmbeddedPointer(InstructionStream host,
-                                                        RelocInfo* rinfo) {
+void Serializer::ObjectSerializer::VisitEmbeddedPointer(
+    Tagged<InstructionStream> host, RelocInfo* rinfo) {
   UNREACHABLE();
 }
 
 void Serializer::ObjectSerializer::VisitExternalReference(
-    InstructionStream host, RelocInfo* rinfo) {
+    Tagged<InstructionStream> host, RelocInfo* rinfo) {
   UNREACHABLE();
 }
 
 void Serializer::ObjectSerializer::VisitInternalReference(
-    InstructionStream host, RelocInfo* rinfo) {
+    Tagged<InstructionStream> host, RelocInfo* rinfo) {
   UNREACHABLE();
 }
 
-void Serializer::ObjectSerializer::VisitOffHeapTarget(InstructionStream host,
-                                                      RelocInfo* rinfo) {
+void Serializer::ObjectSerializer::VisitOffHeapTarget(
+    Tagged<InstructionStream> host, RelocInfo* rinfo) {
   UNREACHABLE();
 }
 
-void Serializer::ObjectSerializer::VisitCodeTarget(InstructionStream host,
-                                                   RelocInfo* rinfo) {
+void Serializer::ObjectSerializer::VisitCodeTarget(
+    Tagged<InstructionStream> host, RelocInfo* rinfo) {
   UNREACHABLE();
 }
 
@@ -1073,7 +1076,7 @@ void Serializer::ObjectSerializer::OutputExternalReference(
 }
 
 void Serializer::ObjectSerializer::VisitExternalPointer(
-    HeapObject host, ExternalPointerSlot slot, ExternalPointerTag tag) {
+    Tagged<HeapObject> host, ExternalPointerSlot slot, ExternalPointerTag tag) {
   PtrComprCageBase cage_base(isolate());
   InstanceType instance_type = object_->map(cage_base)->instance_type();
   if (InstanceTypeChecker::IsForeign(instance_type) ||
@@ -1110,7 +1113,8 @@ void Serializer::ObjectSerializer::VisitExternalPointer(
 }
 
 void Serializer::ObjectSerializer::VisitIndirectPointer(
-    HeapObject host, IndirectPointerSlot slot, IndirectPointerMode mode) {
+    Tagged<HeapObject> host, IndirectPointerSlot slot,
+    IndirectPointerMode mode) {
   DCHECK(V8_CODE_POINTER_SANDBOXING_BOOL);
 
   // The slot must be properly initialized at this point, so will always contain
@@ -1258,7 +1262,7 @@ Handle<FixedArray> ObjectCacheIndexMap::Values(Isolate* isolate) {
   return externals;
 }
 
-bool Serializer::SerializeReadOnlyObjectReference(HeapObject obj,
+bool Serializer::SerializeReadOnlyObjectReference(Tagged<HeapObject> obj,
                                                   SnapshotByteSink* sink) {
   if (!ReadOnlyHeap::Contains(obj)) return false;
 
diff --git a/src/snapshot/serializer.h b/src/snapshot/serializer.h
index afb406e185b..cbc9fc35bba 100644
--- a/src/snapshot/serializer.h
+++ b/src/snapshot/serializer.h
@@ -32,10 +32,12 @@ class CodeAddressMap : public CodeEventLogger {
     isolate_->v8_file_logger()->RemoveLogEventListener(this);
   }
 
-  void CodeMoveEvent(InstructionStream from, InstructionStream to) override {
+  void CodeMoveEvent(Tagged<InstructionStream> from,
+                     Tagged<InstructionStream> to) override {
     address_to_name_map_.Move(from.address(), to.address());
   }
-  void BytecodeMoveEvent(BytecodeArray from, BytecodeArray to) override {
+  void BytecodeMoveEvent(Tagged<BytecodeArray> from,
+                         Tagged<BytecodeArray> to) override {
     address_to_name_map_.Move(from.address(), to.address());
   }
 
@@ -121,8 +123,9 @@ class CodeAddressMap : public CodeEventLogger {
     base::HashMap impl_;
   };
 
-  void LogRecordedBuffer(AbstractCode code, MaybeHandle<SharedFunctionInfo>,
-                         const char* name, int length) override {
+  void LogRecordedBuffer(Tagged<AbstractCode> code,
+                         MaybeHandle<SharedFunctionInfo>, const char* name,
+                         int length) override {
     DisallowGarbageCollection no_gc;
     address_to_name_map_.Insert(code.address(), name, length);
   }
@@ -146,7 +149,7 @@ class ObjectCacheIndexMap {
   // If |obj| is in the map, immediately return true.  Otherwise add it to the
   // map and return false. In either case set |*index_out| to the index
   // associated with the map.
-  bool LookupOrInsert(HeapObject obj, int* index_out) {
+  bool LookupOrInsert(Tagged<HeapObject> obj, int* index_out) {
     auto find_result = map_.FindOrInsert(obj);
     if (!find_result.already_exists) {
       *find_result.entry = next_index_++;
@@ -158,7 +161,7 @@ class ObjectCacheIndexMap {
     return LookupOrInsert(*obj, index_out);
   }
 
-  bool Lookup(HeapObject obj, int* index_out) const {
+  bool Lookup(Tagged<HeapObject> obj, int* index_out) const {
     int* index = map_.Find(obj);
     if (index == nullptr) {
       return false;
@@ -227,14 +230,14 @@ class Serializer : public SerializerDeserializer {
 
   // Compares obj with not_mapped_symbol root. When V8_EXTERNAL_CODE_SPACE is
   // enabled it compares full pointers.
-  V8_INLINE bool IsNotMappedSymbol(HeapObject obj) const;
+  V8_INLINE bool IsNotMappedSymbol(Tagged<HeapObject> obj) const;
 
   void SerializeDeferredObjects();
   void SerializeObject(Handle<HeapObject> o, SlotType slot_type);
   virtual void SerializeObjectImpl(Handle<HeapObject> o,
                                    SlotType slot_type) = 0;
 
-  virtual bool MustBeDeferred(HeapObject object);
+  virtual bool MustBeDeferred(Tagged<HeapObject> object);
 
   void VisitRootPointers(Root root, const char* description,
                          FullObjectSlot start, FullObjectSlot end) override;
@@ -242,7 +245,8 @@ class Serializer : public SerializerDeserializer {
 
   void PutRoot(RootIndex root_index);
   void PutSmiRoot(FullObjectSlot slot);
-  void PutBackReference(HeapObject object, SerializerReference reference);
+  void PutBackReference(Tagged<HeapObject> object,
+                        SerializerReference reference);
   void PutAttachedReference(SerializerReference reference);
   void PutNextChunk(SnapshotSpace space);
   void PutRepeat(int repeat_count);
@@ -255,19 +259,19 @@ class Serializer : public SerializerDeserializer {
   void ResolvePendingForwardReference(int obj);
 
   // Returns true if the object was successfully serialized as a root.
-  bool SerializeRoot(HeapObject obj);
+  bool SerializeRoot(Tagged<HeapObject> obj);
 
   // Returns true if the object was successfully serialized as hot object.
-  bool SerializeHotObject(HeapObject obj);
+  bool SerializeHotObject(Tagged<HeapObject> obj);
 
   // Returns true if the object was successfully serialized as back reference.
-  bool SerializeBackReference(HeapObject obj);
+  bool SerializeBackReference(Tagged<HeapObject> obj);
 
   // Returns true if the object was successfully serialized as pending object.
-  bool SerializePendingObject(HeapObject obj);
+  bool SerializePendingObject(Tagged<HeapObject> obj);
 
   // Returns true if the given heap object is a bytecode handler code object.
-  bool ObjectIsBytecodeHandler(HeapObject obj) const;
+  bool ObjectIsBytecodeHandler(Tagged<HeapObject> obj) const;
 
   ExternalReferenceEncoder::Value EncodeExternalReference(Address addr);
 
@@ -276,7 +280,8 @@ class Serializer : public SerializerDeserializer {
     return external_reference_encoder_.TryEncode(addr);
   }
 
-  bool SerializeReadOnlyObjectReference(HeapObject obj, SnapshotByteSink* sink);
+  bool SerializeReadOnlyObjectReference(Tagged<HeapObject> obj,
+                                        SnapshotByteSink* sink);
 
   // GetInt reads 4 bytes at once, requiring padding at the end.
   // Use padding_offset to specify the space you want to use after padding.
@@ -286,9 +291,9 @@ class Serializer : public SerializerDeserializer {
   // of the serializer.  Initialize it on demand.
   void InitializeCodeAddressMap();
 
-  InstructionStream CopyCode(InstructionStream istream);
+  Tagged<InstructionStream> CopyCode(Tagged<InstructionStream> istream);
 
-  void QueueDeferredObject(HeapObject obj) {
+  void QueueDeferredObject(Tagged<HeapObject> obj) {
     DCHECK_NULL(reference_map_.LookupReference(obj));
     deferred_objects_.Push(obj);
   }
@@ -296,14 +301,14 @@ class Serializer : public SerializerDeserializer {
   // Register that the the given object shouldn't be immediately serialized, but
   // will be serialized later and any references to it should be pending forward
   // references.
-  void RegisterObjectIsPending(HeapObject obj);
+  void RegisterObjectIsPending(Tagged<HeapObject> obj);
 
   // Resolve the given pending object reference with the current object.
-  void ResolvePendingObject(HeapObject obj);
+  void ResolvePendingObject(Tagged<HeapObject> obj);
 
   void OutputStatistics(const char* name);
 
-  void CountAllocation(Map map, int size, SnapshotSpace space);
+  void CountAllocation(Tagged<Map> map, int size, SnapshotSpace space);
 
 #ifdef DEBUG
   void PushStack(Handle<HeapObject> o) { stack_.Push(*o); }
@@ -355,14 +360,14 @@ class Serializer : public SerializerDeserializer {
     HotObjectsList(const HotObjectsList&) = delete;
     HotObjectsList& operator=(const HotObjectsList&) = delete;
 
-    void Add(HeapObject object) {
+    void Add(Tagged<HeapObject> object) {
       circular_queue_[index_] = object.ptr();
       index_ = (index_ + 1) & kSizeMask;
     }
 
     static const int kNotFound = -1;
 
-    int Find(HeapObject object) {
+    int Find(Tagged<HeapObject> object) {
       DCHECK(!AllowGarbageCollection::IsAllowed());
       for (int i = 0; i < kSize; i++) {
         if (circular_queue_[i] == object.ptr()) {
@@ -465,33 +470,36 @@ class Serializer::ObjectSerializer : public ObjectVisitor {
   void Serialize(SlotType slot_type);
   void SerializeObject();
   void SerializeDeferred();
-  void VisitPointers(HeapObject host, ObjectSlot start,
+  void VisitPointers(Tagged<HeapObject> host, ObjectSlot start,
                      ObjectSlot end) override;
-  void VisitPointers(HeapObject host, MaybeObjectSlot start,
+  void VisitPointers(Tagged<HeapObject> host, MaybeObjectSlot start,
                      MaybeObjectSlot end) override;
-  void VisitInstructionStreamPointer(Code host,
+  void VisitInstructionStreamPointer(Tagged<Code> host,
                                      InstructionStreamSlot slot) override;
-  void VisitEmbeddedPointer(InstructionStream host, RelocInfo* target) override;
-  void VisitExternalReference(InstructionStream host,
+  void VisitEmbeddedPointer(Tagged<InstructionStream> host,
+                            RelocInfo* target) override;
+  void VisitExternalReference(Tagged<InstructionStream> host,
                               RelocInfo* rinfo) override;
-  void VisitInternalReference(InstructionStream host,
+  void VisitInternalReference(Tagged<InstructionStream> host,
                               RelocInfo* rinfo) override;
-  void VisitCodeTarget(InstructionStream host, RelocInfo* target) override;
-  void VisitOffHeapTarget(InstructionStream host, RelocInfo* target) override;
+  void VisitCodeTarget(Tagged<InstructionStream> host,
+                       RelocInfo* target) override;
+  void VisitOffHeapTarget(Tagged<InstructionStream> host,
+                          RelocInfo* target) override;
 
-  void VisitExternalPointer(HeapObject host, ExternalPointerSlot slot,
+  void VisitExternalPointer(Tagged<HeapObject> host, ExternalPointerSlot slot,
                             ExternalPointerTag tag) override;
-  void VisitIndirectPointer(HeapObject host, IndirectPointerSlot slot,
+  void VisitIndirectPointer(Tagged<HeapObject> host, IndirectPointerSlot slot,
                             IndirectPointerMode mode) override;
 
   Isolate* isolate() { return isolate_; }
 
  private:
-  void SerializePrologue(SnapshotSpace space, int size, Map map);
+  void SerializePrologue(SnapshotSpace space, int size, Tagged<Map> map);
 
   // This function outputs or skips the raw data between the last pointer and
   // up to the current position.
-  void SerializeContent(Map map, int size);
+  void SerializeContent(Tagged<Map> map, int size);
   void OutputExternalReference(Address target, int target_size, bool sandboxify,
                                ExternalPointerTag tag);
   void OutputRawData(Address up_to);
diff --git a/src/snapshot/shared-heap-serializer.cc b/src/snapshot/shared-heap-serializer.cc
index c75dc98d805..853c3a88d52 100644
--- a/src/snapshot/shared-heap-serializer.cc
+++ b/src/snapshot/shared-heap-serializer.cc
@@ -12,7 +12,7 @@ namespace v8 {
 namespace internal {
 
 // static
-bool SharedHeapSerializer::CanBeInSharedOldSpace(HeapObject obj) {
+bool SharedHeapSerializer::CanBeInSharedOldSpace(Tagged<HeapObject> obj) {
   if (ReadOnlyHeap::Contains(obj)) return false;
   if (IsString(obj)) {
     return IsInternalizedString(obj) ||
@@ -22,7 +22,8 @@ bool SharedHeapSerializer::CanBeInSharedOldSpace(HeapObject obj) {
 }
 
 // static
-bool SharedHeapSerializer::ShouldBeInSharedHeapObjectCache(HeapObject obj) {
+bool SharedHeapSerializer::ShouldBeInSharedHeapObjectCache(
+    Tagged<HeapObject> obj) {
   // To keep the shared heap object cache lean, only include objects that should
   // not be duplicated. Currently, that is only internalized strings. In-place
   // internalizable strings will still be allocated in the shared heap by the
@@ -54,7 +55,7 @@ void SharedHeapSerializer::FinalizeSerialization() {
   // This is called after serialization of the startup and context snapshots
   // which entries are added to the shared heap object cache. Terminate the
   // cache with an undefined.
-  Object undefined = ReadOnlyRoots(isolate()).undefined_value();
+  Tagged<Object> undefined = ReadOnlyRoots(isolate()).undefined_value();
   VisitRootPointer(Root::kSharedHeapObjectCache, nullptr,
                    FullObjectSlot(&undefined));
 
@@ -70,7 +71,7 @@ void SharedHeapSerializer::FinalizeSerialization() {
   IdentityMap<int, base::DefaultAllocationPolicy>::IteratableScope it_scope(
       &serialized_objects_);
   for (auto it = it_scope.begin(); it != it_scope.end(); ++it) {
-    HeapObject obj = HeapObject::cast(it.key());
+    Tagged<HeapObject> obj = HeapObject::cast(it.key());
     CHECK(CanBeInSharedOldSpace(obj));
     CHECK(!ReadOnlyHeap::Contains(obj));
   }
@@ -141,7 +142,7 @@ void SharedHeapSerializer::SerializeStringTable(StringTable* string_table) {
       DCHECK_EQ(root, Root::kStringTable);
       Isolate* isolate = serializer_->isolate();
       for (OffHeapObjectSlot current = start; current < end; ++current) {
-        Object obj = current.load(isolate);
+        Tagged<Object> obj = current.load(isolate);
         if (IsHeapObject(obj)) {
           DCHECK(IsInternalizedString(obj));
           serializer_->SerializeObject(handle(HeapObject::cast(obj), isolate),
@@ -165,14 +166,14 @@ void SharedHeapSerializer::SerializeObjectImpl(Handle<HeapObject> obj,
   DCHECK(CanBeInSharedOldSpace(*obj) || ReadOnlyHeap::Contains(*obj));
   {
     DisallowGarbageCollection no_gc;
-    HeapObject raw = *obj;
+    Tagged<HeapObject> raw = *obj;
     if (SerializeHotObject(raw)) return;
     if (IsRootAndHasBeenSerialized(raw) && SerializeRoot(raw)) return;
   }
   if (SerializeReadOnlyObjectReference(*obj, &sink_)) return;
   {
     DisallowGarbageCollection no_gc;
-    HeapObject raw = *obj;
+    Tagged<HeapObject> raw = *obj;
     if (SerializeBackReference(raw)) return;
     CheckRehashability(raw);
 
diff --git a/src/snapshot/shared-heap-serializer.h b/src/snapshot/shared-heap-serializer.h
index 618b304a1da..57ff0522825 100644
--- a/src/snapshot/shared-heap-serializer.h
+++ b/src/snapshot/shared-heap-serializer.h
@@ -34,9 +34,9 @@ class V8_EXPORT_PRIVATE SharedHeapSerializer : public RootsSerializer {
   bool SerializeUsingSharedHeapObjectCache(SnapshotByteSink* sink,
                                            Handle<HeapObject> obj);
 
-  static bool CanBeInSharedOldSpace(HeapObject obj);
+  static bool CanBeInSharedOldSpace(Tagged<HeapObject> obj);
 
-  static bool ShouldBeInSharedHeapObjectCache(HeapObject obj);
+  static bool ShouldBeInSharedHeapObjectCache(Tagged<HeapObject> obj);
 
  private:
   bool ShouldReconstructSharedHeapObjectCacheForTesting() const;
diff --git a/src/snapshot/snapshot.cc b/src/snapshot/snapshot.cc
index 4dbf6128b63..45f11e89463 100644
--- a/src/snapshot/snapshot.cc
+++ b/src/snapshot/snapshot.cc
@@ -249,9 +249,11 @@ void Snapshot::ClearReconstructableDataForSerialization(
     std::vector<i::Handle<i::SharedFunctionInfo>> sfis_to_clear;
     {
       i::HeapObjectIterator it(isolate->heap());
-      for (i::HeapObject o = it.Next(); !o.is_null(); o = it.Next()) {
+      for (i::Tagged<i::HeapObject> o = it.Next(); !o.is_null();
+           o = it.Next()) {
         if (clear_recompilable_data && IsSharedFunctionInfo(o, cage_base)) {
-          i::SharedFunctionInfo shared = i::SharedFunctionInfo::cast(o);
+          i::Tagged<i::SharedFunctionInfo> shared =
+              i::SharedFunctionInfo::cast(o);
           if (IsScript(shared->script(cage_base), cage_base) &&
               Script::cast(shared->script(cage_base))->type() ==
                   Script::Type::kExtension) {
@@ -261,7 +263,7 @@ void Snapshot::ClearReconstructableDataForSerialization(
             sfis_to_clear.emplace_back(shared, isolate);
           }
         } else if (IsJSRegExp(o, cage_base)) {
-          i::JSRegExp regexp = i::JSRegExp::cast(o);
+          i::Tagged<i::JSRegExp> regexp = i::JSRegExp::cast(o);
           if (regexp->HasCompiledCode()) {
             regexp->DiscardCompiledCodeForSerialization();
           }
@@ -289,13 +291,13 @@ void Snapshot::ClearReconstructableDataForSerialization(
   // Clear JSFunctions.
   {
     i::HeapObjectIterator it(isolate->heap());
-    for (i::HeapObject o = it.Next(); !o.is_null(); o = it.Next()) {
+    for (i::Tagged<i::HeapObject> o = it.Next(); !o.is_null(); o = it.Next()) {
       if (!IsJSFunction(o, cage_base)) continue;
 
-      i::JSFunction fun = i::JSFunction::cast(o);
+      i::Tagged<i::JSFunction> fun = i::JSFunction::cast(o);
       fun->CompleteInobjectSlackTrackingIfActive();
 
-      i::SharedFunctionInfo shared = fun->shared();
+      i::Tagged<i::SharedFunctionInfo> shared = fun->shared();
       if (IsScript(shared->script(cage_base), cage_base) &&
           Script::cast(shared->script(cage_base))->type() ==
               Script::Type::kExtension) {
@@ -342,9 +344,9 @@ void Snapshot::ClearReconstructableDataForSerialization(
     // for the generic wrapper at all.
     i::HeapObjectIterator it(isolate->heap(),
                              HeapObjectIterator::kFilterUnreachable);
-    for (i::HeapObject o = it.Next(); !o.is_null(); o = it.Next()) {
+    for (i::Tagged<i::HeapObject> o = it.Next(); !o.is_null(); o = it.Next()) {
       if (IsJSFunction(o)) {
-        i::JSFunction fun = i::JSFunction::cast(o);
+        i::Tagged<i::JSFunction> fun = i::JSFunction::cast(o);
         if (fun->shared()->HasAsmWasmData()) {
           FATAL("asm.js functions are not supported in snapshots");
         }
diff --git a/src/snapshot/startup-serializer.cc b/src/snapshot/startup-serializer.cc
index ef88d41ba9a..98e9d8a6a53 100644
--- a/src/snapshot/startup-serializer.cc
+++ b/src/snapshot/startup-serializer.cc
@@ -95,7 +95,7 @@ void StartupSerializer::SerializeObjectImpl(Handle<HeapObject> obj,
 #endif  // DEBUG
   {
     DisallowGarbageCollection no_gc;
-    HeapObject raw = *obj;
+    Tagged<HeapObject> raw = *obj;
     DCHECK(!IsInstructionStream(raw));
     if (SerializeHotObject(raw)) return;
     if (IsRootAndHasBeenSerialized(raw) && SerializeRoot(raw)) return;
@@ -139,7 +139,7 @@ void StartupSerializer::SerializeWeakReferencesAndDeferred() {
   // This comes right after serialization of the context snapshot, where we
   // add entries to the startup object cache of the startup snapshot. Add
   // one entry with 'undefined' to terminate the startup object cache.
-  Object undefined = ReadOnlyRoots(isolate()).undefined_value();
+  Tagged<Object> undefined = ReadOnlyRoots(isolate()).undefined_value();
   VisitRootPointer(Root::kStartupObjectCache, nullptr,
                    FullObjectSlot(&undefined));
 
@@ -196,7 +196,7 @@ void StartupSerializer::CheckNoDirtyFinalizationRegistries() {
       isolate->heap()->dirty_js_finalization_registries_list_tail(), isolate));
 }
 
-void SerializedHandleChecker::AddToSet(FixedArray serialized) {
+void SerializedHandleChecker::AddToSet(Tagged<FixedArray> serialized) {
   int length = serialized->length();
   for (int i = 0; i < length; i++) serialized_.insert(serialized->get(i));
 }
diff --git a/src/snapshot/startup-serializer.h b/src/snapshot/startup-serializer.h
index 9e25a407138..e16465d8253 100644
--- a/src/snapshot/startup-serializer.h
+++ b/src/snapshot/startup-serializer.h
@@ -65,7 +65,7 @@ class SerializedHandleChecker : public RootVisitor {
   bool CheckGlobalAndEternalHandles();
 
  private:
-  void AddToSet(FixedArray serialized);
+  void AddToSet(Tagged<FixedArray> serialized);
 
   Isolate* isolate_;
   std::unordered_set<Object, Object::Hasher> serialized_;
diff --git a/src/strings/string-builder-inl.h b/src/strings/string-builder-inl.h
index 5b9ac0a7c07..66f665b0a02 100644
--- a/src/strings/string-builder-inl.h
+++ b/src/strings/string-builder-inl.h
@@ -26,13 +26,15 @@ using StringBuilderSubstringPosition =
                    kStringBuilderConcatHelperPositionBits>;
 
 template <typename sinkchar>
-void StringBuilderConcatHelper(String special, sinkchar* sink,
-                               FixedArray fixed_array, int array_length);
+void StringBuilderConcatHelper(Tagged<String> special, sinkchar* sink,
+                               Tagged<FixedArray> fixed_array,
+                               int array_length);
 
 // Returns the result length of the concatenation.
 // On illegal argument, -1 is returned.
-int StringBuilderConcatLength(int special_length, FixedArray fixed_array,
-                              int array_length, bool* one_byte);
+int StringBuilderConcatLength(int special_length,
+                              Tagged<FixedArray> fixed_array, int array_length,
+                              bool* one_byte);
 
 class FixedArrayBuilder {
  public:
@@ -203,7 +205,7 @@ class IncrementalStringBuilder {
   template <typename DestChar>
   class NoExtend {
    public:
-    NoExtend(String string, int offset,
+    NoExtend(Tagged<String> string, int offset,
              const DisallowGarbageCollection& no_gc) {
       DCHECK(IsSeqOneByteString(string) || IsSeqTwoByteString(string));
       if (sizeof(DestChar) == 1) {
@@ -327,11 +329,11 @@ void IncrementalStringBuilder::Append(SrcChar c) {
   if (sizeof(DestChar) == 1) {
     DCHECK_EQ(String::ONE_BYTE_ENCODING, encoding_);
     SeqOneByteString::cast(*current_part_)
-        .SeqOneByteStringSet(current_index_++, c);
+        ->SeqOneByteStringSet(current_index_++, c);
   } else {
     DCHECK_EQ(String::TWO_BYTE_ENCODING, encoding_);
     SeqTwoByteString::cast(*current_part_)
-        .SeqTwoByteStringSet(current_index_++, c);
+        ->SeqTwoByteStringSet(current_index_++, c);
   }
   if (current_index_ == part_length_) Extend();
   DCHECK(HasValidCurrentIndex());
diff --git a/src/strings/string-builder.cc b/src/strings/string-builder.cc
index c6574b137e0..df3c81fe3c9 100644
--- a/src/strings/string-builder.cc
+++ b/src/strings/string-builder.cc
@@ -12,12 +12,13 @@ namespace v8 {
 namespace internal {
 
 template <typename sinkchar>
-void StringBuilderConcatHelper(String special, sinkchar* sink,
-                               FixedArray fixed_array, int array_length) {
+void StringBuilderConcatHelper(Tagged<String> special, sinkchar* sink,
+                               Tagged<FixedArray> fixed_array,
+                               int array_length) {
   DisallowGarbageCollection no_gc;
   int position = 0;
   for (int i = 0; i < array_length; i++) {
-    Object element = fixed_array->get(i);
+    Tagged<Object> element = fixed_array->get(i);
     if (IsSmi(element)) {
       // Smi encoding of position and length.
       int encoded_slice = Smi::ToInt(element);
@@ -29,7 +30,7 @@ void StringBuilderConcatHelper(String special, sinkchar* sink,
         len = StringBuilderSubstringLength::decode(encoded_slice);
       } else {
         // Position and length encoded in two smis.
-        Object obj = fixed_array->get(++i);
+        Tagged<Object> obj = fixed_array->get(++i);
         DCHECK(IsSmi(obj));
         pos = Smi::ToInt(obj);
         len = -encoded_slice;
@@ -37,7 +38,7 @@ void StringBuilderConcatHelper(String special, sinkchar* sink,
       String::WriteToFlat(special, sink + position, pos, len);
       position += len;
     } else {
-      String string = String::cast(element);
+      Tagged<String> string = String::cast(element);
       int element_length = string->length();
       String::WriteToFlat(string, sink + position, 0, element_length);
       position += element_length;
@@ -45,22 +46,23 @@ void StringBuilderConcatHelper(String special, sinkchar* sink,
   }
 }
 
-template void StringBuilderConcatHelper<uint8_t>(String special, uint8_t* sink,
-                                                 FixedArray fixed_array,
+template void StringBuilderConcatHelper<uint8_t>(Tagged<String> special,
+                                                 uint8_t* sink,
+                                                 Tagged<FixedArray> fixed_array,
                                                  int array_length);
 
-template void StringBuilderConcatHelper<base::uc16>(String special,
-                                                    base::uc16* sink,
-                                                    FixedArray fixed_array,
-                                                    int array_length);
+template void StringBuilderConcatHelper<base::uc16>(
+    Tagged<String> special, base::uc16* sink, Tagged<FixedArray> fixed_array,
+    int array_length);
 
-int StringBuilderConcatLength(int special_length, FixedArray fixed_array,
-                              int array_length, bool* one_byte) {
+int StringBuilderConcatLength(int special_length,
+                              Tagged<FixedArray> fixed_array, int array_length,
+                              bool* one_byte) {
   DisallowGarbageCollection no_gc;
   int position = 0;
   for (int i = 0; i < array_length; i++) {
     int increment = 0;
-    Object elt = fixed_array->get(i);
+    Tagged<Object> elt = fixed_array->get(i);
     if (IsSmi(elt)) {
       // Smi encoding of position and length.
       int smi_value = Smi::ToInt(elt);
@@ -76,7 +78,7 @@ int StringBuilderConcatLength(int special_length, FixedArray fixed_array,
         // Get the position and check that it is a positive smi.
         i++;
         if (i >= array_length) return -1;
-        Object next_smi = fixed_array->get(i);
+        Tagged<Object> next_smi = fixed_array->get(i);
         if (!IsSmi(next_smi)) return -1;
         pos = Smi::ToInt(next_smi);
         if (pos < 0) return -1;
@@ -86,7 +88,7 @@ int StringBuilderConcatLength(int special_length, FixedArray fixed_array,
       if (pos > special_length || len > special_length - pos) return -1;
       increment = len;
     } else if (IsString(elt)) {
-      String element = String::cast(elt);
+      Tagged<String> element = String::cast(elt);
       int element_length = element->length();
       increment = element_length;
       if (*one_byte && !element->IsOneByteRepresentation()) {
diff --git a/src/strings/string-stream.cc b/src/strings/string-stream.cc
index 63104c9b0f3..2d475d820e4 100644
--- a/src/strings/string-stream.cc
+++ b/src/strings/string-stream.cc
@@ -307,7 +307,7 @@ void StringStream::PrintUsingMap(Tagged<JSObject> js_object) {
     PropertyDetails details = descs->GetDetails(i);
     if (details.location() == PropertyLocation::kField) {
       DCHECK_EQ(PropertyKind::kData, details.kind());
-      Object key = descs->GetKey(i);
+      Tagged<Object> key = descs->GetKey(i);
       if (IsString(key) || IsNumber(key)) {
         int len = 3;
         if (IsString(key)) {
@@ -321,7 +321,7 @@ void StringStream::PrintUsingMap(Tagged<JSObject> js_object) {
         }
         Add(": ");
         FieldIndex index = FieldIndex::ForDescriptor(map, i);
-        Object value = js_object->RawFastPropertyAt(index);
+        Tagged<Object> value = js_object->RawFastPropertyAt(index);
         Add("%o\n", value);
       }
     }
@@ -332,7 +332,7 @@ void StringStream::PrintFixedArray(Tagged<FixedArray> array,
                                    unsigned int limit) {
   ReadOnlyRoots roots = array->GetReadOnlyRoots();
   for (unsigned int i = 0; i < 10 && i < limit; i++) {
-    Object element = array->get(i);
+    Tagged<Object> element = array->get(i);
     if (IsTheHole(element, roots)) continue;
     for (int len = 1; len < 18; len++) {
       Put(' ');
@@ -371,7 +371,7 @@ void StringStream::PrintMentionedObjectCache(Isolate* isolate) {
       isolate->string_stream_debug_object_cache();
   Add("-- ObjectCacheKey --\n\n");
   for (size_t i = 0; i < debug_object_cache->size(); i++) {
-    HeapObject printee = *(*debug_object_cache)[i];
+    Tagged<HeapObject> printee = *(*debug_object_cache)[i];
     Add(" #%d# %p: ", static_cast<int>(i),
         reinterpret_cast<void*>(printee.ptr()));
     ShortPrint(printee, this);
@@ -383,7 +383,7 @@ void StringStream::PrintMentionedObjectCache(Isolate* isolate) {
       }
       PrintUsingMap(JSObject::cast(printee));
       if (IsJSArray(printee)) {
-        JSArray array = JSArray::cast(printee);
+        Tagged<JSArray> array = JSArray::cast(printee);
         if (array->HasObjectElements()) {
           unsigned int limit = FixedArray::cast(array->elements())->length();
           unsigned int length = static_cast<uint32_t>(
@@ -418,7 +418,7 @@ void StringStream::PrintFunction(Tagged<JSFunction> fun,
 
 void StringStream::PrintPrototype(Tagged<JSFunction> fun,
                                   Tagged<Object> receiver) {
-  Object name = fun->shared()->Name();
+  Tagged<Object> name = fun->shared()->Name();
   bool print_name = false;
   Isolate* isolate = fun->GetIsolate();
   if (IsNullOrUndefined(receiver, isolate) || IsTheHole(receiver, isolate) ||
@@ -434,7 +434,7 @@ void StringStream::PrintPrototype(Tagged<JSFunction> fun,
                                 kStartAtReceiver);
          !iter.IsAtEnd(); iter.Advance()) {
       if (IsJSProxy(iter.GetCurrent())) break;
-      Object key = iter.GetCurrent<JSObject>()->SlowReverseLookup(fun);
+      Tagged<Object> key = iter.GetCurrent<JSObject>()->SlowReverseLookup(fun);
       if (!IsUndefined(key, isolate)) {
         if (!IsString(name) || !IsString(key) ||
             !String::cast(name)->Equals(String::cast(key))) {
diff --git a/src/strings/string-stream.h b/src/strings/string-stream.h
index 4e56dc7e37f..357241a47c7 100644
--- a/src/strings/string-stream.h
+++ b/src/strings/string-stream.h
@@ -104,13 +104,12 @@ class StringStream final {
         : FmtElm(LC_STR) {
       data_.u_lc_str_ = &value;
     }
-    FmtElm(Object value) : FmtElm(Tagged(value)) {  // NOLINT
-      static_assert(kTaggedCanConvertToRawObjects);
-    }
-    FmtElm(Tagged<Object> value) : FmtElm(OBJ) {  // NOLINT
+    template <typename T>
+    FmtElm(Tagged<T> value) : FmtElm(OBJ) {  // NOLINT
       data_.u_obj_ = value.ptr();
     }
-    FmtElm(Handle<Object> value) : FmtElm(HANDLE) {  // NOLINT
+    template <typename T>
+    FmtElm(Handle<T> value) : FmtElm(HANDLE) {  // NOLINT
       data_.u_handle_ = value.location();
     }
     FmtElm(void* value) : FmtElm(POINTER) {  // NOLINT
diff --git a/src/torque/implementation-visitor.cc b/src/torque/implementation-visitor.cc
index 8832e6cf2ab..50d7b854bfe 100644
--- a/src/torque/implementation-visitor.cc
+++ b/src/torque/implementation-visitor.cc
@@ -4294,18 +4294,18 @@ void CppClassGenerator::GenerateClassCasts() {
                    gen_name_);
   cpp::Function f(&owner, "cast");
   f.SetFlags(cpp::Function::kV8Inline | cpp::Function::kStatic);
-  f.SetReturnType("D");
+  f.SetReturnType("Tagged<D>");
   f.AddParameter("Tagged<Object>", "object");
 
   // V8_INLINE static D cast(Tagged<Object>)
   f.PrintDeclaration(hdr_);
   f.PrintDefinition(inl_, [](std::ostream& stream) {
-    stream << "    return Tagged<D>(D(object.ptr()).ptr());\n";
+    stream << "    return Tagged<D>(D(object.ptr()));\n";
   });
   // V8_INLINE static D unchecked_cast(Tagged<Object>)
   f.SetName("unchecked_cast");
   f.PrintInlineDefinition(hdr_, [](std::ostream& stream) {
-    stream << "    return Tagged<D>(object.ptr());\n";
+    stream << "    return Tagged<D>::unchecked_cast(object);\n";
   });
 }
 
@@ -5101,7 +5101,7 @@ void ImplementationVisitor::GenerateBodyDescriptors(
         // We use an unchecked_cast here because this is used for concurrent
         // marking, where we shouldn't re-read the map.
         h_contents << "    return " << name
-                   << "::unchecked_cast(raw_object).AllocatedSize();\n";
+                   << "::unchecked_cast(raw_object)->AllocatedSize();\n";
       }
       h_contents << "  }\n\n";
 
diff --git a/src/utils/address-map.h b/src/utils/address-map.h
index 2f7c63379ca..da914392554 100644
--- a/src/utils/address-map.h
+++ b/src/utils/address-map.h
@@ -45,12 +45,14 @@ inline uintptr_t PointerToIndexHashMap<Address>::Key(Address value) {
 }
 
 template <>
-inline uintptr_t PointerToIndexHashMap<HeapObject>::Key(HeapObject value) {
+inline uintptr_t PointerToIndexHashMap<Tagged<HeapObject>>::Key(
+    Tagged<HeapObject> value) {
   return value.ptr();
 }
 
 class AddressToIndexHashMap : public PointerToIndexHashMap<Address> {};
-class HeapObjectToIndexHashMap : public PointerToIndexHashMap<HeapObject> {};
+class HeapObjectToIndexHashMap
+    : public PointerToIndexHashMap<Tagged<HeapObject>> {};
 
 class RootIndexMap {
  public:
@@ -59,7 +61,7 @@ class RootIndexMap {
   RootIndexMap& operator=(const RootIndexMap&) = delete;
 
   // Returns true on successful lookup and sets *|out_root_list|.
-  V8_EXPORT_PRIVATE bool Lookup(HeapObject obj,
+  V8_EXPORT_PRIVATE bool Lookup(Tagged<HeapObject> obj,
                                 RootIndex* out_root_list) const {
     Maybe<uint32_t> maybe_index = map_->Get(obj);
     if (maybe_index.IsJust()) {
diff --git a/src/wasm/c-api.cc b/src/wasm/c-api.cc
index 4865790661f..6d043782950 100644
--- a/src/wasm/c-api.cc
+++ b/src/wasm/c-api.cc
@@ -451,7 +451,7 @@ void StoreImpl::SetHostInfo(i::Handle<i::Object> object, void* info,
 }
 
 void* StoreImpl::GetHostInfo(i::Handle<i::Object> key) {
-  i::Object raw =
+  i::Tagged<i::Object> raw =
       i::EphemeronHashTable::cast(host_info_map_->table())->Lookup(key);
   if (IsTheHole(raw, i_isolate())) return nullptr;
   return i::Managed<ManagedData>::cast(raw)->raw()->info;
@@ -1408,7 +1408,8 @@ class SignatureHelper : public i::AllStatic {
     return sig;
   }
 
-  static own<FuncType> Deserialize(i::PodArray<i::wasm::ValueType> sig) {
+  static own<FuncType> Deserialize(
+      i::Tagged<i::PodArray<i::wasm::ValueType>> sig) {
     int result_arity = i::wasm::SerializedSignatureHelper::ReturnCount(sig);
     int param_arity = i::wasm::SerializedSignatureHelper::ParamCount(sig);
     ownvec<ValType> results = ownvec<ValType>::make_uninitialized(result_arity);
@@ -1425,7 +1426,7 @@ class SignatureHelper : public i::AllStatic {
     return FuncType::make(std::move(params), std::move(results));
   }
 
-  static i::PodArray<i::wasm::ValueType> GetSig(
+  static i::Tagged<i::PodArray<i::wasm::ValueType>> GetSig(
       i::Handle<i::JSFunction> function) {
     return i::WasmCapiFunction::cast(*function)->GetSerializedSignature();
   }
@@ -1613,8 +1614,8 @@ void PopArgs(const i::wasm::FunctionSig* sig, Val results[],
   }
 }
 
-own<Trap> CallWasmCapiFunction(i::WasmCapiFunctionData data, const Val args[],
-                               Val results[]) {
+own<Trap> CallWasmCapiFunction(i::Tagged<i::WasmCapiFunctionData> data,
+                               const Val args[], Val results[]) {
   FuncData* func_data =
       i::Managed<FuncData>::cast(data->embedder_data())->raw();
   if (func_data->kind == FuncData::kCallback) {
@@ -1651,7 +1652,7 @@ auto Func::call(const Val args[], Val results[]) const -> own<Trap> {
   auto isolate = store->i_isolate();
   v8::Isolate::Scope isolate_scope(store->isolate());
   i::HandleScope handle_scope(isolate);
-  i::Object raw_function_data =
+  i::Tagged<i::Object> raw_function_data =
       func->v8_object()->shared()->function_data(v8::kAcquireLoad);
 
   // WasmCapiFunctions can be called directly.
@@ -1681,9 +1682,10 @@ auto Func::call(const Val args[], Val results[]) const -> own<Trap> {
     object_ref = i::handle(
         instance->imported_function_refs()->get(function_index), isolate);
     if (IsWasmApiFunctionRef(*object_ref)) {
-      i::JSFunction jsfunc = i::JSFunction::cast(
+      i::Tagged<i::JSFunction> jsfunc = i::JSFunction::cast(
           i::WasmApiFunctionRef::cast(*object_ref)->callable());
-      i::Object data = jsfunc->shared()->function_data(v8::kAcquireLoad);
+      i::Tagged<i::Object> data =
+          jsfunc->shared()->function_data(v8::kAcquireLoad);
       if (IsWasmCapiFunctionData(data)) {
         return CallWasmCapiFunction(i::WasmCapiFunctionData::cast(data), args,
                                     results);
@@ -1770,7 +1772,7 @@ i::Address FuncData::v8_callback(i::Address host_data_foreign,
 
   if (trap) {
     isolate->Throw(*impl(trap.get())->v8_object());
-    i::Object ex = isolate->pending_exception();
+    i::Tagged<i::Object> ex = isolate->pending_exception();
     isolate->clear_pending_exception();
     return ex.ptr();
   }
diff --git a/src/wasm/module-compiler.cc b/src/wasm/module-compiler.cc
index ca1f4b78001..d9a83fb4d1a 100644
--- a/src/wasm/module-compiler.cc
+++ b/src/wasm/module-compiler.cc
@@ -1174,10 +1174,10 @@ class CompileLazyTimingScope {
 
 }  // namespace
 
-bool CompileLazy(Isolate* isolate, WasmInstanceObject instance,
+bool CompileLazy(Isolate* isolate, Tagged<WasmInstanceObject> instance,
                  int func_index) {
   DisallowGarbageCollection no_gc;
-  WasmModuleObject module_object = instance->module_object();
+  Tagged<WasmModuleObject> module_object = instance->module_object();
   NativeModule* native_module = module_object->native_module();
   Counters* counters = isolate->counters();
 
@@ -1225,7 +1225,7 @@ bool CompileLazy(Isolate* isolate, WasmInstanceObject instance,
   DCHECK_EQ(func_index, code->index());
 
   if (WasmCode::ShouldBeLogged(isolate)) {
-    Object url_obj = module_object->script()->name();
+    Tagged<Object> url_obj = module_object->script()->name();
     DCHECK(IsString(url_obj) || IsUndefined(url_obj));
     std::unique_ptr<char[]> url =
         IsString(url_obj) ? String::cast(url_obj)->ToCString() : nullptr;
@@ -1270,12 +1270,13 @@ void ThrowLazyCompilationError(Isolate* isolate,
 
 class TransitiveTypeFeedbackProcessor {
  public:
-  static void Process(WasmInstanceObject instance, int func_index) {
+  static void Process(Tagged<WasmInstanceObject> instance, int func_index) {
     TransitiveTypeFeedbackProcessor{instance, func_index}.ProcessQueue();
   }
 
  private:
-  TransitiveTypeFeedbackProcessor(WasmInstanceObject instance, int func_index)
+  TransitiveTypeFeedbackProcessor(Tagged<WasmInstanceObject> instance,
+                                  int func_index)
       : instance_(instance),
         module_(instance->module()),
         mutex_guard(&module_->type_feedback.mutex),
@@ -1325,7 +1326,8 @@ class TransitiveTypeFeedbackProcessor {
 
 class FeedbackMaker {
  public:
-  FeedbackMaker(WasmInstanceObject instance, int func_index, int num_calls)
+  FeedbackMaker(Tagged<WasmInstanceObject> instance, int func_index,
+                int num_calls)
       : instance_(instance),
         num_imported_functions_(
             static_cast<int>(instance->module()->num_imported_functions)),
@@ -1333,9 +1335,10 @@ class FeedbackMaker {
     result_.reserve(num_calls);
   }
 
-  void AddCandidate(Object maybe_function, int count) {
+  void AddCandidate(Tagged<Object> maybe_function, int count) {
     if (!IsWasmInternalFunction(maybe_function)) return;
-    WasmInternalFunction function = WasmInternalFunction::cast(maybe_function);
+    Tagged<WasmInternalFunction> function =
+        WasmInternalFunction::cast(maybe_function);
     if (function->ref() != instance_) {
       // Not a wasm function, or not a function declared in this instance.
       return;
@@ -1402,25 +1405,26 @@ class FeedbackMaker {
 
 void TransitiveTypeFeedbackProcessor::ProcessFunction(int func_index) {
   int which_vector = declared_function_index(module_, func_index);
-  Object maybe_feedback = instance_->feedback_vectors()->get(which_vector);
+  Tagged<Object> maybe_feedback =
+      instance_->feedback_vectors()->get(which_vector);
   if (!IsFixedArray(maybe_feedback)) return;
-  FixedArray feedback = FixedArray::cast(maybe_feedback);
+  Tagged<FixedArray> feedback = FixedArray::cast(maybe_feedback);
   base::Vector<uint32_t> call_direct_targets =
       module_->type_feedback.feedback_for_function[func_index]
           .call_targets.as_vector();
   DCHECK_EQ(feedback->length(), call_direct_targets.size() * 2);
   FeedbackMaker fm(instance_, func_index, feedback->length() / 2);
   for (int i = 0; i < feedback->length(); i += 2) {
-    Object value = feedback->get(i);
+    Tagged<Object> value = feedback->get(i);
     if (IsWasmInternalFunction(value)) {
       // Monomorphic.
       int count = Smi::cast(feedback->get(i + 1)).value();
       fm.AddCandidate(value, count);
     } else if (IsFixedArray(value)) {
       // Polymorphic.
-      FixedArray polymorphic = FixedArray::cast(value);
+      Tagged<FixedArray> polymorphic = FixedArray::cast(value);
       for (int j = 0; j < polymorphic->length(); j += 2) {
-        Object function = polymorphic->get(j);
+        Tagged<Object> function = polymorphic->get(j);
         int count = Smi::cast(polymorphic->get(j + 1)).value();
         fm.AddCandidate(function, count);
       }
@@ -1446,7 +1450,7 @@ void TransitiveTypeFeedbackProcessor::ProcessFunction(int func_index) {
   feedback_for_function_[func_index].feedback_vector = std::move(result);
 }
 
-void TriggerTierUp(WasmInstanceObject instance, int func_index) {
+void TriggerTierUp(Tagged<WasmInstanceObject> instance, int func_index) {
   NativeModule* native_module = instance->module_object()->native_module();
   CompilationStateImpl* compilation_state =
       Impl(native_module->compilation_state());
@@ -1485,7 +1489,7 @@ void TriggerTierUp(WasmInstanceObject instance, int func_index) {
   compilation_state->AddTopTierPriorityCompilationUnit(tiering_unit, priority);
 }
 
-void TierUpNowForTesting(Isolate* isolate, WasmInstanceObject instance,
+void TierUpNowForTesting(Isolate* isolate, Tagged<WasmInstanceObject> instance,
                          int func_index) {
   NativeModule* native_module = instance->module_object()->native_module();
   if (native_module->enabled_features().has_inlining()) {
@@ -1499,7 +1503,7 @@ void TierUpNowForTesting(Isolate* isolate, WasmInstanceObject instance,
 
 namespace {
 
-void RecordStats(Code code, Counters* counters) {
+void RecordStats(Tagged<Code> code, Counters* counters) {
   if (!code->has_instruction_stream()) return;
   counters->wasm_generated_code_size()->Increment(code->body_size());
   counters->wasm_reloc_size()->Increment(code->relocation_size());
diff --git a/src/wasm/module-compiler.h b/src/wasm/module-compiler.h
index 1df9acf6afc..0921bcc9aab 100644
--- a/src/wasm/module-compiler.h
+++ b/src/wasm/module-compiler.h
@@ -76,7 +76,7 @@ WasmCode* CompileImportWrapper(
 // Triggered by the WasmCompileLazy builtin. The return value indicates whether
 // compilation was successful. Lazy compilation can fail only if validation is
 // also lazy.
-bool CompileLazy(Isolate*, WasmInstanceObject, int func_index);
+bool CompileLazy(Isolate*, Tagged<WasmInstanceObject>, int func_index);
 
 // Throws the compilation error after failed lazy compilation.
 void ThrowLazyCompilationError(Isolate* isolate,
@@ -85,10 +85,10 @@ void ThrowLazyCompilationError(Isolate* isolate,
 
 // Trigger tier-up of a particular function to TurboFan. If tier-up was already
 // triggered, we instead increase the priority with exponential back-off.
-V8_EXPORT_PRIVATE void TriggerTierUp(WasmInstanceObject instance,
+V8_EXPORT_PRIVATE void TriggerTierUp(Tagged<WasmInstanceObject> instance,
                                      int func_index);
 // Synchronous version of the above.
-void TierUpNowForTesting(Isolate* isolate, WasmInstanceObject instance,
+void TierUpNowForTesting(Isolate* isolate, Tagged<WasmInstanceObject> instance,
                          int func_index);
 
 template <typename Key, typename KeyInfo, typename Hash>
diff --git a/src/wasm/module-instantiate.cc b/src/wasm/module-instantiate.cc
index 882c8e01568..86c4d5ce23c 100644
--- a/src/wasm/module-instantiate.cc
+++ b/src/wasm/module-instantiate.cc
@@ -371,7 +371,7 @@ WellKnownImport CheckForWellKnownImport(Handle<WasmInstanceObject> instance,
   static constexpr ValueType kRefExtern = ValueType::Ref(HeapType::kExtern);
   // Check for plain JS functions.
   if (IsJSFunction(*callable)) {
-    SharedFunctionInfo sfi = JSFunction::cast(*callable)->shared();
+    Tagged<SharedFunctionInfo> sfi = JSFunction::cast(*callable)->shared();
     if (!sfi->HasBuiltinId()) return kGeneric;
     // This needs to be a separate switch because it allows other cases than
     // the one below. Merging them would be invalid, because we would then
@@ -490,13 +490,13 @@ WellKnownImport CheckForWellKnownImport(Handle<WasmInstanceObject> instance,
   Handle<JSBoundFunction> bound = Handle<JSBoundFunction>::cast(callable);
   if (bound->bound_arguments()->length() != 0) return kGeneric;
   if (!IsJSFunction(bound->bound_target_function())) return kGeneric;
-  SharedFunctionInfo sfi =
+  Tagged<SharedFunctionInfo> sfi =
       JSFunction::cast(bound->bound_target_function())->shared();
   if (!sfi->HasBuiltinId()) return kGeneric;
   if (sfi->builtin_id() != Builtin::kFunctionPrototypeCall) return kGeneric;
   // Second part: check if the bound receiver is one of the builtins for which
   // we have special-cased support.
-  Object bound_this = bound->bound_this();
+  Tagged<Object> bound_this = bound->bound_this();
   if (!IsJSFunction(bound_this)) return kGeneric;
   sfi = JSFunction::cast(bound_this)->shared();
   if (!sfi->HasBuiltinId()) return kGeneric;
@@ -1309,7 +1309,7 @@ MaybeHandle<WasmInstanceObject> InstanceBuilder::Build() {
 
     if (function.imported) {
       ImportedFunctionEntry entry(instance, module_->start_function_index);
-      Object callable = entry.maybe_callable();
+      Tagged<Object> callable = entry.maybe_callable();
       if (IsJSFunction(callable)) {
         // If the start function was imported and calls into Blink, we have
         // to pretend that the V8 API was used to enter its correct context.
@@ -1692,7 +1692,7 @@ bool InstanceBuilder::ProcessImportedFunction(
       int expected_arity = static_cast<int>(expected_sig->parameter_count());
       if (kind == ImportCallKind::kJSFunctionArityMismatch) {
         Handle<JSFunction> function = Handle<JSFunction>::cast(js_receiver);
-        SharedFunctionInfo shared = function->shared();
+        Tagged<SharedFunctionInfo> shared = function->shared();
         expected_arity =
             shared->internal_formal_parameter_count_without_receiver();
       }
@@ -2080,7 +2080,7 @@ void InstanceBuilder::CompileImportWrappers(
     if (kind == ImportCallKind::kJSFunctionArityMismatch) {
       Handle<JSFunction> function =
           Handle<JSFunction>::cast(resolved.callable());
-      SharedFunctionInfo shared = function->shared();
+      Tagged<SharedFunctionInfo> shared = function->shared();
       expected_arity =
           shared->internal_formal_parameter_count_without_receiver();
     }
@@ -2169,7 +2169,7 @@ int InstanceBuilder::ProcessImports(Handle<WasmInstanceObject> instance) {
               ImportName(index, module_name, import_name).c_str());
           return -1;
         }
-        Object tag = imported_tag->tag();
+        Tagged<Object> tag = imported_tag->tag();
         DCHECK(IsUndefined(instance->tags_table()->get(import.index)));
         instance->tags_table()->set(import.index, tag);
         tags_wrappers_[import.index] = imported_tag;
diff --git a/src/wasm/wasm-code-manager.cc b/src/wasm/wasm-code-manager.cc
index e19b6b25eb0..bfee624b7e5 100644
--- a/src/wasm/wasm-code-manager.cc
+++ b/src/wasm/wasm-code-manager.cc
@@ -888,14 +888,14 @@ void NativeModule::ReserveCodeTableForTesting(uint32_t max_functions) {
   InitializeJumpTableForLazyCompilation(max_functions);
 }
 
-void NativeModule::LogWasmCodes(Isolate* isolate, Script script) {
+void NativeModule::LogWasmCodes(Isolate* isolate, Tagged<Script> script) {
   DisallowGarbageCollection no_gc;
   if (!WasmCode::ShouldBeLogged(isolate)) return;
 
   TRACE_EVENT1("v8.wasm", "wasm.LogWasmCodes", "functions",
                module_->num_declared_functions);
 
-  Object url_obj = script->name();
+  Tagged<Object> url_obj = script->name();
   DCHECK(IsString(url_obj) || IsUndefined(url_obj));
   std::unique_ptr<char[]> source_url =
       IsString(url_obj) ? String::cast(url_obj)->ToCString()
diff --git a/src/wasm/wasm-code-manager.h b/src/wasm/wasm-code-manager.h
index 71d3270d9b9..5c8232fe1cd 100644
--- a/src/wasm/wasm-code-manager.h
+++ b/src/wasm/wasm-code-manager.h
@@ -713,7 +713,7 @@ class V8_EXPORT_PRIVATE NativeModule final {
   // on the fly, and bypass the instance builder pipeline.
   void ReserveCodeTableForTesting(uint32_t max_functions);
 
-  void LogWasmCodes(Isolate*, Script);
+  void LogWasmCodes(Isolate*, Tagged<Script>);
 
   CompilationState* compilation_state() const {
     return compilation_state_.get();
diff --git a/src/wasm/wasm-debug.cc b/src/wasm/wasm-debug.cc
index 4672e668106..98f27d98678 100644
--- a/src/wasm/wasm-debug.cc
+++ b/src/wasm/wasm-debug.cc
@@ -863,15 +863,16 @@ int FindNextBreakablePosition(wasm::NativeModule* native_module, int func_index,
   return 0;
 }
 
-void SetBreakOnEntryFlag(Script script, bool enabled) {
+void SetBreakOnEntryFlag(Tagged<Script> script, bool enabled) {
   if (script->break_on_entry() == enabled) return;
 
   script->set_break_on_entry(enabled);
   // Update the "break_on_entry" flag on all live instances.
-  i::WeakArrayList weak_instance_list = script->wasm_weak_instance_list();
+  i::Tagged<i::WeakArrayList> weak_instance_list =
+      script->wasm_weak_instance_list();
   for (int i = 0; i < weak_instance_list->length(); ++i) {
     if (weak_instance_list->Get(i)->IsCleared()) continue;
-    i::WasmInstanceObject instance = i::WasmInstanceObject::cast(
+    i::Tagged<i::WasmInstanceObject> instance = i::WasmInstanceObject::cast(
         weak_instance_list->Get(i)->GetHeapObject());
     instance->set_break_on_entry(enabled);
   }
@@ -946,7 +947,8 @@ bool WasmScript::SetBreakPointForFunction(Handle<Script> script, int func_index,
 
 namespace {
 
-int GetBreakpointPos(Isolate* isolate, Object break_point_info_or_undef) {
+int GetBreakpointPos(Isolate* isolate,
+                     Tagged<Object> break_point_info_or_undef) {
   if (IsUndefined(break_point_info_or_undef, isolate)) return kMaxInt;
   return BreakPointInfo::cast(break_point_info_or_undef)->source_position();
 }
@@ -963,7 +965,7 @@ int FindBreakpointInfoInsertPos(Isolate* isolate,
   int right = breakpoint_infos->length();  // exclusive
   while (right - left > 1) {
     int mid = left + (right - left) / 2;
-    Object mid_obj = breakpoint_infos->get(mid);
+    Tagged<Object> mid_obj = breakpoint_infos->get(mid);
     if (GetBreakpointPos(isolate, mid_obj) <= position) {
       left = mid;
     } else {
@@ -998,7 +1000,7 @@ bool WasmScript::ClearBreakPoint(Handle<Script> script, int position,
   if (info->GetBreakPointCount(isolate) == 0) {
     // Update array by moving breakpoints up one position.
     for (int i = pos; i < breakpoint_infos->length() - 1; i++) {
-      Object entry = breakpoint_infos->get(i + 1);
+      Tagged<Object> entry = breakpoint_infos->get(i + 1);
       breakpoint_infos->set(i, entry);
       if (IsUndefined(entry, isolate)) break;
     }
@@ -1050,7 +1052,7 @@ bool WasmScript::ClearBreakPointById(Handle<Script> script, int breakpoint_id) {
 }
 
 // static
-void WasmScript::ClearAllBreakpoints(Script script) {
+void WasmScript::ClearAllBreakpoints(Tagged<Script> script) {
   script->set_wasm_breakpoint_infos(
       ReadOnlyRoots(script->GetIsolate()).empty_fixed_array());
   SetBreakOnEntryFlag(script, false);
@@ -1098,7 +1100,7 @@ void WasmScript::AddBreakpointToInfo(Handle<Script> script, int position,
 
   // Move elements [insert_pos, ...] up by one.
   for (int i = breakpoint_infos->length() - 1; i >= insert_pos; --i) {
-    Object entry = breakpoint_infos->get(i);
+    Tagged<Object> entry = breakpoint_infos->get(i);
     if (IsUndefined(entry, isolate)) continue;
     new_breakpoint_infos->set(i + 1, entry);
   }
diff --git a/src/wasm/wasm-js.cc b/src/wasm/wasm-js.cc
index f13ef19932e..199225d230d 100644
--- a/src/wasm/wasm-js.cc
+++ b/src/wasm/wasm-js.cc
@@ -3010,7 +3010,7 @@ void WasmJs::Install(Isolate* isolate, bool exposed_on_global_object) {
   Handle<JSGlobalObject> global = isolate->global_object();
   Handle<Context> context(global->native_context(), isolate);
   // Install the JS API once only.
-  Object prev = context->get(Context::WASM_MODULE_CONSTRUCTOR_INDEX);
+  Tagged<Object> prev = context->get(Context::WASM_MODULE_CONSTRUCTOR_INDEX);
   if (!IsUndefined(prev, isolate)) {
     DCHECK(IsJSFunction(prev));
     return;
diff --git a/src/wasm/wasm-objects-inl.h b/src/wasm/wasm-objects-inl.h
index a896151e623..0214a536fa2 100644
--- a/src/wasm/wasm-objects-inl.h
+++ b/src/wasm/wasm-objects-inl.h
@@ -265,7 +265,8 @@ void WasmInstanceObject::clear_padding() {
   }
 }
 
-WasmMemoryObject WasmInstanceObject::memory_object(int memory_index) const {
+Tagged<WasmMemoryObject> WasmInstanceObject::memory_object(
+    int memory_index) const {
   return WasmMemoryObject::cast(memory_objects()->get(memory_index));
 }
 
@@ -451,7 +452,7 @@ MaybeHandle<Object> WasmObject::ToWasmValue(Isolate* isolate,
 // Conversions from Numeric objects.
 // static
 template <typename ElementType>
-ElementType WasmObject::FromNumber(Object value) {
+ElementType WasmObject::FromNumber(Tagged<Object> value) {
   // The value must already be prepared for storing to numeric fields.
   DCHECK(IsNumber(value));
   if (IsSmi(value)) {
@@ -525,18 +526,18 @@ void WasmObject::WriteValueAt(Isolate* isolate, Handle<HeapObject> obj,
   }
 }
 
-wasm::StructType* WasmStruct::type(Map map) {
-  WasmTypeInfo type_info = map->wasm_type_info();
+wasm::StructType* WasmStruct::type(Tagged<Map> map) {
+  Tagged<WasmTypeInfo> type_info = map->wasm_type_info();
   return reinterpret_cast<wasm::StructType*>(type_info->native_type());
 }
 
-wasm::StructType* WasmStruct::GcSafeType(Map map) {
+wasm::StructType* WasmStruct::GcSafeType(Tagged<Map> map) {
   DCHECK_EQ(WASM_STRUCT_TYPE, map->instance_type());
-  HeapObject raw = HeapObject::cast(map->constructor_or_back_pointer());
+  Tagged<HeapObject> raw = HeapObject::cast(map->constructor_or_back_pointer());
   // The {WasmTypeInfo} might be in the middle of being moved, which is why we
   // can't read its map for a checked cast. But we can rely on its native type
   // pointer being intact in the old location.
-  WasmTypeInfo type_info = WasmTypeInfo::unchecked_cast(raw);
+  Tagged<WasmTypeInfo> type_info = WasmTypeInfo::unchecked_cast(raw);
   return reinterpret_cast<wasm::StructType*>(type_info->native_type());
 }
 
@@ -550,7 +551,7 @@ int WasmStruct::Size(const wasm::StructType* type) {
 }
 
 // static
-void WasmStruct::EncodeInstanceSizeInMap(int instance_size, Map map) {
+void WasmStruct::EncodeInstanceSizeInMap(int instance_size, Tagged<Map> map) {
   // WasmStructs can be bigger than the {map.instance_size_in_words} field
   // can describe; yet we have to store the instance size somewhere on the
   // map so that the GC can read it without relying on any other objects
@@ -564,12 +565,14 @@ void WasmStruct::EncodeInstanceSizeInMap(int instance_size, Map map) {
 }
 
 // static
-int WasmStruct::DecodeInstanceSizeFromMap(Map map) {
+int WasmStruct::DecodeInstanceSizeFromMap(Tagged<Map> map) {
   return (map->WasmByte2() << (8 + kObjectAlignmentBits)) |
          (map->WasmByte1() << kObjectAlignmentBits);
 }
 
-int WasmStruct::GcSafeSize(Map map) { return DecodeInstanceSizeFromMap(map); }
+int WasmStruct::GcSafeSize(Tagged<Map> map) {
+  return DecodeInstanceSizeFromMap(map);
+}
 
 wasm::StructType* WasmStruct::type() const { return type(map()); }
 
@@ -602,25 +605,25 @@ void WasmStruct::SetField(Isolate* isolate, Handle<WasmStruct> obj,
   WriteValueAt(isolate, obj, field_type, offset, value);
 }
 
-wasm::ArrayType* WasmArray::type(Map map) {
+wasm::ArrayType* WasmArray::type(Tagged<Map> map) {
   DCHECK_EQ(WASM_ARRAY_TYPE, map->instance_type());
-  WasmTypeInfo type_info = map->wasm_type_info();
+  Tagged<WasmTypeInfo> type_info = map->wasm_type_info();
   return reinterpret_cast<wasm::ArrayType*>(type_info->native_type());
 }
 
-wasm::ArrayType* WasmArray::GcSafeType(Map map) {
+wasm::ArrayType* WasmArray::GcSafeType(Tagged<Map> map) {
   DCHECK_EQ(WASM_ARRAY_TYPE, map->instance_type());
-  HeapObject raw = HeapObject::cast(map->constructor_or_back_pointer());
+  Tagged<HeapObject> raw = HeapObject::cast(map->constructor_or_back_pointer());
   // The {WasmTypeInfo} might be in the middle of being moved, which is why we
   // can't read its map for a checked cast. But we can rely on its native type
   // pointer being intact in the old location.
-  WasmTypeInfo type_info = WasmTypeInfo::unchecked_cast(raw);
+  Tagged<WasmTypeInfo> type_info = WasmTypeInfo::unchecked_cast(raw);
   return reinterpret_cast<wasm::ArrayType*>(type_info->native_type());
 }
 
 wasm::ArrayType* WasmArray::type() const { return type(map()); }
 
-int WasmArray::SizeFor(Map map, int length) {
+int WasmArray::SizeFor(Tagged<Map> map, int length) {
   int element_size = DecodeElementSizeFromMap(map);
   return kHeaderSize + RoundUp(element_size * length, kTaggedSize);
 }
@@ -653,12 +656,14 @@ Handle<Object> WasmArray::GetElement(Isolate* isolate, Handle<WasmArray> array,
 }
 
 // static
-void WasmArray::EncodeElementSizeInMap(int element_size, Map map) {
+void WasmArray::EncodeElementSizeInMap(int element_size, Tagged<Map> map) {
   map->SetWasmByte1(element_size);
 }
 
 // static
-int WasmArray::DecodeElementSizeFromMap(Map map) { return map->WasmByte1(); }
+int WasmArray::DecodeElementSizeFromMap(Tagged<Map> map) {
+  return map->WasmByte1();
+}
 
 EXTERNAL_POINTER_ACCESSORS(WasmContinuationObject, jmpbuf, Address,
                            kJmpbufOffset, kWasmContinuationJmpbufTag)
diff --git a/src/wasm/wasm-objects.cc b/src/wasm/wasm-objects.cc
index 2d682466fba..5ffb545a6a4 100644
--- a/src/wasm/wasm-objects.cc
+++ b/src/wasm/wasm-objects.cc
@@ -657,7 +657,7 @@ Handle<WasmIndirectFunctionTable> WasmIndirectFunctionTable::New(
 }
 
 void WasmIndirectFunctionTable::Set(uint32_t index, int sig_id,
-                                    Address call_target, Object ref) {
+                                    Address call_target, Tagged<Object> ref) {
   Isolate* isolate = GetIsolateFromWritableObject(*this);
   sig_ids()->set(index, sig_id);
   targets()->set<kWasmIndirectFunctionTargetTag>(index, isolate, call_target);
@@ -715,8 +715,8 @@ void WasmIndirectFunctionTable::Resize(Isolate* isolate,
 
 namespace {
 
-void SetInstanceMemory(WasmInstanceObject instance, JSArrayBuffer buffer,
-                       int memory_index) {
+void SetInstanceMemory(Tagged<WasmInstanceObject> instance,
+                       Tagged<JSArrayBuffer> buffer, int memory_index) {
   DisallowHeapAllocation no_gc;
   const WasmModule* module = instance->module();
   const wasm::WasmMemory& memory = module->memories[memory_index];
@@ -856,19 +856,19 @@ void WasmMemoryObject::UseInInstance(Isolate* isolate,
   memory->set_instances(*new_instances);
 }
 
-void WasmMemoryObject::SetNewBuffer(JSArrayBuffer new_buffer) {
+void WasmMemoryObject::SetNewBuffer(Tagged<JSArrayBuffer> new_buffer) {
   DisallowGarbageCollection no_gc;
   set_array_buffer(new_buffer);
   if (has_instances()) {
-    WeakArrayList instances = this->instances();
+    Tagged<WeakArrayList> instances = this->instances();
     for (int i = 0, len = instances->length(); i < len; ++i) {
       MaybeObject elem = instances->Get(i);
       if (elem->IsCleared()) continue;
-      WasmInstanceObject instance =
+      Tagged<WasmInstanceObject> instance =
           WasmInstanceObject::cast(elem->GetHeapObjectAssumeWeak());
       // TODO(clemens): Avoid the iteration by also remembering the memory index
       // if we ever see larger numbers of memories.
-      FixedArray memory_objects = instance->memory_objects();
+      Tagged<FixedArray> memory_objects = instance->memory_objects();
       int num_memories = memory_objects->length();
       for (int mem_idx = 0; mem_idx < num_memories; ++mem_idx) {
         if (memory_objects->get(mem_idx) == *this) {
@@ -1100,7 +1100,7 @@ void ImportedFunctionEntry::SetWasmToJs(
       index_, wasm_to_js_wrapper->instruction_start());
 }
 
-void ImportedFunctionEntry::SetWasmToWasm(WasmInstanceObject instance,
+void ImportedFunctionEntry::SetWasmToWasm(Tagged<WasmInstanceObject> instance,
                                           Address call_target) {
   TRACE_IFT("Import Wasm 0x%" PRIxPTR "[%d] = {instance=0x%" PRIxPTR
             ", target=0x%" PRIxPTR "}\n",
@@ -1111,17 +1111,17 @@ void ImportedFunctionEntry::SetWasmToWasm(WasmInstanceObject instance,
 
 // Returns an empty Object() if no callable is available, a JSReceiver
 // otherwise.
-Object ImportedFunctionEntry::maybe_callable() {
-  Object value = object_ref();
+Tagged<Object> ImportedFunctionEntry::maybe_callable() {
+  Tagged<Object> value = object_ref();
   if (!IsWasmApiFunctionRef(value)) return Object();
   return JSReceiver::cast(WasmApiFunctionRef::cast(value)->callable());
 }
 
-JSReceiver ImportedFunctionEntry::callable() {
+Tagged<JSReceiver> ImportedFunctionEntry::callable() {
   return JSReceiver::cast(WasmApiFunctionRef::cast(object_ref())->callable());
 }
 
-Object ImportedFunctionEntry::object_ref() {
+Tagged<Object> ImportedFunctionEntry::object_ref() {
   return instance_->imported_function_refs()->get(index_);
 }
 
@@ -1161,7 +1161,7 @@ void WasmInstanceObject::SetRawMemory(int memory_index, uint8_t* mem_start,
                          ? wasm::max_mem64_bytes()
                          : wasm::max_mem32_bytes());
   // All memory bases and sizes are stored in a FixedAddressArray.
-  FixedAddressArray bases_and_sizes = memory_bases_and_sizes();
+  Tagged<FixedAddressArray> bases_and_sizes = memory_bases_and_sizes();
   bases_and_sizes->set_sandboxed_pointer(memory_index * 2,
                                          reinterpret_cast<Address>(mem_start));
   bases_and_sizes->set(memory_index * 2 + 1, mem_size);
@@ -1230,12 +1230,15 @@ Handle<WasmInstanceObject> WasmInstanceObject::New(
     // Some constants:
     uint8_t* empty_backing_store_buffer =
         reinterpret_cast<uint8_t*>(EmptyBackingStoreBuffer());
-    FixedArray empty_fixed_array = ReadOnlyRoots(isolate).empty_fixed_array();
-    ByteArray empty_byte_array = ReadOnlyRoots(isolate).empty_byte_array();
-    ExternalPointerArray empty_external_pointer_array =
+    Tagged<FixedArray> empty_fixed_array =
+        ReadOnlyRoots(isolate).empty_fixed_array();
+    Tagged<ByteArray> empty_byte_array =
+        ReadOnlyRoots(isolate).empty_byte_array();
+    Tagged<ExternalPointerArray> empty_external_pointer_array =
         ReadOnlyRoots(isolate).empty_external_pointer_array();
 
-    WasmInstanceObject instance = WasmInstanceObject::cast(*instance_object);
+    Tagged<WasmInstanceObject> instance =
+        WasmInstanceObject::cast(*instance_object);
     instance->clear_padding();
     instance->set_imported_function_targets(*imported_function_targets);
     instance->set_imported_mutable_globals(*imported_mutable_globals);
@@ -1301,7 +1304,8 @@ Handle<WasmInstanceObject> WasmInstanceObject::New(
   return Handle<WasmInstanceObject>::cast(instance_object);
 }
 
-void WasmInstanceObject::InitDataSegmentArrays(WasmModuleObject module_object) {
+void WasmInstanceObject::InitDataSegmentArrays(
+    Tagged<WasmModuleObject> module_object) {
   auto module = module_object->module();
   auto wire_bytes = module_object->native_module()->wire_bytes();
   auto num_data_segments = module->num_declared_data_segments;
@@ -1431,7 +1435,7 @@ base::Optional<MessageTemplate> WasmInstanceObject::InitTableEntries(
 
 MaybeHandle<WasmInternalFunction> WasmInstanceObject::GetWasmInternalFunction(
     Isolate* isolate, Handle<WasmInstanceObject> instance, int index) {
-  Object val = instance->wasm_internal_functions()->get(index);
+  Tagged<Object> val = instance->wasm_internal_functions()->get(index);
   if (IsSmi(val)) return {};
   return handle(WasmInternalFunction::cast(val), isolate);
 }
@@ -1549,7 +1553,7 @@ Handle<JSFunction> WasmInternalFunction::GetOrCreateExternal(
   return result;
 }
 
-HeapObject WasmInternalFunction::external() {
+Tagged<HeapObject> WasmInternalFunction::external() {
   return this->TorqueGeneratedWasmInternalFunction::external();
 }
 
@@ -1652,7 +1656,7 @@ void WasmInstanceObject::ImportWasmJSFunctionIntoTable(
     } else if (UseGenericWasmToJSWrapper(kind, sig, resolved.suspend())) {
       call_target = isolate->builtins()
                         ->code(Builtin::kWasmToJsWrapperAsm)
-                        .instruction_start();
+                        ->instruction_start();
     } else {
       wasm::CompilationEnv env = native_module->CreateCompilationEnv();
       wasm::WasmCompilationResult result =
@@ -1849,9 +1853,10 @@ bool WasmTagObject::MatchesSignature(uint32_t expected_canonical_type_index) {
 }
 
 const wasm::FunctionSig* WasmCapiFunction::GetSignature(Zone* zone) const {
-  WasmCapiFunctionData function_data = shared()->wasm_capi_function_data();
+  Tagged<WasmCapiFunctionData> function_data =
+      shared()->wasm_capi_function_data();
   return wasm::SerializedSignatureHelper::DeserializeSignature(
-      zone, function_data.serialized_signature());
+      zone, function_data->serialized_signature());
 }
 
 bool WasmCapiFunction::MatchesSignature(
@@ -2092,10 +2097,10 @@ uint32_t WasmExceptionPackage::GetEncodedSize(const wasm::WasmTagSig* sig) {
   return encoded_size;
 }
 
-bool WasmExportedFunction::IsWasmExportedFunction(Object object) {
+bool WasmExportedFunction::IsWasmExportedFunction(Tagged<Object> object) {
   if (!IsJSFunction(object)) return false;
-  JSFunction js_function = JSFunction::cast(object);
-  Code code = js_function->code();
+  Tagged<JSFunction> js_function = JSFunction::cast(object);
+  Tagged<Code> code = js_function->code();
   if (CodeKind::JS_TO_WASM_FUNCTION != code->kind() &&
       code->builtin_id() != Builtin::kJSToWasmWrapper &&
       code->builtin_id() != Builtin::kWasmReturnPromiseOnSuspend) {
@@ -2105,9 +2110,9 @@ bool WasmExportedFunction::IsWasmExportedFunction(Object object) {
   return true;
 }
 
-bool WasmCapiFunction::IsWasmCapiFunction(Object object) {
+bool WasmCapiFunction::IsWasmCapiFunction(Tagged<Object> object) {
   if (!IsJSFunction(object)) return false;
-  JSFunction js_function = JSFunction::cast(object);
+  Tagged<JSFunction> js_function = JSFunction::cast(object);
   // TODO(jkummerow): Enable this when there is a JavaScript wrapper
   // able to call this function.
   // if (js_function->code()->kind() != CodeKind::WASM_TO_CAPI_FUNCTION) {
@@ -2143,7 +2148,7 @@ Handle<WasmCapiFunction> WasmCapiFunction::New(
   return Handle<WasmCapiFunction>::cast(result);
 }
 
-WasmInstanceObject WasmExportedFunction::instance() {
+Tagged<WasmInstanceObject> WasmExportedFunction::instance() {
   return shared()->wasm_exported_function_data()->instance();
 }
 
@@ -2252,9 +2257,9 @@ std::unique_ptr<char[]> WasmExportedFunction::GetDebugName(
 }
 
 // static
-bool WasmJSFunction::IsWasmJSFunction(Object object) {
+bool WasmJSFunction::IsWasmJSFunction(Tagged<Object> object) {
   if (!IsJSFunction(object)) return false;
-  JSFunction js_function = JSFunction::cast(object);
+  Tagged<JSFunction> js_function = JSFunction::cast(object);
   return js_function->shared()->HasWasmJSFunctionData();
 }
 
@@ -2338,7 +2343,7 @@ Handle<WasmJSFunction> WasmJSFunction::New(Isolate* isolate,
       int expected_arity = parameter_count;
       wasm::ImportCallKind kind = wasm::kDefaultImportCallKind;
       if (IsJSFunction(*callable)) {
-        SharedFunctionInfo shared =
+        Tagged<SharedFunctionInfo> shared =
             Handle<JSFunction>::cast(callable)->shared();
         expected_arity =
             shared->internal_formal_parameter_count_without_receiver();
@@ -2372,7 +2377,7 @@ Handle<WasmJSFunction> WasmJSFunction::New(Isolate* isolate,
   return Handle<WasmJSFunction>::cast(js_function);
 }
 
-JSReceiver WasmJSFunction::GetCallable() const {
+Tagged<JSReceiver> WasmJSFunction::GetCallable() const {
   return JSReceiver::cast(
       WasmApiFunctionRef::cast(
           shared()->wasm_js_function_data()->internal()->ref())
@@ -2387,9 +2392,9 @@ wasm::Suspend WasmJSFunction::GetSuspend() const {
 }
 
 const wasm::FunctionSig* WasmJSFunction::GetSignature(Zone* zone) const {
-  WasmJSFunctionData function_data = shared()->wasm_js_function_data();
+  Tagged<WasmJSFunctionData> function_data = shared()->wasm_js_function_data();
   return wasm::SerializedSignatureHelper::DeserializeSignature(
-      zone, function_data.serialized_signature());
+      zone, function_data->serialized_signature());
 }
 
 bool WasmJSFunction::MatchesSignature(
@@ -2407,11 +2412,12 @@ bool WasmJSFunction::MatchesSignature(
          other_canonical_sig_index;
 }
 
-PodArray<wasm::ValueType> WasmCapiFunction::GetSerializedSignature() const {
+Tagged<PodArray<wasm::ValueType>> WasmCapiFunction::GetSerializedSignature()
+    const {
   return shared()->wasm_capi_function_data()->serialized_signature();
 }
 
-bool WasmExternalFunction::IsWasmExternalFunction(Object object) {
+bool WasmExternalFunction::IsWasmExternalFunction(Tagged<Object> object) {
   return WasmExportedFunction::IsWasmExportedFunction(object) ||
          WasmJSFunction::IsWasmJSFunction(object);
 }
@@ -2422,7 +2428,7 @@ MaybeHandle<WasmInternalFunction> WasmInternalFunction::FromExternal(
   if (WasmExportedFunction::IsWasmExportedFunction(*external) ||
       WasmJSFunction::IsWasmJSFunction(*external) ||
       WasmCapiFunction::IsWasmCapiFunction(*external)) {
-    WasmFunctionData data = WasmFunctionData::cast(
+    Tagged<WasmFunctionData> data = WasmFunctionData::cast(
         Handle<JSFunction>::cast(external)->shared()->function_data(
             kAcquireLoad));
     return handle(data->internal(), isolate);
@@ -2608,7 +2614,8 @@ MaybeHandle<Object> JSToWasmObject(Isolate* isolate, Handle<Object> value,
       auto type_canonicalizer = GetWasmEngine()->type_canonicalizer();
 
       if (WasmExportedFunction::IsWasmExportedFunction(*value)) {
-        WasmExportedFunction function = WasmExportedFunction::cast(*value);
+        Tagged<WasmExportedFunction> function =
+            WasmExportedFunction::cast(*value);
         uint32_t real_type_index = function->shared()
                                        ->wasm_exported_function_data()
                                        ->canonical_type_index();
@@ -2640,7 +2647,7 @@ MaybeHandle<Object> JSToWasmObject(Isolate* isolate, Handle<Object> value,
         return WasmInternalFunction::FromExternal(value, isolate);
       } else if (IsWasmStruct(*value) || IsWasmArray(*value)) {
         auto wasm_obj = Handle<WasmObject>::cast(value);
-        WasmTypeInfo type_info = wasm_obj->map()->wasm_type_info();
+        Tagged<WasmTypeInfo> type_info = wasm_obj->map()->wasm_type_info();
         uint32_t real_idx = type_info->type_index();
         const WasmModule* real_module =
             WasmInstanceObject::cast(type_info->instance())->module();
diff --git a/src/wasm/wasm-objects.h b/src/wasm/wasm-objects.h
index 706474110ab..ba01c79b48c 100644
--- a/src/wasm/wasm-objects.h
+++ b/src/wasm/wasm-objects.h
@@ -106,11 +106,12 @@ class ImportedFunctionEntry {
                                      const wasm::FunctionSig* sig);
 
   // Initialize this entry as a Wasm to Wasm call.
-  void SetWasmToWasm(WasmInstanceObject target_instance, Address call_target);
+  void SetWasmToWasm(Tagged<WasmInstanceObject> target_instance,
+                     Address call_target);
 
-  JSReceiver callable();
-  Object maybe_callable();
-  Object object_ref();
+  Tagged<JSReceiver> callable();
+  Tagged<Object> maybe_callable();
+  Tagged<Object> object_ref();
   Address target();
   void set_target(Address new_target);
 
@@ -284,7 +285,7 @@ class WasmMemoryObject
 
   // Assign a new (grown) buffer to this memory, also updating the shortcut
   // fields of all instances that use this memory.
-  void SetNewBuffer(JSArrayBuffer new_buffer);
+  void SetNewBuffer(Tagged<JSArrayBuffer> new_buffer);
 
   V8_EXPORT_PRIVATE static int32_t Grow(Isolate*, Handle<WasmMemoryObject>,
                                         uint32_t pages);
@@ -385,7 +386,7 @@ class V8_EXPORT_PRIVATE WasmInstanceObject : public JSObject {
   // is deterministic. Depending on the V8 build mode there could be no padding.
   V8_INLINE void clear_padding();
 
-  inline WasmMemoryObject memory_object(int memory_index) const;
+  inline Tagged<WasmMemoryObject> memory_object(int memory_index) const;
   inline uint8_t* memory_base(int memory_index) const;
   inline size_t memory_size(int memory_index) const;
 
@@ -578,7 +579,7 @@ class V8_EXPORT_PRIVATE WasmInstanceObject : public JSObject {
   OBJECT_CONSTRUCTORS(WasmInstanceObject, JSObject);
 
  private:
-  void InitDataSegmentArrays(WasmModuleObject);
+  void InitDataSegmentArrays(Tagged<WasmModuleObject>);
 };
 
 // Representation of WebAssembly.Exception JavaScript-level object.
@@ -647,10 +648,10 @@ bool UseGenericWasmToJSWrapper(wasm::ImportCallKind kind,
 // Representation of WebAssembly.Function JavaScript-level object.
 class WasmExportedFunction : public JSFunction {
  public:
-  WasmInstanceObject instance();
+  Tagged<WasmInstanceObject> instance();
   V8_EXPORT_PRIVATE int function_index();
 
-  V8_EXPORT_PRIVATE static bool IsWasmExportedFunction(Object object);
+  V8_EXPORT_PRIVATE static bool IsWasmExportedFunction(Tagged<Object> object);
 
   V8_EXPORT_PRIVATE static Handle<WasmExportedFunction> New(
       Isolate* isolate, Handle<WasmInstanceObject> instance,
@@ -675,14 +676,14 @@ class WasmExportedFunction : public JSFunction {
 // Representation of WebAssembly.Function JavaScript-level object.
 class WasmJSFunction : public JSFunction {
  public:
-  static bool IsWasmJSFunction(Object object);
+  static bool IsWasmJSFunction(Tagged<Object> object);
 
   static Handle<WasmJSFunction> New(Isolate* isolate,
                                     const wasm::FunctionSig* sig,
                                     Handle<JSReceiver> callable,
                                     wasm::Suspend suspend);
 
-  JSReceiver GetCallable() const;
+  Tagged<JSReceiver> GetCallable() const;
   wasm::Suspend GetSuspend() const;
   // Deserializes the signature of this function using the provided zone. Note
   // that lifetime of the signature is hence directly coupled to the zone.
@@ -696,13 +697,13 @@ class WasmJSFunction : public JSFunction {
 // An external function exposed to Wasm via the C/C++ API.
 class WasmCapiFunction : public JSFunction {
  public:
-  static bool IsWasmCapiFunction(Object object);
+  static bool IsWasmCapiFunction(Tagged<Object> object);
 
   static Handle<WasmCapiFunction> New(
       Isolate* isolate, Address call_target, Handle<Foreign> embedder_data,
       Handle<PodArray<wasm::ValueType>> serialized_signature);
 
-  PodArray<wasm::ValueType> GetSerializedSignature() const;
+  Tagged<PodArray<wasm::ValueType>> GetSerializedSignature() const;
   // Checks whether the given {sig} has the same parameter types as the
   // serialized signature stored within this C-API function object.
   bool MatchesSignature(uint32_t other_canonical_sig_index) const;
@@ -719,7 +720,7 @@ class WasmCapiFunction : public JSFunction {
 // // TODO(wasm): Potentially {WasmCapiFunction} will be added here as well.
 class WasmExternalFunction : public JSFunction {
  public:
-  static bool IsWasmExternalFunction(Object object);
+  static bool IsWasmExternalFunction(Tagged<Object> object);
 
   DECL_CAST(WasmExternalFunction)
   OBJECT_CONSTRUCTORS(WasmExternalFunction, JSFunction);
@@ -747,7 +748,7 @@ class WasmIndirectFunctionTable
   static void Resize(Isolate* isolate, Handle<WasmIndirectFunctionTable> table,
                      uint32_t new_size);
   V8_EXPORT_PRIVATE void Set(uint32_t index, int sig_id, Address call_target,
-                             Object ref);
+                             Tagged<Object> ref);
   void Clear(uint32_t index);
 
   DECL_PRINTER(WasmIndirectFunctionTable)
@@ -847,7 +848,7 @@ class WasmInternalFunction
  private:
   // Make this private so it is not use by accident. Use {GetOrCreateExternal}
   // instead.
-  HeapObject external();
+  Tagged<HeapObject> external();
 };
 
 // Information for a WasmJSFunction which is referenced as the function data of
@@ -935,7 +936,7 @@ class WasmScript : public AllStatic {
                                                     int breakpoint_id);
 
   // Remove all set breakpoints.
-  static void ClearAllBreakpoints(Script);
+  static void ClearAllBreakpoints(Tagged<Script>);
 
   // Get a list of all possible breakpoints within a given range of this module.
   V8_EXPORT_PRIVATE static bool GetPossibleBreakpoints(
@@ -1016,20 +1017,21 @@ class WasmObject : public TorqueGeneratedWasmObject<WasmObject, JSReceiver> {
 
  private:
   template <typename ElementType>
-  static ElementType FromNumber(Object value);
+  static ElementType FromNumber(Tagged<Object> value);
 
   TQ_OBJECT_CONSTRUCTORS(WasmObject)
 };
 
 class WasmStruct : public TorqueGeneratedWasmStruct<WasmStruct, WasmObject> {
  public:
-  static inline wasm::StructType* type(Map map);
+  static inline wasm::StructType* type(Tagged<Map> map);
   inline wasm::StructType* type() const;
-  static inline wasm::StructType* GcSafeType(Map map);
+  static inline wasm::StructType* GcSafeType(Tagged<Map> map);
   static inline int Size(const wasm::StructType* type);
-  static inline int GcSafeSize(Map map);
-  static inline void EncodeInstanceSizeInMap(int instance_size, Map map);
-  static inline int DecodeInstanceSizeFromMap(Map map);
+  static inline int GcSafeSize(Tagged<Map> map);
+  static inline void EncodeInstanceSizeInMap(int instance_size,
+                                             Tagged<Map> map);
+  static inline int DecodeInstanceSizeFromMap(Tagged<Map> map);
 
   // Returns the address of the field at given offset.
   inline Address RawFieldAddress(int raw_offset);
@@ -1056,16 +1058,16 @@ class WasmStruct : public TorqueGeneratedWasmStruct<WasmStruct, WasmObject> {
 
 class WasmArray : public TorqueGeneratedWasmArray<WasmArray, WasmObject> {
  public:
-  static inline wasm::ArrayType* type(Map map);
+  static inline wasm::ArrayType* type(Tagged<Map> map);
   inline wasm::ArrayType* type() const;
-  static inline wasm::ArrayType* GcSafeType(Map map);
+  static inline wasm::ArrayType* GcSafeType(Tagged<Map> map);
 
   // Get the {ObjectSlot} corresponding to the element at {index}. Requires that
   // this is a reference array.
   inline ObjectSlot ElementSlot(uint32_t index);
   V8_EXPORT_PRIVATE wasm::WasmValue GetElement(uint32_t index);
 
-  static inline int SizeFor(Map map, int length);
+  static inline int SizeFor(Tagged<Map> map, int length);
 
   // Returns boxed value of the array's element.
   static inline Handle<Object> GetElement(Isolate* isolate,
@@ -1090,8 +1092,8 @@ class WasmArray : public TorqueGeneratedWasmArray<WasmArray, WasmObject> {
     return MaxLength(type->element_type().value_kind_size());
   }
 
-  static inline void EncodeElementSizeInMap(int element_size, Map map);
-  static inline int DecodeElementSizeFromMap(Map map);
+  static inline void EncodeElementSizeInMap(int element_size, Tagged<Map> map);
+  static inline int DecodeElementSizeFromMap(Tagged<Map> map);
 
   DECL_PRINTER(WasmArray)
 
diff --git a/test/cctest/compiler/codegen-tester.h b/test/cctest/compiler/codegen-tester.h
index a7ee6092ede..168a63e6744 100644
--- a/test/cctest/compiler/codegen-tester.h
+++ b/test/cctest/compiler/codegen-tester.h
@@ -60,12 +60,12 @@ class RawMachineAssemblerTester : public HandleAndZoneScope,
 
   ~RawMachineAssemblerTester() override = default;
 
-  void CheckNumber(double expected, Object number) {
+  void CheckNumber(double expected, Tagged<Object> number) {
     CHECK(Object::SameValue(*this->isolate()->factory()->NewNumber(expected),
                             number));
   }
 
-  void CheckString(const char* expected, Object string) {
+  void CheckString(const char* expected, Tagged<Object> string) {
     CHECK(Object::SameValue(
         *this->isolate()->factory()->InternalizeUtf8String(expected), string));
   }
diff --git a/test/cctest/compiler/function-tester.cc b/test/cctest/compiler/function-tester.cc
index c11e3a03fa8..9a9d91d72ae 100644
--- a/test/cctest/compiler/function-tester.cc
+++ b/test/cctest/compiler/function-tester.cc
@@ -130,7 +130,7 @@ Handle<Object> FunctionTester::false_value() {
 
 Handle<JSFunction> FunctionTester::ForMachineGraph(Graph* graph,
                                                    int param_count) {
-  JSFunction p;
+  Tagged<JSFunction> p;
   {  // because of the implicit handle scope of FunctionTester.
     FunctionTester f(graph, param_count);
     p = *f.function;
diff --git a/test/cctest/compiler/test-atomic-load-store-codegen.cc b/test/cctest/compiler/test-atomic-load-store-codegen.cc
index 39b776108d6..b3e0e163077 100644
--- a/test/cctest/compiler/test-atomic-load-store-codegen.cc
+++ b/test/cctest/compiler/test-atomic-load-store-codegen.cc
@@ -88,7 +88,8 @@ void CheckEq(CType in_value, CType out_value) {
 #ifdef V8_COMPRESS_POINTERS
 // Specializations for checking the result of compressing store.
 template <>
-void CheckEq<Object>(Object in_value, Object out_value) {
+void CheckEq<Tagged<Object>>(Tagged<Object> in_value,
+                             Tagged<Object> out_value) {
   // Compare only lower 32-bits of the value because tagged load/stores are
   // 32-bit operations anyway.
   CHECK_EQ(static_cast<Tagged_t>(in_value.ptr()),
@@ -96,24 +97,25 @@ void CheckEq<Object>(Object in_value, Object out_value) {
 }
 
 template <>
-void CheckEq<HeapObject>(HeapObject in_value, HeapObject out_value) {
-  return CheckEq<Object>(in_value, out_value);
+void CheckEq<Tagged<HeapObject>>(Tagged<HeapObject> in_value,
+                                 Tagged<HeapObject> out_value) {
+  return CheckEq<Tagged<Object>>(in_value, out_value);
 }
 
 template <>
-void CheckEq<Smi>(Smi in_value, Smi out_value) {
-  return CheckEq<Object>(in_value, out_value);
+void CheckEq<Tagged<Smi>>(Tagged<Smi> in_value, Tagged<Smi> out_value) {
+  return CheckEq<Tagged<Object>>(in_value, out_value);
 }
 #endif
 
-template <typename TaggedT>
-void InitBuffer(TaggedT* buffer, size_t length, MachineType type) {
-  const size_t kBufferSize = sizeof(TaggedT) * length;
+template <typename T>
+void InitBuffer(Tagged<T>* buffer, size_t length, MachineType type) {
+  const size_t kBufferSize = sizeof(Tagged<T>) * length;
 
   // Tagged field loads require values to be properly tagged because of
   // pointer decompression that may be happenning during load.
   Isolate* isolate = CcTest::InitIsolateOnce();
-  Smi* smi_view = reinterpret_cast<Smi*>(&buffer[0]);
+  Tagged<Smi>* smi_view = reinterpret_cast<Tagged<Smi>*>(&buffer[0]);
   if (type.IsTaggedSigned()) {
     for (size_t i = 0; i < length; i++) {
       smi_view[i] = Smi::FromInt(static_cast<int>(i + kBufferSize) ^ 0xABCDEF0);
@@ -130,18 +132,19 @@ void InitBuffer(TaggedT* buffer, size_t length, MachineType type) {
   }
 }
 
-template <typename TaggedT>
+template <typename T>
 void AtomicLoadTagged(MachineType type, AtomicMemoryOrder order) {
   const int kNumElems = 16;
-  TaggedT buffer[kNumElems];
+  Tagged<T> buffer[kNumElems];
 
   InitBuffer(buffer, kNumElems, type);
 
   for (int i = 0; i < kNumElems; i++) {
-    BufferedRawMachineAssemblerTester<TaggedT> m;
-    TaggedT* base_pointer = &buffer[0];
+    BufferedRawMachineAssemblerTester<Tagged<T>> m;
+    Tagged<T>* base_pointer = &buffer[0];
     if (COMPRESS_POINTERS_BOOL) {
-      base_pointer = reinterpret_cast<TaggedT*>(LSB(base_pointer, kTaggedSize));
+      base_pointer =
+          reinterpret_cast<Tagged<T>*>(LSB(base_pointer, kTaggedSize));
     }
     Node* base = m.PointerConstant(base_pointer);
     Node* index = m.Int32Constant(i * sizeof(buffer[0]));
@@ -153,7 +156,7 @@ void AtomicLoadTagged(MachineType type, AtomicMemoryOrder order) {
       load = m.AtomicLoad(params, base, index);
     }
     m.Return(load);
-    CheckEq<TaggedT>(buffer[i], m.Call());
+    CheckEq<Tagged<T>>(buffer[i], m.Call());
   }
 }
 }  // namespace
@@ -241,7 +244,7 @@ TEST(SeqCstStoreInteger) {
 }
 
 namespace {
-template <typename TaggedT>
+template <typename T>
 void AtomicStoreTagged(MachineType type, AtomicMemoryOrder order) {
   // This tests that tagged values are correctly transferred by atomic loads and
   // stores from in_buffer to out_buffer. For each particular element in
@@ -249,17 +252,17 @@ void AtomicStoreTagged(MachineType type, AtomicMemoryOrder order) {
   // indices are zapped, to test instructions of the correct width are emitted.
 
   const int kNumElems = 16;
-  TaggedT in_buffer[kNumElems];
-  TaggedT out_buffer[kNumElems];
+  Tagged<T> in_buffer[kNumElems];
+  Tagged<T> out_buffer[kNumElems];
   uintptr_t zap_data[] = {kZapValue, kZapValue};
-  TaggedT zap_value;
+  Tagged<T> zap_value;
 
-  static_assert(sizeof(TaggedT) <= sizeof(zap_data));
-  MemCopy(&zap_value, &zap_data, sizeof(TaggedT));
+  static_assert(sizeof(Tagged<T>) <= sizeof(zap_data));
+  MemCopy(&zap_value, &zap_data, sizeof(Tagged<T>));
   InitBuffer(in_buffer, kNumElems, type);
 
 #ifdef V8_TARGET_BIG_ENDIAN
-  int offset = sizeof(TaggedT) - ElementSizeInBytes(type.representation());
+  int offset = sizeof(Tagged<T>) - ElementSizeInBytes(type.representation());
 #else
   int offset = 0;
 #endif
@@ -270,9 +273,9 @@ void AtomicStoreTagged(MachineType type, AtomicMemoryOrder order) {
     RawMachineAssemblerTester<int32_t> m;
     int32_t OK = 0x29000 + x;
     Node* in_base = m.PointerConstant(in_buffer);
-    Node* in_index = m.IntPtrConstant(x * sizeof(TaggedT) + offset);
+    Node* in_index = m.IntPtrConstant(x * sizeof(Tagged<T>) + offset);
     Node* out_base = m.PointerConstant(out_buffer);
-    Node* out_index = m.IntPtrConstant(y * sizeof(TaggedT) + offset);
+    Node* out_index = m.IntPtrConstant(y * sizeof(Tagged<T>) + offset);
 
     Node* load;
     AtomicLoadParameters load_params(type, order);
@@ -295,7 +298,7 @@ void AtomicStoreTagged(MachineType type, AtomicMemoryOrder order) {
     CHECK_NE(in_buffer[x], out_buffer[y]);
     CHECK_EQ(OK, m.Call());
     // Mostly same as CHECK_EQ() but customized for compressed tagged values.
-    CheckEq<TaggedT>(in_buffer[x], out_buffer[y]);
+    CheckEq<Tagged<T>>(in_buffer[x], out_buffer[y]);
     for (int32_t z = 0; z < kNumElems; z++) {
       if (z != y) CHECK_EQ(zap_value, out_buffer[z]);
     }
diff --git a/test/cctest/compiler/test-code-generator.cc b/test/cctest/compiler/test-code-generator.cc
index 5e530978326..4e1a4330ede 100644
--- a/test/cctest/compiler/test-code-generator.cc
+++ b/test/cctest/compiler/test-code-generator.cc
@@ -284,7 +284,7 @@ void PrintStateValue(std::ostream& os, Isolate* isolate, Handle<Object> value,
       os << Object::Number(*value);
       break;
     case MachineRepresentation::kSimd128: {
-      FixedArray vector = FixedArray::cast(*value);
+      Tagged<FixedArray> vector = FixedArray::cast(*value);
       os << "[";
       for (int lane = 0; lane < 4; lane++) {
         os << Smi::cast(vector->get(lane)).value();
@@ -774,7 +774,7 @@ class TestEnvironment : public HandleAndZoneScope {
     return static_cast<int>(std::distance(layout.cbegin(), it));
   }
 
-  Object GetMoveSource(Handle<FixedArray> state, MoveOperands* move) {
+  Tagged<Object> GetMoveSource(Handle<FixedArray> state, MoveOperands* move) {
     InstructionOperand from = move->source();
     if (from.IsConstant()) {
       Constant constant = instructions_.GetConstant(
@@ -823,7 +823,7 @@ class TestEnvironment : public HandleAndZoneScope {
     for (auto move : *moves) {
       int to_index = OperandToStatePosition(
           TeardownLayout(), AllocatedOperand::cast(move->destination()));
-      Object source = GetMoveSource(state_out, move);
+      Tagged<Object> source = GetMoveSource(state_out, move);
       state_out->set(to_index, source);
     }
     return state_out;
@@ -838,7 +838,7 @@ class TestEnvironment : public HandleAndZoneScope {
     for (auto move : *moves) {
       int to_index = OperandToStatePosition(
           TeardownLayout(), AllocatedOperand::cast(move->destination()));
-      Object source = GetMoveSource(state_in, move);
+      Tagged<Object> source = GetMoveSource(state_in, move);
       state_out->set(to_index, source);
     }
     // If we generated redundant moves, they were eliminated automatically and
diff --git a/test/cctest/compiler/test-concurrent-shared-function-info.cc b/test/cctest/compiler/test-concurrent-shared-function-info.cc
index afe770a48e8..8045f460a5f 100644
--- a/test/cctest/compiler/test-concurrent-shared-function-info.cc
+++ b/test/cctest/compiler/test-concurrent-shared-function-info.cc
@@ -29,10 +29,11 @@ enum class SfiState {
   PreparedForDebugExecution,
 };
 
-void ExpectSharedFunctionInfoState(Isolate* isolate, SharedFunctionInfo sfi,
+void ExpectSharedFunctionInfoState(Isolate* isolate,
+                                   Tagged<SharedFunctionInfo> sfi,
                                    SfiState expectedState) {
-  Object function_data = sfi->function_data(kAcquireLoad);
-  HeapObject script = sfi->script(kAcquireLoad);
+  Tagged<Object> function_data = sfi->function_data(kAcquireLoad);
+  Tagged<HeapObject> script = sfi->script(kAcquireLoad);
   switch (expectedState) {
     case SfiState::Compiled:
       CHECK(IsBytecodeArray(function_data) ||
@@ -45,14 +46,14 @@ void ExpectSharedFunctionInfoState(Isolate* isolate, SharedFunctionInfo sfi,
             (IsCode(function_data) &&
              Code::cast(function_data)->kind() == CodeKind::BASELINE));
       CHECK(IsScript(script));
-      DebugInfo debug_info = sfi->GetDebugInfo(isolate);
+      Tagged<DebugInfo> debug_info = sfi->GetDebugInfo(isolate);
       CHECK(!debug_info->HasInstrumentedBytecodeArray());
       break;
     }
     case SfiState::PreparedForDebugExecution: {
       CHECK(IsBytecodeArray(function_data));
       CHECK(IsScript(script));
-      DebugInfo debug_info = sfi->GetDebugInfo(isolate);
+      Tagged<DebugInfo> debug_info = sfi->GetDebugInfo(isolate);
       CHECK(debug_info->HasInstrumentedBytecodeArray());
       break;
     }
@@ -177,7 +178,7 @@ TEST(TestConcurrentSharedFunctionInfo) {
 
     // PreparedForDebugExecution ==> DebugInfo
     {
-      DebugInfo debug_info = test_sfi->GetDebugInfo(isolate);
+      Tagged<DebugInfo> debug_info = test_sfi->GetDebugInfo(isolate);
       debug_info->ClearBreakInfo(isolate);
       ExpectSharedFunctionInfoState(isolate, *test_sfi, SfiState::DebugInfo);
     }
diff --git a/test/cctest/compiler/test-representation-change.cc b/test/cctest/compiler/test-representation-change.cc
index 0e2f2291efc..053a5f1c9d9 100644
--- a/test/cctest/compiler/test-representation-change.cc
+++ b/test/cctest/compiler/test-representation-change.cc
@@ -77,7 +77,7 @@ class RepresentationChangerTester : public HandleAndZoneScope,
     CHECK_FLOAT_EQ(expected, fval);
   }
 
-  void CheckHeapConstant(Node* n, HeapObject expected) {
+  void CheckHeapConstant(Node* n, Tagged<HeapObject> expected) {
     HeapObjectMatcher m(n);
     CHECK(m.HasResolvedValue());
     CHECK_EQ(expected, *m.ResolvedValue());
diff --git a/test/cctest/compiler/test-run-load-store.cc b/test/cctest/compiler/test-run-load-store.cc
index d433b79fbd1..78b9d3c0cd1 100644
--- a/test/cctest/compiler/test-run-load-store.cc
+++ b/test/cctest/compiler/test-run-load-store.cc
@@ -8,8 +8,10 @@
 
 #include "src/base/bits.h"
 #include "src/base/overflowing-math.h"
+#include "src/base/template-utils.h"
 #include "src/base/utils/random-number-generator.h"
 #include "src/objects/objects-inl.h"
+#include "src/objects/tagged.h"
 #include "test/cctest/cctest.h"
 #include "test/cctest/compiler/codegen-tester.h"
 #include "test/common/value-helper.h"
@@ -201,7 +203,8 @@ void CheckEq(CType in_value, CType out_value) {
 #ifdef V8_COMPRESS_POINTERS
 // Specializations for checking the result of compressing store.
 template <>
-void CheckEq<Object>(Object in_value, Object out_value) {
+void CheckEq<Tagged<Object>>(Tagged<Object> in_value,
+                             Tagged<Object> out_value) {
   // Compare only lower 32-bits of the value because tagged load/stores are
   // 32-bit operations anyway.
   CHECK_EQ(static_cast<Tagged_t>(in_value.ptr()),
@@ -209,13 +212,14 @@ void CheckEq<Object>(Object in_value, Object out_value) {
 }
 
 template <>
-void CheckEq<HeapObject>(HeapObject in_value, HeapObject out_value) {
-  return CheckEq<Object>(in_value, out_value);
+void CheckEq<Tagged<HeapObject>>(Tagged<HeapObject> in_value,
+                                 Tagged<HeapObject> out_value) {
+  return CheckEq<Tagged<Object>>(in_value, out_value);
 }
 
 template <>
-void CheckEq<Smi>(Smi in_value, Smi out_value) {
-  return CheckEq<Object>(in_value, out_value);
+void CheckEq<Tagged<Smi>>(Tagged<Smi> in_value, Tagged<Smi> out_value) {
+  return CheckEq<Tagged<Object>>(in_value, out_value);
 }
 #endif
 
@@ -386,10 +390,12 @@ TEST(RunLoadImmIndex) {
   RunLoadImmIndex<int32_t>(MachineType::Int32(), TestAlignment::kAligned);
   RunLoadImmIndex<uint32_t>(MachineType::Uint32(), TestAlignment::kAligned);
   RunLoadImmIndex<void*>(MachineType::Pointer(), TestAlignment::kAligned);
-  RunLoadImmIndex<Smi>(MachineType::TaggedSigned(), TestAlignment::kAligned);
-  RunLoadImmIndex<HeapObject>(MachineType::TaggedPointer(),
-                              TestAlignment::kAligned);
-  RunLoadImmIndex<Object>(MachineType::AnyTagged(), TestAlignment::kAligned);
+  RunLoadImmIndex<Tagged<Smi>>(MachineType::TaggedSigned(),
+                               TestAlignment::kAligned);
+  RunLoadImmIndex<Tagged<HeapObject>>(MachineType::TaggedPointer(),
+                                      TestAlignment::kAligned);
+  RunLoadImmIndex<Tagged<Object>>(MachineType::AnyTagged(),
+                                  TestAlignment::kAligned);
   RunLoadImmIndex<float>(MachineType::Float32(), TestAlignment::kAligned);
   RunLoadImmIndex<double>(MachineType::Float64(), TestAlignment::kAligned);
 #if V8_TARGET_ARCH_64_BIT
@@ -420,10 +426,12 @@ TEST(RunLoadStore) {
   RunLoadStore<int32_t>(MachineType::Int32(), TestAlignment::kAligned);
   RunLoadStore<uint32_t>(MachineType::Uint32(), TestAlignment::kAligned);
   RunLoadStore<void*>(MachineType::Pointer(), TestAlignment::kAligned);
-  RunLoadStore<Smi>(MachineType::TaggedSigned(), TestAlignment::kAligned);
-  RunLoadStore<HeapObject>(MachineType::TaggedPointer(),
-                           TestAlignment::kAligned);
-  RunLoadStore<Object>(MachineType::AnyTagged(), TestAlignment::kAligned);
+  RunLoadStore<Tagged<Smi>>(MachineType::TaggedSigned(),
+                            TestAlignment::kAligned);
+  RunLoadStore<Tagged<HeapObject>>(MachineType::TaggedPointer(),
+                                   TestAlignment::kAligned);
+  RunLoadStore<Tagged<Object>>(MachineType::AnyTagged(),
+                               TestAlignment::kAligned);
   RunLoadStore<float>(MachineType::Float32(), TestAlignment::kAligned);
   RunLoadStore<double>(MachineType::Float64(), TestAlignment::kAligned);
 #if V8_TARGET_ARCH_64_BIT
diff --git a/test/cctest/compiler/test-run-machops.cc b/test/cctest/compiler/test-run-machops.cc
index 598a7a57c08..261192764a9 100644
--- a/test/cctest/compiler/test-run-machops.cc
+++ b/test/cctest/compiler/test-run-machops.cc
@@ -1172,7 +1172,7 @@ TEST(RunDiamondPhiConst) {
 
 
 TEST(RunDiamondPhiNumber) {
-  RawMachineAssemblerTester<Object> m(MachineType::Int32());
+  RawMachineAssemblerTester<Tagged<Object>> m(MachineType::Int32());
   double false_val = -11.1;
   double true_val = 200.1;
   Node* true_node = m.NumberConstant(true_val);
@@ -1185,7 +1185,7 @@ TEST(RunDiamondPhiNumber) {
 
 
 TEST(RunDiamondPhiString) {
-  RawMachineAssemblerTester<Object> m(MachineType::Int32());
+  RawMachineAssemblerTester<Tagged<Object>> m(MachineType::Int32());
   const char* false_val = "false";
   const char* true_val = "true";
   Node* true_node = m.StringConstant(true_val);
@@ -5179,7 +5179,7 @@ TEST(RunRefDiamond) {
   const int magic = 99644;
   Handle<String> rexpected =
       CcTest::i_isolate()->factory()->InternalizeUtf8String("A");
-  String buffer;
+  Tagged<String> buffer;
 
   RawMachineLabel blocka, blockb, end;
   Node* k1 = m.StringConstant("A");
@@ -5214,7 +5214,7 @@ TEST(RunDoubleRefDiamond) {
   double dconstant = 99.99;
   Handle<String> rexpected =
       CcTest::i_isolate()->factory()->InternalizeUtf8String("AX");
-  String rbuffer;
+  Tagged<String> rbuffer;
 
   RawMachineLabel blocka, blockb, end;
   Node* d1 = m.Float64Constant(dconstant);
@@ -5255,7 +5255,7 @@ TEST(RunDoubleRefDoubleDiamond) {
   double dconstant = 99.997;
   Handle<String> rexpected =
       CcTest::i_isolate()->factory()->InternalizeUtf8String("AD");
-  String rbuffer;
+  Tagged<String> rbuffer;
 
   RawMachineLabel blocka, blockb, mid, blockd, blocke, end;
   Node* d1 = m.Float64Constant(dconstant);
@@ -5760,7 +5760,7 @@ TEST(RunSpillConstantsAndParameters) {
 
 
 TEST(RunNewSpaceConstantsInPhi) {
-  RawMachineAssemblerTester<Object> m(MachineType::Int32());
+  RawMachineAssemblerTester<Tagged<Object>> m(MachineType::Int32());
 
   Isolate* isolate = CcTest::i_isolate();
   Handle<HeapNumber> true_val = isolate->factory()->NewHeapNumber(11.2);
diff --git a/test/cctest/heap/heap-utils.cc b/test/cctest/heap/heap-utils.cc
index fec22e337c6..06023378e6a 100644
--- a/test/cctest/heap/heap-utils.cc
+++ b/test/cctest/heap/heap-utils.cc
@@ -161,12 +161,13 @@ void FillPageInPagedSpace(Page* page,
   // Collect all free list block sizes
   page->ForAllFreeListCategories(
       [&available_sizes](FreeListCategory* category) {
-        category->IterateNodesForTesting([&available_sizes](FreeSpace node) {
-          int node_size = node->Size();
-          if (node_size >= kMaxRegularHeapObjectSize) {
-            available_sizes.push_back(node_size);
-          }
-        });
+        category->IterateNodesForTesting(
+            [&available_sizes](Tagged<FreeSpace> node) {
+              int node_size = node->Size();
+              if (node_size >= kMaxRegularHeapObjectSize) {
+                available_sizes.push_back(node_size);
+              }
+            });
       });
 
   Isolate* isolate = heap->isolate();
@@ -194,11 +195,12 @@ void FillPageInPagedSpace(Page* page,
         remaining_sizes.push_back({});
         std::vector<int>& sizes_in_category =
             remaining_sizes[remaining_sizes.size() - 1];
-        category->IterateNodesForTesting([&sizes_in_category](FreeSpace node) {
-          int node_size = node->Size();
-          DCHECK_LT(0, FixedArrayLenFromSize(node_size));
-          sizes_in_category.push_back(node_size);
-        });
+        category->IterateNodesForTesting(
+            [&sizes_in_category](Tagged<FreeSpace> node) {
+              int node_size = node->Size();
+              DCHECK_LT(0, FixedArrayLenFromSize(node_size));
+              sizes_in_category.push_back(node_size);
+            });
       });
   for (auto it = remaining_sizes.rbegin(); it != remaining_sizes.rend(); ++it) {
     std::vector<int> sizes_in_category = *it;
@@ -392,7 +394,7 @@ void ForceEvacuationCandidate(Page* page) {
   }
 }
 
-bool InCorrectGeneration(HeapObject object) {
+bool InCorrectGeneration(Tagged<HeapObject> object) {
   return v8_flags.single_generation ? !i::Heap::InYoungGeneration(object)
                                     : i::Heap::InYoungGeneration(object);
 }
diff --git a/test/cctest/heap/heap-utils.h b/test/cctest/heap/heap-utils.h
index 03b6222478a..97682735041 100644
--- a/test/cctest/heap/heap-utils.h
+++ b/test/cctest/heap/heap-utils.h
@@ -67,7 +67,7 @@ bool InYoungGeneration(v8::Isolate* isolate, const GlobalOrPersistent& global) {
   return i::Heap::InYoungGeneration(*v8::Utils::OpenHandle(*tmp));
 }
 
-bool InCorrectGeneration(HeapObject object);
+bool InCorrectGeneration(Tagged<HeapObject> object);
 
 template <typename GlobalOrPersistent>
 bool InCorrectGeneration(v8::Isolate* isolate,
diff --git a/test/cctest/heap/test-alloc.cc b/test/cctest/heap/test-alloc.cc
index 49d65f6eb33..d69a1e5197c 100644
--- a/test/cctest/heap/test-alloc.cc
+++ b/test/cctest/heap/test-alloc.cc
@@ -50,7 +50,7 @@ Handle<Object> HeapTester::TestAllocateAfterFailures() {
   AlwaysAllocateScopeForTesting scope(heap);
   int size = FixedArray::SizeFor(100);
   // Young generation.
-  HeapObject obj =
+  Tagged<HeapObject> obj =
       heap->AllocateRaw(size, AllocationType::kYoung).ToObjectChecked();
   // In order to pass heap verification on Isolate teardown, mark the
   // allocated area as a filler.
diff --git a/test/cctest/heap/test-array-buffer-tracker.cc b/test/cctest/heap/test-array-buffer-tracker.cc
index b8068ea6d27..4932b6e5758 100644
--- a/test/cctest/heap/test-array-buffer-tracker.cc
+++ b/test/cctest/heap/test-array-buffer-tracker.cc
@@ -37,7 +37,7 @@ bool IsTracked(i::Heap* heap, i::ArrayBufferExtension* extension) {
   return in_young || in_old;
 }
 
-bool IsTracked(i::Heap* heap, i::JSArrayBuffer buffer) {
+bool IsTracked(i::Heap* heap, i::Tagged<i::JSArrayBuffer> buffer) {
   return IsTracked(heap, buffer->extension());
 }
 
@@ -266,7 +266,7 @@ TEST(ArrayBuffer_LivePromotion) {
   v8::Isolate* isolate = env->GetIsolate();
   Heap* heap = reinterpret_cast<Isolate*>(isolate)->heap();
 
-  JSArrayBuffer raw_ab;
+  Tagged<JSArrayBuffer> raw_ab;
   {
     v8::HandleScope handle_scope(isolate);
     Handle<FixedArray> root =
diff --git a/test/cctest/heap/test-concurrent-allocation.cc b/test/cctest/heap/test-concurrent-allocation.cc
index 3991c457864..c46814e15ca 100644
--- a/test/cctest/heap/test-concurrent-allocation.cc
+++ b/test/cctest/heap/test-concurrent-allocation.cc
@@ -35,10 +35,10 @@ namespace internal {
 
 namespace {
 void CreateFixedArray(Heap* heap, Address start, int size) {
-  HeapObject object = HeapObject::FromAddress(start);
+  Tagged<HeapObject> object = HeapObject::FromAddress(start);
   object->set_map_after_allocation(ReadOnlyRoots(heap).fixed_array_map(),
                                    SKIP_WRITE_BARRIER);
-  FixedArray array = FixedArray::cast(object);
+  Tagged<FixedArray> array = FixedArray::cast(object);
   int length = (size - FixedArray::kHeaderSize) / kTaggedSize;
   array->set_length(length);
   MemsetTagged(array->data_start(), ReadOnlyRoots(heap).undefined_value(),
@@ -404,7 +404,7 @@ UNINITIALIZED_TEST(ConcurrentBlackAllocation) {
 
   for (int i = 0; i < kNumIterations * kObjectsAllocatedPerIteration; i++) {
     Address address = objects[i];
-    HeapObject object = HeapObject::FromAddress(address);
+    Tagged<HeapObject> object = HeapObject::FromAddress(address);
 
     if (i < kWhiteIterations * kObjectsAllocatedPerIteration) {
       CHECK(heap->marking_state()->IsUnmarked(object));
@@ -418,8 +418,8 @@ UNINITIALIZED_TEST(ConcurrentBlackAllocation) {
 
 class ConcurrentWriteBarrierThread final : public v8::base::Thread {
  public:
-  ConcurrentWriteBarrierThread(Heap* heap, FixedArray fixed_array,
-                               HeapObject value)
+  ConcurrentWriteBarrierThread(Heap* heap, Tagged<FixedArray> fixed_array,
+                               Tagged<HeapObject> value)
       : v8::base::Thread(base::Thread::Options("ThreadWithLocalHeap")),
         heap_(heap),
         fixed_array_(fixed_array),
@@ -450,8 +450,8 @@ UNINITIALIZED_TEST(ConcurrentWriteBarrier) {
   Isolate* i_isolate = reinterpret_cast<Isolate*>(isolate);
   Heap* heap = i_isolate->heap();
 
-  FixedArray fixed_array;
-  HeapObject value;
+  Tagged<FixedArray> fixed_array;
+  Tagged<HeapObject> value;
   {
     HandleScope handle_scope(i_isolate);
     Handle<FixedArray> fixed_array_handle(
@@ -482,7 +482,8 @@ UNINITIALIZED_TEST(ConcurrentWriteBarrier) {
 
 class ConcurrentRecordRelocSlotThread final : public v8::base::Thread {
  public:
-  ConcurrentRecordRelocSlotThread(Heap* heap, Code code, HeapObject value)
+  ConcurrentRecordRelocSlotThread(Heap* heap, Tagged<Code> code,
+                                  Tagged<HeapObject> value)
       : v8::base::Thread(base::Thread::Options("ThreadWithLocalHeap")),
         heap_(heap),
         code_(code),
@@ -494,7 +495,7 @@ class ConcurrentRecordRelocSlotThread final : public v8::base::Thread {
     // Modification of InstructionStream object requires write access.
     RwxMemoryWriteScopeForTesting rwx_write_scope;
     DisallowGarbageCollection no_gc;
-    InstructionStream istream = code_->instruction_stream();
+    Tagged<InstructionStream> istream = code_->instruction_stream();
     int mode_mask = RelocInfo::EmbeddedObjectModeMask();
     CodePageMemoryModificationScope memory_modification_scope(istream);
     for (RelocIterator it(code_, mode_mask); !it.done(); it.next()) {
@@ -524,8 +525,8 @@ UNINITIALIZED_TEST(ConcurrentRecordRelocSlot) {
   Isolate* i_isolate = reinterpret_cast<Isolate*>(isolate);
   Heap* heap = i_isolate->heap();
   {
-    Code code;
-    HeapObject value;
+    Tagged<Code> code;
+    Tagged<HeapObject> value;
     {
       HandleScope handle_scope(i_isolate);
       uint8_t buffer[i::Assembler::kDefaultBufferSize];
diff --git a/test/cctest/heap/test-heap.cc b/test/cctest/heap/test-heap.cc
index 20087d45c05..0ebad72323c 100644
--- a/test/cctest/heap/test-heap.cc
+++ b/test/cctest/heap/test-heap.cc
@@ -1740,7 +1740,7 @@ void CompilationCacheRegeneration(bool retain_root_sfi, bool flush_root_sfi,
     bool root_sfi_still_exists = false;
     MaybeObject maybe_root_sfi =
         script->shared_function_infos()->Get(kFunctionLiteralIdTopLevel);
-    if (HeapObject sfi_or_undefined;
+    if (Tagged<HeapObject> sfi_or_undefined;
         maybe_root_sfi.GetHeapObject(&sfi_or_undefined)) {
       root_sfi_still_exists = !IsUndefined(sfi_or_undefined);
     }
diff --git a/test/cctest/heap/test-mark-compact.cc b/test/cctest/heap/test-mark-compact.cc
index 79c88bf39e0..0dd9f97d8c0 100644
--- a/test/cctest/heap/test-mark-compact.cc
+++ b/test/cctest/heap/test-mark-compact.cc
@@ -80,7 +80,7 @@ TEST(Promotion) {
 // allocation failure.
 AllocationResult HeapTester::AllocateMapForTest(Isolate* isolate) {
   Heap* heap = isolate->heap();
-  HeapObject obj;
+  Tagged<HeapObject> obj;
   AllocationResult alloc = heap->AllocateRaw(Map::kSize, AllocationType::kMap);
   if (!alloc.To(&obj)) return alloc;
   obj->set_map_after_allocation(ReadOnlyRoots(heap).meta_map(),
@@ -96,14 +96,14 @@ AllocationResult HeapTester::AllocateFixedArrayForTest(
     Heap* heap, int length, AllocationType allocation) {
   DCHECK(length >= 0 && length <= FixedArray::kMaxLength);
   int size = FixedArray::SizeFor(length);
-  HeapObject obj;
+  Tagged<HeapObject> obj;
   {
     AllocationResult result = heap->AllocateRaw(size, allocation);
     if (!result.To(&obj)) return result;
   }
   obj->set_map_after_allocation(ReadOnlyRoots(heap).fixed_array_map(),
                                 SKIP_WRITE_BARRIER);
-  FixedArray array = FixedArray::cast(obj);
+  Tagged<FixedArray> array = FixedArray::cast(obj);
   array->set_length(length);
   MemsetTagged(array->data_start(), ReadOnlyRoots(heap).undefined_value(),
                length);
diff --git a/test/cctest/heap/test-spaces.cc b/test/cctest/heap/test-spaces.cc
index b3a9332d58b..d4dcf619b3d 100644
--- a/test/cctest/heap/test-spaces.cc
+++ b/test/cctest/heap/test-spaces.cc
@@ -377,10 +377,10 @@ TEST(OldLargeObjectSpace) {
 
   int lo_size = Page::kPageSize;
 
-  Object obj = lo->AllocateRaw(lo_size).ToObjectChecked();
+  Tagged<Object> obj = lo->AllocateRaw(lo_size).ToObjectChecked();
   CHECK(IsHeapObject(obj));
 
-  HeapObject ho = HeapObject::cast(obj);
+  Tagged<HeapObject> ho = HeapObject::cast(obj);
 
   CHECK(lo->Contains(HeapObject::cast(obj)));
 
@@ -467,28 +467,29 @@ TEST(SizeOfInitialHeap) {
 }
 #endif  // DEBUG
 
-static HeapObject AllocateUnaligned(NewSpace* space, int size) {
+static Tagged<HeapObject> AllocateUnaligned(NewSpace* space, int size) {
   AllocationResult allocation = space->AllocateRaw(size, kTaggedAligned);
   CHECK(!allocation.IsFailure());
-  HeapObject filler;
+  Tagged<HeapObject> filler;
   CHECK(allocation.To(&filler));
   space->heap()->CreateFillerObjectAt(filler.address(), size);
   return filler;
 }
 
-static HeapObject AllocateUnaligned(PagedSpace* space, int size) {
+static Tagged<HeapObject> AllocateUnaligned(PagedSpace* space, int size) {
   AllocationResult allocation = space->AllocateRaw(size, kTaggedAligned);
   CHECK(!allocation.IsFailure());
-  HeapObject filler;
+  Tagged<HeapObject> filler;
   CHECK(allocation.To(&filler));
   space->heap()->CreateFillerObjectAt(filler.address(), size);
   return filler;
 }
 
-static HeapObject AllocateUnaligned(OldLargeObjectSpace* space, int size) {
+static Tagged<HeapObject> AllocateUnaligned(OldLargeObjectSpace* space,
+                                            int size) {
   AllocationResult allocation = space->AllocateRaw(size);
   CHECK(!allocation.IsFailure());
-  HeapObject filler;
+  Tagged<HeapObject> filler;
   CHECK(allocation.To(&filler));
   return filler;
 }
@@ -654,7 +655,7 @@ HEAP_TEST(Regress777177) {
     heap::SimulateFullSpace(old_space);
     AllocationResult result =
         old_space->AllocateRaw(filler_size, kTaggedAligned);
-    HeapObject obj = result.ToObjectChecked();
+    Tagged<HeapObject> obj = result.ToObjectChecked();
     heap->CreateFillerObjectAt(obj.address(), filler_size);
   }
 
@@ -663,7 +664,7 @@ HEAP_TEST(Regress777177) {
     // top_on_previous_step_ to the next page.
     AllocationResult result =
         old_space->AllocateRaw(max_object_size, kTaggedAligned);
-    HeapObject obj = result.ToObjectChecked();
+    Tagged<HeapObject> obj = result.ToObjectChecked();
     // Simulate allocation folding moving the top pointer back.
     old_space->SetTopAndLimit(obj.address(), old_space->limit(),
                               old_space->limit());
@@ -673,7 +674,7 @@ HEAP_TEST(Regress777177) {
     // This triggers assert in crbug.com/777177.
     AllocationResult result =
         old_space->AllocateRaw(filler_size, kTaggedAligned);
-    HeapObject obj = result.ToObjectChecked();
+    Tagged<HeapObject> obj = result.ToObjectChecked();
     heap->CreateFillerObjectAt(obj.address(), filler_size);
   }
   old_space->RemoveAllocationObserver(&observer);
@@ -703,7 +704,7 @@ HEAP_TEST(Regress791582) {
   {
     AllocationResult result =
         new_space->AllocateRaw(until_page_end, kTaggedAligned);
-    HeapObject obj = result.ToObjectChecked();
+    Tagged<HeapObject> obj = result.ToObjectChecked();
     heap->CreateFillerObjectAt(obj.address(), until_page_end);
     // Simulate allocation folding moving the top pointer back.
     *new_space->allocation_top_address() = obj.address();
@@ -712,7 +713,7 @@ HEAP_TEST(Regress791582) {
   {
     // This triggers assert in crbug.com/791582
     AllocationResult result = new_space->AllocateRaw(256, kTaggedAligned);
-    HeapObject obj = result.ToObjectChecked();
+    Tagged<HeapObject> obj = result.ToObjectChecked();
     heap->CreateFillerObjectAt(obj.address(), 256);
   }
   new_space->RemoveAllocationObserver(&observer);
@@ -738,7 +739,8 @@ TEST(ShrinkPageToHighWaterMarkFreeSpaceEnd) {
   old_space->FreeLinearAllocationArea();
   old_space->ResetFreeList();
 
-  HeapObject filler = HeapObject::FromAddress(array->address() + array->Size());
+  Tagged<HeapObject> filler =
+      HeapObject::FromAddress(array->address() + array->Size());
   CHECK(IsFreeSpace(filler));
   size_t shrunk = old_space->ShrinkPageToHighWaterMark(page);
   size_t should_have_shrunk = RoundDown(
@@ -791,7 +793,8 @@ TEST(ShrinkPageToHighWaterMarkOneWordFiller) {
   old_space->FreeLinearAllocationArea();
   old_space->ResetFreeList();
 
-  HeapObject filler = HeapObject::FromAddress(array->address() + array->Size());
+  Tagged<HeapObject> filler =
+      HeapObject::FromAddress(array->address() + array->Size());
   CHECK_EQ(filler->map(),
            ReadOnlyRoots(CcTest::heap()).one_pointer_filler_map());
 
@@ -819,7 +822,8 @@ TEST(ShrinkPageToHighWaterMarkTwoWordFiller) {
   old_space->FreeLinearAllocationArea();
   old_space->ResetFreeList();
 
-  HeapObject filler = HeapObject::FromAddress(array->address() + array->Size());
+  Tagged<HeapObject> filler =
+      HeapObject::FromAddress(array->address() + array->Size());
   CHECK_EQ(filler->map(),
            ReadOnlyRoots(CcTest::heap()).two_pointer_filler_map());
 
@@ -952,7 +956,7 @@ TEST(ReadOnlySpaceMetrics_AlignedAllocations) {
 
   int alignment = USE_ALLOCATION_ALIGNMENT_BOOL ? kDoubleSize : kTaggedSize;
 
-  HeapObject object =
+  Tagged<HeapObject> object =
       faked_space->AllocateRaw(object_size, kDoubleAligned).ToObjectChecked();
   CHECK_EQ(object.address() % alignment, 0);
   object =
diff --git a/test/cctest/heap/test-weak-references.cc b/test/cctest/heap/test-weak-references.cc
index 092cf2cd72e..a3aff017de6 100644
--- a/test/cctest/heap/test-weak-references.cc
+++ b/test/cctest/heap/test-weak-references.cc
@@ -58,7 +58,7 @@ TEST(WeakReferencesBasic) {
     CHECK(IsCode(*code));
 
     lh->set_data1(HeapObjectReference::Weak(*code));
-    HeapObject code_heap_object;
+    Tagged<HeapObject> code_heap_object;
     CHECK(lh->data1()->GetHeapObjectIfWeak(&code_heap_object));
     CHECK_EQ(*code, code_heap_object);
 
@@ -100,7 +100,7 @@ TEST(WeakReferencesOldToOld) {
   heap::InvokeMajorGC(heap);
   CHECK(heap->InOldSpace(*fixed_array));
 
-  HeapObject heap_object;
+  Tagged<HeapObject> heap_object;
   CHECK(lh->data1()->GetHeapObjectIfWeak(&heap_object));
   CHECK_EQ(heap_object, *fixed_array);
 }
@@ -126,7 +126,7 @@ TEST(WeakReferencesOldToNew) {
 
   heap::InvokeMajorGC(heap);
 
-  HeapObject heap_object;
+  Tagged<HeapObject> heap_object;
   CHECK(lh->data1()->GetHeapObjectIfWeak(&heap_object));
   CHECK_EQ(heap_object, *fixed_array);
 }
@@ -152,7 +152,7 @@ TEST(WeakReferencesOldToNewScavenged) {
 
   heap::InvokeMinorGC(heap);
 
-  HeapObject heap_object;
+  Tagged<HeapObject> heap_object;
   CHECK(lh->data1()->GetHeapObjectIfWeak(&heap_object));
   CHECK_EQ(heap_object, *fixed_array);
 }
@@ -193,7 +193,7 @@ TEST(ObjectMovesBeforeClearingWeakField) {
   HandleScope outer_scope(isolate);
   Handle<LoadHandler> lh = CreateLoadHandlerForTest(factory);
   CHECK(InCorrectGeneration(*lh));
-  LoadHandler lh_location = *lh;
+  Tagged<LoadHandler> lh_location = *lh;
   {
     HandleScope inner_scope(isolate);
     // Create a new FixedArray which the LoadHandler will point to.
@@ -210,7 +210,7 @@ TEST(ObjectMovesBeforeClearingWeakField) {
 
   // Scavenger will move *lh.
   heap::InvokeMinorGC(heap);
-  LoadHandler new_lh_location = *lh;
+  Tagged<LoadHandler> new_lh_location = *lh;
   CHECK_NE(lh_location, new_lh_location);
   CHECK(lh->data1()->IsWeak());
 
@@ -275,7 +275,7 @@ TEST(ObjectWithWeakReferencePromoted) {
   CHECK(heap->InOldSpace(*lh));
   CHECK(heap->InOldSpace(*fixed_array));
 
-  HeapObject heap_object;
+  Tagged<HeapObject> heap_object;
   CHECK(lh->data1()->GetHeapObjectIfWeak(&heap_object));
   CHECK_EQ(heap_object, *fixed_array);
 }
@@ -372,7 +372,7 @@ TEST(WeakArraysBasic) {
   CHECK(Heap::InYoungGeneration(*array));
 
   for (int i = 0; i < length; ++i) {
-    HeapObject heap_object;
+    Tagged<HeapObject> heap_object;
     CHECK(array->Get(i)->GetHeapObjectIfStrong(&heap_object));
     CHECK_EQ(heap_object, ReadOnlyRoots(heap).undefined_value());
   }
@@ -404,7 +404,7 @@ TEST(WeakArraysBasic) {
   // TODO(marja): update this when/if we do handle weak references in the new
   // space.
   heap::InvokeMinorGC(heap);
-  HeapObject heap_object;
+  Tagged<HeapObject> heap_object;
   CHECK(array->Get(0)->GetHeapObjectIfWeak(&heap_object));
   CHECK_EQ(Smi::cast(FixedArray::cast(heap_object)->get(0)).value(), 2016);
   CHECK(array->Get(1)->GetHeapObjectIfWeak(&heap_object));
@@ -501,7 +501,7 @@ TEST(WeakArrayListBasic) {
   // TODO(marja): update this when/if we do handle weak references in the new
   // space.
   heap::InvokeMinorGC(heap);
-  HeapObject heap_object;
+  Tagged<HeapObject> heap_object;
   CHECK_EQ(array->length(), 8);
   CHECK(array->Get(0)->GetHeapObjectIfWeak(&heap_object));
   CHECK_EQ(Smi::cast(FixedArray::cast(heap_object)->get(0)).value(), 2016);
@@ -711,9 +711,9 @@ TEST(PrototypeUsersBasic) {
 
 namespace {
 
-HeapObject saved_heap_object;
+Tagged<HeapObject> saved_heap_object;
 
-static void TestCompactCallback(HeapObject value, int old_index,
+static void TestCompactCallback(Tagged<HeapObject> value, int old_index,
                                 int new_index) {
   saved_heap_object = value;
   CHECK_EQ(old_index, 2);
@@ -759,7 +759,7 @@ TEST(PrototypeUsersCompacted) {
   CHECK(array->Get(3)->IsCleared());
 
   CHECK_EQ(array->length(), 3 + PrototypeUsers::kFirstIndex);
-  WeakArrayList new_array =
+  Tagged<WeakArrayList> new_array =
       PrototypeUsers::Compact(array, heap, TestCompactCallback);
   CHECK_EQ(new_array->length(), 1 + PrototypeUsers::kFirstIndex);
   CHECK_EQ(saved_heap_object, *live_map);
diff --git a/test/cctest/heap/test-write-barrier.cc b/test/cctest/heap/test-write-barrier.cc
index 61a5613e186..583f2298fe5 100644
--- a/test/cctest/heap/test-write-barrier.cc
+++ b/test/cctest/heap/test-write-barrier.cc
@@ -38,9 +38,9 @@ HEAP_TEST(WriteBarrier_Marking) {
     objects->set(2, *value2);
   }
   heap::SimulateIncrementalMarking(CcTest::heap(), false);
-  FixedArray host = FixedArray::cast(objects->get(0));
-  HeapObject value1 = HeapObject::cast(objects->get(1));
-  HeapObject value2 = HeapObject::cast(objects->get(2));
+  Tagged<FixedArray> host = FixedArray::cast(objects->get(0));
+  Tagged<HeapObject> value1 = HeapObject::cast(objects->get(1));
+  Tagged<HeapObject> value2 = HeapObject::cast(objects->get(2));
   CHECK(heap->marking_state()->IsUnmarked(host));
   CHECK(heap->marking_state()->IsUnmarked(value1));
   // Trigger the barrier for the unmarked host and expect the bail out.
@@ -78,7 +78,7 @@ HEAP_TEST(WriteBarrier_MarkingExtension) {
     objects->set(0, *host);
   }
   heap::SimulateIncrementalMarking(CcTest::heap(), false);
-  JSArrayBuffer host = JSArrayBuffer::cast(objects->get(0));
+  Tagged<JSArrayBuffer> host = JSArrayBuffer::cast(objects->get(0));
   CHECK(heap->marking_state()->IsUnmarked(host));
   CHECK(!extension->IsMarked());
   WriteBarrier::Marking(host, extension);
diff --git a/test/cctest/test-accessor-assembler.cc b/test/cctest/test-accessor-assembler.cc
index 10c3249f83b..00772cc8b26 100644
--- a/test/cctest/test-accessor-assembler.cc
+++ b/test/cctest/test-accessor-assembler.cc
@@ -82,7 +82,7 @@ void TestStubCacheOffsetCalculation(StubCache::Table table) {
       }
       Handle<Object> result = ft.Call(name, map).ToHandleChecked();
 
-      Smi expected = Smi::FromInt(expected_result & Smi::kMaxValue);
+      Tagged<Smi> expected = Smi::FromInt(expected_result & Smi::kMaxValue);
       CHECK_EQ(expected, Smi::cast(*result));
     }
   }
diff --git a/test/cctest/test-api-typed-array.cc b/test/cctest/test-api-typed-array.cc
index ae45bccecc7..0513656fe56 100644
--- a/test/cctest/test-api-typed-array.cc
+++ b/test/cctest/test-api-typed-array.cc
@@ -18,7 +18,7 @@ namespace {
 
 void CheckElementValue(i::Isolate* isolate, int expected,
                        i::Handle<i::Object> obj, int offset) {
-  i::Object element =
+  i::Tagged<i::Object> element =
       *i::Object::GetElement(isolate, obj, offset).ToHandleChecked();
   CHECK_EQ(expected, i::Smi::ToInt(element));
 }
diff --git a/test/cctest/test-api.cc b/test/cctest/test-api.cc
index e44ca42a8e6..adb4b4277fb 100644
--- a/test/cctest/test-api.cc
+++ b/test/cctest/test-api.cc
@@ -10659,7 +10659,7 @@ THREADED_TEST(ShadowObjectAndDataProperty) {
   // slow_stub bailout which would mean a trip to the runtime on all
   // subsequent stores, and a lack of feedback for the optimizing
   // compiler downstream.
-  i::HeapObject heap_object;
+  i::Tagged<i::HeapObject> heap_object;
   CHECK(nexus.GetFeedback().GetHeapObject(&heap_object));
   CHECK(IsPropertyCell(heap_object));
 }
@@ -10703,7 +10703,7 @@ THREADED_TEST(ShadowObjectAndDataPropertyTurbo) {
   CHECK_EQ(i::FeedbackSlotKind::kStoreGlobalSloppy, nexus.kind());
   CompileRun("%OptimizeFunctionOnNextCall(foo); foo(1)");
   CHECK_EQ(i::InlineCacheState::MONOMORPHIC, nexus.ic_state());
-  i::HeapObject heap_object;
+  i::Tagged<i::HeapObject> heap_object;
   CHECK(nexus.GetFeedback().GetHeapObject(&heap_object));
   CHECK(IsPropertyCell(heap_object));
 }
@@ -12654,7 +12654,7 @@ TEST(CallHandlerAsFunctionHasNoSideEffectNotSupported) {
             .IsEmpty());
 
   // Side-effect-free version is not supported.
-  i::FunctionTemplateInfo cons = i::FunctionTemplateInfo::cast(
+  i::Tagged<i::FunctionTemplateInfo> cons = i::FunctionTemplateInfo::cast(
       v8::Utils::OpenHandle(*templ)->constructor());
   i::Heap* heap = reinterpret_cast<i::Isolate*>(isolate)->heap();
   i::CallHandlerInfo handler_info =
@@ -14859,7 +14859,8 @@ static void MorphAString(i::String string,
     CHECK(string->map() == roots.external_one_byte_string_map());
     // Morph external string to be TwoByte string.
     string->set_map(roots.external_two_byte_string_map());
-    i::ExternalTwoByteString morphed = i::ExternalTwoByteString::cast(string);
+    i::Tagged<i::ExternalTwoByteString> morphed =
+        i::ExternalTwoByteString::cast(string);
     CcTest::heap()->UpdateExternalString(morphed, string->length(), 0);
     morphed->SetResource(isolate, uc16_resource);
   } else {
@@ -14867,7 +14868,8 @@ static void MorphAString(i::String string,
     CHECK(string->map() == roots.external_two_byte_string_map());
     // Morph external string to be one-byte string.
     string->set_map(roots.external_one_byte_string_map());
-    i::ExternalOneByteString morphed = i::ExternalOneByteString::cast(string);
+    i::Tagged<i::ExternalOneByteString> morphed =
+        i::ExternalOneByteString::cast(string);
     CcTest::heap()->UpdateExternalString(morphed, string->length(), 0);
     morphed->SetResource(isolate, one_byte_resource);
   }
diff --git a/test/cctest/test-code-stub-assembler.cc b/test/cctest/test-code-stub-assembler.cc
index a54527f1f82..7708e77d2da 100644
--- a/test/cctest/test-code-stub-assembler.cc
+++ b/test/cctest/test-code-stub-assembler.cc
@@ -502,7 +502,7 @@ TEST(ComputeIntegerHash) {
     Handle<Object> result = ft.Call(key).ToHandleChecked();
 
     uint32_t hash = ComputeSeededHash(k, HashSeed(isolate));
-    Smi expected = Smi::FromInt(hash);
+    Tagged<Smi> expected = Smi::FromInt(hash);
     CHECK_EQ(expected, Smi::cast(*result));
   }
 }
@@ -1946,9 +1946,9 @@ TEST(AllocationFoldingCSA) {
     } else {
       CHECK(IsAligned(result->address(), kTaggedSize));
     }
-    ByteArray prev_array;
+    Tagged<ByteArray> prev_array;
     for (int i = 1; i <= kNumArrays; ++i) {
-      ByteArray current_array = ByteArray::cast(result->get(i - 1));
+      Tagged<ByteArray> current_array = ByteArray::cast(result->get(i - 1));
       if (V8_COMPRESS_POINTERS_8GB_BOOL) {
         CHECK(IsAligned(current_array.address(), kObjectAlignment8GbHeap));
       } else {
@@ -2648,8 +2648,9 @@ class AppendJSArrayCodeStubAssembler : public CodeStubAssembler {
     CHECK_EQ(result_size < 6 ? *undefined_value : *o4, *obj);
   }
 
-  static void TestAppendJSArray(Isolate* isolate, ElementsKind kind, Object o1,
-                                Object o2, Object o3, Object o4,
+  static void TestAppendJSArray(Isolate* isolate, ElementsKind kind,
+                                Tagged<Object> o1, Tagged<Object> o2,
+                                Tagged<Object> o3, Tagged<Object> o4,
                                 int initial_size, int result_size) {
     CodeAssemblerTester asm_tester(isolate, i::JSParameterCount(kNumParams));
     AppendJSArrayCodeStubAssembler m(asm_tester.state(), kind);
@@ -3574,7 +3575,7 @@ TEST(CloneEmptyFixedArray) {
 
   Handle<FixedArray> source(isolate->factory()->empty_fixed_array());
   Handle<Object> result_raw = ft.Call(source).ToHandleChecked();
-  FixedArray result(FixedArray::cast(*result_raw));
+  Tagged<FixedArray> result(FixedArray::cast(*result_raw));
   CHECK_EQ(0, result->length());
   CHECK_EQ(*(isolate->factory()->empty_fixed_array()), result);
 }
@@ -3592,7 +3593,7 @@ TEST(CloneFixedArray) {
   Handle<FixedArray> source(isolate->factory()->NewFixedArrayWithHoles(5));
   source->set(1, Smi::FromInt(1234));
   Handle<Object> result_raw = ft.Call(source).ToHandleChecked();
-  FixedArray result(FixedArray::cast(*result_raw));
+  Tagged<FixedArray> result(FixedArray::cast(*result_raw));
   CHECK_EQ(5, result->length());
   CHECK(IsTheHole(result->get(0), isolate));
   CHECK_EQ(Smi::cast(result->get(1)).value(), 1234);
@@ -3615,7 +3616,7 @@ TEST(CloneFixedArrayCOW) {
   source->set(1, Smi::FromInt(1234));
   source->set_map(ReadOnlyRoots(isolate).fixed_cow_array_map());
   Handle<Object> result_raw = ft.Call(source).ToHandleChecked();
-  FixedArray result(FixedArray::cast(*result_raw));
+  Tagged<FixedArray> result(FixedArray::cast(*result_raw));
   CHECK_EQ(*source, result);
 }
 
@@ -3639,7 +3640,7 @@ TEST(ExtractFixedArrayCOWForceCopy) {
   source->set(1, Smi::FromInt(1234));
   source->set_map(ReadOnlyRoots(isolate).fixed_cow_array_map());
   Handle<Object> result_raw = ft.Call(source).ToHandleChecked();
-  FixedArray result(FixedArray::cast(*result_raw));
+  Tagged<FixedArray> result(FixedArray::cast(*result_raw));
   CHECK_NE(*source, result);
   CHECK_EQ(5, result->length());
   CHECK(IsTheHole(result->get(0), isolate));
@@ -3672,7 +3673,7 @@ TEST(ExtractFixedArraySimple) {
       ft.Call(source, Handle<Smi>(Smi::FromInt(1), isolate),
               Handle<Smi>(Smi::FromInt(2), isolate))
           .ToHandleChecked();
-  FixedArray result(FixedArray::cast(*result_raw));
+  Tagged<FixedArray> result(FixedArray::cast(*result_raw));
   CHECK_EQ(2, result->length());
   CHECK_EQ(Smi::cast(result->get(0)).value(), 1234);
   CHECK(IsTheHole(result->get(1), isolate));
@@ -3698,7 +3699,7 @@ TEST(ExtractFixedArraySimpleSmiConstant) {
   Handle<FixedArray> source(isolate->factory()->NewFixedArrayWithHoles(5));
   source->set(1, Smi::FromInt(1234));
   Handle<Object> result_raw = ft.Call(source).ToHandleChecked();
-  FixedArray result(FixedArray::cast(*result_raw));
+  Tagged<FixedArray> result(FixedArray::cast(*result_raw));
   CHECK_EQ(2, result->length());
   CHECK_EQ(Smi::cast(result->get(0)).value(), 1234);
   CHECK(IsTheHole(result->get(1), isolate));
@@ -3724,7 +3725,7 @@ TEST(ExtractFixedArraySimpleIntPtrConstant) {
   Handle<FixedArray> source(isolate->factory()->NewFixedArrayWithHoles(5));
   source->set(1, Smi::FromInt(1234));
   Handle<Object> result_raw = ft.Call(source).ToHandleChecked();
-  FixedArray result(FixedArray::cast(*result_raw));
+  Tagged<FixedArray> result(FixedArray::cast(*result_raw));
   CHECK_EQ(2, result->length());
   CHECK_EQ(Smi::cast(result->get(0)).value(), 1234);
   CHECK(IsTheHole(result->get(1), isolate));
@@ -3748,7 +3749,7 @@ TEST(ExtractFixedArraySimpleIntPtrConstantNoDoubles) {
   Handle<FixedArray> source(isolate->factory()->NewFixedArrayWithHoles(5));
   source->set(1, Smi::FromInt(1234));
   Handle<Object> result_raw = ft.Call(source).ToHandleChecked();
-  FixedArray result(FixedArray::cast(*result_raw));
+  Tagged<FixedArray> result(FixedArray::cast(*result_raw));
   CHECK_EQ(2, result->length());
   CHECK_EQ(Smi::cast(result->get(0)).value(), 1234);
   CHECK(IsTheHole(result->get(1), isolate));
@@ -3773,7 +3774,7 @@ TEST(ExtractFixedArraySimpleIntPtrParameters) {
       ft.Call(source, Handle<Smi>(Smi::FromInt(1), isolate),
               Handle<Smi>(Smi::FromInt(2), isolate))
           .ToHandleChecked();
-  FixedArray result(FixedArray::cast(*result_raw));
+  Tagged<FixedArray> result(FixedArray::cast(*result_raw));
   CHECK_EQ(2, result->length());
   CHECK_EQ(Smi::cast(result->get(0)).value(), 1234);
   CHECK(IsTheHole(result->get(1), isolate));
@@ -3789,7 +3790,8 @@ TEST(ExtractFixedArraySimpleIntPtrParameters) {
       ft.Call(source_double, Handle<Smi>(Smi::FromInt(1), isolate),
               Handle<Smi>(Smi::FromInt(2), isolate))
           .ToHandleChecked();
-  FixedDoubleArray double_result = FixedDoubleArray::cast(*double_result_raw);
+  Tagged<FixedDoubleArray> double_result =
+      FixedDoubleArray::cast(*double_result_raw);
   CHECK_EQ(2, double_result->length());
   CHECK_EQ(double_result->get_scalar(0), 11);
   CHECK_EQ(double_result->get_scalar(1), 12);
diff --git a/test/cctest/test-debug.cc b/test/cctest/test-debug.cc
index 53393dc8680..d3cecd7e670 100644
--- a/test/cctest/test-debug.cc
+++ b/test/cctest/test-debug.cc
@@ -162,7 +162,7 @@ void CheckDebuggerUnloaded() {
 
   // Iterate the heap and check that there are no debugger related objects left.
   HeapObjectIterator iterator(CcTest::heap());
-  for (HeapObject obj = iterator.Next(); !obj.is_null();
+  for (Tagged<HeapObject> obj = iterator.Next(); !obj.is_null();
        obj = iterator.Next()) {
     CHECK(!IsDebugInfo(obj));
   }
@@ -3352,7 +3352,7 @@ TEST(DebugScriptLineEndsAreAscending) {
         v8::internal::Script::cast(instances->get(i)), CcTest::i_isolate());
 
     v8::internal::Script::InitLineEnds(CcTest::i_isolate(), new_script);
-    v8::internal::FixedArray ends =
+    v8::internal::Tagged<v8::internal::FixedArray> ends =
         v8::internal::FixedArray::cast(new_script->line_ends());
     CHECK_GT(ends->length(), 0);
 
@@ -4601,10 +4601,10 @@ TEST(DebugEvaluateNoSideEffect) {
   std::vector<i::Handle<i::JSFunction>> all_functions;
   {
     i::HeapObjectIterator iterator(isolate->heap());
-    for (i::HeapObject obj = iterator.Next(); !obj.is_null();
+    for (i::Tagged<i::HeapObject> obj = iterator.Next(); !obj.is_null();
          obj = iterator.Next()) {
       if (!IsJSFunction(obj)) continue;
-      i::JSFunction fun = i::JSFunction::cast(obj);
+      i::Tagged<i::JSFunction> fun = i::JSFunction::cast(obj);
       all_functions.emplace_back(fun, isolate);
     }
   }
@@ -4696,7 +4696,7 @@ UNINITIALIZED_TEST(LoadedAtStartupScripts) {
     {
       i::DisallowGarbageCollection no_gc;
       i::Script::Iterator iterator(i_isolate);
-      for (i::Script script = iterator.Next(); !script.is_null();
+      for (i::Tagged<i::Script> script = iterator.Next(); !script.is_null();
            script = iterator.Next()) {
         if (script->type() == i::Script::Type::kNative &&
             IsUndefined(script->name(), i_isolate)) {
diff --git a/test/cctest/test-descriptor-array.cc b/test/cctest/test-descriptor-array.cc
index e331e8207cb..d8976a700f4 100644
--- a/test/cctest/test-descriptor-array.cc
+++ b/test/cctest/test-descriptor-array.cc
@@ -51,12 +51,12 @@ void CheckDescriptorArrayLookups(Isolate* isolate, Handle<Map> map,
   // Test C++ implementation.
   {
     DisallowGarbageCollection no_gc;
-    DescriptorArray descriptors = map->instance_descriptors(isolate);
+    Tagged<DescriptorArray> descriptors = map->instance_descriptors(isolate);
     DCHECK(descriptors->IsSortedNoDuplicates());
     int nof_descriptors = descriptors->number_of_descriptors();
 
     for (size_t i = 0; i < names.size(); ++i) {
-      Name name = *names[i];
+      Tagged<Name> name = *names[i];
       InternalIndex index = descriptors->Search(name, nof_descriptors, false);
       CHECK(index.is_found());
       CHECK_EQ(i, index.as_uint32());
@@ -85,12 +85,12 @@ void CheckTransitionArrayLookups(Isolate* isolate,
     DCHECK(transitions->IsSortedNoDuplicates());
 
     for (size_t i = 0; i < maps.size(); ++i) {
-      Map expected_map = *maps[i];
-      Name name = expected_map->instance_descriptors(isolate)->GetKey(
+      Tagged<Map> expected_map = *maps[i];
+      Tagged<Name> name = expected_map->instance_descriptors(isolate)->GetKey(
           expected_map->LastAdded());
 
-      Map map = transitions->SearchAndGetTargetForTesting(PropertyKind::kData,
-                                                          name, NONE);
+      Tagged<Map> map = transitions->SearchAndGetTargetForTesting(
+          PropertyKind::kData, name, NONE);
       CHECK(!map.is_null());
       CHECK_EQ(expected_map, map);
     }
diff --git a/test/cctest/test-field-type-tracking.cc b/test/cctest/test-field-type-tracking.cc
index 29606da0980..5cf5f6af665 100644
--- a/test/cctest/test-field-type-tracking.cc
+++ b/test/cctest/test-field-type-tracking.cc
@@ -62,8 +62,10 @@ static Handle<AccessorPair> CreateAccessorPair(bool with_getter,
 }
 
 // Check cached migration target map after Map::Update() and Map::TryUpdate()
-static void CheckMigrationTarget(Isolate* isolate, Map old_map, Map new_map) {
-  Map target = TransitionsAccessor(isolate, old_map).GetMigrationTarget();
+static void CheckMigrationTarget(Isolate* isolate, Tagged<Map> old_map,
+                                 Tagged<Map> new_map) {
+  Tagged<Map> target =
+      TransitionsAccessor(isolate, old_map).GetMigrationTarget();
   if (target.is_null()) return;
   CHECK_EQ(new_map, target);
   CHECK_EQ(MapUpdater::TryUpdateNoLock(isolate, old_map,
@@ -237,7 +239,8 @@ class Expectations {
     }
   }
 
-  bool Check(DescriptorArray descriptors, InternalIndex descriptor) const {
+  bool Check(Tagged<DescriptorArray> descriptors,
+             InternalIndex descriptor) const {
     PropertyDetails details = descriptors->GetDetails(descriptor);
 
     if (details.kind() != kinds_[descriptor.as_int()]) return false;
@@ -252,10 +255,10 @@ class Expectations {
 
     if (!details.representation().Equals(expected_representation)) return false;
 
-    Object expected_value = *values_[descriptor.as_int()];
+    Tagged<Object> expected_value = *values_[descriptor.as_int()];
     if (details.location() == PropertyLocation::kField) {
       if (details.kind() == PropertyKind::kData) {
-        FieldType type = descriptors->GetFieldType(descriptor);
+        Tagged<FieldType> type = descriptors->GetFieldType(descriptor);
         return FieldType::cast(expected_value) == type;
       } else {
         // kAccessor
@@ -263,22 +266,22 @@ class Expectations {
       }
     } else {
       CHECK_EQ(PropertyKind::kAccessor, details.kind());
-      Object value = descriptors->GetStrongValue(descriptor);
+      Tagged<Object> value = descriptors->GetStrongValue(descriptor);
       if (value == expected_value) return true;
       if (!IsAccessorPair(value)) return false;
-      AccessorPair pair = AccessorPair::cast(value);
+      Tagged<AccessorPair> pair = AccessorPair::cast(value);
       return pair->Equals(expected_value, *setter_values_[descriptor.as_int()]);
     }
     UNREACHABLE();
   }
 
-  bool Check(Map map, int expected_nof) const {
+  bool Check(Tagged<Map> map, int expected_nof) const {
     CHECK_EQ(elements_kind_, map->elements_kind());
     CHECK(number_of_properties_ <= MAX_PROPERTIES);
     CHECK_EQ(expected_nof, map->NumberOfOwnDescriptors());
     CHECK(!map->is_dictionary_map());
 
-    DescriptorArray descriptors = map->instance_descriptors();
+    Tagged<DescriptorArray> descriptors = map->instance_descriptors();
     CHECK(expected_nof <= number_of_properties_);
     for (InternalIndex i : InternalIndex::Range(expected_nof)) {
       if (!Check(descriptors, i)) {
@@ -292,9 +295,11 @@ class Expectations {
     return true;
   }
 
-  bool Check(Map map) const { return Check(map, number_of_properties_); }
+  bool Check(Tagged<Map> map) const {
+    return Check(map, number_of_properties_);
+  }
 
-  bool CheckNormalized(Map map) const {
+  bool CheckNormalized(Tagged<Map> map) const {
     CHECK(map->is_dictionary_map());
     CHECK_EQ(elements_kind_, map->elements_kind());
     // TODO(leszeks): Iterate over the key/value pairs of the map and compare
@@ -584,7 +589,7 @@ TEST(ReconfigureAccessorToNonExistingDataFieldHeavy) {
   // Check that the property contains |value|.
   CHECK_EQ(1, obj->map()->NumberOfOwnDescriptors());
   FieldIndex index = FieldIndex::ForDescriptor(obj->map(), first);
-  Object the_value = obj->RawFastPropertyAt(index);
+  Tagged<Object> the_value = obj->RawFastPropertyAt(index);
   CHECK(IsSmi(the_value));
   CHECK_EQ(42, Smi::ToInt(the_value));
 }
@@ -753,9 +758,9 @@ void TestGeneralizeField(int detach_property_at_index, int property_index,
 
   {
     // Check that all previous maps are not stable.
-    Map tmp = *new_map;
+    Tagged<Map> tmp = *new_map;
     while (true) {
-      Object back = tmp->GetBackPointer();
+      Tagged<Object> back = tmp->GetBackPointer();
       if (IsUndefined(back, isolate)) break;
       tmp = Map::cast(back);
       CHECK(!tmp->is_stable());
@@ -1831,7 +1836,7 @@ static void TestReconfigureElementsKind_GeneralizeFieldInPlace(
   {
     MapHandles map_list;
     map_list.push_back(updated_map);
-    Map transitioned_map = map2->FindElementsKindTransitionedMap(
+    Tagged<Map> transitioned_map = map2->FindElementsKindTransitionedMap(
         isolate, map_list, ConcurrencyMode::kSynchronous);
     CHECK_EQ(*updated_map, transitioned_map);
   }
diff --git a/test/cctest/test-inobject-slack-tracking.cc b/test/cctest/test-inobject-slack-tracking.cc
index 897002ff366..a01e03bba96 100644
--- a/test/cctest/test-inobject-slack-tracking.cc
+++ b/test/cctest/test-inobject-slack-tracking.cc
@@ -72,13 +72,14 @@ static inline Handle<T> CompileRunI(const char* script) {
   return OpenHandle<T>(CompileRun(script));
 }
 
-static Object GetFieldValue(JSObject obj, int property_index) {
+static Tagged<Object> GetFieldValue(Tagged<JSObject> obj, int property_index) {
   FieldIndex index = FieldIndex::ForPropertyIndex(obj->map(), property_index);
   return obj->RawFastPropertyAt(index);
 }
 
-static double GetDoubleFieldValue(JSObject obj, FieldIndex field_index) {
-  Object value = obj->RawFastPropertyAt(field_index);
+static double GetDoubleFieldValue(Tagged<JSObject> obj,
+                                  FieldIndex field_index) {
+  Tagged<Object> value = obj->RawFastPropertyAt(field_index);
   if (IsHeapNumber(value)) {
     return HeapNumber::cast(value)->value();
   } else {
@@ -86,12 +87,12 @@ static double GetDoubleFieldValue(JSObject obj, FieldIndex field_index) {
   }
 }
 
-static double GetDoubleFieldValue(JSObject obj, int property_index) {
+static double GetDoubleFieldValue(Tagged<JSObject> obj, int property_index) {
   FieldIndex index = FieldIndex::ForPropertyIndex(obj->map(), property_index);
   return GetDoubleFieldValue(obj, index);
 }
 
-bool IsObjectShrinkable(JSObject obj) {
+bool IsObjectShrinkable(Tagged<JSObject> obj) {
   Handle<Map> filler_map =
       CcTest::i_isolate()->factory()->one_pointer_filler_map();
 
diff --git a/test/cctest/test-js-weak-refs.cc b/test/cctest/test-js-weak-refs.cc
index 5247242be80..b7968ca4061 100644
--- a/test/cctest/test-js-weak-refs.cc
+++ b/test/cctest/test-js-weak-refs.cc
@@ -105,14 +105,15 @@ Handle<WeakCell> FinalizationRegistryRegister(
 }
 
 void NullifyWeakCell(Handle<WeakCell> weak_cell, Isolate* isolate) {
-  auto empty_func = [](HeapObject object, ObjectSlot slot, Object target) {};
+  auto empty_func = [](Tagged<HeapObject> object, ObjectSlot slot,
+                       Tagged<Object> target) {};
   weak_cell->Nullify(isolate, empty_func);
 #ifdef VERIFY_HEAP
   weak_cell->WeakCellVerify(isolate);
 #endif  // VERIFY_HEAP
 }
 
-Object PopClearedCellHoldings(
+Tagged<Object> PopClearedCellHoldings(
     Handle<JSFinalizationRegistry> finalization_registry, Isolate* isolate) {
   // PopClearedCell is implemented in Torque. Reproduce that implementation here
   // for testing.
@@ -123,7 +124,7 @@ Object PopClearedCellHoldings(
   weak_cell->set_next(ReadOnlyRoots(isolate).undefined_value());
 
   if (IsWeakCell(finalization_registry->cleared_cells())) {
-    WeakCell cleared_cells_head =
+    Tagged<WeakCell> cleared_cells_head =
         WeakCell::cast(finalization_registry->cleared_cells());
     DCHECK_EQ(cleared_cells_head->prev(), *weak_cell);
     cleared_cells_head->set_prev(ReadOnlyRoots(isolate).undefined_value());
@@ -141,7 +142,8 @@ Object PopClearedCellHoldings(
 
 // Usage: VerifyWeakCellChain(isolate, list_head, n, cell1, cell2, ..., celln);
 // verifies that list_head == cell1 and cell1, cell2, ..., celln. form a list.
-void VerifyWeakCellChain(Isolate* isolate, Object list_head, int n_args, ...) {
+void VerifyWeakCellChain(Isolate* isolate, Tagged<Object> list_head, int n_args,
+                         ...) {
   CHECK_GE(n_args, 0);
 
   va_list args;
@@ -151,12 +153,12 @@ void VerifyWeakCellChain(Isolate* isolate, Object list_head, int n_args, ...) {
     // Verify empty list
     CHECK(IsUndefined(list_head, isolate));
   } else {
-    WeakCell current = WeakCell::cast(Object(va_arg(args, Address)));
+    Tagged<WeakCell> current = WeakCell::cast(Object(va_arg(args, Address)));
     CHECK_EQ(current, list_head);
     CHECK(IsUndefined(current->prev(), isolate));
 
     for (int i = 1; i < n_args; i++) {
-      WeakCell next = WeakCell::cast(Object(va_arg(args, Address)));
+      Tagged<WeakCell> next = WeakCell::cast(Object(va_arg(args, Address)));
       CHECK_EQ(current->next(), next);
       CHECK_EQ(next->prev(), current);
       current = next;
@@ -168,14 +170,15 @@ void VerifyWeakCellChain(Isolate* isolate, Object list_head, int n_args, ...) {
 
 // Like VerifyWeakCellChain but verifies the chain created with key_list_prev
 // and key_list_next instead of prev and next.
-void VerifyWeakCellKeyChain(Isolate* isolate, SimpleNumberDictionary key_map,
-                            Object unregister_token, int n_args, ...) {
+void VerifyWeakCellKeyChain(Isolate* isolate,
+                            Tagged<SimpleNumberDictionary> key_map,
+                            Tagged<Object> unregister_token, int n_args, ...) {
   CHECK_GE(n_args, 0);
 
   va_list args;
   va_start(args, n_args);
 
-  Object hash = Object::GetHash(unregister_token);
+  Tagged<Object> hash = Object::GetHash(unregister_token);
   InternalIndex entry = InternalIndex::NotFound();
   if (!IsUndefined(hash, isolate)) {
     uint32_t key = Smi::ToInt(hash);
@@ -186,13 +189,13 @@ void VerifyWeakCellKeyChain(Isolate* isolate, SimpleNumberDictionary key_map,
     CHECK(entry.is_not_found());
   } else {
     CHECK(entry.is_found());
-    WeakCell current = WeakCell::cast(Object(va_arg(args, Address)));
-    Object list_head = key_map->ValueAt(entry);
+    Tagged<WeakCell> current = WeakCell::cast(Object(va_arg(args, Address)));
+    Tagged<Object> list_head = key_map->ValueAt(entry);
     CHECK_EQ(current, list_head);
     CHECK(IsUndefined(current->key_list_prev(), isolate));
 
     for (int i = 1; i < n_args; i++) {
-      WeakCell next = WeakCell::cast(Object(va_arg(args, Address)));
+      Tagged<WeakCell> next = WeakCell::cast(Object(va_arg(args, Address)));
       CHECK_EQ(current->key_list_next(), next);
       CHECK_EQ(next->key_list_prev(), current);
       current = next;
@@ -272,7 +275,7 @@ TEST(TestRegisterWithKey) {
       finalization_registry, js_object, undefined, token1, isolate);
 
   {
-    SimpleNumberDictionary key_map =
+    Tagged<SimpleNumberDictionary> key_map =
         SimpleNumberDictionary::cast(finalization_registry->key_map());
     VerifyWeakCellKeyChain(isolate, key_map, *token1, 1, *weak_cell1);
     VerifyWeakCellKeyChain(isolate, key_map, *token2, 0);
@@ -284,7 +287,7 @@ TEST(TestRegisterWithKey) {
       finalization_registry, js_object, undefined, token2, isolate);
 
   {
-    SimpleNumberDictionary key_map =
+    Tagged<SimpleNumberDictionary> key_map =
         SimpleNumberDictionary::cast(finalization_registry->key_map());
     VerifyWeakCellKeyChain(isolate, key_map, *token1, 1, *weak_cell1);
     VerifyWeakCellKeyChain(isolate, key_map, *token2, 1, *weak_cell2);
@@ -296,7 +299,7 @@ TEST(TestRegisterWithKey) {
       finalization_registry, js_object, undefined, token1, isolate);
 
   {
-    SimpleNumberDictionary key_map =
+    Tagged<SimpleNumberDictionary> key_map =
         SimpleNumberDictionary::cast(finalization_registry->key_map());
     VerifyWeakCellKeyChain(isolate, key_map, *token1, 2, *weak_cell3,
                            *weak_cell1);
@@ -398,13 +401,15 @@ TEST(TestJSFinalizationRegistryPopClearedCellHoldings1) {
   NullifyWeakCell(weak_cell3, isolate);
 
   CHECK(finalization_registry->NeedsCleanup());
-  Object cleared1 = PopClearedCellHoldings(finalization_registry, isolate);
+  Tagged<Object> cleared1 =
+      PopClearedCellHoldings(finalization_registry, isolate);
   CHECK_EQ(cleared1, *holdings3);
   CHECK(IsUndefined(weak_cell3->prev(), isolate));
   CHECK(IsUndefined(weak_cell3->next(), isolate));
 
   CHECK(finalization_registry->NeedsCleanup());
-  Object cleared2 = PopClearedCellHoldings(finalization_registry, isolate);
+  Tagged<Object> cleared2 =
+      PopClearedCellHoldings(finalization_registry, isolate);
   CHECK_EQ(cleared2, *holdings2);
   CHECK(IsUndefined(weak_cell2->prev(), isolate));
   CHECK(IsUndefined(weak_cell2->next(), isolate));
@@ -414,7 +419,8 @@ TEST(TestJSFinalizationRegistryPopClearedCellHoldings1) {
   NullifyWeakCell(weak_cell1, isolate);
 
   CHECK(finalization_registry->NeedsCleanup());
-  Object cleared3 = PopClearedCellHoldings(finalization_registry, isolate);
+  Tagged<Object> cleared3 =
+      PopClearedCellHoldings(finalization_registry, isolate);
   CHECK_EQ(cleared3, *holdings1);
   CHECK(IsUndefined(weak_cell1->prev(), isolate));
   CHECK(IsUndefined(weak_cell1->next(), isolate));
@@ -451,26 +457,28 @@ TEST(TestJSFinalizationRegistryPopClearedCellHoldings2) {
   // Nullifying doesn't affect the key chains (just moves WeakCells from
   // active_cells to cleared_cells).
   {
-    SimpleNumberDictionary key_map =
+    Tagged<SimpleNumberDictionary> key_map =
         SimpleNumberDictionary::cast(finalization_registry->key_map());
     VerifyWeakCellKeyChain(isolate, key_map, *token1, 2, *weak_cell2,
                            *weak_cell1);
   }
 
-  Object cleared1 = PopClearedCellHoldings(finalization_registry, isolate);
+  Tagged<Object> cleared1 =
+      PopClearedCellHoldings(finalization_registry, isolate);
   CHECK_EQ(cleared1, *holdings2);
 
   {
-    SimpleNumberDictionary key_map =
+    Tagged<SimpleNumberDictionary> key_map =
         SimpleNumberDictionary::cast(finalization_registry->key_map());
     VerifyWeakCellKeyChain(isolate, key_map, *token1, 1, *weak_cell1);
   }
 
-  Object cleared2 = PopClearedCellHoldings(finalization_registry, isolate);
+  Tagged<Object> cleared2 =
+      PopClearedCellHoldings(finalization_registry, isolate);
   CHECK_EQ(cleared2, *holdings1);
 
   {
-    SimpleNumberDictionary key_map =
+    Tagged<SimpleNumberDictionary> key_map =
         SimpleNumberDictionary::cast(finalization_registry->key_map());
     VerifyWeakCellKeyChain(isolate, key_map, *token1, 0);
   }
@@ -505,7 +513,7 @@ TEST(TestUnregisterActiveCells) {
                       *weak_cell2b, *weak_cell2a, *weak_cell1b, *weak_cell1a);
   VerifyWeakCellChain(isolate, finalization_registry->cleared_cells(), 0);
   {
-    SimpleNumberDictionary key_map =
+    Tagged<SimpleNumberDictionary> key_map =
         SimpleNumberDictionary::cast(finalization_registry->key_map());
     VerifyWeakCellKeyChain(isolate, key_map, *token1, 2, *weak_cell1b,
                            *weak_cell1a);
@@ -515,7 +523,7 @@ TEST(TestUnregisterActiveCells) {
 
   JSFinalizationRegistry::Unregister(finalization_registry, token1, isolate);
   {
-    SimpleNumberDictionary key_map =
+    Tagged<SimpleNumberDictionary> key_map =
         SimpleNumberDictionary::cast(finalization_registry->key_map());
     VerifyWeakCellKeyChain(isolate, key_map, *token1, 0);
     VerifyWeakCellKeyChain(isolate, key_map, *token2, 2, *weak_cell2b,
@@ -560,7 +568,7 @@ TEST(TestUnregisterActiveAndClearedCells) {
   VerifyWeakCellChain(isolate, finalization_registry->cleared_cells(), 1,
                       *weak_cell2a);
   {
-    SimpleNumberDictionary key_map =
+    Tagged<SimpleNumberDictionary> key_map =
         SimpleNumberDictionary::cast(finalization_registry->key_map());
     VerifyWeakCellKeyChain(isolate, key_map, *token1, 2, *weak_cell1b,
                            *weak_cell1a);
@@ -575,7 +583,7 @@ TEST(TestUnregisterActiveAndClearedCells) {
                       *weak_cell1b, *weak_cell1a);
   VerifyWeakCellChain(isolate, finalization_registry->cleared_cells(), 0);
   {
-    SimpleNumberDictionary key_map =
+    Tagged<SimpleNumberDictionary> key_map =
         SimpleNumberDictionary::cast(finalization_registry->key_map());
     VerifyWeakCellKeyChain(isolate, key_map, *token1, 2, *weak_cell1b,
                            *weak_cell1a);
@@ -604,7 +612,7 @@ TEST(TestWeakCellUnregisterTwice) {
                       *weak_cell1);
   VerifyWeakCellChain(isolate, finalization_registry->cleared_cells(), 0);
   {
-    SimpleNumberDictionary key_map =
+    Tagged<SimpleNumberDictionary> key_map =
         SimpleNumberDictionary::cast(finalization_registry->key_map());
     VerifyWeakCellKeyChain(isolate, key_map, *token1, 1, *weak_cell1);
   }
@@ -614,7 +622,7 @@ TEST(TestWeakCellUnregisterTwice) {
   VerifyWeakCellChain(isolate, finalization_registry->active_cells(), 0);
   VerifyWeakCellChain(isolate, finalization_registry->cleared_cells(), 0);
   {
-    SimpleNumberDictionary key_map =
+    Tagged<SimpleNumberDictionary> key_map =
         SimpleNumberDictionary::cast(finalization_registry->key_map());
     VerifyWeakCellKeyChain(isolate, key_map, *token1, 0);
   }
@@ -624,7 +632,7 @@ TEST(TestWeakCellUnregisterTwice) {
   VerifyWeakCellChain(isolate, finalization_registry->active_cells(), 0);
   VerifyWeakCellChain(isolate, finalization_registry->cleared_cells(), 0);
   {
-    SimpleNumberDictionary key_map =
+    Tagged<SimpleNumberDictionary> key_map =
         SimpleNumberDictionary::cast(finalization_registry->key_map());
     VerifyWeakCellKeyChain(isolate, key_map, *token1, 0);
   }
@@ -648,13 +656,14 @@ TEST(TestWeakCellUnregisterPopped) {
   NullifyWeakCell(weak_cell1, isolate);
 
   CHECK(finalization_registry->NeedsCleanup());
-  Object cleared1 = PopClearedCellHoldings(finalization_registry, isolate);
+  Tagged<Object> cleared1 =
+      PopClearedCellHoldings(finalization_registry, isolate);
   CHECK_EQ(cleared1, *holdings1);
 
   VerifyWeakCellChain(isolate, finalization_registry->active_cells(), 0);
   VerifyWeakCellChain(isolate, finalization_registry->cleared_cells(), 0);
   {
-    SimpleNumberDictionary key_map =
+    Tagged<SimpleNumberDictionary> key_map =
         SimpleNumberDictionary::cast(finalization_registry->key_map());
     VerifyWeakCellKeyChain(isolate, key_map, *token1, 0);
   }
@@ -664,7 +673,7 @@ TEST(TestWeakCellUnregisterPopped) {
   VerifyWeakCellChain(isolate, finalization_registry->active_cells(), 0);
   VerifyWeakCellChain(isolate, finalization_registry->cleared_cells(), 0);
   {
-    SimpleNumberDictionary key_map =
+    Tagged<SimpleNumberDictionary> key_map =
         SimpleNumberDictionary::cast(finalization_registry->key_map());
     VerifyWeakCellKeyChain(isolate, key_map, *token1, 0);
   }
@@ -849,7 +858,7 @@ TEST(TestRemoveUnregisterToken) {
   VerifyWeakCellChain(isolate, finalization_registry->cleared_cells(), 1,
                       *weak_cell2a);
   {
-    SimpleNumberDictionary key_map =
+    Tagged<SimpleNumberDictionary> key_map =
         SimpleNumberDictionary::cast(finalization_registry->key_map());
     VerifyWeakCellKeyChain(isolate, key_map, *token1, 2, *weak_cell1b,
                            *weak_cell1a);
@@ -860,7 +869,7 @@ TEST(TestRemoveUnregisterToken) {
   finalization_registry->RemoveUnregisterToken(
       JSReceiver::cast(*token2), isolate,
       JSFinalizationRegistry::kKeepMatchedCellsInRegistry,
-      [](HeapObject, ObjectSlot, Object) {});
+      [](Tagged<HeapObject>, ObjectSlot, Tagged<Object>) {});
 
   // Both weak_cell2a and weak_cell2b remain on the weak cell chains.
   VerifyWeakCellChain(isolate, finalization_registry->active_cells(), 3,
@@ -870,7 +879,7 @@ TEST(TestRemoveUnregisterToken) {
 
   // But both weak_cell2a and weak_cell2b are removed from the key chain.
   {
-    SimpleNumberDictionary key_map =
+    Tagged<SimpleNumberDictionary> key_map =
         SimpleNumberDictionary::cast(finalization_registry->key_map());
     VerifyWeakCellKeyChain(isolate, key_map, *token1, 2, *weak_cell1b,
                            *weak_cell1a);
diff --git a/test/cctest/test-log-stack-tracer.cc b/test/cctest/test-log-stack-tracer.cc
index ac7b8d41203..824f42178b8 100644
--- a/test/cctest/test-log-stack-tracer.cc
+++ b/test/cctest/test-log-stack-tracer.cc
@@ -43,9 +43,9 @@
 namespace v8 {
 namespace internal {
 
-static bool IsAddressWithinFuncCode(JSFunction function, Isolate* isolate,
-                                    void* addr) {
-  i::AbstractCode code = function->abstract_code(isolate);
+static bool IsAddressWithinFuncCode(Tagged<JSFunction> function,
+                                    Isolate* isolate, void* addr) {
+  i::Tagged<i::AbstractCode> code = function->abstract_code(isolate);
   return code->contains(isolate, reinterpret_cast<Address>(addr));
 }
 
@@ -55,7 +55,7 @@ static bool IsAddressWithinFuncCode(v8::Local<v8::Context> context,
   v8::Local<v8::Value> func =
       context->Global()->Get(context, v8_str(func_name)).ToLocalChecked();
   CHECK(func->IsFunction());
-  JSFunction js_func = JSFunction::cast(*v8::Utils::OpenHandle(*func));
+  Tagged<JSFunction> js_func = JSFunction::cast(*v8::Utils::OpenHandle(*func));
   return IsAddressWithinFuncCode(js_func, isolate, addr);
 }
 
diff --git a/test/cctest/test-mementos.cc b/test/cctest/test-mementos.cc
index 46041195085..e64f5239459 100644
--- a/test/cctest/test-mementos.cc
+++ b/test/cctest/test-mementos.cc
@@ -50,7 +50,7 @@ static void SetUpNewSpaceWithPoisonedMementoAtTop() {
 
   // Create an allocation memento behind the string with a garbage allocation
   // site pointer.
-  AllocationMemento memento = AllocationMemento::unchecked_cast(
+  Tagged<AllocationMemento> memento = AllocationMemento::unchecked_cast(
       Object(new_space->top() + kHeapObjectTag));
   memento->set_map_after_allocation(
       ReadOnlyRoots(heap).allocation_memento_map(), SKIP_WRITE_BARRIER);
diff --git a/test/cctest/test-orderedhashtable.cc b/test/cctest/test-orderedhashtable.cc
index b7fba5f87e5..dbaf97eb225 100644
--- a/test/cctest/test-orderedhashtable.cc
+++ b/test/cctest/test-orderedhashtable.cc
@@ -59,19 +59,20 @@ Handle<OrderedNameDictionary> Add(Isolate* isolate,
 // version for
 // OrderedHashMap, OrderedHashSet
 template <typename T>
-bool HasKey(Isolate* isolate, Handle<T> table, Object key) {
+bool HasKey(Isolate* isolate, Handle<T> table, Tagged<Object> key) {
   return T::HasKey(isolate, *table, key);
 }
 
 template <>
-bool HasKey(Isolate* isolate, Handle<OrderedNameDictionary> table, Object key) {
+bool HasKey(Isolate* isolate, Handle<OrderedNameDictionary> table,
+            Tagged<Object> key) {
   return table->FindEntry(isolate, key).is_found();
 }
 
 // version for
 // OrderedHashTable, OrderedHashSet
 template <typename T>
-Handle<T> Delete(Isolate* isolate, Handle<T> table, Object key) {
+Handle<T> Delete(Isolate* isolate, Handle<T> table, Tagged<Object> key) {
   T::Delete(isolate, *table, key);
   return table;
 }
@@ -79,7 +80,7 @@ Handle<T> Delete(Isolate* isolate, Handle<T> table, Object key) {
 template <>
 Handle<OrderedNameDictionary> Delete(Isolate* isolate,
                                      Handle<OrderedNameDictionary> table,
-                                     Object key) {
+                                     Tagged<Object> key) {
   // OrderedNameDictionary doesn't have Delete, but only DeleteEntry, which
   // requires the key to be deleted to be present
   InternalIndex entry = table->FindEntry(isolate, key);
@@ -298,8 +299,8 @@ TEST(SmallOrderedHashMapDuplicateHashCode) {
   CopyHashCode(key1, key2);
 
   CHECK(!Object::SameValue(*key1, *key2));
-  Object hash1 = Object::GetHash(*key1);
-  Object hash2 = Object::GetHash(*key2);
+  Tagged<Object> hash1 = Object::GetHash(*key1);
+  Tagged<Object> hash2 = Object::GetHash(*key2);
   CHECK_EQ(hash1, hash2);
 
   map = SmallOrderedHashMap::Add(isolate, map, key2, value).ToHandleChecked();
diff --git a/test/cctest/test-regexp.cc b/test/cctest/test-regexp.cc
index 76237d90454..c5da72b59d1 100644
--- a/test/cctest/test-regexp.cc
+++ b/test/cctest/test-regexp.cc
@@ -109,11 +109,12 @@ class InterruptTest {
     i::Handle<i::JSRegExp> regexp = Utils::OpenHandle(*re);
     // We executed on a two-byte subject so far, so we expect only bytecode for
     // two-byte to be present.
-    i::Object one_byte_code = regexp->bytecode(/* is_latin1 */ true);
+    i::Tagged<i::Object> one_byte_code = regexp->bytecode(/* is_latin1 */ true);
     CHECK(IsSmi(one_byte_code));
     CHECK_EQ(i::Smi::cast(one_byte_code).value(),
              i::JSRegExp::kUninitializedValue);
-    i::Object two_byte_code = regexp->bytecode(/* is_latin1 */ false);
+    i::Tagged<i::Object> two_byte_code =
+        regexp->bytecode(/* is_latin1 */ false);
     CHECK(IsByteArray(two_byte_code));
 
     // Transition the subject string to one-byte by internalizing it.
@@ -342,6 +343,6 @@ TEST(InterruptAndTransitionSubjectFromTwoByteToOneByte) {
   // After the test, we expect that bytecode for a one-byte subject has been
   // installed during the interrupt.
   i::Handle<i::JSRegExp> regexp = Utils::OpenHandle(*test.GetRegExp());
-  i::Object one_byte_code = regexp->bytecode(/* is_latin1 */ true);
+  i::Tagged<i::Object> one_byte_code = regexp->bytecode(/* is_latin1 */ true);
   CHECK(IsByteArray(one_byte_code));
 }
diff --git a/test/cctest/test-serialize.cc b/test/cctest/test-serialize.cc
index fa3d1932597..c7cf36027b2 100644
--- a/test/cctest/test-serialize.cc
+++ b/test/cctest/test-serialize.cc
@@ -4937,7 +4937,7 @@ void CheckSFIsAreWeak(WeakFixedArray sfis, Isolate* isolate) {
   int no_of_weak = 0;
   for (int i = 0; i < sfis->length(); ++i) {
     MaybeObject maybe_object = sfis->Get(i);
-    HeapObject heap_object;
+    Tagged<HeapObject> heap_object;
     CHECK(maybe_object->IsWeakOrCleared() ||
           (maybe_object->GetHeapObjectIfStrong(&heap_object) &&
            IsUndefined(heap_object, isolate)));
diff --git a/test/cctest/test-shared-strings.cc b/test/cctest/test-shared-strings.cc
index 95bd3318f8b..0364edbf721 100644
--- a/test/cctest/test-shared-strings.cc
+++ b/test/cctest/test-shared-strings.cc
@@ -477,10 +477,11 @@ class ConcurrentStringTableLookupThread final
 
   void RunForString(Handle<String> input_string, int counter) override {
     CHECK(input_string->IsShared());
-    Object result = Object(StringTable::TryStringToIndexOrLookupExisting(
-        i_isolate, input_string->ptr()));
+    Tagged<Object> result =
+        Object(StringTable::TryStringToIndexOrLookupExisting(
+            i_isolate, input_string->ptr()));
     if (IsString(result)) {
-      String internalized = String::cast(result);
+      Tagged<String> internalized = String::cast(result);
       CHECK(IsInternalizedString(internalized));
       CHECK_IMPLIES(IsInternalizedString(*input_string),
                     *input_string == internalized);
@@ -1621,8 +1622,8 @@ void CreateExternalResources(Isolate* i_isolate, Handle<FixedArray> strings,
 }
 
 void CheckStringAndResource(
-    String string, int index, bool should_be_alive, String deleted_string,
-    bool check_transition, bool shared_resources,
+    Tagged<String> string, int index, bool should_be_alive,
+    Tagged<String> deleted_string, bool check_transition, bool shared_resources,
     const std::vector<std::unique_ptr<ConcurrentExternalizationThread>>&
         threads) {
   if (check_transition) {
@@ -1729,7 +1730,7 @@ void TestConcurrentExternalization(bool share_resources) {
   for (int i = 0; i < shared_strings->length(); i++) {
     Handle<String> input_string(String::cast(shared_strings->get(i)),
                                 i_isolate);
-    String string = *input_string;
+    Tagged<String> string = *input_string;
     CheckStringAndResource(string, i, true, String{}, true, share_resources,
                            threads);
   }
@@ -1825,7 +1826,7 @@ void TestConcurrentExternalizationWithDeadStrings(bool share_resources,
     Handle<String> input_string(String::cast(shared_strings->get(i)),
                                 i_isolate);
     const bool should_be_alive = i % 3 != 0;
-    String string = *input_string;
+    Tagged<String> string = *input_string;
     CheckStringAndResource(string, i, should_be_alive, *empty_string,
                            transition_with_stack, share_resources, threads);
   }
@@ -1843,7 +1844,7 @@ void TestConcurrentExternalizationWithDeadStrings(bool share_resources,
       Handle<String> input_string(String::cast(shared_strings->get(i)),
                                   i_isolate);
       const bool should_be_alive = i % 3 != 0;
-      String string = *input_string;
+      Tagged<String> string = *input_string;
       CheckStringAndResource(string, i, should_be_alive, *empty_string, true,
                              share_resources, threads);
     }
@@ -1935,7 +1936,7 @@ void TestConcurrentExternalizationAndInternalization(
   for (int i = 0; i < shared_strings->length(); i++) {
     Handle<String> input_string(String::cast(shared_strings->get(i)),
                                 i_isolate);
-    String string = *input_string;
+    Tagged<String> string = *input_string;
     if (hit_or_miss == kTestHit) {
       CHECK(IsThinString(string));
       string = ThinString::cast(string)->actual();
diff --git a/test/cctest/test-smi-lexicographic-compare.cc b/test/cctest/test-smi-lexicographic-compare.cc
index 929ed91ebdd..973a9155f80 100644
--- a/test/cctest/test-smi-lexicographic-compare.cc
+++ b/test/cctest/test-smi-lexicographic-compare.cc
@@ -21,7 +21,7 @@ void AddSigned(std::set<Smi>* smis, int64_t x) {
 }
 
 // Uses std::lexicographical_compare twice to convert the result to -1, 0 or 1.
-int ExpectedCompareResult(Smi a, Smi b) {
+int ExpectedCompareResult(Tagged<Smi> a, Tagged<Smi> b) {
   std::string str_a = std::to_string(a.value());
   std::string str_b = std::to_string(b.value());
   bool expected_a_lt_b = std::lexicographical_compare(
@@ -39,7 +39,7 @@ int ExpectedCompareResult(Smi a, Smi b) {
   }
 }
 
-bool Test(Isolate* isolate, Smi a, Smi b) {
+bool Test(Isolate* isolate, Tagged<Smi> a, Tagged<Smi> b) {
   int actual = Smi(Smi::LexicographicCompare(isolate, a, b)).value();
   int expected = ExpectedCompareResult(a, b);
 
@@ -68,8 +68,8 @@ TEST(TestSmiLexicographicCompare) {
     }
   }
 
-  for (Smi a : smis) {
-    for (Smi b : smis) {
+  for (Tagged<Smi> a : smis) {
+    for (Tagged<Smi> b : smis) {
       CHECK(Test(isolate, a, b));
     }
   }
diff --git a/test/cctest/test-strings.cc b/test/cctest/test-strings.cc
index cae6af059b6..c9db5aa25f5 100644
--- a/test/cctest/test-strings.cc
+++ b/test/cctest/test-strings.cc
@@ -307,7 +307,7 @@ void ConsStringGenerationData::Reset() {
   rng_.init();
 }
 
-void AccumulateStats(ConsString cons_string, ConsStringStats* stats) {
+void AccumulateStats(Tagged<ConsString> cons_string, ConsStringStats* stats) {
   int left_length = cons_string->first()->length();
   int right_length = cons_string->second()->length();
   CHECK(cons_string->length() == left_length + right_length);
@@ -344,11 +344,11 @@ void AccumulateStats(Handle<String> cons_string, ConsStringStats* stats) {
   stats->chars_ += cons_string->length();
 }
 
-void AccumulateStatsWithOperator(ConsString cons_string,
+void AccumulateStatsWithOperator(Tagged<ConsString> cons_string,
                                  ConsStringStats* stats) {
   ConsStringIterator iter(cons_string);
   int offset;
-  for (String string = iter.Next(&offset); !string.is_null();
+  for (Tagged<String> string = iter.Next(&offset); !string.is_null();
        string = iter.Next(&offset)) {
     // Accumulate stats.
     CHECK_EQ(0, offset);
@@ -611,7 +611,8 @@ TEST(ConsStringWithEmptyFirstFlatten) {
   CHECK_EQ(initial_length, flat->length());
 }
 
-static void VerifyCharacterStream(String flat_string, String cons_string) {
+static void VerifyCharacterStream(Tagged<String> flat_string,
+                                  Tagged<String> cons_string) {
   // Do not want to test ConString traversal on flat string.
   CHECK(flat_string->IsFlat() && !IsConsString(flat_string));
   CHECK(IsConsString(cons_string));
@@ -677,7 +678,7 @@ void TestStringCharacterStream(BuildString build, int test_cases) {
     VerifyConsString(cons_string, &data);
     // TODO(leszeks): Remove Tagged cast when .first() returns a Tagged.
     static_assert(kTaggedCanConvertToRawObjects);
-    String flat_string_ptr =
+    Tagged<String> flat_string_ptr =
         IsConsString(*flat_string)
             ? Tagged(Tagged<ConsString>::cast(*flat_string)->first())
             : *flat_string;
diff --git a/test/cctest/test-swiss-name-dictionary.cc b/test/cctest/test-swiss-name-dictionary.cc
index 0aabd5981df..f5010ffc23b 100644
--- a/test/cctest/test-swiss-name-dictionary.cc
+++ b/test/cctest/test-swiss-name-dictionary.cc
@@ -113,7 +113,7 @@ void RuntimeTestRunner::CheckEnumerationOrder(
   ReadOnlyRoots roots(isolate_);
   int i = 0;
   for (InternalIndex index : table->IterateEntriesOrdered()) {
-    Object key;
+    Tagged<Object> key;
     if (table->ToKey(roots, index, &key)) {
       CHECK_LT(i, expected_keys.size());
       Handle<Name> expected_key =
diff --git a/test/cctest/test-transitions.cc b/test/cctest/test-transitions.cc
index 5de11ec0a68..fb2e88daafd 100644
--- a/test/cctest/test-transitions.cc
+++ b/test/cctest/test-transitions.cc
@@ -71,8 +71,8 @@ TEST(TransitionArray_SimpleFieldTransitions) {
                                                  attributes));
     CHECK_EQ(2, transitions.NumberOfTransitions());
     for (int i = 0; i < 2; i++) {
-      Name key = transitions.GetKey(i);
-      Map target = transitions.GetTarget(i);
+      Tagged<Name> key = transitions.GetKey(i);
+      Tagged<Map> target = transitions.GetTarget(i);
       CHECK((key == *name1 && target == *map1) ||
             (key == *name2 && target == *map2));
     }
@@ -134,8 +134,8 @@ TEST(TransitionArray_FullFieldTransitions) {
                                                  attributes));
     CHECK_EQ(2, transitions.NumberOfTransitions());
     for (int i = 0; i < 2; i++) {
-      Name key = transitions.GetKey(i);
-      Map target = transitions.GetTarget(i);
+      Tagged<Name> key = transitions.GetKey(i);
+      Tagged<Map> target = transitions.GetTarget(i);
       CHECK((key == *name1 && target == *map1) ||
             (key == *name2 && target == *map2));
     }
@@ -180,8 +180,8 @@ TEST(TransitionArray_DifferentFieldNames) {
                            *names[i], PropertyKind::kData, attributes));
   }
   for (int i = 0; i < PROPS_COUNT; i++) {
-    Name key = transitions.GetKey(i);
-    Map target = transitions.GetTarget(i);
+    Tagged<Name> key = transitions.GetKey(i);
+    Tagged<Map> target = transitions.GetTarget(i);
     for (int j = 0; j < PROPS_COUNT; j++) {
       if (*names[i] == key) {
         CHECK_EQ(*maps[i], target);
@@ -296,8 +296,8 @@ TEST(TransitionArray_SameFieldNamesDifferentAttributes) {
   // Ensure that info about the other fields still valid.
   CHECK_EQ(PROPS_COUNT + ATTRS_COUNT, transitions.NumberOfTransitions());
   for (int i = 0; i < PROPS_COUNT + ATTRS_COUNT; i++) {
-    Name key = transitions.GetKey(i);
-    Map target = transitions.GetTarget(i);
+    Tagged<Name> key = transitions.GetKey(i);
+    Tagged<Map> target = transitions.GetTarget(i);
     if (key == *name) {
       // Attributes transition.
       PropertyAttributes attributes =
diff --git a/test/cctest/test-transitions.h b/test/cctest/test-transitions.h
index 28f055c217f..cc76223c12e 100644
--- a/test/cctest/test-transitions.h
+++ b/test/cctest/test-transitions.h
@@ -12,7 +12,7 @@ namespace internal {
 
 class TestTransitionsAccessor : public TransitionsAccessor {
  public:
-  TestTransitionsAccessor(Isolate* isolate, Map map)
+  TestTransitionsAccessor(Isolate* isolate, Tagged<Map> map)
       : TransitionsAccessor(isolate, map) {}
   TestTransitionsAccessor(Isolate* isolate, Handle<Map> map)
       : TransitionsAccessor(isolate, *map) {}
@@ -27,7 +27,9 @@ class TestTransitionsAccessor : public TransitionsAccessor {
 
   int Capacity() { return TransitionsAccessor::Capacity(); }
 
-  TransitionArray transitions() { return TransitionsAccessor::transitions(); }
+  Tagged<TransitionArray> transitions() {
+    return TransitionsAccessor::transitions();
+  }
 };
 
 }  // namespace internal
diff --git a/test/cctest/test-unwinder-code-pages.cc b/test/cctest/test-unwinder-code-pages.cc
index 3ae9d213f13..c3afa102150 100644
--- a/test/cctest/test-unwinder-code-pages.cc
+++ b/test/cctest/test-unwinder-code-pages.cc
@@ -168,7 +168,7 @@ TEST(Unwind_BuiltinPCInMiddle_Success_CodePagesAPI) {
   register_state.fp = stack;
 
   // Put the current PC inside of a valid builtin.
-  Code builtin = *BUILTIN_CODE(i_isolate, StringEqual);
+  Tagged<Code> builtin = *BUILTIN_CODE(i_isolate, StringEqual);
   const uintptr_t offset = 40;
   CHECK_LT(offset, builtin->instruction_size());
   register_state.pc =
@@ -225,7 +225,7 @@ TEST(Unwind_BuiltinPCAtStart_Success_CodePagesAPI) {
 
   // Put the current PC at the start of a valid builtin, so that we are setting
   // up the frame.
-  Code builtin = *BUILTIN_CODE(i_isolate, StringEqual);
+  Tagged<Code> builtin = *BUILTIN_CODE(i_isolate, StringEqual);
   register_state.pc = reinterpret_cast<void*>(builtin->instruction_start());
 
   bool unwound = v8::Unwinder::TryUnwindV8Frames(
@@ -296,7 +296,7 @@ TEST(Unwind_CodeObjectPCInMiddle_Success_CodePagesAPI) {
       Handle<JSFunction>::cast(v8::Utils::OpenHandle(*local_foo));
 
   // Put the current PC inside of the created code object.
-  Code code = foo->code();
+  Tagged<Code> code = foo->code();
   // We don't produce optimized code when run with --no-turbofan and
   // --no-maglev.
   if (!code->is_optimized_code()) return;
@@ -455,7 +455,7 @@ TEST(Unwind_JSEntry_Fail_CodePagesAPI) {
   CHECK_LE(pages_length, arraysize(code_pages));
   RegisterState register_state;
 
-  Code js_entry = *BUILTIN_CODE(i_isolate, JSEntry);
+  Tagged<Code> js_entry = *BUILTIN_CODE(i_isolate, JSEntry);
   uint8_t* start = reinterpret_cast<uint8_t*>(js_entry->instruction_start());
   register_state.pc = start + 10;
 
@@ -637,7 +637,7 @@ TEST(PCIsInV8_InJSEntryRange_CodePagesAPI) {
       isolate->CopyCodePages(arraysize(code_pages), code_pages);
   CHECK_LE(pages_length, arraysize(code_pages));
 
-  Code js_entry = *BUILTIN_CODE(i_isolate, JSEntry);
+  Tagged<Code> js_entry = *BUILTIN_CODE(i_isolate, JSEntry);
   uint8_t* start = reinterpret_cast<uint8_t*>(js_entry->instruction_start());
   size_t length = js_entry->instruction_size();
 
diff --git a/test/cctest/wasm/test-run-wasm-wrappers.cc b/test/cctest/wasm/test-run-wasm-wrappers.cc
index 8525524f29e..4278a13dfd9 100644
--- a/test/cctest/wasm/test-run-wasm-wrappers.cc
+++ b/test/cctest/wasm/test-run-wasm-wrappers.cc
@@ -33,12 +33,12 @@ Handle<WasmInstanceObject> CompileModule(Zone* zone, Isolate* isolate,
   return maybe_instance.ToHandleChecked();
 }
 
-bool IsGeneric(Code wrapper) {
+bool IsGeneric(Tagged<Code> wrapper) {
   return wrapper->is_builtin() &&
          wrapper->builtin_id() == Builtin::kJSToWasmWrapper;
 }
 
-bool IsSpecific(Code wrapper) {
+bool IsSpecific(Tagged<Code> wrapper) {
   return wrapper->kind() == CodeKind::JS_TO_WASM_FUNCTION;
 }
 
@@ -175,7 +175,7 @@ TEST(WrapperReplacement) {
     }
 
     // Get the wrapper-code object after the wrapper replacement.
-    Code wrapper_after_call = main_function_data->wrapper_code();
+    Tagged<Code> wrapper_after_call = main_function_data->wrapper_code();
 
     // Verify that the budget has been exhausted.
     CHECK_EQ(main_function_data->wrapper_budget(), 0);
diff --git a/test/cctest/wasm/test-wasm-breakpoints.cc b/test/cctest/wasm/test-wasm-breakpoints.cc
index fab849e2748..41d4d793c5a 100644
--- a/test/cctest/wasm/test-wasm-breakpoints.cc
+++ b/test/cctest/wasm/test-wasm-breakpoints.cc
@@ -286,7 +286,7 @@ WASM_COMPILED_EXEC_TEST(WasmCollectPossibleBreakpoints) {
 
   runner.Build({WASM_NOP, WASM_I32_ADD(WASM_ZERO, WASM_ONE)});
 
-  WasmInstanceObject instance = *runner.builder().instance_object();
+  Tagged<WasmInstanceObject> instance = *runner.builder().instance_object();
   NativeModule* native_module = instance->module_object()->native_module();
 
   std::vector<debug::Location> locations;
diff --git a/test/cctest/wasm/test-wasm-serialization.cc b/test/cctest/wasm/test-wasm-serialization.cc
index 15a7eb64611..786025e7287 100644
--- a/test/cctest/wasm/test-wasm-serialization.cc
+++ b/test/cctest/wasm/test-wasm-serialization.cc
@@ -221,7 +221,7 @@ TEST(DeserializeWithSourceUrl) {
     const std::string url = "http://example.com/example.wasm";
     Handle<WasmModuleObject> module_object;
     CHECK(test.Deserialize(base::VectorOf(url)).ToHandle(&module_object));
-    String url_str = String::cast(module_object->script()->name());
+    Tagged<String> url_str = String::cast(module_object->script()->name());
     CHECK_EQ(url, url_str->ToCString().get());
   }
   test.CollectGarbage();
diff --git a/test/cctest/wasm/test-wasm-stack.cc b/test/cctest/wasm/test-wasm-stack.cc
index ec16e22343b..8f317aa648a 100644
--- a/test/cctest/wasm/test-wasm-stack.cc
+++ b/test/cctest/wasm/test-wasm-stack.cc
@@ -107,7 +107,7 @@ void CheckComputeLocation(v8::internal::Isolate* i_isolate, Handle<Object> exc,
   //               whether Script::PositionInfo.column should be the offset
   //               relative to the module or relative to the function.
   // CHECK_EQ(topLocation.column - 1, message->GetColumnNumber());
-  String scriptSource = message->GetSource();
+  Tagged<String> scriptSource = message->GetSource();
   CHECK(IsString(scriptSource));
   if (stackFrame->IsWasm()) {
     CHECK_EQ(scriptSource->length(), 0);
diff --git a/test/cctest/wasm/wasm-run-utils.cc b/test/cctest/wasm/wasm-run-utils.cc
index 5df399c9225..472e4b3f6ea 100644
--- a/test/cctest/wasm/wasm-run-utils.cc
+++ b/test/cctest/wasm/wasm-run-utils.cc
@@ -503,7 +503,8 @@ void WasmFunctionCompiler::Build(base::Vector<const uint8_t> bytes) {
       native_module->PublishCode(native_module->AddCompiledCode(*result));
   DCHECK_NOT_NULL(code);
   DisallowGarbageCollection no_gc;
-  Script script = builder_->instance_object()->module_object()->script();
+  Tagged<Script> script =
+      builder_->instance_object()->module_object()->script();
   std::unique_ptr<char[]> source_url =
       String::cast(script->name())->ToCString();
   if (WasmCode::ShouldBeLogged(isolate())) {
diff --git a/test/common/call-tester.h b/test/common/call-tester.h
index ec05ea00d2f..e30281d04d0 100644
--- a/test/common/call-tester.h
+++ b/test/common/call-tester.h
@@ -42,11 +42,11 @@ class CallHelper {
 
 template <>
 template <typename... Params>
-Object CallHelper<Object>::Call(Params... args) {
+Tagged<Object> CallHelper<Tagged<Object>>::Call(Params... args) {
   CSignature::VerifyParams<Params...>(csig_);
   Address entry = Generate();
   auto fn = GeneratedCode<Address, Params...>::FromAddress(isolate_, entry);
-  return Object(fn.Call(args...));
+  return Tagged<Object>(fn.Call(args...));
 }
 
 // A call helper that calls the given code object assuming C calling convention.
diff --git a/test/mkgrokdump/mkgrokdump.cc b/test/mkgrokdump/mkgrokdump.cc
index ff5483216aa..76cfb882708 100644
--- a/test/mkgrokdump/mkgrokdump.cc
+++ b/test/mkgrokdump/mkgrokdump.cc
@@ -45,7 +45,7 @@ class MockArrayBufferAllocator : public v8::ArrayBuffer::Allocator {
 };
 
 static void DumpKnownMap(FILE* out, i::Heap* heap, const char* space_name,
-                         i::HeapObject object) {
+                         i::Tagged<i::HeapObject> object) {
 #define RO_ROOT_LIST_CASE(type, name, CamelName) \
   if (root_name == nullptr && object == roots.name()) root_name = #CamelName;
 #define MUTABLE_ROOT_LIST_CASE(type, name, CamelName) \
@@ -53,7 +53,7 @@ static void DumpKnownMap(FILE* out, i::Heap* heap, const char* space_name,
 
   i::ReadOnlyRoots roots(heap);
   const char* root_name = nullptr;
-  i::Map map = i::Map::cast(object);
+  i::Tagged<i::Map> map = i::Map::cast(object);
   intptr_t root_ptr =
       static_cast<intptr_t>(map.ptr()) & (i::Page::kPageSize - 1);
 
@@ -69,7 +69,7 @@ static void DumpKnownMap(FILE* out, i::Heap* heap, const char* space_name,
 }
 
 static void DumpKnownObject(FILE* out, i::Heap* heap, const char* space_name,
-                            i::HeapObject object) {
+                            i::Tagged<i::HeapObject> object) {
 #define RO_ROOT_LIST_CASE(type, name, CamelName)        \
   if (root_name == nullptr && object == roots.name()) { \
     root_name = #CamelName;                             \
@@ -142,14 +142,14 @@ static int DumpHeapConstants(FILE* out, const char* argv0) {
       i::PrintF(out, "\n# List of known V8 maps.\n");
       i::PrintF(out, "KNOWN_MAPS = {\n");
       i::ReadOnlyHeapObjectIterator ro_iterator(read_only_heap);
-      for (i::HeapObject object = ro_iterator.Next(); !object.is_null();
-           object = ro_iterator.Next()) {
+      for (i::Tagged<i::HeapObject> object = ro_iterator.Next();
+           !object.is_null(); object = ro_iterator.Next()) {
         if (!IsMap(object)) continue;
         DumpKnownMap(out, heap, i::ToString(i::RO_SPACE), object);
       }
 
       i::PagedSpaceObjectIterator iterator(heap, heap->old_space());
-      for (i::HeapObject object = iterator.Next(); !object.is_null();
+      for (i::Tagged<i::HeapObject> object = iterator.Next(); !object.is_null();
            object = iterator.Next()) {
         if (!IsMap(object)) continue;
         DumpKnownMap(out, heap, i::ToString(heap->old_space()->identity()),
diff --git a/test/unittests/api/deserialize-unittest.cc b/test/unittests/api/deserialize-unittest.cc
index a6a701d5f9f..d175ccf64f2 100644
--- a/test/unittests/api/deserialize-unittest.cc
+++ b/test/unittests/api/deserialize-unittest.cc
@@ -293,22 +293,22 @@ class MergeDeserializedCodeTest : public DeserializeTest {
   };
 
   template <typename T>
-  static i::SharedFunctionInfo GetSharedFunctionInfo(
+  static i::Tagged<i::SharedFunctionInfo> GetSharedFunctionInfo(
       Local<T> function_or_script) {
     i::Handle<i::JSFunction> i_function =
         i::Handle<i::JSFunction>::cast(Utils::OpenHandle(*function_or_script));
     return i_function->shared();
   }
 
-  static i::MaybeObject WeakOrSmi(i::Object obj) {
+  static i::MaybeObject WeakOrSmi(i::Tagged<i::Object> obj) {
     return IsSmi(obj)
                ? i::MaybeObject::FromSmi(i::Smi::cast(obj))
                : i::MaybeObject::MakeWeak(i::MaybeObject::FromObject(obj));
   }
 
   void ValidateStandaloneGraphAndPopulateArray(
-      i::SharedFunctionInfo toplevel_sfi, i::WeakFixedArray array,
-      bool lazy_should_be_compiled = false,
+      i::Tagged<i::SharedFunctionInfo> toplevel_sfi,
+      i::Tagged<i::WeakFixedArray> array, bool lazy_should_be_compiled = false,
       bool eager_should_be_compiled = true) {
     i::DisallowGarbageCollection no_gc;
     CHECK(toplevel_sfi->is_compiled());
@@ -317,12 +317,12 @@ class MergeDeserializedCodeTest : public DeserializeTest {
                WeakOrSmi(toplevel_sfi->function_data(kAcquireLoad)));
     array->Set(kToplevelFeedbackMetadata,
                WeakOrSmi(toplevel_sfi->feedback_metadata()));
-    i::Script script = i::Script::cast(toplevel_sfi->script());
+    i::Tagged<i::Script> script = i::Script::cast(toplevel_sfi->script());
     array->Set(kScript, WeakOrSmi(script));
-    i::WeakFixedArray sfis = script->shared_function_infos();
+    i::Tagged<i::WeakFixedArray> sfis = script->shared_function_infos();
     CHECK_EQ(sfis->length(), 4);
     CHECK_EQ(sfis->Get(0), WeakOrSmi(toplevel_sfi));
-    i::SharedFunctionInfo eager =
+    i::Tagged<i::SharedFunctionInfo> eager =
         i::SharedFunctionInfo::cast(sfis->Get(1).GetHeapObjectAssumeWeak());
     CHECK_EQ(eager->is_compiled(), eager_should_be_compiled);
     array->Set(kEagerSfi, WeakOrSmi(eager));
@@ -330,7 +330,7 @@ class MergeDeserializedCodeTest : public DeserializeTest {
       array->Set(kEagerFunctionData,
                  WeakOrSmi(eager->function_data(kAcquireLoad)));
       array->Set(kEagerFeedbackMetadata, WeakOrSmi(eager->feedback_metadata()));
-      i::SharedFunctionInfo iife =
+      i::Tagged<i::SharedFunctionInfo> iife =
           i::SharedFunctionInfo::cast(sfis->Get(2).GetHeapObjectAssumeWeak());
       CHECK(iife->is_compiled());
       array->Set(kIifeSfi, WeakOrSmi(iife));
@@ -338,7 +338,7 @@ class MergeDeserializedCodeTest : public DeserializeTest {
                  WeakOrSmi(iife->function_data(kAcquireLoad)));
       array->Set(kIifeFeedbackMetadata, WeakOrSmi(iife->feedback_metadata()));
     }
-    i::SharedFunctionInfo lazy =
+    i::Tagged<i::SharedFunctionInfo> lazy =
         i::SharedFunctionInfo::cast(sfis->Get(3).GetHeapObjectAssumeWeak());
     CHECK_EQ(lazy->is_compiled(), lazy_should_be_compiled);
     array->Set(kLazySfi, WeakOrSmi(lazy));
@@ -349,7 +349,7 @@ class MergeDeserializedCodeTest : public DeserializeTest {
                         i::Isolate* i_isolate) {
     for (int index = 0; index < kScriptObjectsCount; ++index) {
       if ((sfis_to_age & (1 << index)) == (1 << index)) {
-        i::SharedFunctionInfo sfi = i::SharedFunctionInfo::cast(
+        i::Tagged<i::SharedFunctionInfo> sfi = i::SharedFunctionInfo::cast(
             original_objects->Get(index).GetHeapObjectAssumeWeak());
         i::SharedFunctionInfo::EnsureOldForTesting(sfi);
       }
@@ -374,13 +374,13 @@ class MergeDeserializedCodeTest : public DeserializeTest {
   };
 
   void RetainObjects(ScriptObjectFlag to_retain,
-                     i::WeakFixedArray original_objects,
-                     i::FixedArray retained_original_objects,
+                     i::Tagged<i::WeakFixedArray> original_objects,
+                     i::Tagged<i::FixedArray> retained_original_objects,
                      i::Isolate* i_isolate) {
     for (int index = 0; index < kScriptObjectsCount; ++index) {
       if ((to_retain & (1 << index)) == (1 << index)) {
         i::MaybeObject maybe = original_objects->Get(index);
-        if (i::HeapObject heap_object;
+        if (i::Tagged<i::HeapObject> heap_object;
             maybe.GetHeapObjectIfWeak(&heap_object)) {
           retained_original_objects->set(index, heap_object);
           continue;
diff --git a/test/unittests/assembler/macro-assembler-x64-unittest.cc b/test/unittests/assembler/macro-assembler-x64-unittest.cc
index b08f153ecf8..07563d48d65 100644
--- a/test/unittests/assembler/macro-assembler-x64-unittest.cc
+++ b/test/unittests/assembler/macro-assembler-x64-unittest.cc
@@ -165,9 +165,9 @@ TEST_F(MacroAssemblerX64Test, Smi) {
     bool is_in_range = number >= Smi::kMinValue && number <= Smi::kMaxValue;
     CHECK_EQ(is_in_range, is_valid);
     if (is_valid) {
-      Smi smi_from_intptr = Smi::FromIntptr(number);
+      Tagged<Smi> smi_from_intptr = Smi::FromIntptr(number);
       if (static_cast<int>(number) == number) {  // Is a 32-bit int.
-        Smi smi_from_int = Smi::FromInt(static_cast<int32_t>(number));
+        Tagged<Smi> smi_from_int = Smi::FromInt(static_cast<int32_t>(number));
         CHECK_EQ(smi_from_int, smi_from_intptr);
       }
       int64_t smi_value = smi_from_intptr.value();
@@ -176,7 +176,8 @@ TEST_F(MacroAssemblerX64Test, Smi) {
   }
 }
 
-static void TestMoveSmi(MacroAssembler* masm, Label* exit, int id, Smi value) {
+static void TestMoveSmi(MacroAssembler* masm, Label* exit, int id,
+                        Tagged<Smi> value) {
   __ movl(rax, Immediate(id));
   __ Move(rcx, value);
   __ Move(rdx, static_cast<intptr_t>(value.ptr()));
@@ -545,7 +546,7 @@ TEST_F(MacroAssemblerX64Test, EmbeddedObj) {
 #endif
   using myF0 = Address();
   auto f = GeneratedCode<myF0>::FromAddress(isolate, code->instruction_start());
-  Object result = Object(f.Call());
+  Tagged<Object> result = Object(f.Call());
   CHECK_EQ(old_array->ptr(), result.ptr());
 
   // Collect garbage to ensure reloc info can be walked by the heap.
diff --git a/test/unittests/codegen/code-pages-unittest.cc b/test/unittests/codegen/code-pages-unittest.cc
index e5a8106c3b4..3a8236ba979 100644
--- a/test/unittests/codegen/code-pages-unittest.cc
+++ b/test/unittests/codegen/code-pages-unittest.cc
@@ -150,11 +150,11 @@ TEST_F(CodePagesTest, OptimizedCodeWithCodeRange) {
   Handle<JSFunction> foo =
       Handle<JSFunction>::cast(v8::Utils::OpenHandle(*local_foo));
 
-  Code code = foo->code();
+  Tagged<Code> code = foo->code();
   // We don't produce optimized code when run with --no-turbofan and
   // --no-maglev.
   if (!code->is_optimized_code()) return;
-  InstructionStream foo_code = code->instruction_stream();
+  Tagged<InstructionStream> foo_code = code->instruction_stream();
 
   EXPECT_TRUE(i_isolate()->heap()->InSpace(foo_code, CODE_SPACE));
 
@@ -200,11 +200,11 @@ TEST_F(CodePagesTest, OptimizedCodeWithCodePages) {
         EXPECT_TRUE(v8_flags.always_sparkplug);
         return;
       }
-      Code code = foo->code();
+      Tagged<Code> code = foo->code();
       // We don't produce optimized code when run with --no-turbofan and
       // --no-maglev.
       if (!code->is_optimized_code()) return;
-      InstructionStream foo_code = code->instruction_stream();
+      Tagged<InstructionStream> foo_code = code->instruction_stream();
 
       EXPECT_TRUE(i_isolate()->heap()->InSpace(foo_code, CODE_SPACE));
 
diff --git a/test/unittests/compiler/codegen-tester.h b/test/unittests/compiler/codegen-tester.h
index 969c8a97806..aced406a34d 100644
--- a/test/unittests/compiler/codegen-tester.h
+++ b/test/unittests/compiler/codegen-tester.h
@@ -60,12 +60,12 @@ class RawMachineAssemblerTester : public CallHelper<ReturnType>,
 
   ~RawMachineAssemblerTester() override = default;
 
-  void CheckNumber(double expected, Object number) {
+  void CheckNumber(double expected, Tagged<Object> number) {
     CHECK(Object::SameValue(*this->isolate()->factory()->NewNumber(expected),
                             number));
   }
 
-  void CheckString(const char* expected, Object string) {
+  void CheckString(const char* expected, Tagged<Object> string) {
     CHECK(Object::SameValue(
         *this->isolate()->factory()->InternalizeUtf8String(expected), string));
   }
diff --git a/test/unittests/compiler/codegen-unittest.cc b/test/unittests/compiler/codegen-unittest.cc
index 1d931204d80..ce112c0cf1e 100644
--- a/test/unittests/compiler/codegen-unittest.cc
+++ b/test/unittests/compiler/codegen-unittest.cc
@@ -21,7 +21,7 @@ class CodeGenTest : public TestWithIsolateAndZone {
 // TODO(dcarney): on x64 Smis are generated with the SmiConstantRegister
 #if !V8_TARGET_ARCH_X64
     if (Smi::IsValid(v)) {
-      RawMachineAssemblerTester<Object> m(i_isolate(), zone());
+      RawMachineAssemblerTester<Tagged<Object>> m(i_isolate(), zone());
       m.Return(m.NumberConstant(v));
       CHECK_EQ(Smi::FromInt(v), m.Call());
     }
@@ -29,14 +29,14 @@ class CodeGenTest : public TestWithIsolateAndZone {
   }
 
   void RunNumberConstant(double v) {
-    RawMachineAssemblerTester<Object> m(i_isolate(), zone());
+    RawMachineAssemblerTester<Tagged<Object>> m(i_isolate(), zone());
 #if V8_TARGET_ARCH_X64
     // TODO(dcarney): on x64 Smis are generated with the SmiConstantRegister
     Handle<Object> number = m.isolate()->factory()->NewNumber(v);
     if (IsSmi(*number)) return;
 #endif
     m.Return(m.NumberConstant(v));
-    Object result = m.Call();
+    Tagged<Object> result = m.Call();
     m.CheckNumber(v, result);
   }
 };
@@ -366,13 +366,13 @@ TEST_F(CodeGenTest, RunNumberConstants) {
 }
 
 TEST_F(CodeGenTest, RunEmptyString) {
-  RawMachineAssemblerTester<Object> m(i_isolate(), zone());
+  RawMachineAssemblerTester<Tagged<Object>> m(i_isolate(), zone());
   m.Return(m.StringConstant("empty"));
   m.CheckString("empty", m.Call());
 }
 
 TEST_F(CodeGenTest, RunHeapConstant) {
-  RawMachineAssemblerTester<Object> m(i_isolate(), zone());
+  RawMachineAssemblerTester<Tagged<Object>> m(i_isolate(), zone());
   m.Return(m.StringConstant("empty"));
   m.CheckString("empty", m.Call());
 }
@@ -381,7 +381,7 @@ TEST_F(CodeGenTest, RunHeapNumberConstant) {
   RawMachineAssemblerTester<void*> m(i_isolate(), zone());
   Handle<HeapObject> number = m.isolate()->factory()->NewHeapNumber(100.5);
   m.Return(m.HeapConstant(number));
-  HeapObject result =
+  Tagged<HeapObject> result =
       HeapObject::cast(Object(reinterpret_cast<Address>(m.Call())));
   CHECK_EQ(result, *number);
 }
diff --git a/test/unittests/compiler/compiler-unittest.cc b/test/unittests/compiler/compiler-unittest.cc
index 82a1cce7274..b327ad8e836 100644
--- a/test/unittests/compiler/compiler-unittest.cc
+++ b/test/unittests/compiler/compiler-unittest.cc
@@ -34,7 +34,7 @@ static Handle<Object> GetGlobalProperty(const char* name) {
       .ToHandleChecked();
 }
 
-static void SetGlobalProperty(const char* name, Object value) {
+static void SetGlobalProperty(const char* name, Tagged<Object> value) {
   Isolate* isolate = reinterpret_cast<i::Isolate*>(v8::Isolate::GetCurrent());
   Handle<Object> object(value, isolate);
   Handle<String> internalized_name =
@@ -299,7 +299,7 @@ TEST_F(CompilerTest, FeedbackVectorPreservedAcrossRecompiles) {
   FeedbackSlot slot_for_a(0);
   MaybeObject object = feedback_vector->Get(slot_for_a);
   {
-    HeapObject heap_object;
+    Tagged<HeapObject> heap_object;
     EXPECT_TRUE(object->GetHeapObjectIfWeak(&heap_object));
     EXPECT_TRUE(IsJSFunction(heap_object));
   }
@@ -311,7 +311,7 @@ TEST_F(CompilerTest, FeedbackVectorPreservedAcrossRecompiles) {
   EXPECT_TRUE(f->HasAttachedOptimizedCode());
   object = f->feedback_vector()->Get(slot_for_a);
   {
-    HeapObject heap_object;
+    Tagged<HeapObject> heap_object;
     EXPECT_TRUE(object->GetHeapObjectIfWeak(&heap_object));
     EXPECT_TRUE(IsJSFunction(heap_object));
   }
diff --git a/test/unittests/heap/heap-utils.cc b/test/unittests/heap/heap-utils.cc
index 46c67149729..c2176a551df 100644
--- a/test/unittests/heap/heap-utils.cc
+++ b/test/unittests/heap/heap-utils.cc
@@ -81,12 +81,13 @@ void FillPageInPagedSpace(Page* page,
   // Collect all free list block sizes
   page->ForAllFreeListCategories(
       [&available_sizes](FreeListCategory* category) {
-        category->IterateNodesForTesting([&available_sizes](FreeSpace node) {
-          int node_size = node->Size();
-          if (node_size >= kMaxRegularHeapObjectSize) {
-            available_sizes.push_back(node_size);
-          }
-        });
+        category->IterateNodesForTesting(
+            [&available_sizes](Tagged<FreeSpace> node) {
+              int node_size = node->Size();
+              if (node_size >= kMaxRegularHeapObjectSize) {
+                available_sizes.push_back(node_size);
+              }
+            });
       });
 
   Isolate* isolate = heap->isolate();
@@ -114,11 +115,12 @@ void FillPageInPagedSpace(Page* page,
         remaining_sizes.push_back({});
         std::vector<int>& sizes_in_category =
             remaining_sizes[remaining_sizes.size() - 1];
-        category->IterateNodesForTesting([&sizes_in_category](FreeSpace node) {
-          int node_size = node->Size();
-          DCHECK_LT(0, FixedArrayLenFromSize(node_size));
-          sizes_in_category.push_back(node_size);
-        });
+        category->IterateNodesForTesting(
+            [&sizes_in_category](Tagged<FreeSpace> node) {
+              int node_size = node->Size();
+              DCHECK_LT(0, FixedArrayLenFromSize(node_size));
+              sizes_in_category.push_back(node_size);
+            });
       });
   for (auto it = remaining_sizes.rbegin(); it != remaining_sizes.rend(); ++it) {
     std::vector<int> sizes_in_category = *it;
@@ -279,7 +281,7 @@ void HeapInternalsBase::FillCurrentPage(
     FillCurrentSemiSpacePage(space, out_handles);
 }
 
-bool IsNewObjectInCorrectGeneration(HeapObject object) {
+bool IsNewObjectInCorrectGeneration(Tagged<HeapObject> object) {
   return v8_flags.single_generation ? !i::Heap::InYoungGeneration(object)
                                     : i::Heap::InYoungGeneration(object);
 }
diff --git a/test/unittests/heap/heap-utils.h b/test/unittests/heap/heap-utils.h
index 24d266351d4..6d0862dcf23 100644
--- a/test/unittests/heap/heap-utils.h
+++ b/test/unittests/heap/heap-utils.h
@@ -153,7 +153,7 @@ bool InYoungGeneration(v8::Isolate* isolate, const GlobalOrPersistent& global) {
   return Heap::InYoungGeneration(*v8::Utils::OpenHandle(*tmp));
 }
 
-bool IsNewObjectInCorrectGeneration(HeapObject object);
+bool IsNewObjectInCorrectGeneration(Tagged<HeapObject> object);
 
 template <typename GlobalOrPersistent>
 bool IsNewObjectInCorrectGeneration(v8::Isolate* isolate,
diff --git a/test/unittests/heap/inner-pointer-resolution-unittest.cc b/test/unittests/heap/inner-pointer-resolution-unittest.cc
index 9c295e62d4a..b613065b5cf 100644
--- a/test/unittests/heap/inner-pointer-resolution-unittest.cc
+++ b/test/unittests/heap/inner-pointer-resolution-unittest.cc
@@ -652,9 +652,9 @@ TEST_F(InnerPointerResolutionHeapTest, UnusedRegularYoungPages) {
     auto page3 = Page::FromHeapObject(obj3);
     EXPECT_TRUE(page3 == page1 || page3 == page2);
     if (page3 == page1) {
-      EXPECT_EQ(obj3.address(), obj1.address() + obj1.Size(cage_base));
+      EXPECT_EQ(obj3.address(), obj1.address() + obj1->Size(cage_base));
     } else {
-      EXPECT_EQ(obj3.address(), obj2.address() + obj2.Size(cage_base));
+      EXPECT_EQ(obj3.address(), obj2.address() + obj2->Size(cage_base));
     }
 
     // Keep inner pointers to all objects.
@@ -665,12 +665,12 @@ TEST_F(InnerPointerResolutionHeapTest, UnusedRegularYoungPages) {
     // Keep pointers to the end of the pages, after the objects.
     outside_ptr1 = page1->area_end() - 3 * kTaggedSize;
     outside_ptr2 = page2->area_end() - 2 * kTaggedSize;
-    EXPECT_LE(obj1.address() + obj1.Size(cage_base), outside_ptr1);
-    EXPECT_LE(obj2.address() + obj2.Size(cage_base), outside_ptr2);
+    EXPECT_LE(obj1.address() + obj1->Size(cage_base), outside_ptr1);
+    EXPECT_LE(obj2.address() + obj2->Size(cage_base), outside_ptr2);
     if (page3 == page1) {
-      EXPECT_LE(obj3.address() + obj3.Size(cage_base), outside_ptr1);
+      EXPECT_LE(obj3.address() + obj3->Size(cage_base), outside_ptr1);
     } else {
-      EXPECT_LE(obj3.address() + obj3.Size(cage_base), outside_ptr2);
+      EXPECT_LE(obj3.address() + obj3->Size(cage_base), outside_ptr2);
     }
 
     // Ensure the young generation space is iterable.
diff --git a/test/unittests/heap/iterators-unittest.cc b/test/unittests/heap/iterators-unittest.cc
index 8d2f1f5878e..870af6d221b 100644
--- a/test/unittests/heap/iterators-unittest.cc
+++ b/test/unittests/heap/iterators-unittest.cc
@@ -46,16 +46,16 @@ TEST_F(IteratorsTest, CombinedHeapObjectIteratorNullPastEnd) {
 
 namespace {
 // An arbitrary object guaranteed to live on the non-read-only heap.
-Object CreateWritableObject(v8::Isolate* isolate) {
+Tagged<Object> CreateWritableObject(v8::Isolate* isolate) {
   return *v8::Utils::OpenHandle(*v8::Object::New(isolate));
 }
 }  // namespace
 
 TEST_F(IteratorsTest, ReadOnlyHeapObjectIterator) {
   HandleScope handle_scope(i_isolate());
-  const Object sample_object = CreateWritableObject(v8_isolate());
+  const Tagged<Object> sample_object = CreateWritableObject(v8_isolate());
   ReadOnlyHeapObjectIterator iterator(i_isolate()->read_only_heap());
-  for (HeapObject obj = iterator.Next(); !obj.is_null();
+  for (Tagged<HeapObject> obj = iterator.Next(); !obj.is_null();
        obj = iterator.Next()) {
     CHECK(ReadOnlyHeap::Contains(obj));
     CHECK(!i_isolate()->heap()->Contains(obj));
@@ -66,10 +66,10 @@ TEST_F(IteratorsTest, ReadOnlyHeapObjectIterator) {
 TEST_F(IteratorsTest, HeapObjectIterator) {
   Heap* const heap = i_isolate()->heap();
   HandleScope handle_scope(i_isolate());
-  const Object sample_object = CreateWritableObject(v8_isolate());
+  const Tagged<Object> sample_object = CreateWritableObject(v8_isolate());
   bool seen_sample_object = false;
   HeapObjectIterator iterator(heap);
-  for (HeapObject obj = iterator.Next(); !obj.is_null();
+  for (Tagged<HeapObject> obj = iterator.Next(); !obj.is_null();
        obj = iterator.Next()) {
     CHECK_IMPLIES(!v8_flags.enable_third_party_heap,
                   !ReadOnlyHeap::Contains(obj));
@@ -82,10 +82,10 @@ TEST_F(IteratorsTest, HeapObjectIterator) {
 TEST_F(IteratorsTest, CombinedHeapObjectIterator) {
   Heap* const heap = i_isolate()->heap();
   HandleScope handle_scope(i_isolate());
-  const Object sample_object = CreateWritableObject(v8_isolate());
+  const Tagged<Object> sample_object = CreateWritableObject(v8_isolate());
   bool seen_sample_object = false;
   CombinedHeapObjectIterator iterator(heap);
-  for (HeapObject obj = iterator.Next(); !obj.is_null();
+  for (Tagged<HeapObject> obj = iterator.Next(); !obj.is_null();
        obj = iterator.Next()) {
     CHECK(IsValidHeapObject(heap, obj));
     if (sample_object.SafeEquals(obj)) seen_sample_object = true;
diff --git a/test/unittests/heap/lab-unittest.cc b/test/unittests/heap/lab-unittest.cc
index bf3b749ce39..3cc21c0386f 100644
--- a/test/unittests/heap/lab-unittest.cc
+++ b/test/unittests/heap/lab-unittest.cc
@@ -25,7 +25,7 @@ Address AllocateLabBackingStore(Heap* heap, size_t size_in_bytes) {
 bool AllocateFromLab(Heap* heap, LocalAllocationBuffer* lab,
                      size_t size_in_bytes,
                      AllocationAlignment alignment = kTaggedAligned) {
-  HeapObject obj;
+  Tagged<HeapObject> obj;
   AllocationResult result =
       lab->AllocateRawAligned(static_cast<int>(size_in_bytes), alignment);
   if (result.To(&obj)) {
@@ -38,7 +38,7 @@ bool AllocateFromLab(Heap* heap, LocalAllocationBuffer* lab,
 void VerifyIterable(Address base, Address limit,
                     std::vector<size_t> expected_size) {
   EXPECT_LE(base, limit);
-  HeapObject object;
+  Tagged<HeapObject> object;
   size_t counter = 0;
   while (base < limit) {
     object = HeapObject::FromAddress(base);
diff --git a/test/unittests/heap/marking-worklist-unittest.cc b/test/unittests/heap/marking-worklist-unittest.cc
index 45bbdad4be1..fc45d75511f 100644
--- a/test/unittests/heap/marking-worklist-unittest.cc
+++ b/test/unittests/heap/marking-worklist-unittest.cc
@@ -21,13 +21,13 @@ using MarkingWorklistTest = TestWithContext;
 TEST_F(MarkingWorklistTest, PushPop) {
   MarkingWorklists holder;
   MarkingWorklists::Local worklists(&holder);
-  HeapObject pushed_object =
+  Tagged<HeapObject> pushed_object =
       HeapObject::cast(i_isolate()
                            ->roots_table()
                            .slot(RootIndex::kFirstStrongRoot)
                            .load(i_isolate()));
   worklists.Push(pushed_object);
-  HeapObject popped_object;
+  Tagged<HeapObject> popped_object;
   EXPECT_TRUE(worklists.Pop(&popped_object));
   EXPECT_EQ(popped_object, pushed_object);
 }
@@ -35,13 +35,13 @@ TEST_F(MarkingWorklistTest, PushPop) {
 TEST_F(MarkingWorklistTest, PushPopOnHold) {
   MarkingWorklists holder;
   MarkingWorklists::Local worklists(&holder);
-  HeapObject pushed_object =
+  Tagged<HeapObject> pushed_object =
       HeapObject::cast(i_isolate()
                            ->roots_table()
                            .slot(RootIndex::kFirstStrongRoot)
                            .load(i_isolate()));
   worklists.PushOnHold(pushed_object);
-  HeapObject popped_object;
+  Tagged<HeapObject> popped_object;
   EXPECT_TRUE(worklists.PopOnHold(&popped_object));
   EXPECT_EQ(popped_object, pushed_object);
 }
@@ -50,7 +50,7 @@ TEST_F(MarkingWorklistTest, MergeOnHold) {
   MarkingWorklists holder;
   MarkingWorklists::Local main_worklists(&holder);
   MarkingWorklists::Local worker_worklists(&holder);
-  HeapObject pushed_object =
+  Tagged<HeapObject> pushed_object =
       HeapObject::cast(i_isolate()
                            ->roots_table()
                            .slot(RootIndex::kFirstStrongRoot)
@@ -58,7 +58,7 @@ TEST_F(MarkingWorklistTest, MergeOnHold) {
   worker_worklists.PushOnHold(pushed_object);
   worker_worklists.Publish();
   main_worklists.MergeOnHold();
-  HeapObject popped_object;
+  Tagged<HeapObject> popped_object;
   EXPECT_TRUE(main_worklists.Pop(&popped_object));
   EXPECT_EQ(popped_object, pushed_object);
 }
@@ -67,14 +67,14 @@ TEST_F(MarkingWorklistTest, ShareWorkIfGlobalPoolIsEmpty) {
   MarkingWorklists holder;
   MarkingWorklists::Local main_worklists(&holder);
   MarkingWorklists::Local worker_worklists(&holder);
-  HeapObject pushed_object =
+  Tagged<HeapObject> pushed_object =
       HeapObject::cast(i_isolate()
                            ->roots_table()
                            .slot(RootIndex::kFirstStrongRoot)
                            .load(i_isolate()));
   main_worklists.Push(pushed_object);
   main_worklists.ShareWork();
-  HeapObject popped_object;
+  Tagged<HeapObject> popped_object;
   EXPECT_TRUE(worker_worklists.Pop(&popped_object));
   EXPECT_EQ(popped_object, pushed_object);
 }
@@ -84,7 +84,7 @@ TEST_F(MarkingWorklistTest, ContextWorklistsPushPop) {
   MarkingWorklists holder;
   holder.CreateContextWorklists({context});
   MarkingWorklists::Local worklists(&holder);
-  HeapObject pushed_object =
+  Tagged<HeapObject> pushed_object =
       HeapObject::cast(i_isolate()
                            ->roots_table()
                            .slot(RootIndex::kFirstStrongRoot)
@@ -92,7 +92,7 @@ TEST_F(MarkingWorklistTest, ContextWorklistsPushPop) {
   worklists.SwitchToContext(context);
   worklists.Push(pushed_object);
   worklists.SwitchToSharedForTesting();
-  HeapObject popped_object;
+  Tagged<HeapObject> popped_object;
   EXPECT_TRUE(worklists.Pop(&popped_object));
   EXPECT_EQ(popped_object, pushed_object);
   holder.ReleaseContextWorklists();
@@ -103,7 +103,7 @@ TEST_F(MarkingWorklistTest, ContextWorklistsEmpty) {
   MarkingWorklists holder;
   holder.CreateContextWorklists({context});
   MarkingWorklists::Local worklists(&holder);
-  HeapObject pushed_object =
+  Tagged<HeapObject> pushed_object =
       HeapObject::cast(i_isolate()
                            ->roots_table()
                            .slot(RootIndex::kFirstStrongRoot)
@@ -113,7 +113,7 @@ TEST_F(MarkingWorklistTest, ContextWorklistsEmpty) {
   EXPECT_FALSE(worklists.IsEmpty());
   worklists.SwitchToSharedForTesting();
   EXPECT_FALSE(worklists.IsEmpty());
-  HeapObject popped_object;
+  Tagged<HeapObject> popped_object;
   EXPECT_TRUE(worklists.Pop(&popped_object));
   EXPECT_EQ(popped_object, pushed_object);
   EXPECT_TRUE(worklists.IsEmpty());
@@ -127,7 +127,7 @@ TEST_F(MarkingWorklistTest, ContextWorklistCrossTask) {
   holder.CreateContextWorklists({context1, context2});
   MarkingWorklists::Local main_worklists(&holder);
   MarkingWorklists::Local worker_worklists(&holder);
-  HeapObject pushed_object =
+  Tagged<HeapObject> pushed_object =
       HeapObject::cast(i_isolate()
                            ->roots_table()
                            .slot(RootIndex::kFirstStrongRoot)
@@ -136,7 +136,7 @@ TEST_F(MarkingWorklistTest, ContextWorklistCrossTask) {
   main_worklists.Push(pushed_object);
   main_worklists.ShareWork();
   worker_worklists.SwitchToContext(context2);
-  HeapObject popped_object;
+  Tagged<HeapObject> popped_object;
   EXPECT_TRUE(worker_worklists.Pop(&popped_object));
   EXPECT_EQ(popped_object, pushed_object);
   EXPECT_EQ(context1, worker_worklists.Context());
diff --git a/test/unittests/heap/persistent-handles-unittest.cc b/test/unittests/heap/persistent-handles-unittest.cc
index ddd917446ee..bab9f436b0b 100644
--- a/test/unittests/heap/persistent-handles-unittest.cc
+++ b/test/unittests/heap/persistent-handles-unittest.cc
@@ -117,7 +117,8 @@ class PersistentHandlesThread final : public v8::base::Thread {
  public:
   PersistentHandlesThread(Heap* heap, std::vector<Handle<HeapNumber>> handles,
                           std::unique_ptr<PersistentHandles> ph,
-                          HeapNumber number, base::Semaphore* sema_started,
+                          Tagged<HeapNumber> number,
+                          base::Semaphore* sema_started,
                           base::Semaphore* sema_gc_finished)
       : v8::base::Thread(base::Thread::Options("ThreadWithLocalHeap")),
         heap_(heap),
diff --git a/test/unittests/heap/shared-heap-unittest.cc b/test/unittests/heap/shared-heap-unittest.cc
index fc9b860a3fb..53eae905c32 100644
--- a/test/unittests/heap/shared-heap-unittest.cc
+++ b/test/unittests/heap/shared-heap-unittest.cc
@@ -220,7 +220,7 @@ void AllocateInSharedHeap(int iterations = 100) {
     }
 
     for (int i = 0; i < kKeptAliveInHeap; i++) {
-      FixedArray array = FixedArray::cast(arrays_in_heap->get(i));
+      Tagged<FixedArray> array = FixedArray::cast(arrays_in_heap->get(i));
       CHECK_EQ(array->length(), 100);
     }
   });
diff --git a/test/unittests/heap/spaces-unittest.cc b/test/unittests/heap/spaces-unittest.cc
index 87b9dccf806..47cfa34b444 100644
--- a/test/unittests/heap/spaces-unittest.cc
+++ b/test/unittests/heap/spaces-unittest.cc
@@ -45,7 +45,7 @@ TEST_F(SpacesTest, CompactionSpaceMerge) {
   const int kExpectedPages =
       (kNumObjects + kNumObjectsPerPage - 1) / kNumObjectsPerPage;
   for (int i = 0; i < kNumObjects; i++) {
-    HeapObject object =
+    Tagged<HeapObject> object =
         compaction_space->AllocateRawUnaligned(kMaxRegularHeapObjectSize)
             .ToObjectChecked();
     heap->CreateFillerObjectAt(object.address(), kMaxRegularHeapObjectSize);
@@ -62,13 +62,13 @@ TEST_F(SpacesTest, CompactionSpaceMerge) {
 
 TEST_F(SpacesTest, WriteBarrierFromHeapObject) {
   constexpr Address address1 = Page::kPageSize;
-  HeapObject object1 = HeapObject::unchecked_cast(Object(address1));
+  Tagged<HeapObject> object1 = HeapObject::unchecked_cast(Object(address1));
   BasicMemoryChunk* chunk1 = BasicMemoryChunk::FromHeapObject(object1);
   heap_internals::MemoryChunk* slim_chunk1 =
       heap_internals::MemoryChunk::FromHeapObject(object1);
   EXPECT_EQ(static_cast<void*>(chunk1), static_cast<void*>(slim_chunk1));
   constexpr Address address2 = 2 * Page::kPageSize - 1;
-  HeapObject object2 = HeapObject::unchecked_cast(Object(address2));
+  Tagged<HeapObject> object2 = HeapObject::unchecked_cast(Object(address2));
   BasicMemoryChunk* chunk2 = BasicMemoryChunk::FromHeapObject(object2);
   heap_internals::MemoryChunk* slim_chunk2 =
       heap_internals::MemoryChunk::FromHeapObject(object2);
diff --git a/test/unittests/interpreter/bytecode-array-iterator-unittest.cc b/test/unittests/interpreter/bytecode-array-iterator-unittest.cc
index 95f7d24f38d..1d876db11d1 100644
--- a/test/unittests/interpreter/bytecode-array-iterator-unittest.cc
+++ b/test/unittests/interpreter/bytecode-array-iterator-unittest.cc
@@ -31,9 +31,9 @@ TEST_F(BytecodeArrayIteratorTest, IteratesBytecodeArray) {
                               HashSeed(isolate()));
   double heap_num_0 = 2.718;
   double heap_num_1 = 2.0 * Smi::kMaxValue;
-  Smi zero = Smi::zero();
-  Smi smi_0 = Smi::FromInt(64);
-  Smi smi_1 = Smi::FromInt(-65536);
+  Tagged<Smi> zero = Smi::zero();
+  Tagged<Smi> smi_0 = Smi::FromInt(64);
+  Tagged<Smi> smi_1 = Smi::FromInt(-65536);
   Register reg_0(0);
   Register reg_16(16);  // Something not eligible for short Star.
   RegisterList pair = BytecodeUtils::NewRegisterList(0, 2);
diff --git a/test/unittests/interpreter/bytecode-array-random-iterator-unittest.cc b/test/unittests/interpreter/bytecode-array-random-iterator-unittest.cc
index fd6579b76ee..785cdc43708 100644
--- a/test/unittests/interpreter/bytecode-array-random-iterator-unittest.cc
+++ b/test/unittests/interpreter/bytecode-array-random-iterator-unittest.cc
@@ -31,9 +31,9 @@ TEST_F(BytecodeArrayRandomIteratorTest, InvalidBeforeStart) {
                               HashSeed(isolate()));
   double heap_num_0 = 2.718;
   double heap_num_1 = 2.0 * Smi::kMaxValue;
-  Smi zero = Smi::zero();
-  Smi smi_0 = Smi::FromInt(64);
-  Smi smi_1 = Smi::FromInt(-65536);
+  Tagged<Smi> zero = Smi::zero();
+  Tagged<Smi> smi_0 = Smi::FromInt(64);
+  Tagged<Smi> smi_1 = Smi::FromInt(-65536);
   Register reg_0(0);
   Register reg_1(1);
   RegisterList pair = BytecodeUtils::NewRegisterList(0, 2);
@@ -83,9 +83,9 @@ TEST_F(BytecodeArrayRandomIteratorTest, InvalidAfterEnd) {
                               HashSeed(isolate()));
   double heap_num_0 = 2.718;
   double heap_num_1 = 2.0 * Smi::kMaxValue;
-  Smi zero = Smi::zero();
-  Smi smi_0 = Smi::FromInt(64);
-  Smi smi_1 = Smi::FromInt(-65536);
+  Tagged<Smi> zero = Smi::zero();
+  Tagged<Smi> smi_0 = Smi::FromInt(64);
+  Tagged<Smi> smi_1 = Smi::FromInt(-65536);
   Register reg_0(0);
   Register reg_1(1);
   RegisterList pair = BytecodeUtils::NewRegisterList(0, 2);
@@ -135,9 +135,9 @@ TEST_F(BytecodeArrayRandomIteratorTest, AccessesFirst) {
                               HashSeed(isolate()));
   double heap_num_0 = 2.718;
   double heap_num_1 = 2.0 * Smi::kMaxValue;
-  Smi zero = Smi::zero();
-  Smi smi_0 = Smi::FromInt(64);
-  Smi smi_1 = Smi::FromInt(-65536);
+  Tagged<Smi> zero = Smi::zero();
+  Tagged<Smi> smi_0 = Smi::FromInt(64);
+  Tagged<Smi> smi_1 = Smi::FromInt(-65536);
   Register reg_0(0);
   Register reg_1(1);
   RegisterList pair = BytecodeUtils::NewRegisterList(0, 2);
@@ -192,9 +192,9 @@ TEST_F(BytecodeArrayRandomIteratorTest, AccessesLast) {
                               HashSeed(isolate()));
   double heap_num_0 = 2.718;
   double heap_num_1 = 2.0 * Smi::kMaxValue;
-  Smi zero = Smi::zero();
-  Smi smi_0 = Smi::FromInt(64);
-  Smi smi_1 = Smi::FromInt(-65536);
+  Tagged<Smi> zero = Smi::zero();
+  Tagged<Smi> smi_0 = Smi::FromInt(64);
+  Tagged<Smi> smi_1 = Smi::FromInt(-65536);
   Register reg_0(0);
   Register reg_1(1);
   RegisterList pair = BytecodeUtils::NewRegisterList(0, 2);
@@ -249,9 +249,9 @@ TEST_F(BytecodeArrayRandomIteratorTest, RandomAccessValid) {
                               HashSeed(isolate()));
   double heap_num_0 = 2.718;
   double heap_num_1 = 2.0 * Smi::kMaxValue;
-  Smi zero = Smi::zero();
-  Smi smi_0 = Smi::FromInt(64);
-  Smi smi_1 = Smi::FromInt(-65536);
+  Tagged<Smi> zero = Smi::zero();
+  Tagged<Smi> smi_0 = Smi::FromInt(64);
+  Tagged<Smi> smi_1 = Smi::FromInt(-65536);
   Register reg_0(0);
   Register reg_16(16);  // Something not eligible for short Star.
   RegisterList pair = BytecodeUtils::NewRegisterList(0, 2);
@@ -427,9 +427,9 @@ TEST_F(BytecodeArrayRandomIteratorTest, IteratesBytecodeArray) {
                               HashSeed(isolate()));
   double heap_num_0 = 2.718;
   double heap_num_1 = 2.0 * Smi::kMaxValue;
-  Smi zero = Smi::zero();
-  Smi smi_0 = Smi::FromInt(64);
-  Smi smi_1 = Smi::FromInt(-65536);
+  Tagged<Smi> zero = Smi::zero();
+  Tagged<Smi> smi_0 = Smi::FromInt(64);
+  Tagged<Smi> smi_1 = Smi::FromInt(-65536);
   Register reg_0(0);
   Register reg_16(16);  // Something not eligible for short Star.
   RegisterList pair = BytecodeUtils::NewRegisterList(0, 2);
@@ -680,9 +680,9 @@ TEST_F(BytecodeArrayRandomIteratorTest, IteratesBytecodeArrayBackwards) {
                               HashSeed(isolate()));
   double heap_num_0 = 2.718;
   double heap_num_1 = 2.0 * Smi::kMaxValue;
-  Smi zero = Smi::zero();
-  Smi smi_0 = Smi::FromInt(64);
-  Smi smi_1 = Smi::FromInt(-65536);
+  Tagged<Smi> zero = Smi::zero();
+  Tagged<Smi> smi_0 = Smi::FromInt(64);
+  Tagged<Smi> smi_1 = Smi::FromInt(-65536);
   Register reg_0(0);
   Register reg_16(16);  // Something not eligible for short Star.
   RegisterList pair = BytecodeUtils::NewRegisterList(0, 2);
diff --git a/test/unittests/interpreter/bytecode-expectations-printer.cc b/test/unittests/interpreter/bytecode-expectations-printer.cc
index 70e025f8d8a..d67ed01459c 100644
--- a/test/unittests/interpreter/bytecode-expectations-printer.cc
+++ b/test/unittests/interpreter/bytecode-expectations-printer.cc
@@ -284,8 +284,8 @@ void BytecodeExpectationsPrinter::PrintSourcePosition(
   }
 }
 
-void BytecodeExpectationsPrinter::PrintV8String(std::ostream* stream,
-                                                i::String string) const {
+void BytecodeExpectationsPrinter::PrintV8String(
+    std::ostream* stream, i::Tagged<i::String> string) const {
   *stream << '"';
   for (int i = 0, length = string->length(); i < length; ++i) {
     *stream << i::AsEscapedUC16ForJSON(string->Get(i));
@@ -341,7 +341,7 @@ void BytecodeExpectationsPrinter::PrintBytecodeSequence(
 }
 
 void BytecodeExpectationsPrinter::PrintConstantPool(
-    std::ostream* stream, i::FixedArray constant_pool) const {
+    std::ostream* stream, i::Tagged<i::FixedArray> constant_pool) const {
   *stream << "constant pool: [\n";
   int num_constants = constant_pool->length();
   if (num_constants > 0) {
diff --git a/test/unittests/interpreter/bytecode-expectations-printer.h b/test/unittests/interpreter/bytecode-expectations-printer.h
index 7a77c772dbc..8c8c44b0ba0 100644
--- a/test/unittests/interpreter/bytecode-expectations-printer.h
+++ b/test/unittests/interpreter/bytecode-expectations-printer.h
@@ -70,14 +70,14 @@ class BytecodeExpectationsPrinter final {
   void PrintSourcePosition(std::ostream* stream,
                            SourcePositionTableIterator* source_iterator,
                            int bytecode_offset) const;
-  void PrintV8String(std::ostream* stream, i::String string) const;
+  void PrintV8String(std::ostream* stream, i::Tagged<i::String> string) const;
   void PrintConstant(std::ostream* stream, i::Handle<i::Object> constant) const;
   void PrintFrameSize(std::ostream* stream,
                       i::Handle<i::BytecodeArray> bytecode_array) const;
   void PrintBytecodeSequence(std::ostream* stream,
                              i::Handle<i::BytecodeArray> bytecode_array) const;
   void PrintConstantPool(std::ostream* stream,
-                         i::FixedArray constant_pool) const;
+                         i::Tagged<i::FixedArray> constant_pool) const;
   void PrintCodeSnippet(std::ostream* stream, const std::string& body) const;
   void PrintBytecodeArray(std::ostream* stream,
                           i::Handle<i::BytecodeArray> bytecode_array) const;
diff --git a/test/unittests/interpreter/constant-array-builder-unittest.cc b/test/unittests/interpreter/constant-array-builder-unittest.cc
index da7589423d2..542877fa9f6 100644
--- a/test/unittests/interpreter/constant-array-builder-unittest.cc
+++ b/test/unittests/interpreter/constant-array-builder-unittest.cc
@@ -137,7 +137,7 @@ TEST_F(ConstantArrayBuilderTest, AllocateEntriesWithIdx8Reservations) {
       CHECK_EQ(operand_size, OperandSize::kByte);
     }
     for (size_t i = 0; i < duplicates_in_idx8_space; i++) {
-      Smi value = Smi::FromInt(static_cast<int>(2 * k8BitCapacity + i));
+      Tagged<Smi> value = Smi::FromInt(static_cast<int>(2 * k8BitCapacity + i));
       size_t index = builder.CommitReservedEntry(OperandSize::kByte, value);
       CHECK_EQ(index, k8BitCapacity - reserved + i);
     }
@@ -154,13 +154,13 @@ TEST_F(ConstantArrayBuilderTest, AllocateEntriesWithIdx8Reservations) {
 
     // Check all committed values match expected
     for (size_t i = 0; i < k8BitCapacity - reserved; i++) {
-      Object value = constant_array->get(static_cast<int>(i));
-      Smi smi = Smi::FromInt(static_cast<int>(i));
+      Tagged<Object> value = constant_array->get(static_cast<int>(i));
+      Tagged<Smi> smi = Smi::FromInt(static_cast<int>(i));
       CHECK(Object::SameValue(value, smi));
     }
     for (size_t i = k8BitCapacity; i < 2 * k8BitCapacity + reserved; i++) {
-      Object value = constant_array->get(static_cast<int>(i));
-      Smi smi = Smi::FromInt(static_cast<int>(i - reserved));
+      Tagged<Object> value = constant_array->get(static_cast<int>(i));
+      Tagged<Smi> smi = Smi::FromInt(static_cast<int>(i - reserved));
       CHECK(Object::SameValue(value, smi));
     }
   }
@@ -205,7 +205,7 @@ TEST_F(ConstantArrayBuilderTest, AllocateEntriesWithWideReservations) {
     CHECK_EQ(constant_array->length(),
              static_cast<int>(k8BitCapacity + reserved));
     for (size_t i = 0; i < k8BitCapacity + reserved; i++) {
-      Object value = constant_array->get(static_cast<int>(i));
+      Tagged<Object> value = constant_array->get(static_cast<int>(i));
       CHECK(Object::SameValue(value,
                               *isolate()->factory()->NewNumberFromSize(i)));
     }
@@ -235,8 +235,9 @@ TEST_F(ConstantArrayBuilderTest, GapFilledWhenLowReservationCommitted) {
   Handle<FixedArray> constant_array = builder.ToFixedArray(isolate());
   CHECK_EQ(constant_array->length(), static_cast<int>(2 * k8BitCapacity));
   for (size_t i = 0; i < k8BitCapacity; i++) {
-    Object original = constant_array->get(static_cast<int>(k8BitCapacity + i));
-    Object duplicate = constant_array->get(static_cast<int>(i));
+    Tagged<Object> original =
+        constant_array->get(static_cast<int>(k8BitCapacity + i));
+    Tagged<Object> duplicate = constant_array->get(static_cast<int>(i));
     CHECK(Object::SameValue(original, duplicate));
     Handle<Object> reference = isolate()->factory()->NewNumberFromSize(i);
     CHECK(Object::SameValue(original, *reference));
diff --git a/test/unittests/interpreter/interpreter-tester.h b/test/unittests/interpreter/interpreter-tester.h
index 5f3f8406400..34839d978d1 100644
--- a/test/unittests/interpreter/interpreter-tester.h
+++ b/test/unittests/interpreter/interpreter-tester.h
@@ -35,7 +35,7 @@ class InterpreterCallable {
  public:
   virtual ~InterpreterCallable() = default;
 
-  FeedbackVector vector() const { return function_->feedback_vector(); }
+  Tagged<FeedbackVector> vector() const { return function_->feedback_vector(); }
 
  protected:
   InterpreterCallable(Isolate* isolate, Handle<JSFunction> function)
diff --git a/test/unittests/interpreter/interpreter-unittest.cc b/test/unittests/interpreter/interpreter-unittest.cc
index b6fb521c53d..e4022a33741 100644
--- a/test/unittests/interpreter/interpreter-unittest.cc
+++ b/test/unittests/interpreter/interpreter-unittest.cc
@@ -4763,7 +4763,7 @@ TEST_F(InterpreterTest, InterpreterWithNativeStack) {
   i::Handle<i::JSFunction> f = i::Handle<i::JSFunction>::cast(o);
 
   CHECK(f->shared()->HasBytecodeArray());
-  i::Code code = f->shared()->GetCode(i_isolate());
+  i::Tagged<i::Code> code = f->shared()->GetCode(i_isolate());
   i::Handle<i::Code> interpreter_entry_trampoline =
       BUILTIN_CODE(i_isolate(), InterpreterEntryTrampoline);
 
@@ -4779,24 +4779,24 @@ TEST_F(InterpreterTest, InterpreterGetBytecodeHandler) {
   Interpreter* interpreter = i_isolate()->interpreter();
 
   // Test that single-width bytecode handlers deserializer correctly.
-  Code wide_handler =
+  Tagged<Code> wide_handler =
       interpreter->GetBytecodeHandler(Bytecode::kWide, OperandScale::kSingle);
 
   CHECK_EQ(wide_handler->builtin_id(), Builtin::kWideHandler);
 
-  Code add_handler =
+  Tagged<Code> add_handler =
       interpreter->GetBytecodeHandler(Bytecode::kAdd, OperandScale::kSingle);
 
   CHECK_EQ(add_handler->builtin_id(), Builtin::kAddHandler);
 
   // Test that double-width bytecode handlers deserializer correctly, including
   // an illegal bytecode handler since there is no Wide.Wide handler.
-  Code wide_wide_handler =
+  Tagged<Code> wide_wide_handler =
       interpreter->GetBytecodeHandler(Bytecode::kWide, OperandScale::kDouble);
 
   CHECK_EQ(wide_wide_handler->builtin_id(), Builtin::kIllegalHandler);
 
-  Code add_wide_handler =
+  Tagged<Code> add_wide_handler =
       interpreter->GetBytecodeHandler(Bytecode::kAdd, OperandScale::kDouble);
 
   CHECK_EQ(add_wide_handler->builtin_id(), Builtin::kAddWideHandler);
@@ -4821,7 +4821,8 @@ TEST_F(InterpreterTest, InterpreterCollectSourcePositions) {
 
   Compiler::CollectSourcePositions(i_isolate(), sfi);
 
-  ByteArray source_position_table = bytecode_array->SourcePositionTable();
+  Tagged<ByteArray> source_position_table =
+      bytecode_array->SourcePositionTable();
   CHECK(bytecode_array->HasSourcePositionTable());
   CHECK_GT(source_position_table->length(), 0);
 }
@@ -4849,7 +4850,8 @@ TEST_F(InterpreterTest, InterpreterCollectSourcePositions_StackOverflow) {
   i_isolate()->stack_guard()->SetStackLimit(GetCurrentStackPosition());
   Compiler::CollectSourcePositions(i_isolate(), sfi);
   // Stack overflowed so source position table can be returned but is empty.
-  ByteArray source_position_table = bytecode_array->SourcePositionTable();
+  Tagged<ByteArray> source_position_table =
+      bytecode_array->SourcePositionTable();
   CHECK(!bytecode_array->HasSourcePositionTable());
   CHECK_EQ(source_position_table->length(), 0);
 
@@ -4983,21 +4985,22 @@ TEST_F(InterpreterTest, InterpreterCollectSourcePositions_GenerateStackTrace) {
   }
 
   CHECK(bytecode_array->HasSourcePositionTable());
-  ByteArray source_position_table = bytecode_array->SourcePositionTable();
+  Tagged<ByteArray> source_position_table =
+      bytecode_array->SourcePositionTable();
   CHECK_GT(source_position_table->length(), 0);
 }
 
 TEST_F(InterpreterTest, InterpreterLookupNameOfBytecodeHandler) {
   Interpreter* interpreter = i_isolate()->interpreter();
-  Code ldaLookupSlot = interpreter->GetBytecodeHandler(Bytecode::kLdaLookupSlot,
-                                                       OperandScale::kSingle);
+  Tagged<Code> ldaLookupSlot = interpreter->GetBytecodeHandler(
+      Bytecode::kLdaLookupSlot, OperandScale::kSingle);
   CheckStringEqual("LdaLookupSlotHandler",
                    Builtins::name(ldaLookupSlot->builtin_id()));
-  Code wideLdaLookupSlot = interpreter->GetBytecodeHandler(
+  Tagged<Code> wideLdaLookupSlot = interpreter->GetBytecodeHandler(
       Bytecode::kLdaLookupSlot, OperandScale::kDouble);
   CheckStringEqual("LdaLookupSlotWideHandler",
                    Builtins::name(wideLdaLookupSlot->builtin_id()));
-  Code extraWideLdaLookupSlot = interpreter->GetBytecodeHandler(
+  Tagged<Code> extraWideLdaLookupSlot = interpreter->GetBytecodeHandler(
       Bytecode::kLdaLookupSlot, OperandScale::kQuadruple);
   CheckStringEqual("LdaLookupSlotExtraWideHandler",
                    Builtins::name(extraWideLdaLookupSlot->builtin_id()));
diff --git a/test/unittests/logging/log-unittest.cc b/test/unittests/logging/log-unittest.cc
index 093bdc6e2d3..1ecff11b914 100644
--- a/test/unittests/logging/log-unittest.cc
+++ b/test/unittests/logging/log-unittest.cc
@@ -903,7 +903,7 @@ void ValidateMapDetailsLogging(v8::Isolate* isolate,
   i::HeapObjectIterator iterator(heap);
   i::DisallowGarbageCollection no_gc;
   size_t i = 0;
-  for (i::HeapObject obj = iterator.Next(); !obj.is_null();
+  for (i::Tagged<i::HeapObject> obj = iterator.Next(); !obj.is_null();
        obj = iterator.Next()) {
     if (!IsMap(obj)) continue;
     i++;
diff --git a/test/unittests/maglev/node-type-unittest.cc b/test/unittests/maglev/node-type-unittest.cc
index b127689d2f1..ce34c2b1b5f 100644
--- a/test/unittests/maglev/node-type-unittest.cc
+++ b/test/unittests/maglev/node-type-unittest.cc
@@ -89,9 +89,9 @@ TEST_F(MaglevTest, NodeTypeIsClosedUnderIntersect) {
 // Ensure StaticTypeForMap is consistent with actual maps.
 TEST_F(MaglevTest, NodeTypeApproximationIsConsistent) {
   for (auto idx = RootIndex::kFirstRoot; idx <= RootIndex::kLastRoot; ++idx) {
-    Object obj = isolate()->roots_table().slot(idx).load(isolate());
+    Tagged<Object> obj = isolate()->roots_table().slot(idx).load(isolate());
     if (obj.ptr() == kNullAddress || !IsMap(obj)) continue;
-    Map map = Map::cast(obj);
+    Tagged<Map> map = Map::cast(obj);
     compiler::MapRef map_ref = MakeRef(broker(), map);
 
     for (NodeType a : kAllNodeTypes) {
@@ -106,9 +106,9 @@ TEST_F(MaglevTest, NodeTypeApproximationIsConsistent) {
 // Ensure CombineType is consistent with actual maps.
 TEST_F(MaglevTest, NodeTypeCombineIsConsistent) {
   for (auto idx = RootIndex::kFirstRoot; idx <= RootIndex::kLastRoot; ++idx) {
-    Object obj = isolate()->roots_table().slot(idx).load(isolate());
+    Tagged<Object> obj = isolate()->roots_table().slot(idx).load(isolate());
     if (obj.ptr() == kNullAddress || !IsMap(obj)) continue;
-    Map map = Map::cast(obj);
+    Tagged<Map> map = Map::cast(obj);
     compiler::MapRef map_ref = MakeRef(broker(), map);
 
     for (NodeType a : kAllNodeTypes) {
diff --git a/test/unittests/numbers/conversions-unittest.cc b/test/unittests/numbers/conversions-unittest.cc
index 2ac238eaef1..7251d3de180 100644
--- a/test/unittests/numbers/conversions-unittest.cc
+++ b/test/unittests/numbers/conversions-unittest.cc
@@ -395,7 +395,7 @@ TEST_F(ConversionsTest, NoHandlesForTryNumberToSize) {
   size_t result = 0;
   {
     SealHandleScope no_handles(i_isolate());
-    Smi smi = Smi::FromInt(1);
+    Tagged<Smi> smi = Smi::FromInt(1);
     CHECK(TryNumberToSize(smi, &result));
     CHECK_EQ(result, 1u);
   }
diff --git a/test/unittests/objects/concurrent-js-array-unittest.cc b/test/unittests/objects/concurrent-js-array-unittest.cc
index 675fb764a18..7d5058faf09 100644
--- a/test/unittests/objects/concurrent-js-array-unittest.cc
+++ b/test/unittests/objects/concurrent-js-array-unittest.cc
@@ -63,7 +63,7 @@ class BackgroundThread final : public v8::base::Thread {
         continue;
       }
 
-      base::Optional<Object> result =
+      base::Optional<Tagged<Object>> result =
           ConcurrentLookupIterator::TryGetOwnCowElement(
               isolate, FixedArray::cast(*elements), elements_kind,
               Smi::ToInt(x->length(isolate, kRelaxedLoad)), kIndex);
diff --git a/test/unittests/objects/concurrent-script-context-table-unittest.cc b/test/unittests/objects/concurrent-script-context-table-unittest.cc
index b33df0f3f41..0e457f8937d 100644
--- a/test/unittests/objects/concurrent-script-context-table-unittest.cc
+++ b/test/unittests/objects/concurrent-script-context-table-unittest.cc
@@ -45,7 +45,7 @@ class ScriptContextTableAccessUsedThread final : public v8::base::Thread {
     sema_started_->Signal();
 
     for (int i = 0; i < script_context_table_->used(kAcquireLoad); ++i) {
-      Context context = script_context_table_->get_context(i);
+      Tagged<Context> context = script_context_table_->get_context(i);
       EXPECT_TRUE(context->IsScriptContext());
     }
   }
diff --git a/test/unittests/objects/dictionary-unittest.cc b/test/unittests/objects/dictionary-unittest.cc
index 0b09bbefbb0..d8398460d89 100644
--- a/test/unittests/objects/dictionary-unittest.cc
+++ b/test/unittests/objects/dictionary-unittest.cc
@@ -100,7 +100,7 @@ class DictionaryTest : public TestWithHeapInternalsAndContext {
     for (int i = 0; i < 100; i++) {
       Handle<JSReceiver> key = factory->NewJSArray(7);
       CHECK_EQ(table->Lookup(key), roots.the_hole_value());
-      Object identity_hash = key->GetIdentityHash();
+      Tagged<Object> identity_hash = key->GetIdentityHash();
       CHECK_EQ(roots.undefined_value(), identity_hash);
     }
   }
@@ -161,7 +161,7 @@ class DictionaryTest : public TestWithHeapInternalsAndContext {
     for (int i = 0; i < 100; i++) {
       Handle<JSReceiver> key = factory->NewJSArray(7);
       CHECK(!table->Has(isolate(), key));
-      Object identity_hash = key->GetIdentityHash();
+      Tagged<Object> identity_hash = key->GetIdentityHash();
       CHECK_EQ(ReadOnlyRoots(heap()).undefined_value(), identity_hash);
     }
   }
@@ -228,9 +228,9 @@ TEST_F(DictionaryTest, HashSet) {
   TestHashSet(ObjectHashSet::New(isolate(), 23));
 }
 
-class ObjectHashTableTest : public ObjectHashTable {
+class ObjectHashTableTest {
  public:
-  explicit ObjectHashTableTest(ObjectHashTable o) : ObjectHashTable(o) {}
+  explicit ObjectHashTableTest(Tagged<ObjectHashTable> o) : table_(o) {}
 
   // For every object, add a `->` operator which returns a pointer to this
   // object. This will allow smoother transition between T and Tagged<T>.
@@ -238,16 +238,21 @@ class ObjectHashTableTest : public ObjectHashTable {
   const ObjectHashTableTest* operator->() const { return this; }
 
   void insert(InternalIndex entry, int key, int value) {
-    set(EntryToIndex(entry), Smi::FromInt(key));
-    set(EntryToIndex(entry) + 1, Smi::FromInt(value));
+    table_->set(table_->EntryToIndex(entry), Smi::FromInt(key));
+    table_->set(table_->EntryToIndex(entry) + 1, Smi::FromInt(value));
   }
 
   int lookup(int key, Isolate* isolate) {
     Handle<Object> key_obj(Smi::FromInt(key), isolate);
-    return Smi::ToInt(Lookup(key_obj));
+    return Smi::ToInt(table_->Lookup(key_obj));
   }
 
-  int capacity() { return Capacity(); }
+  int capacity() { return table_->Capacity(); }
+
+  void Rehash(Isolate* isolate) { table_->Rehash(isolate); }
+
+ private:
+  Tagged<ObjectHashTable> table_;
 };
 
 TEST_F(DictionaryTest, HashTableRehash) {
diff --git a/test/unittests/objects/feedback-vector-unittest.cc b/test/unittests/objects/feedback-vector-unittest.cc
index 19510c30454..2b8be204ead 100644
--- a/test/unittests/objects/feedback-vector-unittest.cc
+++ b/test/unittests/objects/feedback-vector-unittest.cc
@@ -85,7 +85,7 @@ TEST_F(FeedbackVectorTest, VectorStructure) {
     spec.AddForInSlot();
     vector = NewFeedbackVector(isolate, &spec);
     FeedbackVectorHelper helper(vector);
-    FeedbackCell cell = *vector->GetClosureFeedbackCell(0);
+    Tagged<FeedbackCell> cell = *vector->GetClosureFeedbackCell(0);
     CHECK_EQ(cell->value(), *factory->undefined_value());
   }
 }
@@ -194,7 +194,7 @@ TEST_F(FeedbackVectorTest, VectorCallICStateApply) {
   FeedbackNexus nexus(feedback_vector, slot);
   CHECK_EQ(InlineCacheState::MONOMORPHIC, nexus.ic_state());
   CHECK_EQ(CallFeedbackContent::kReceiver, nexus.GetCallFeedbackContent());
-  HeapObject heap_object;
+  Tagged<HeapObject> heap_object;
   CHECK(nexus.GetFeedback()->GetHeapObjectIfWeak(&heap_object));
   CHECK_EQ(*F, heap_object);
 
@@ -233,7 +233,7 @@ TEST_F(FeedbackVectorTest, VectorCallFeedback) {
   FeedbackNexus nexus(feedback_vector, slot);
 
   CHECK_EQ(InlineCacheState::MONOMORPHIC, nexus.ic_state());
-  HeapObject heap_object;
+  Tagged<HeapObject> heap_object;
   CHECK(nexus.GetFeedback()->GetHeapObjectIfWeak(&heap_object));
   CHECK_EQ(*foo, heap_object);
 
@@ -265,7 +265,7 @@ TEST_F(FeedbackVectorTest, VectorPolymorphicCallFeedback) {
   FeedbackNexus nexus(feedback_vector, slot);
 
   CHECK_EQ(InlineCacheState::POLYMORPHIC, nexus.ic_state());
-  HeapObject heap_object;
+  Tagged<HeapObject> heap_object;
   CHECK(nexus.GetFeedback()->GetHeapObjectIfWeak(&heap_object));
   CHECK(IsFeedbackCell(heap_object, isolate));
   // Ensure this is the feedback cell for the closure returned by
@@ -293,7 +293,7 @@ TEST_F(FeedbackVectorTest, VectorCallFeedbackForArray) {
   FeedbackNexus nexus(feedback_vector, slot);
 
   CHECK_EQ(InlineCacheState::MONOMORPHIC, nexus.ic_state());
-  HeapObject heap_object;
+  Tagged<HeapObject> heap_object;
   CHECK(nexus.GetFeedback()->GetHeapObjectIfWeak(&heap_object));
   CHECK_EQ(*isolate->array_function(), heap_object);
 
@@ -546,7 +546,7 @@ TEST_F(FeedbackVectorTest, VectorLoadICOnSmi) {
   FeedbackNexus nexus(feedback_vector, slot);
   CHECK_EQ(InlineCacheState::MONOMORPHIC, nexus.ic_state());
   // Verify that the monomorphic map is the one we expect.
-  Map number_map = ReadOnlyRoots(heap).heap_number_map();
+  Tagged<Map> number_map = ReadOnlyRoots(heap).heap_number_map();
   CHECK_EQ(number_map, nexus.GetFirstMap());
 
   // Now go polymorphic on o.
diff --git a/test/unittests/objects/object-unittest.cc b/test/unittests/objects/object-unittest.cc
index 933138f0ac8..b9bf3a11514 100644
--- a/test/unittests/objects/object-unittest.cc
+++ b/test/unittests/objects/object-unittest.cc
@@ -352,7 +352,8 @@ TEST_F(ObjectTest, EnumCache) {
     CHECK_EQ(c->map()->instance_descriptors()->enum_cache(),
              *factory->empty_enum_cache());
 
-    EnumCache enum_cache = cc->map()->instance_descriptors()->enum_cache();
+    Tagged<EnumCache> enum_cache =
+        cc->map()->instance_descriptors()->enum_cache();
     CHECK_NE(enum_cache, *factory->empty_enum_cache());
     CHECK_EQ(enum_cache->keys()->length(), 3);
     CHECK_EQ(enum_cache->indices()->length(), 3);
@@ -369,7 +370,8 @@ TEST_F(ObjectTest, EnumCache) {
 
     // The enum cache is shared on the descriptor array of maps {a}, {b} and
     // {c} only.
-    EnumCache enum_cache = a->map()->instance_descriptors()->enum_cache();
+    Tagged<EnumCache> enum_cache =
+        a->map()->instance_descriptors()->enum_cache();
     CHECK_NE(enum_cache, *factory->empty_enum_cache());
     CHECK_NE(cc->map()->instance_descriptors()->enum_cache(),
              *factory->empty_enum_cache());
@@ -397,7 +399,8 @@ TEST_F(ObjectTest, EnumCache) {
     CHECK_EQ(c->map()->EnumLength(), 3);
     CHECK_EQ(cc->map()->EnumLength(), 3);
 
-    EnumCache enum_cache = c->map()->instance_descriptors()->enum_cache();
+    Tagged<EnumCache> enum_cache =
+        c->map()->instance_descriptors()->enum_cache();
     CHECK_NE(enum_cache, *factory->empty_enum_cache());
     // The keys and indices caches are updated.
     CHECK_EQ(enum_cache, *previous_enum_cache);
@@ -433,7 +436,8 @@ TEST_F(ObjectTest, EnumCache) {
     CHECK_EQ(c->map()->EnumLength(), 3);
     CHECK_EQ(cc->map()->EnumLength(), 3);
 
-    EnumCache enum_cache = c->map()->instance_descriptors()->enum_cache();
+    Tagged<EnumCache> enum_cache =
+        c->map()->instance_descriptors()->enum_cache();
     CHECK_NE(enum_cache, *factory->empty_enum_cache());
     // The keys and indices caches are not updated.
     CHECK_EQ(enum_cache, *previous_enum_cache);
@@ -657,7 +661,7 @@ TEST_F(ObjectTest, ConstructorInstanceTypes) {
 
   DisallowGarbageCollection no_gc;
   for (int i = 0; i < Context::NATIVE_CONTEXT_SLOTS; i++) {
-    Object value = context->get(i);
+    Tagged<Object> value = context->get(i);
     if (!IsJSFunction(value)) continue;
     InstanceType instance_type =
         JSFunction::cast(value)->map()->instance_type();
diff --git a/test/unittests/objects/roots-unittest.cc b/test/unittests/objects/roots-unittest.cc
index cd4c098d5a7..f4acd2ad70a 100644
--- a/test/unittests/objects/roots-unittest.cc
+++ b/test/unittests/objects/roots-unittest.cc
@@ -18,7 +18,7 @@ namespace internal {
 using RootsTest = TestWithIsolate;
 
 namespace {
-AllocationSpace GetSpaceFromObject(Object object) {
+AllocationSpace GetSpaceFromObject(Tagged<Object> object) {
   DCHECK(IsHeapObject(object));
   BasicMemoryChunk* chunk =
       BasicMemoryChunk::FromHeapObject(HeapObject::cast(object));
diff --git a/test/unittests/objects/weakmaps-unittest.cc b/test/unittests/objects/weakmaps-unittest.cc
index 65341d69606..799a5667c2d 100644
--- a/test/unittests/objects/weakmaps-unittest.cc
+++ b/test/unittests/objects/weakmaps-unittest.cc
@@ -157,7 +157,8 @@ TEST_F(WeakMapsTest, Shrinking) {
 }
 
 namespace {
-bool EphemeronHashTableContainsKey(EphemeronHashTable table, HeapObject key) {
+bool EphemeronHashTableContainsKey(Tagged<EphemeronHashTable> table,
+                                   Tagged<HeapObject> key) {
   for (InternalIndex i : table->IterateEntries()) {
     if (table->KeyAt(i) == key) return true;
   }
diff --git a/test/unittests/parser/parse-decision-unittest.cc b/test/unittests/parser/parse-decision-unittest.cc
index 338f5375589..3ac74b7ffe2 100644
--- a/test/unittests/parser/parse-decision-unittest.cc
+++ b/test/unittests/parser/parse-decision-unittest.cc
@@ -45,7 +45,7 @@ void GetTopLevelFunctionInfo(
   SharedFunctionInfo::ScriptIterator iterator(
       toplevel_fn->GetIsolate(), Script::cast(toplevel_fn->shared()->script()));
 
-  for (SharedFunctionInfo shared = iterator.Next(); !shared.is_null();
+  for (Tagged<SharedFunctionInfo> shared = iterator.Next(); !shared.is_null();
        shared = iterator.Next()) {
     std::unique_ptr<char[]> name = String::cast(shared->Name())->ToCString();
     is_compiled->insert(std::make_pair(name.get(), shared->is_compiled()));
diff --git a/test/unittests/parser/preparser-unittest.cc b/test/unittests/parser/preparser-unittest.cc
index 891c642033a..3f266213d62 100644
--- a/test/unittests/parser/preparser-unittest.cc
+++ b/test/unittests/parser/preparser-unittest.cc
@@ -884,10 +884,10 @@ TEST_F(PreParserTest, ProducingAndConsumingByteData) {
     i::ZoneConsumedPreparseData::ByteData::ReadingScope reading_scope(
         &bytes_for_reading, wrapper);
 
-    CHECK_EQ(wrapper.data_length(), kDataSize);
+    CHECK_EQ(wrapper->data_length(), kDataSize);
 
     for (int i = 0; i < kDataSize; i++) {
-      CHECK_EQ(copied_buffer.at(i), wrapper.get(i));
+      CHECK_EQ(copied_buffer.at(i), wrapper->get(i));
     }
 
 #ifdef DEBUG
diff --git a/test/unittests/parser/scanner-streams-unittest.cc b/test/unittests/parser/scanner-streams-unittest.cc
index a243be5f49d..9dde5636f1d 100644
--- a/test/unittests/parser/scanner-streams-unittest.cc
+++ b/test/unittests/parser/scanner-streams-unittest.cc
@@ -784,7 +784,7 @@ TEST_F(ScannerStreamsTest, RelocatingCharacterStream) {
   CHECK_EQ('a', two_byte_string_stream->Advance());
   CHECK_EQ('b', two_byte_string_stream->Advance());
   CHECK_EQ(size_t{2}, two_byte_string_stream->pos());
-  i::String raw = *two_byte_string;
+  i::Tagged<i::String> raw = *two_byte_string;
   // We need to invoke GC without stack, otherwise no compaction is performed.
   i::DisableConservativeStackScanningScopeForTesting no_stack_scanning(
       i_isolate()->heap());
@@ -828,7 +828,7 @@ TEST_F(ScannerStreamsTest, RelocatingUnbufferedCharacterStream) {
   CHECK_EQ('c', two_byte_string_stream->Advance());
   CHECK_EQ(size_t{3}, two_byte_string_stream->pos());
 
-  i::String raw = *two_byte_string;
+  i::Tagged<i::String> raw = *two_byte_string;
   // We need to invoke GC without stack, otherwise no compaction is performed.
   i::DisableConservativeStackScanningScopeForTesting no_stack_scanning(
       i_isolate()->heap());
diff --git a/test/unittests/regexp/regexp-unittest.cc b/test/unittests/regexp/regexp-unittest.cc
index 1d47f823d61..3bc03741a39 100644
--- a/test/unittests/regexp/regexp-unittest.cc
+++ b/test/unittests/regexp/regexp-unittest.cc
@@ -657,11 +657,9 @@ static Handle<JSRegExp> CreateJSRegExp(Handle<String> source, Handle<Code> code,
   return regexp;
 }
 
-static ArchRegExpMacroAssembler::Result Execute(JSRegExp regexp, String input,
-                                                int start_offset,
-                                                Address input_start,
-                                                Address input_end,
-                                                int* captures) {
+static ArchRegExpMacroAssembler::Result Execute(
+    Tagged<JSRegExp> regexp, Tagged<String> input, int start_offset,
+    Address input_start, Address input_end, int* captures) {
   return static_cast<NativeRegExpMacroAssembler::Result>(
       NativeRegExpMacroAssembler::ExecuteForTesting(
           input, start_offset, reinterpret_cast<uint8_t*>(input_start),
@@ -2310,8 +2308,8 @@ TEST_F(RegExpTestWithContext, UnicodePropertyEscapeCodeSize) {
 
   static constexpr int kMaxSize = 200 * KB;
   static constexpr bool kIsNotLatin1 = false;
-  Object maybe_code = re->code(kIsNotLatin1);
-  Object maybe_bytecode = re->bytecode(kIsNotLatin1);
+  Tagged<Object> maybe_code = re->code(kIsNotLatin1);
+  Tagged<Object> maybe_bytecode = re->bytecode(kIsNotLatin1);
   if (IsByteArray(maybe_bytecode)) {
     // On x64, excessive inlining produced >250KB.
     CHECK_LT(ByteArray::cast(maybe_bytecode)->Size(), kMaxSize);
diff --git a/test/unittests/tasks/background-compile-task-unittest.cc b/test/unittests/tasks/background-compile-task-unittest.cc
index 9f49bfa153e..dc9b5696c9c 100644
--- a/test/unittests/tasks/background-compile-task-unittest.cc
+++ b/test/unittests/tasks/background-compile-task-unittest.cc
@@ -111,7 +111,7 @@ TEST_F(BackgroundCompileTaskTest, CompileAndRun) {
       task.get(), isolate(), Compiler::KEEP_EXCEPTION));
   ASSERT_TRUE(shared->is_compiled());
 
-  Smi value = Smi::cast(*RunJS("f(100);"));
+  Tagged<Smi> value = Smi::cast(*RunJS("f(100);"));
   ASSERT_TRUE(value == Smi::FromInt(160));
 }
 
diff --git a/test/unittests/test-utils.h b/test/unittests/test-utils.h
index 36cb143ade1..431536cd0a9 100644
--- a/test/unittests/test-utils.h
+++ b/test/unittests/test-utils.h
@@ -511,10 +511,10 @@ class V8_NODISCARD SaveFlags {
 };
 
 // For GTest.
-inline void PrintTo(Object o, ::std::ostream* os) {
+inline void PrintTo(Tagged<Object> o, ::std::ostream* os) {
   *os << reinterpret_cast<void*>(o.ptr());
 }
-inline void PrintTo(Smi o, ::std::ostream* os) {
+inline void PrintTo(Tagged<Smi> o, ::std::ostream* os) {
   *os << reinterpret_cast<void*>(o.ptr());
 }
 
@@ -527,7 +527,7 @@ static inline uint16_t* AsciiToTwoByteString(const char* source) {
 
 class TestTransitionsAccessor : public TransitionsAccessor {
  public:
-  TestTransitionsAccessor(Isolate* isolate, Map map)
+  TestTransitionsAccessor(Isolate* isolate, Tagged<Map> map)
       : TransitionsAccessor(isolate, map) {}
   TestTransitionsAccessor(Isolate* isolate, Handle<Map> map)
       : TransitionsAccessor(isolate, *map) {}
@@ -542,7 +542,9 @@ class TestTransitionsAccessor : public TransitionsAccessor {
 
   int Capacity() { return TransitionsAccessor::Capacity(); }
 
-  TransitionArray transitions() { return TransitionsAccessor::transitions(); }
+  Tagged<TransitionArray> transitions() {
+    return TransitionsAccessor::transitions();
+  }
 };
 
 // Helper class that allows to write tests in a slot size independent manner.
@@ -583,14 +585,15 @@ class FakeCodeEventLogger : public i::CodeEventLogger {
   explicit FakeCodeEventLogger(i::Isolate* isolate)
       : CodeEventLogger(isolate) {}
 
-  void CodeMoveEvent(i::InstructionStream from,
-                     i::InstructionStream to) override {}
-  void BytecodeMoveEvent(i::BytecodeArray from, i::BytecodeArray to) override {}
+  void CodeMoveEvent(i::Tagged<i::InstructionStream> from,
+                     i::Tagged<i::InstructionStream> to) override {}
+  void BytecodeMoveEvent(i::Tagged<i::BytecodeArray> from,
+                         i::Tagged<i::BytecodeArray> to) override {}
   void CodeDisableOptEvent(i::Handle<i::AbstractCode> code,
                            i::Handle<i::SharedFunctionInfo> shared) override {}
 
  private:
-  void LogRecordedBuffer(i::AbstractCode code,
+  void LogRecordedBuffer(i::Tagged<i::AbstractCode> code,
                          i::MaybeHandle<i::SharedFunctionInfo> maybe_shared,
                          const char* name, int length) override {}
 #if V8_ENABLE_WEBASSEMBLY
diff --git a/tools/debug_helper/debug-macro-shims.h b/tools/debug_helper/debug-macro-shims.h
index 02deb3d766f..4bd1dc7bb6d 100644
--- a/tools/debug_helper/debug-macro-shims.h
+++ b/tools/debug_helper/debug-macro-shims.h
@@ -79,7 +79,7 @@ inline Value<intptr_t> Signed(d::MemoryAccessor accessor, uintptr_t u) {
   return {d::MemoryAccessResult::kOk, static_cast<intptr_t>(u)};
 }
 inline Value<int32_t> SmiUntag(d::MemoryAccessor accessor, uintptr_t s_t) {
-  Smi s(s_t);
+  Tagged<Smi> s(s_t);
   return {d::MemoryAccessResult::kOk, s.value()};
 }
 inline Value<uintptr_t> SmiFromInt32(d::MemoryAccessor accessor, int32_t i) {
-- 
2.35.1

