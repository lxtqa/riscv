From 65a226a63fd7c10e761ae82b00b8490b879a8fae Mon Sep 17 00:00:00 2001
From: Nico Hartmann <nicohartmann@chromium.org>
Date: Mon, 12 Jun 2023 12:47:05 +0200
Subject: [PATCH] [compiler] Add Adapter template argument to
 InstructionSelector

This is a preparation to generalize instruction selection to handle
both Turbofan sea of nodes as well as Turboshaft cfg as inputs.

Also adapt the following implementations:
 - X64InstructionSelector
 - IA32InstructionSelector
 - ArmInstructionSelector
 - Arm64InstructionSelector

Bug: v8:12783
Change-Id: I692bc86b3fca0c1506130934fb9de8521650791c
Reviewed-on: https://chromium-review.googlesource.com/c/v8/v8/+/4575071
Commit-Queue: Nico Hartmann <nicohartmann@chromium.org>
Reviewed-by: Darius Mercadier <dmercadier@chromium.org>
Cr-Commit-Position: refs/heads/main@{#88196}
---
 .../backend/arm/instruction-selector-arm.cc   | 1028 +++++++++------
 .../arm64/instruction-selector-arm64.cc       | 1156 ++++++++++-------
 .../backend/ia32/instruction-selector-ia32.cc |  994 ++++++++------
 .../backend/instruction-selector-impl.h       |   23 +-
 src/compiler/backend/instruction-selector.cc  |  755 +++++++----
 src/compiler/backend/instruction-selector.h   |   27 +-
 .../backend/x64/instruction-selector-x64.cc   | 1110 ++++++++++------
 src/compiler/pipeline.cc                      |   71 +-
 src/flags/flag-definitions.h                  |    3 +
 9 files changed, 3263 insertions(+), 1904 deletions(-)

diff --git a/src/compiler/backend/arm/instruction-selector-arm.cc b/src/compiler/backend/arm/instruction-selector-arm.cc
index 1a6b14a2d16..7fd1224818a 100644
--- a/src/compiler/backend/arm/instruction-selector-arm.cc
+++ b/src/compiler/backend/arm/instruction-selector-arm.cc
@@ -15,10 +15,13 @@ namespace internal {
 namespace compiler {
 
 // Adds Arm-specific methods for generating InstructionOperands.
-class ArmOperandGenerator : public OperandGenerator {
+template <typename Adapter>
+class ArmOperandGeneratorT : public OperandGeneratorT<Adapter> {
  public:
-  explicit ArmOperandGenerator(InstructionSelector* selector)
-      : OperandGenerator(selector) {}
+  OPERAND_GENERATOR_T_BOILERPLATE(Adapter)
+
+  explicit ArmOperandGeneratorT(InstructionSelectorT<Adapter>* selector)
+      : super(selector) {}
 
   bool CanBeImmediate(int32_t value) const {
     return Assembler::ImmediateFitsAddrMode1Instruction(value);
@@ -79,24 +82,27 @@ class ArmOperandGenerator : public OperandGenerator {
 
 namespace {
 
-void VisitRR(InstructionSelector* selector, InstructionCode opcode,
+template <typename Adapter>
+void VisitRR(InstructionSelectorT<Adapter>* selector, InstructionCode opcode,
              Node* node) {
-  ArmOperandGenerator g(selector);
+  ArmOperandGeneratorT<Adapter> g(selector);
   selector->Emit(opcode, g.DefineAsRegister(node),
                  g.UseRegister(node->InputAt(0)));
 }
 
-void VisitRRR(InstructionSelector* selector, InstructionCode opcode,
+template <typename Adapter>
+void VisitRRR(InstructionSelectorT<Adapter>* selector, InstructionCode opcode,
               Node* node) {
-  ArmOperandGenerator g(selector);
+  ArmOperandGeneratorT<Adapter> g(selector);
   selector->Emit(opcode, g.DefineAsRegister(node),
                  g.UseRegister(node->InputAt(0)),
                  g.UseRegister(node->InputAt(1)));
 }
 
-void VisitSimdShiftRRR(InstructionSelector* selector, ArchOpcode opcode,
-                       Node* node, int width) {
-  ArmOperandGenerator g(selector);
+template <typename Adapter>
+void VisitSimdShiftRRR(InstructionSelectorT<Adapter>* selector,
+                       ArchOpcode opcode, Node* node, int width) {
+  ArmOperandGeneratorT<Adapter> g(selector);
   Int32Matcher m(node->InputAt(1));
   if (m.HasResolvedValue()) {
     if (m.IsMultipleOf(width)) {
@@ -112,9 +118,10 @@ void VisitSimdShiftRRR(InstructionSelector* selector, ArchOpcode opcode,
 }
 
 #if V8_ENABLE_WEBASSEMBLY
-void VisitRRRShuffle(InstructionSelector* selector, ArchOpcode opcode,
+template <typename Adapter>
+void VisitRRRShuffle(InstructionSelectorT<Adapter>* selector, ArchOpcode opcode,
                      Node* node) {
-  ArmOperandGenerator g(selector);
+  ArmOperandGeneratorT<Adapter> g(selector);
   // Swap inputs to save an instruction in the CodeGenerator for High ops.
   if (opcode == kArmS32x4ZipRight || opcode == kArmS32x4UnzipRight ||
       opcode == kArmS32x4TransposeRight || opcode == kArmS16x8ZipRight ||
@@ -134,28 +141,32 @@ void VisitRRRShuffle(InstructionSelector* selector, ArchOpcode opcode,
 }
 #endif  // V8_ENABLE_WEBASSEMBLY
 
-void VisitRRI(InstructionSelector* selector, ArchOpcode opcode, Node* node) {
-  ArmOperandGenerator g(selector);
+template <typename Adapter>
+void VisitRRI(InstructionSelectorT<Adapter>* selector, ArchOpcode opcode,
+              Node* node) {
+  ArmOperandGeneratorT<Adapter> g(selector);
   int32_t imm = OpParameter<int32_t>(node->op());
   selector->Emit(opcode, g.DefineAsRegister(node),
                  g.UseRegister(node->InputAt(0)), g.UseImmediate(imm));
 }
 
-void VisitRRIR(InstructionSelector* selector, ArchOpcode opcode, Node* node) {
-  ArmOperandGenerator g(selector);
+template <typename Adapter>
+void VisitRRIR(InstructionSelectorT<Adapter>* selector, ArchOpcode opcode,
+               Node* node) {
+  ArmOperandGeneratorT<Adapter> g(selector);
   int32_t imm = OpParameter<int32_t>(node->op());
   selector->Emit(opcode, g.DefineAsRegister(node),
                  g.UseRegister(node->InputAt(0)), g.UseImmediate(imm),
                  g.UseUniqueRegister(node->InputAt(1)));
 }
 
-template <IrOpcode::Value kOpcode, int kImmMin, int kImmMax,
+template <typename Adapter, IrOpcode::Value kOpcode, int kImmMin, int kImmMax,
           AddressingMode kImmMode, AddressingMode kRegMode>
-bool TryMatchShift(InstructionSelector* selector,
+bool TryMatchShift(InstructionSelectorT<Adapter>* selector,
                    InstructionCode* opcode_return, Node* node,
                    InstructionOperand* value_return,
                    InstructionOperand* shift_return) {
-  ArmOperandGenerator g(selector);
+  ArmOperandGeneratorT<Adapter> g(selector);
   if (node->opcode() == kOpcode) {
     Int32BinopMatcher m(node);
     *value_return = g.UseRegister(m.left().node());
@@ -171,13 +182,13 @@ bool TryMatchShift(InstructionSelector* selector,
   return false;
 }
 
-template <IrOpcode::Value kOpcode, int kImmMin, int kImmMax,
+template <typename Adapter, IrOpcode::Value kOpcode, int kImmMin, int kImmMax,
           AddressingMode kImmMode>
-bool TryMatchShiftImmediate(InstructionSelector* selector,
+bool TryMatchShiftImmediate(InstructionSelectorT<Adapter>* selector,
                             InstructionCode* opcode_return, Node* node,
                             InstructionOperand* value_return,
                             InstructionOperand* shift_return) {
-  ArmOperandGenerator g(selector);
+  ArmOperandGeneratorT<Adapter> g(selector);
   if (node->opcode() == kOpcode) {
     Int32BinopMatcher m(node);
     if (m.right().IsInRange(kImmMin, kImmMax)) {
@@ -190,48 +201,58 @@ bool TryMatchShiftImmediate(InstructionSelector* selector,
   return false;
 }
 
-bool TryMatchROR(InstructionSelector* selector, InstructionCode* opcode_return,
-                 Node* node, InstructionOperand* value_return,
+template <typename Adapter>
+bool TryMatchROR(InstructionSelectorT<Adapter>* selector,
+                 InstructionCode* opcode_return, Node* node,
+                 InstructionOperand* value_return,
                  InstructionOperand* shift_return) {
-  return TryMatchShift<IrOpcode::kWord32Ror, 1, 31, kMode_Operand2_R_ROR_I,
-                       kMode_Operand2_R_ROR_R>(selector, opcode_return, node,
-                                               value_return, shift_return);
+  return TryMatchShift<Adapter, IrOpcode::kWord32Ror, 1, 31,
+                       kMode_Operand2_R_ROR_I, kMode_Operand2_R_ROR_R>(
+      selector, opcode_return, node, value_return, shift_return);
 }
 
-bool TryMatchASR(InstructionSelector* selector, InstructionCode* opcode_return,
-                 Node* node, InstructionOperand* value_return,
+template <typename Adapter>
+bool TryMatchASR(InstructionSelectorT<Adapter>* selector,
+                 InstructionCode* opcode_return, Node* node,
+                 InstructionOperand* value_return,
                  InstructionOperand* shift_return) {
-  return TryMatchShift<IrOpcode::kWord32Sar, 1, 32, kMode_Operand2_R_ASR_I,
-                       kMode_Operand2_R_ASR_R>(selector, opcode_return, node,
-                                               value_return, shift_return);
+  return TryMatchShift<Adapter, IrOpcode::kWord32Sar, 1, 32,
+                       kMode_Operand2_R_ASR_I, kMode_Operand2_R_ASR_R>(
+      selector, opcode_return, node, value_return, shift_return);
 }
 
-bool TryMatchLSL(InstructionSelector* selector, InstructionCode* opcode_return,
-                 Node* node, InstructionOperand* value_return,
+template <typename Adapter>
+bool TryMatchLSL(InstructionSelectorT<Adapter>* selector,
+                 InstructionCode* opcode_return, Node* node,
+                 InstructionOperand* value_return,
                  InstructionOperand* shift_return) {
-  return TryMatchShift<IrOpcode::kWord32Shl, 0, 31, kMode_Operand2_R_LSL_I,
-                       kMode_Operand2_R_LSL_R>(selector, opcode_return, node,
-                                               value_return, shift_return);
+  return TryMatchShift<Adapter, IrOpcode::kWord32Shl, 0, 31,
+                       kMode_Operand2_R_LSL_I, kMode_Operand2_R_LSL_R>(
+      selector, opcode_return, node, value_return, shift_return);
 }
 
-bool TryMatchLSLImmediate(InstructionSelector* selector,
+template <typename Adapter>
+bool TryMatchLSLImmediate(InstructionSelectorT<Adapter>* selector,
                           InstructionCode* opcode_return, Node* node,
                           InstructionOperand* value_return,
                           InstructionOperand* shift_return) {
-  return TryMatchShiftImmediate<IrOpcode::kWord32Shl, 0, 31,
+  return TryMatchShiftImmediate<Adapter, IrOpcode::kWord32Shl, 0, 31,
                                 kMode_Operand2_R_LSL_I>(
       selector, opcode_return, node, value_return, shift_return);
 }
 
-bool TryMatchLSR(InstructionSelector* selector, InstructionCode* opcode_return,
-                 Node* node, InstructionOperand* value_return,
+template <typename Adapter>
+bool TryMatchLSR(InstructionSelectorT<Adapter>* selector,
+                 InstructionCode* opcode_return, Node* node,
+                 InstructionOperand* value_return,
                  InstructionOperand* shift_return) {
-  return TryMatchShift<IrOpcode::kWord32Shr, 1, 32, kMode_Operand2_R_LSR_I,
-                       kMode_Operand2_R_LSR_R>(selector, opcode_return, node,
-                                               value_return, shift_return);
+  return TryMatchShift<Adapter, IrOpcode::kWord32Shr, 1, 32,
+                       kMode_Operand2_R_LSR_I, kMode_Operand2_R_LSR_R>(
+      selector, opcode_return, node, value_return, shift_return);
 }
 
-bool TryMatchShift(InstructionSelector* selector,
+template <typename Adapter>
+bool TryMatchShift(InstructionSelectorT<Adapter>* selector,
                    InstructionCode* opcode_return, Node* node,
                    InstructionOperand* value_return,
                    InstructionOperand* shift_return) {
@@ -242,11 +263,12 @@ bool TryMatchShift(InstructionSelector* selector,
       TryMatchROR(selector, opcode_return, node, value_return, shift_return));
 }
 
-bool TryMatchImmediateOrShift(InstructionSelector* selector,
+template <typename Adapter>
+bool TryMatchImmediateOrShift(InstructionSelectorT<Adapter>* selector,
                               InstructionCode* opcode_return, Node* node,
                               size_t* input_count_return,
                               InstructionOperand* inputs) {
-  ArmOperandGenerator g(selector);
+  ArmOperandGeneratorT<Adapter> g(selector);
   if (g.CanBeImmediate(node, *opcode_return)) {
     *opcode_return |= AddressingModeField::encode(kMode_Operand2_I);
     inputs[0] = g.UseImmediate(node);
@@ -260,10 +282,11 @@ bool TryMatchImmediateOrShift(InstructionSelector* selector,
   return false;
 }
 
-void VisitBinop(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitBinop(InstructionSelectorT<Adapter>* selector, Node* node,
                 InstructionCode opcode, InstructionCode reverse_opcode,
                 FlagsContinuation* cont) {
-  ArmOperandGenerator g(selector);
+  ArmOperandGeneratorT<Adapter> g(selector);
   Int32BinopMatcher m(node);
   InstructionOperand inputs[3];
   size_t input_count = 0;
@@ -310,17 +333,19 @@ void VisitBinop(InstructionSelector* selector, Node* node,
                                  inputs, cont);
 }
 
-void VisitBinop(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitBinop(InstructionSelectorT<Adapter>* selector, Node* node,
                 InstructionCode opcode, InstructionCode reverse_opcode) {
   FlagsContinuation cont;
   VisitBinop(selector, node, opcode, reverse_opcode, &cont);
 }
 
-void EmitDiv(InstructionSelector* selector, ArchOpcode div_opcode,
+template <typename Adapter>
+void EmitDiv(InstructionSelectorT<Adapter>* selector, ArchOpcode div_opcode,
              ArchOpcode f64i32_opcode, ArchOpcode i32f64_opcode,
              InstructionOperand result_operand, InstructionOperand left_operand,
              InstructionOperand right_operand) {
-  ArmOperandGenerator g(selector);
+  ArmOperandGeneratorT<Adapter> g(selector);
   if (selector->IsSupported(SUDIV)) {
     selector->Emit(div_opcode, result_operand, left_operand, right_operand);
     return;
@@ -335,18 +360,22 @@ void EmitDiv(InstructionSelector* selector, ArchOpcode div_opcode,
   selector->Emit(i32f64_opcode, result_operand, result_double_operand);
 }
 
-void VisitDiv(InstructionSelector* selector, Node* node, ArchOpcode div_opcode,
-              ArchOpcode f64i32_opcode, ArchOpcode i32f64_opcode) {
-  ArmOperandGenerator g(selector);
+template <typename Adapter>
+void VisitDiv(InstructionSelectorT<Adapter>* selector, Node* node,
+              ArchOpcode div_opcode, ArchOpcode f64i32_opcode,
+              ArchOpcode i32f64_opcode) {
+  ArmOperandGeneratorT<Adapter> g(selector);
   Int32BinopMatcher m(node);
   EmitDiv(selector, div_opcode, f64i32_opcode, i32f64_opcode,
           g.DefineAsRegister(node), g.UseRegister(m.left().node()),
           g.UseRegister(m.right().node()));
 }
 
-void VisitMod(InstructionSelector* selector, Node* node, ArchOpcode div_opcode,
-              ArchOpcode f64i32_opcode, ArchOpcode i32f64_opcode) {
-  ArmOperandGenerator g(selector);
+template <typename Adapter>
+void VisitMod(InstructionSelectorT<Adapter>* selector, Node* node,
+              ArchOpcode div_opcode, ArchOpcode f64i32_opcode,
+              ArchOpcode i32f64_opcode) {
+  ArmOperandGeneratorT<Adapter> g(selector);
   Int32BinopMatcher m(node);
   InstructionOperand div_operand = g.TempRegister();
   InstructionOperand result_operand = g.DefineAsRegister(node);
@@ -370,11 +399,12 @@ void VisitMod(InstructionSelector* selector, Node* node, ArchOpcode div_opcode,
 // vld1 and vst1, when given two registers, will post-increment the offset, i.e.
 // perform the operation at base, then add offset to base. What we intend is to
 // access at (base+offset).
-void EmitAddBeforeS128LoadStore(InstructionSelector* selector,
+template <typename Adapter>
+void EmitAddBeforeS128LoadStore(InstructionSelectorT<Adapter>* selector,
                                 InstructionCode* opcode_return,
                                 size_t* input_count_return,
                                 InstructionOperand* inputs) {
-  ArmOperandGenerator g(selector);
+  ArmOperandGeneratorT<Adapter> g(selector);
   InstructionOperand addr = g.TempRegister();
   InstructionCode op = kArmAdd;
   op |= AddressingModeField::encode(kMode_Operand2_R);
@@ -384,9 +414,10 @@ void EmitAddBeforeS128LoadStore(InstructionSelector* selector,
   inputs[0] = addr;
 }
 
-void EmitLoad(InstructionSelector* selector, InstructionCode opcode,
+template <typename Adapter>
+void EmitLoad(InstructionSelectorT<Adapter>* selector, InstructionCode opcode,
               InstructionOperand* output, Node* base, Node* index) {
-  ArmOperandGenerator g(selector);
+  ArmOperandGeneratorT<Adapter> g(selector);
   InstructionOperand inputs[3];
   size_t input_count = 2;
 
@@ -435,9 +466,10 @@ void EmitLoad(InstructionSelector* selector, InstructionCode opcode,
   selector->Emit(opcode, 1, output, input_count, inputs);
 }
 
-void EmitStore(InstructionSelector* selector, InstructionCode opcode,
+template <typename Adapter>
+void EmitStore(InstructionSelectorT<Adapter>* selector, InstructionCode opcode,
                size_t input_count, InstructionOperand* inputs, Node* index) {
-  ArmOperandGenerator g(selector);
+  ArmOperandGeneratorT<Adapter> g(selector);
   ArchOpcode arch_opcode = ArchOpcodeField::decode(opcode);
 
   if (g.CanBeImmediate(index, opcode)) {
@@ -459,9 +491,10 @@ void EmitStore(InstructionSelector* selector, InstructionCode opcode,
   selector->Emit(opcode, 0, nullptr, input_count, inputs);
 }
 
-void VisitPairAtomicBinOp(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitPairAtomicBinOp(InstructionSelectorT<Adapter>* selector, Node* node,
                           ArchOpcode opcode) {
-  ArmOperandGenerator g(selector);
+  ArmOperandGeneratorT<Adapter> g(selector);
   Node* base = node->InputAt(0);
   Node* index = node->InputAt(1);
   Node* value = node->InputAt(2);
@@ -497,7 +530,8 @@ void VisitPairAtomicBinOp(InstructionSelector* selector, Node* node,
 
 }  // namespace
 
-void InstructionSelector::VisitStackSlot(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitStackSlot(Node* node) {
   StackSlotRepresentation rep = StackSlotRepresentationOf(node->op());
   int slot = frame_->AllocateSpillSlot(rep.size(), rep.alignment());
   OperandGenerator g(this);
@@ -506,19 +540,21 @@ void InstructionSelector::VisitStackSlot(Node* node) {
        sequence()->AddImmediate(Constant(slot)), 0, nullptr);
 }
 
-void InstructionSelector::VisitAbortCSADcheck(Node* node) {
-  ArmOperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitAbortCSADcheck(Node* node) {
+  ArmOperandGeneratorT<Adapter> g(this);
   Emit(kArchAbortCSADcheck, g.NoOutput(), g.UseFixed(node->InputAt(0), r1));
 }
 
-void InstructionSelector::VisitStoreLane(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitStoreLane(Node* node) {
   StoreLaneParameters params = StoreLaneParametersOf(node->op());
   LoadStoreLaneParams f(params.rep, params.laneidx);
   InstructionCode opcode =
       f.low_op ? kArmS128StoreLaneLow : kArmS128StoreLaneHigh;
   opcode |= MiscField::encode(f.sz);
 
-  ArmOperandGenerator g(this);
+  ArmOperandGeneratorT<Adapter> g(this);
   InstructionOperand inputs[4];
   size_t input_count = 4;
   inputs[0] = g.UseRegister(node->InputAt(2));
@@ -529,14 +565,15 @@ void InstructionSelector::VisitStoreLane(Node* node) {
   Emit(opcode, 0, nullptr, input_count, inputs);
 }
 
-void InstructionSelector::VisitLoadLane(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitLoadLane(Node* node) {
   LoadLaneParameters params = LoadLaneParametersOf(node->op());
   LoadStoreLaneParams f(params.rep.representation(), params.laneidx);
   InstructionCode opcode =
       f.low_op ? kArmS128LoadLaneLow : kArmS128LoadLaneHigh;
   opcode |= MiscField::encode(f.sz);
 
-  ArmOperandGenerator g(this);
+  ArmOperandGeneratorT<Adapter> g(this);
   InstructionOperand output = g.DefineSameAsFirst(node);
   InstructionOperand inputs[4];
   size_t input_count = 4;
@@ -548,7 +585,8 @@ void InstructionSelector::VisitLoadLane(Node* node) {
   Emit(opcode, 1, &output, input_count, inputs);
 }
 
-void InstructionSelector::VisitLoadTransform(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitLoadTransform(Node* node) {
   LoadTransformParameters params = LoadTransformParametersOf(node->op());
   InstructionCode opcode = kArchNop;
   switch (params.transformation) {
@@ -592,7 +630,7 @@ void InstructionSelector::VisitLoadTransform(Node* node) {
       UNIMPLEMENTED();
   }
 
-  ArmOperandGenerator g(this);
+  ArmOperandGeneratorT<Adapter> g(this);
   InstructionOperand output = g.DefineAsRegister(node);
   InstructionOperand inputs[2];
   size_t input_count = 2;
@@ -602,9 +640,10 @@ void InstructionSelector::VisitLoadTransform(Node* node) {
   Emit(opcode, 1, &output, input_count, inputs);
 }
 
-void InstructionSelector::VisitLoad(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitLoad(Node* node) {
   LoadRepresentation load_rep = LoadRepresentationOf(node->op());
-  ArmOperandGenerator g(this);
+  ArmOperandGeneratorT<Adapter> g(this);
   Node* base = node->InputAt(0);
   Node* index = node->InputAt(1);
 
@@ -646,7 +685,8 @@ void InstructionSelector::VisitLoad(Node* node) {
   EmitLoad(this, opcode, &output, base, index);
 }
 
-void InstructionSelector::VisitProtectedLoad(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitProtectedLoad(Node* node) {
   // TODO(eholk)
   UNIMPLEMENTED();
 }
@@ -698,10 +738,11 @@ ArchOpcode GetAtomicStoreOpcode(MachineRepresentation rep) {
   }
 }
 
-void VisitStoreCommon(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitStoreCommon(InstructionSelectorT<Adapter>* selector, Node* node,
                       StoreRepresentation store_rep,
                       base::Optional<AtomicMemoryOrder> atomic_order) {
-  ArmOperandGenerator g(selector);
+  ArmOperandGeneratorT<Adapter> g(selector);
   Node* base = node->InputAt(0);
   Node* index = node->InputAt(1);
   Node* value = node->InputAt(2);
@@ -794,22 +835,28 @@ void VisitStoreCommon(InstructionSelector* selector, Node* node,
 
 }  // namespace
 
-void InstructionSelector::VisitStorePair(Node* node) { UNREACHABLE(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitStorePair(Node* node) {
+  UNREACHABLE();
+}
 
-void InstructionSelector::VisitStore(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitStore(Node* node) {
   VisitStoreCommon(this, node, StoreRepresentationOf(node->op()),
                    base::nullopt);
 }
 
-void InstructionSelector::VisitProtectedStore(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitProtectedStore(Node* node) {
   // TODO(eholk)
   UNIMPLEMENTED();
 }
 
-void InstructionSelector::VisitUnalignedLoad(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitUnalignedLoad(Node* node) {
   MachineRepresentation load_rep =
       LoadRepresentationOf(node->op()).representation();
-  ArmOperandGenerator g(this);
+  ArmOperandGeneratorT<Adapter> g(this);
   Node* base = node->InputAt(0);
   Node* index = node->InputAt(1);
 
@@ -871,8 +918,9 @@ void InstructionSelector::VisitUnalignedLoad(Node* node) {
   }
 }
 
-void InstructionSelector::VisitUnalignedStore(Node* node) {
-  ArmOperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitUnalignedStore(Node* node) {
+  ArmOperandGeneratorT<Adapter> g(this);
   Node* base = node->InputAt(0);
   Node* index = node->InputAt(1);
   Node* value = node->InputAt(2);
@@ -959,9 +1007,10 @@ void InstructionSelector::VisitUnalignedStore(Node* node) {
 
 namespace {
 
-void EmitBic(InstructionSelector* selector, Node* node, Node* left,
+template <typename Adapter>
+void EmitBic(InstructionSelectorT<Adapter>* selector, Node* node, Node* left,
              Node* right) {
-  ArmOperandGenerator g(selector);
+  ArmOperandGeneratorT<Adapter> g(selector);
   InstructionCode opcode = kArmBic;
   InstructionOperand value_operand;
   InstructionOperand shift_operand;
@@ -975,20 +1024,22 @@ void EmitBic(InstructionSelector* selector, Node* node, Node* left,
                  g.UseRegister(right));
 }
 
-void EmitUbfx(InstructionSelector* selector, Node* node, Node* left,
+template <typename Adapter>
+void EmitUbfx(InstructionSelectorT<Adapter>* selector, Node* node, Node* left,
               uint32_t lsb, uint32_t width) {
   DCHECK_LE(lsb, 31u);
   DCHECK_LE(1u, width);
   DCHECK_LE(width, 32u - lsb);
-  ArmOperandGenerator g(selector);
+  ArmOperandGeneratorT<Adapter> g(selector);
   selector->Emit(kArmUbfx, g.DefineAsRegister(node), g.UseRegister(left),
                  g.TempImmediate(lsb), g.TempImmediate(width));
 }
 
 }  // namespace
 
-void InstructionSelector::VisitWord32And(Node* node) {
-  ArmOperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32And(Node* node) {
+  ArmOperandGeneratorT<Adapter> g(this);
   Int32BinopMatcher m(node);
   if (m.left().IsWord32Xor() && CanCover(node, m.left().node())) {
     Int32BinopMatcher mleft(m.left().node());
@@ -1084,12 +1135,14 @@ void InstructionSelector::VisitWord32And(Node* node) {
   VisitBinop(this, node, kArmAnd, kArmAnd);
 }
 
-void InstructionSelector::VisitWord32Or(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32Or(Node* node) {
   VisitBinop(this, node, kArmOrr, kArmOrr);
 }
 
-void InstructionSelector::VisitWord32Xor(Node* node) {
-  ArmOperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32Xor(Node* node) {
+  ArmOperandGeneratorT<Adapter> g(this);
   Int32BinopMatcher m(node);
   if (m.right().Is(-1)) {
     InstructionCode opcode = kArmMvn;
@@ -1107,13 +1160,14 @@ void InstructionSelector::VisitWord32Xor(Node* node) {
   VisitBinop(this, node, kArmEor, kArmEor);
 }
 
-void InstructionSelector::VisitStackPointerGreaterThan(
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitStackPointerGreaterThan(
     Node* node, FlagsContinuation* cont) {
   StackCheckKind kind = StackCheckKindOf(node->op());
   InstructionCode opcode =
       kArchStackPointerGreaterThan | MiscField::encode(static_cast<int>(kind));
 
-  ArmOperandGenerator g(this);
+  ArmOperandGeneratorT<Adapter> g(this);
 
   // No outputs.
   InstructionOperand* const outputs = nullptr;
@@ -1138,10 +1192,10 @@ void InstructionSelector::VisitStackPointerGreaterThan(
 
 namespace {
 
-template <typename TryMatchShift>
-void VisitShift(InstructionSelector* selector, Node* node,
+template <typename Adapter, typename TryMatchShift>
+void VisitShift(InstructionSelectorT<Adapter>* selector, Node* node,
                 TryMatchShift try_match_shift, FlagsContinuation* cont) {
-  ArmOperandGenerator g(selector);
+  ArmOperandGeneratorT<Adapter> g(selector);
   InstructionCode opcode = kArmMov;
   InstructionOperand inputs[2];
   size_t input_count = 2;
@@ -1162,8 +1216,8 @@ void VisitShift(InstructionSelector* selector, Node* node,
                                  inputs, cont);
 }
 
-template <typename TryMatchShift>
-void VisitShift(InstructionSelector* selector, Node* node,
+template <typename Adapter, typename TryMatchShift>
+void VisitShift(InstructionSelectorT<Adapter>* selector, Node* node,
                 TryMatchShift try_match_shift) {
   FlagsContinuation cont;
   VisitShift(selector, node, try_match_shift, &cont);
@@ -1171,12 +1225,14 @@ void VisitShift(InstructionSelector* selector, Node* node,
 
 }  // namespace
 
-void InstructionSelector::VisitWord32Shl(Node* node) {
-  VisitShift(this, node, TryMatchLSL);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32Shl(Node* node) {
+  VisitShift(this, node, TryMatchLSL<Adapter>);
 }
 
-void InstructionSelector::VisitWord32Shr(Node* node) {
-  ArmOperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32Shr(Node* node) {
+  ArmOperandGeneratorT<Adapter> g(this);
   Int32BinopMatcher m(node);
   if (IsSupported(ARMv7) && m.left().IsWord32And() &&
       m.right().IsInRange(0, 31)) {
@@ -1193,11 +1249,12 @@ void InstructionSelector::VisitWord32Shr(Node* node) {
       }
     }
   }
-  VisitShift(this, node, TryMatchLSR);
+  VisitShift(this, node, TryMatchLSR<Adapter>);
 }
 
-void InstructionSelector::VisitWord32Sar(Node* node) {
-  ArmOperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32Sar(Node* node) {
+  ArmOperandGeneratorT<Adapter> g(this);
   Int32BinopMatcher m(node);
   if (CanCover(m.node(), m.left().node()) && m.left().IsWord32Shl()) {
     Int32BinopMatcher mleft(m.left().node());
@@ -1220,11 +1277,12 @@ void InstructionSelector::VisitWord32Sar(Node* node) {
       }
     }
   }
-  VisitShift(this, node, TryMatchASR);
+  VisitShift(this, node, TryMatchASR<Adapter>);
 }
 
-void InstructionSelector::VisitInt32PairAdd(Node* node) {
-  ArmOperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32PairAdd(Node* node) {
+  ArmOperandGeneratorT<Adapter> g(this);
 
   Node* projection1 = NodeProperties::FindProjection(node, 1);
   if (projection1) {
@@ -1248,8 +1306,9 @@ void InstructionSelector::VisitInt32PairAdd(Node* node) {
   }
 }
 
-void InstructionSelector::VisitInt32PairSub(Node* node) {
-  ArmOperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32PairSub(Node* node) {
+  ArmOperandGeneratorT<Adapter> g(this);
 
   Node* projection1 = NodeProperties::FindProjection(node, 1);
   if (projection1) {
@@ -1273,8 +1332,9 @@ void InstructionSelector::VisitInt32PairSub(Node* node) {
   }
 }
 
-void InstructionSelector::VisitInt32PairMul(Node* node) {
-  ArmOperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32PairMul(Node* node) {
+  ArmOperandGeneratorT<Adapter> g(this);
   Node* projection1 = NodeProperties::FindProjection(node, 1);
   if (projection1) {
     InstructionOperand inputs[] = {g.UseUniqueRegister(node->InputAt(0)),
@@ -1298,9 +1358,10 @@ void InstructionSelector::VisitInt32PairMul(Node* node) {
 
 namespace {
 // Shared routine for multiple shift operations.
-void VisitWord32PairShift(InstructionSelector* selector, InstructionCode opcode,
-                          Node* node) {
-  ArmOperandGenerator g(selector);
+template <typename Adapter>
+void VisitWord32PairShift(InstructionSelectorT<Adapter>* selector,
+                          InstructionCode opcode, Node* node) {
+  ArmOperandGeneratorT<Adapter> g(selector);
   // We use g.UseUniqueRegister here to guarantee that there is
   // no register aliasing of input registers with output registers.
   Int32Matcher m(node->InputAt(2));
@@ -1332,45 +1393,65 @@ void VisitWord32PairShift(InstructionSelector* selector, InstructionCode opcode,
   selector->Emit(opcode, output_count, outputs, 3, inputs, temp_count, temps);
 }
 }  // namespace
-void InstructionSelector::VisitWord32PairShl(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32PairShl(Node* node) {
   VisitWord32PairShift(this, kArmLslPair, node);
 }
 
-void InstructionSelector::VisitWord32PairShr(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32PairShr(Node* node) {
   VisitWord32PairShift(this, kArmLsrPair, node);
 }
 
-void InstructionSelector::VisitWord32PairSar(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32PairSar(Node* node) {
   VisitWord32PairShift(this, kArmAsrPair, node);
 }
 
-void InstructionSelector::VisitWord32Rol(Node* node) { UNREACHABLE(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32Rol(Node* node) {
+  UNREACHABLE();
+}
 
-void InstructionSelector::VisitWord32Ror(Node* node) {
-  VisitShift(this, node, TryMatchROR);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32Ror(Node* node) {
+  VisitShift(this, node, TryMatchROR<Adapter>);
 }
 
-void InstructionSelector::VisitWord32Ctz(Node* node) { UNREACHABLE(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32Ctz(Node* node) {
+  UNREACHABLE();
+}
 
-void InstructionSelector::VisitWord32ReverseBits(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32ReverseBits(Node* node) {
   DCHECK(IsSupported(ARMv7));
   VisitRR(this, kArmRbit, node);
 }
 
-void InstructionSelector::VisitWord64ReverseBytes(Node* node) { UNREACHABLE(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64ReverseBytes(Node* node) {
+  UNREACHABLE();
+}
 
-void InstructionSelector::VisitWord32ReverseBytes(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32ReverseBytes(Node* node) {
   VisitRR(this, kArmRev, node);
 }
 
-void InstructionSelector::VisitSimd128ReverseBytes(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitSimd128ReverseBytes(Node* node) {
   UNREACHABLE();
 }
 
-void InstructionSelector::VisitWord32Popcnt(Node* node) { UNREACHABLE(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32Popcnt(Node* node) {
+  UNREACHABLE();
+}
 
-void InstructionSelector::VisitInt32Add(Node* node) {
-  ArmOperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32Add(Node* node) {
+  ArmOperandGeneratorT<Adapter> g(this);
   Int32BinopMatcher m(node);
   if (CanCover(node, m.left().node())) {
     switch (m.left().opcode()) {
@@ -1487,8 +1568,9 @@ void InstructionSelector::VisitInt32Add(Node* node) {
   VisitBinop(this, node, kArmAdd, kArmAdd);
 }
 
-void InstructionSelector::VisitInt32Sub(Node* node) {
-  ArmOperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32Sub(Node* node) {
+  ArmOperandGeneratorT<Adapter> g(this);
   Int32BinopMatcher m(node);
   if (IsSupported(ARMv7) && m.right().IsInt32Mul() &&
       CanCover(node, m.right().node())) {
@@ -1502,9 +1584,10 @@ void InstructionSelector::VisitInt32Sub(Node* node) {
 
 namespace {
 
-void EmitInt32MulWithOverflow(InstructionSelector* selector, Node* node,
-                              FlagsContinuation* cont) {
-  ArmOperandGenerator g(selector);
+template <typename Adapter>
+void EmitInt32MulWithOverflow(InstructionSelectorT<Adapter>* selector,
+                              Node* node, FlagsContinuation* cont) {
+  ArmOperandGeneratorT<Adapter> g(selector);
   Int32BinopMatcher m(node);
   InstructionOperand result_operand = g.DefineAsRegister(node);
   InstructionOperand temp_operand = g.TempRegister();
@@ -1523,8 +1606,9 @@ void EmitInt32MulWithOverflow(InstructionSelector* selector, Node* node,
 
 }  // namespace
 
-void InstructionSelector::VisitInt32Mul(Node* node) {
-  ArmOperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32Mul(Node* node) {
+  ArmOperandGeneratorT<Adapter> g(this);
   Int32BinopMatcher m(node);
   if (m.right().HasResolvedValue() && m.right().ResolvedValue() > 0) {
     int32_t value = m.right().ResolvedValue();
@@ -1546,27 +1630,32 @@ void InstructionSelector::VisitInt32Mul(Node* node) {
   VisitRRR(this, kArmMul, node);
 }
 
-void InstructionSelector::VisitUint32MulHigh(Node* node) {
-  ArmOperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitUint32MulHigh(Node* node) {
+  ArmOperandGeneratorT<Adapter> g(this);
   InstructionOperand outputs[] = {g.TempRegister(), g.DefineAsRegister(node)};
   InstructionOperand inputs[] = {g.UseRegister(node->InputAt(0)),
                                  g.UseRegister(node->InputAt(1))};
   Emit(kArmUmull, arraysize(outputs), outputs, arraysize(inputs), inputs);
 }
 
-void InstructionSelector::VisitInt32Div(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32Div(Node* node) {
   VisitDiv(this, node, kArmSdiv, kArmVcvtF64S32, kArmVcvtS32F64);
 }
 
-void InstructionSelector::VisitUint32Div(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitUint32Div(Node* node) {
   VisitDiv(this, node, kArmUdiv, kArmVcvtF64U32, kArmVcvtU32F64);
 }
 
-void InstructionSelector::VisitInt32Mod(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32Mod(Node* node) {
   VisitMod(this, node, kArmSdiv, kArmVcvtF64S32, kArmVcvtS32F64);
 }
 
-void InstructionSelector::VisitUint32Mod(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitUint32Mod(Node* node) {
   VisitMod(this, node, kArmUdiv, kArmVcvtF64U32, kArmVcvtU32F64);
 }
 
@@ -1625,33 +1714,37 @@ void InstructionSelector::VisitUint32Mod(Node* node) {
   V(Float32Min, kArmFloat32Min) \
   V(Float64Min, kArmFloat64Min)
 
-#define RR_VISITOR(Name, opcode)                      \
-  void InstructionSelector::Visit##Name(Node* node) { \
-    VisitRR(this, opcode, node);                      \
+#define RR_VISITOR(Name, opcode)                                \
+  template <typename Adapter>                                   \
+  void InstructionSelectorT<Adapter>::Visit##Name(Node* node) { \
+    VisitRR(this, opcode, node);                                \
   }
 RR_OP_LIST(RR_VISITOR)
 #undef RR_VISITOR
 #undef RR_OP_LIST
 
-#define RR_VISITOR_V8(Name, opcode)                   \
-  void InstructionSelector::Visit##Name(Node* node) { \
-    DCHECK(CpuFeatures::IsSupported(ARMv8));          \
-    VisitRR(this, opcode, node);                      \
+#define RR_VISITOR_V8(Name, opcode)                             \
+  template <typename Adapter>                                   \
+  void InstructionSelectorT<Adapter>::Visit##Name(Node* node) { \
+    DCHECK(CpuFeatures::IsSupported(ARMv8));                    \
+    VisitRR(this, opcode, node);                                \
   }
 RR_OP_LIST_V8(RR_VISITOR_V8)
 #undef RR_VISITOR_V8
 #undef RR_OP_LIST_V8
 
-#define RRR_VISITOR(Name, opcode)                     \
-  void InstructionSelector::Visit##Name(Node* node) { \
-    VisitRRR(this, opcode, node);                     \
+#define RRR_VISITOR(Name, opcode)                               \
+  template <typename Adapter>                                   \
+  void InstructionSelectorT<Adapter>::Visit##Name(Node* node) { \
+    VisitRRR(this, opcode, node);                               \
   }
 RRR_OP_LIST(RRR_VISITOR)
 #undef RRR_VISITOR
 #undef RRR_OP_LIST
 
-void InstructionSelector::VisitFloat32Add(Node* node) {
-  ArmOperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat32Add(Node* node) {
+  ArmOperandGeneratorT<Adapter> g(this);
   Float32BinopMatcher m(node);
   if (m.left().IsFloat32Mul() && CanCover(node, m.left().node())) {
     Float32BinopMatcher mleft(m.left().node());
@@ -1670,8 +1763,9 @@ void InstructionSelector::VisitFloat32Add(Node* node) {
   VisitRRR(this, kArmVaddF32, node);
 }
 
-void InstructionSelector::VisitFloat64Add(Node* node) {
-  ArmOperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64Add(Node* node) {
+  ArmOperandGeneratorT<Adapter> g(this);
   Float64BinopMatcher m(node);
   if (m.left().IsFloat64Mul() && CanCover(node, m.left().node())) {
     Float64BinopMatcher mleft(m.left().node());
@@ -1690,8 +1784,9 @@ void InstructionSelector::VisitFloat64Add(Node* node) {
   VisitRRR(this, kArmVaddF64, node);
 }
 
-void InstructionSelector::VisitFloat32Sub(Node* node) {
-  ArmOperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat32Sub(Node* node) {
+  ArmOperandGeneratorT<Adapter> g(this);
   Float32BinopMatcher m(node);
   if (m.right().IsFloat32Mul() && CanCover(node, m.right().node())) {
     Float32BinopMatcher mright(m.right().node());
@@ -1703,8 +1798,9 @@ void InstructionSelector::VisitFloat32Sub(Node* node) {
   VisitRRR(this, kArmVsubF32, node);
 }
 
-void InstructionSelector::VisitFloat64Sub(Node* node) {
-  ArmOperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64Sub(Node* node) {
+  ArmOperandGeneratorT<Adapter> g(this);
   Float64BinopMatcher m(node);
   if (m.right().IsFloat64Mul() && CanCover(node, m.right().node())) {
     Float64BinopMatcher mright(m.right().node());
@@ -1716,37 +1812,43 @@ void InstructionSelector::VisitFloat64Sub(Node* node) {
   VisitRRR(this, kArmVsubF64, node);
 }
 
-void InstructionSelector::VisitFloat64Mod(Node* node) {
-  ArmOperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64Mod(Node* node) {
+  ArmOperandGeneratorT<Adapter> g(this);
   Emit(kArmVmodF64, g.DefineAsFixed(node, d0), g.UseFixed(node->InputAt(0), d0),
        g.UseFixed(node->InputAt(1), d1))
       ->MarkAsCall();
 }
 
-void InstructionSelector::VisitFloat64Ieee754Binop(Node* node,
-                                                   InstructionCode opcode) {
-  ArmOperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64Ieee754Binop(
+    Node* node, InstructionCode opcode) {
+  ArmOperandGeneratorT<Adapter> g(this);
   Emit(opcode, g.DefineAsFixed(node, d0), g.UseFixed(node->InputAt(0), d0),
        g.UseFixed(node->InputAt(1), d1))
       ->MarkAsCall();
 }
 
-void InstructionSelector::VisitFloat64Ieee754Unop(Node* node,
-                                                  InstructionCode opcode) {
-  ArmOperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64Ieee754Unop(
+    Node* node, InstructionCode opcode) {
+  ArmOperandGeneratorT<Adapter> g(this);
   Emit(opcode, g.DefineAsFixed(node, d0), g.UseFixed(node->InputAt(0), d0))
       ->MarkAsCall();
 }
 
-void InstructionSelector::EmitMoveParamToFPR(Node* node, int index) {}
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::EmitMoveParamToFPR(Node* node, int index) {}
 
-void InstructionSelector::EmitMoveFPRToParam(InstructionOperand* op,
-                                             LinkageLocation location) {}
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::EmitMoveFPRToParam(
+    InstructionOperand* op, LinkageLocation location) {}
 
-void InstructionSelector::EmitPrepareArguments(
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::EmitPrepareArguments(
     ZoneVector<PushParameter>* arguments, const CallDescriptor* call_descriptor,
     Node* node) {
-  ArmOperandGenerator g(this);
+  ArmOperandGeneratorT<Adapter> g(this);
 
   // Prepare for C function call.
   if (call_descriptor->IsCFunctionCall()) {
@@ -1777,10 +1879,11 @@ void InstructionSelector::EmitPrepareArguments(
   }
 }
 
-void InstructionSelector::EmitPrepareResults(
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::EmitPrepareResults(
     ZoneVector<PushParameter>* results, const CallDescriptor* call_descriptor,
     Node* node) {
-  ArmOperandGenerator g(this);
+  ArmOperandGeneratorT<Adapter> g(this);
 
   for (PushParameter output : *results) {
     if (!output.location.IsCallerFrameSlot()) continue;
@@ -1802,21 +1905,26 @@ void InstructionSelector::EmitPrepareResults(
   }
 }
 
-bool InstructionSelector::IsTailCallAddressImmediate() { return false; }
+template <typename Adapter>
+bool InstructionSelectorT<Adapter>::IsTailCallAddressImmediate() {
+  return false;
+}
 
 namespace {
 
 // Shared routine for multiple compare operations.
-void VisitCompare(InstructionSelector* selector, InstructionCode opcode,
-                  InstructionOperand left, InstructionOperand right,
-                  FlagsContinuation* cont) {
+template <typename Adapter>
+void VisitCompare(InstructionSelectorT<Adapter>* selector,
+                  InstructionCode opcode, InstructionOperand left,
+                  InstructionOperand right, FlagsContinuation* cont) {
   selector->EmitWithContinuation(opcode, left, right, cont);
 }
 
 // Shared routine for multiple float32 compare operations.
-void VisitFloat32Compare(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitFloat32Compare(InstructionSelectorT<Adapter>* selector, Node* node,
                          FlagsContinuation* cont) {
-  ArmOperandGenerator g(selector);
+  ArmOperandGeneratorT<Adapter> g(selector);
   Float32BinopMatcher m(node);
   if (m.right().Is(0.0f)) {
     VisitCompare(selector, kArmVcmpF32, g.UseRegister(m.left().node()),
@@ -1832,9 +1940,10 @@ void VisitFloat32Compare(InstructionSelector* selector, Node* node,
 }
 
 // Shared routine for multiple float64 compare operations.
-void VisitFloat64Compare(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitFloat64Compare(InstructionSelectorT<Adapter>* selector, Node* node,
                          FlagsContinuation* cont) {
-  ArmOperandGenerator g(selector);
+  ArmOperandGeneratorT<Adapter> g(selector);
   Float64BinopMatcher m(node);
   if (m.right().Is(0.0)) {
     VisitCompare(selector, kArmVcmpF64, g.UseRegister(m.left().node()),
@@ -1901,11 +2010,10 @@ FlagsCondition MapForFlagSettingBinop(FlagsCondition cond) {
 // (a <ops> b), b.<cond'>
 // where <ops> is the flag setting version of <op>, and if so,
 // updates {node}, {opcode} and {cont} accordingly.
-void MaybeReplaceCmpZeroWithFlagSettingBinop(InstructionSelector* selector,
-                                             Node** node, Node* binop,
-                                             InstructionCode* opcode,
-                                             FlagsCondition cond,
-                                             FlagsContinuation* cont) {
+template <typename Adapter>
+void MaybeReplaceCmpZeroWithFlagSettingBinop(
+    InstructionSelectorT<Adapter>* selector, Node** node, Node* binop,
+    InstructionCode* opcode, FlagsCondition cond, FlagsContinuation* cont) {
   InstructionCode binop_opcode;
   InstructionCode no_output_opcode;
   switch (binop->opcode()) {
@@ -1944,9 +2052,10 @@ void MaybeReplaceCmpZeroWithFlagSettingBinop(InstructionSelector* selector,
 }
 
 // Shared routine for multiple word compare operations.
-void VisitWordCompare(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitWordCompare(InstructionSelectorT<Adapter>* selector, Node* node,
                       InstructionCode opcode, FlagsContinuation* cont) {
-  ArmOperandGenerator g(selector);
+  ArmOperandGeneratorT<Adapter> g(selector);
   Int32BinopMatcher m(node);
   InstructionOperand inputs[3];
   size_t input_count = 0;
@@ -1989,7 +2098,8 @@ void VisitWordCompare(InstructionSelector* selector, Node* node,
                                  inputs, cont);
 }
 
-void VisitWordCompare(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitWordCompare(InstructionSelectorT<Adapter>* selector, Node* node,
                       FlagsContinuation* cont) {
   InstructionCode opcode = kArmCmp;
   Int32BinopMatcher m(node);
@@ -2022,8 +2132,9 @@ void VisitWordCompare(InstructionSelector* selector, Node* node,
 }  // namespace
 
 // Shared routine for word comparisons against zero.
-void InstructionSelector::VisitWordCompareZero(Node* user, Node* value,
-                                               FlagsContinuation* cont) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWordCompareZero(
+    Node* user, Node* value, FlagsContinuation* cont) {
   // Try to combine with comparisons against 0 by simply inverting the branch.
   while (value->opcode() == IrOpcode::kWord32Equal && CanCover(user, value)) {
     Int32BinopMatcher m(value);
@@ -2112,13 +2223,13 @@ void InstructionSelector::VisitWordCompareZero(Node* user, Node* value,
       case IrOpcode::kWord32Xor:
         return VisitWordCompare(this, value, kArmTeq, cont);
       case IrOpcode::kWord32Sar:
-        return VisitShift(this, value, TryMatchASR, cont);
+        return VisitShift(this, value, TryMatchASR<Adapter>, cont);
       case IrOpcode::kWord32Shl:
-        return VisitShift(this, value, TryMatchLSL, cont);
+        return VisitShift(this, value, TryMatchLSL<Adapter>, cont);
       case IrOpcode::kWord32Shr:
-        return VisitShift(this, value, TryMatchLSR, cont);
+        return VisitShift(this, value, TryMatchLSR<Adapter>, cont);
       case IrOpcode::kWord32Ror:
-        return VisitShift(this, value, TryMatchROR, cont);
+        return VisitShift(this, value, TryMatchROR<Adapter>, cont);
       case IrOpcode::kStackPointerGreaterThan:
         cont->OverwriteAndNegateIfEqual(kStackPointerGreaterThanCondition);
         return VisitStackPointerGreaterThan(value, cont);
@@ -2132,15 +2243,17 @@ void InstructionSelector::VisitWordCompareZero(Node* user, Node* value,
   }
 
   // Continuation could not be combined with a compare, emit compare against 0.
-  ArmOperandGenerator g(this);
+  ArmOperandGeneratorT<Adapter> g(this);
   InstructionCode const opcode =
       kArmTst | AddressingModeField::encode(kMode_Operand2_R);
   InstructionOperand const value_operand = g.UseRegister(value);
   EmitWithContinuation(opcode, value_operand, value_operand, cont);
 }
 
-void InstructionSelector::VisitSwitch(Node* node, const SwitchInfo& sw) {
-  ArmOperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitSwitch(Node* node,
+                                                const SwitchInfo& sw) {
+  ArmOperandGeneratorT<Adapter> g(this);
   InstructionOperand value_operand = g.UseRegister(node->InputAt(0));
 
   // Emit either ArchTableSwitch or ArchBinarySearchSwitch.
@@ -2170,7 +2283,8 @@ void InstructionSelector::VisitSwitch(Node* node, const SwitchInfo& sw) {
   return EmitBinarySearchSwitch(sw, value_operand);
 }
 
-void InstructionSelector::VisitWord32Equal(Node* const node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32Equal(Node* const node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kEqual, node);
   Int32BinopMatcher m(node);
   if (m.right().Is(0)) {
@@ -2179,29 +2293,34 @@ void InstructionSelector::VisitWord32Equal(Node* const node) {
   VisitWordCompare(this, node, &cont);
 }
 
-void InstructionSelector::VisitInt32LessThan(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32LessThan(Node* node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kSignedLessThan, node);
   VisitWordCompare(this, node, &cont);
 }
 
-void InstructionSelector::VisitInt32LessThanOrEqual(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32LessThanOrEqual(Node* node) {
   FlagsContinuation cont =
       FlagsContinuation::ForSet(kSignedLessThanOrEqual, node);
   VisitWordCompare(this, node, &cont);
 }
 
-void InstructionSelector::VisitUint32LessThan(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitUint32LessThan(Node* node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kUnsignedLessThan, node);
   VisitWordCompare(this, node, &cont);
 }
 
-void InstructionSelector::VisitUint32LessThanOrEqual(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitUint32LessThanOrEqual(Node* node) {
   FlagsContinuation cont =
       FlagsContinuation::ForSet(kUnsignedLessThanOrEqual, node);
   VisitWordCompare(this, node, &cont);
 }
 
-void InstructionSelector::VisitInt32AddWithOverflow(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32AddWithOverflow(Node* node) {
   if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
     FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
     return VisitBinop(this, node, kArmAdd, kArmAdd, &cont);
@@ -2210,7 +2329,8 @@ void InstructionSelector::VisitInt32AddWithOverflow(Node* node) {
   VisitBinop(this, node, kArmAdd, kArmAdd, &cont);
 }
 
-void InstructionSelector::VisitInt32SubWithOverflow(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32SubWithOverflow(Node* node) {
   if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
     FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
     return VisitBinop(this, node, kArmSub, kArmRsb, &cont);
@@ -2219,7 +2339,8 @@ void InstructionSelector::VisitInt32SubWithOverflow(Node* node) {
   VisitBinop(this, node, kArmSub, kArmRsb, &cont);
 }
 
-void InstructionSelector::VisitInt32MulWithOverflow(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32MulWithOverflow(Node* node) {
   if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
     // ARM doesn't set the overflow flag for multiplication, so we need to test
     // on kNotEqual. Here is the code sequence used:
@@ -2232,40 +2353,47 @@ void InstructionSelector::VisitInt32MulWithOverflow(Node* node) {
   EmitInt32MulWithOverflow(this, node, &cont);
 }
 
-void InstructionSelector::VisitFloat32Equal(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat32Equal(Node* node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kEqual, node);
   VisitFloat32Compare(this, node, &cont);
 }
 
-void InstructionSelector::VisitFloat32LessThan(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat32LessThan(Node* node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kFloatLessThan, node);
   VisitFloat32Compare(this, node, &cont);
 }
 
-void InstructionSelector::VisitFloat32LessThanOrEqual(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat32LessThanOrEqual(Node* node) {
   FlagsContinuation cont =
       FlagsContinuation::ForSet(kFloatLessThanOrEqual, node);
   VisitFloat32Compare(this, node, &cont);
 }
 
-void InstructionSelector::VisitFloat64Equal(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64Equal(Node* node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kEqual, node);
   VisitFloat64Compare(this, node, &cont);
 }
 
-void InstructionSelector::VisitFloat64LessThan(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64LessThan(Node* node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kFloatLessThan, node);
   VisitFloat64Compare(this, node, &cont);
 }
 
-void InstructionSelector::VisitFloat64LessThanOrEqual(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64LessThanOrEqual(Node* node) {
   FlagsContinuation cont =
       FlagsContinuation::ForSet(kFloatLessThanOrEqual, node);
   VisitFloat64Compare(this, node, &cont);
 }
 
-void InstructionSelector::VisitFloat64InsertLowWord32(Node* node) {
-  ArmOperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64InsertLowWord32(Node* node) {
+  ArmOperandGeneratorT<Adapter> g(this);
   Node* left = node->InputAt(0);
   Node* right = node->InputAt(1);
   if (left->opcode() == IrOpcode::kFloat64InsertHighWord32 &&
@@ -2279,8 +2407,9 @@ void InstructionSelector::VisitFloat64InsertLowWord32(Node* node) {
        g.UseRegister(right));
 }
 
-void InstructionSelector::VisitFloat64InsertHighWord32(Node* node) {
-  ArmOperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64InsertHighWord32(Node* node) {
+  ArmOperandGeneratorT<Adapter> g(this);
   Node* left = node->InputAt(0);
   Node* right = node->InputAt(1);
   if (left->opcode() == IrOpcode::kFloat64InsertLowWord32 &&
@@ -2294,19 +2423,21 @@ void InstructionSelector::VisitFloat64InsertHighWord32(Node* node) {
        g.UseRegister(right));
 }
 
-void InstructionSelector::VisitMemoryBarrier(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitMemoryBarrier(Node* node) {
   // Use DMB ISH for both acquire-release and sequentially consistent barriers.
-  ArmOperandGenerator g(this);
+  ArmOperandGeneratorT<Adapter> g(this);
   Emit(kArmDmbIsh, g.NoOutput());
 }
 
-void InstructionSelector::VisitWord32AtomicLoad(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32AtomicLoad(Node* node) {
   // The memory order is ignored as both acquire and sequentially consistent
   // loads can emit LDR; DMB ISH.
   // https://www.cl.cam.ac.uk/~pes20/cpp/cpp0xmappings.html
   AtomicLoadParameters atomic_load_params = AtomicLoadParametersOf(node->op());
   LoadRepresentation load_rep = atomic_load_params.representation();
-  ArmOperandGenerator g(this);
+  ArmOperandGeneratorT<Adapter> g(this);
   Node* base = node->InputAt(0);
   Node* index = node->InputAt(1);
   ArchOpcode opcode;
@@ -2330,14 +2461,16 @@ void InstructionSelector::VisitWord32AtomicLoad(Node* node) {
        g.DefineAsRegister(node), g.UseRegister(base), g.UseRegister(index));
 }
 
-void InstructionSelector::VisitWord32AtomicStore(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32AtomicStore(Node* node) {
   AtomicStoreParameters store_params = AtomicStoreParametersOf(node->op());
   VisitStoreCommon(this, node, store_params.store_representation(),
                    store_params.order());
 }
 
-void InstructionSelector::VisitWord32AtomicExchange(Node* node) {
-  ArmOperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32AtomicExchange(Node* node) {
+  ArmOperandGeneratorT<Adapter> g(this);
   Node* base = node->InputAt(0);
   Node* index = node->InputAt(1);
   Node* value = node->InputAt(2);
@@ -2370,8 +2503,10 @@ void InstructionSelector::VisitWord32AtomicExchange(Node* node) {
   Emit(code, 1, outputs, input_count, inputs, arraysize(temps), temps);
 }
 
-void InstructionSelector::VisitWord32AtomicCompareExchange(Node* node) {
-  ArmOperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32AtomicCompareExchange(
+    Node* node) {
+  ArmOperandGeneratorT<Adapter> g(this);
   Node* base = node->InputAt(0);
   Node* index = node->InputAt(1);
   Node* old_value = node->InputAt(2);
@@ -2407,10 +2542,11 @@ void InstructionSelector::VisitWord32AtomicCompareExchange(Node* node) {
   Emit(code, 1, outputs, input_count, inputs, arraysize(temps), temps);
 }
 
-void InstructionSelector::VisitWord32AtomicBinaryOperation(
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32AtomicBinaryOperation(
     Node* node, ArchOpcode int8_op, ArchOpcode uint8_op, ArchOpcode int16_op,
     ArchOpcode uint16_op, ArchOpcode word32_op) {
-  ArmOperandGenerator g(this);
+  ArmOperandGeneratorT<Adapter> g(this);
   Node* base = node->InputAt(0);
   Node* index = node->InputAt(1);
   Node* value = node->InputAt(2);
@@ -2444,11 +2580,12 @@ void InstructionSelector::VisitWord32AtomicBinaryOperation(
   Emit(code, 1, outputs, input_count, inputs, arraysize(temps), temps);
 }
 
-#define VISIT_ATOMIC_BINOP(op)                                           \
-  void InstructionSelector::VisitWord32Atomic##op(Node* node) {          \
-    VisitWord32AtomicBinaryOperation(                                    \
-        node, kAtomic##op##Int8, kAtomic##op##Uint8, kAtomic##op##Int16, \
-        kAtomic##op##Uint16, kAtomic##op##Word32);                       \
+#define VISIT_ATOMIC_BINOP(op)                                            \
+  template <typename Adapter>                                             \
+  void InstructionSelectorT<Adapter>::VisitWord32Atomic##op(Node* node) { \
+    VisitWord32AtomicBinaryOperation(                                     \
+        node, kAtomic##op##Int8, kAtomic##op##Uint8, kAtomic##op##Int16,  \
+        kAtomic##op##Uint16, kAtomic##op##Word32);                        \
   }
 VISIT_ATOMIC_BINOP(Add)
 VISIT_ATOMIC_BINOP(Sub)
@@ -2457,8 +2594,9 @@ VISIT_ATOMIC_BINOP(Or)
 VISIT_ATOMIC_BINOP(Xor)
 #undef VISIT_ATOMIC_BINOP
 
-void InstructionSelector::VisitWord32AtomicPairLoad(Node* node) {
-  ArmOperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32AtomicPairLoad(Node* node) {
+  ArmOperandGeneratorT<Adapter> g(this);
   Node* base = node->InputAt(0);
   Node* index = node->InputAt(1);
   InstructionOperand inputs[3];
@@ -2491,8 +2629,9 @@ void InstructionSelector::VisitWord32AtomicPairLoad(Node* node) {
        temp_count, temps);
 }
 
-void InstructionSelector::VisitWord32AtomicPairStore(Node* node) {
-  ArmOperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32AtomicPairStore(Node* node) {
+  ArmOperandGeneratorT<Adapter> g(this);
   Node* base = node->InputAt(0);
   Node* index = node->InputAt(1);
   Node* value_low = node->InputAt(2);
@@ -2508,28 +2647,34 @@ void InstructionSelector::VisitWord32AtomicPairStore(Node* node) {
   Emit(code, 0, nullptr, arraysize(inputs), inputs, arraysize(temps), temps);
 }
 
-void InstructionSelector::VisitWord32AtomicPairAdd(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32AtomicPairAdd(Node* node) {
   VisitPairAtomicBinOp(this, node, kArmWord32AtomicPairAdd);
 }
 
-void InstructionSelector::VisitWord32AtomicPairSub(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32AtomicPairSub(Node* node) {
   VisitPairAtomicBinOp(this, node, kArmWord32AtomicPairSub);
 }
 
-void InstructionSelector::VisitWord32AtomicPairAnd(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32AtomicPairAnd(Node* node) {
   VisitPairAtomicBinOp(this, node, kArmWord32AtomicPairAnd);
 }
 
-void InstructionSelector::VisitWord32AtomicPairOr(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32AtomicPairOr(Node* node) {
   VisitPairAtomicBinOp(this, node, kArmWord32AtomicPairOr);
 }
 
-void InstructionSelector::VisitWord32AtomicPairXor(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32AtomicPairXor(Node* node) {
   VisitPairAtomicBinOp(this, node, kArmWord32AtomicPairXor);
 }
 
-void InstructionSelector::VisitWord32AtomicPairExchange(Node* node) {
-  ArmOperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32AtomicPairExchange(Node* node) {
+  ArmOperandGeneratorT<Adapter> g(this);
   Node* base = node->InputAt(0);
   Node* index = node->InputAt(1);
   Node* value = node->InputAt(2);
@@ -2562,8 +2707,10 @@ void InstructionSelector::VisitWord32AtomicPairExchange(Node* node) {
        temps);
 }
 
-void InstructionSelector::VisitWord32AtomicPairCompareExchange(Node* node) {
-  ArmOperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32AtomicPairCompareExchange(
+    Node* node) {
+  ArmOperandGeneratorT<Adapter> g(this);
   AddressingMode addressing_mode = kMode_Offset_RR;
   InstructionOperand inputs[] = {g.UseFixed(node->InputAt(2), r4),
                                  g.UseFixed(node->InputAt(3), r5),
@@ -2739,22 +2886,25 @@ void InstructionSelector::VisitWord32AtomicPairCompareExchange(Node* node) {
   V(S128Xor, kArmS128Xor)                             \
   V(S128AndNot, kArmS128AndNot)
 
-void InstructionSelector::VisitI32x4DotI16x8S(Node* node) {
-  ArmOperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI32x4DotI16x8S(Node* node) {
+  ArmOperandGeneratorT<Adapter> g(this);
   Emit(kArmI32x4DotI16x8S, g.DefineAsRegister(node),
        g.UseUniqueRegister(node->InputAt(0)),
        g.UseUniqueRegister(node->InputAt(1)));
 }
 
-void InstructionSelector::VisitI16x8DotI8x16I7x16S(Node* node) {
-  ArmOperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI16x8DotI8x16I7x16S(Node* node) {
+  ArmOperandGeneratorT<Adapter> g(this);
   Emit(kArmI16x8DotI8x16S, g.DefineAsRegister(node),
        g.UseUniqueRegister(node->InputAt(0)),
        g.UseUniqueRegister(node->InputAt(1)));
 }
 
-void InstructionSelector::VisitI32x4DotI8x16I7x16AddS(Node* node) {
-  ArmOperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI32x4DotI8x16I7x16AddS(Node* node) {
+  ArmOperandGeneratorT<Adapter> g(this);
   InstructionOperand temps[] = {g.TempSimd128Register()};
   Emit(kArmI32x4DotI8x16AddS, g.DefineSameAsInput(node, 2),
        g.UseUniqueRegister(node->InputAt(0)),
@@ -2762,8 +2912,9 @@ void InstructionSelector::VisitI32x4DotI8x16I7x16AddS(Node* node) {
        g.UseUniqueRegister(node->InputAt(2)), arraysize(temps), temps);
 }
 
-void InstructionSelector::VisitS128Const(Node* node) {
-  ArmOperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitS128Const(Node* node) {
+  ArmOperandGeneratorT<Adapter> g(this);
   uint32_t val[kSimd128Size / sizeof(uint32_t)];
   memcpy(val, S128ImmediateParameterOf(node->op()).data(), kSimd128Size);
   // If all bytes are zeros, avoid emitting code for generic constants.
@@ -2781,22 +2932,26 @@ void InstructionSelector::VisitS128Const(Node* node) {
   }
 }
 
-void InstructionSelector::VisitS128Zero(Node* node) {
-  ArmOperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitS128Zero(Node* node) {
+  ArmOperandGeneratorT<Adapter> g(this);
   Emit(kArmS128Zero, g.DefineAsRegister(node));
 }
 
-#define SIMD_VISIT_SPLAT(Type)                               \
-  void InstructionSelector::Visit##Type##Splat(Node* node) { \
-    VisitRR(this, kArm##Type##Splat, node);                  \
+#define SIMD_VISIT_SPLAT(Type)                                         \
+  template <typename Adapter>                                          \
+  void InstructionSelectorT<Adapter>::Visit##Type##Splat(Node* node) { \
+    VisitRR(this, kArm##Type##Splat, node);                            \
   }
 SIMD_TYPE_LIST(SIMD_VISIT_SPLAT)
 SIMD_VISIT_SPLAT(F64x2)
 #undef SIMD_VISIT_SPLAT
 
-#define SIMD_VISIT_EXTRACT_LANE(Type, Sign)                              \
-  void InstructionSelector::Visit##Type##ExtractLane##Sign(Node* node) { \
-    VisitRRI(this, kArm##Type##ExtractLane##Sign, node);                 \
+#define SIMD_VISIT_EXTRACT_LANE(Type, Sign)                           \
+  template <typename Adapter>                                         \
+  void InstructionSelectorT<Adapter>::Visit##Type##ExtractLane##Sign( \
+      Node* node) {                                                   \
+    VisitRRI(this, kArm##Type##ExtractLane##Sign, node);              \
   }
 SIMD_VISIT_EXTRACT_LANE(F64x2, )
 SIMD_VISIT_EXTRACT_LANE(F32x4, )
@@ -2807,92 +2962,99 @@ SIMD_VISIT_EXTRACT_LANE(I8x16, U)
 SIMD_VISIT_EXTRACT_LANE(I8x16, S)
 #undef SIMD_VISIT_EXTRACT_LANE
 
-#define SIMD_VISIT_REPLACE_LANE(Type)                              \
-  void InstructionSelector::Visit##Type##ReplaceLane(Node* node) { \
-    VisitRRIR(this, kArm##Type##ReplaceLane, node);                \
+#define SIMD_VISIT_REPLACE_LANE(Type)                                        \
+  template <typename Adapter>                                                \
+  void InstructionSelectorT<Adapter>::Visit##Type##ReplaceLane(Node* node) { \
+    VisitRRIR(this, kArm##Type##ReplaceLane, node);                          \
   }
 SIMD_TYPE_LIST(SIMD_VISIT_REPLACE_LANE)
 SIMD_VISIT_REPLACE_LANE(F64x2)
 #undef SIMD_VISIT_REPLACE_LANE
 #undef SIMD_TYPE_LIST
 
-#define SIMD_VISIT_UNOP(Name, instruction)            \
-  void InstructionSelector::Visit##Name(Node* node) { \
-    VisitRR(this, instruction, node);                 \
+#define SIMD_VISIT_UNOP(Name, instruction)                      \
+  template <typename Adapter>                                   \
+  void InstructionSelectorT<Adapter>::Visit##Name(Node* node) { \
+    VisitRR(this, instruction, node);                           \
   }
 SIMD_UNOP_LIST(SIMD_VISIT_UNOP)
 #undef SIMD_VISIT_UNOP
 #undef SIMD_UNOP_LIST
 
-#define SIMD_VISIT_SHIFT_OP(Name, width)              \
-  void InstructionSelector::Visit##Name(Node* node) { \
-    VisitSimdShiftRRR(this, kArm##Name, node, width); \
+#define SIMD_VISIT_SHIFT_OP(Name, width)                        \
+  template <typename Adapter>                                   \
+  void InstructionSelectorT<Adapter>::Visit##Name(Node* node) { \
+    VisitSimdShiftRRR(this, kArm##Name, node, width);           \
   }
 SIMD_SHIFT_OP_LIST(SIMD_VISIT_SHIFT_OP)
 #undef SIMD_VISIT_SHIFT_OP
 #undef SIMD_SHIFT_OP_LIST
 
-#define SIMD_VISIT_BINOP(Name, instruction)           \
-  void InstructionSelector::Visit##Name(Node* node) { \
-    VisitRRR(this, instruction, node);                \
+#define SIMD_VISIT_BINOP(Name, instruction)                     \
+  template <typename Adapter>                                   \
+  void InstructionSelectorT<Adapter>::Visit##Name(Node* node) { \
+    VisitRRR(this, instruction, node);                          \
   }
 SIMD_BINOP_LIST(SIMD_VISIT_BINOP)
 #undef SIMD_VISIT_BINOP
 #undef SIMD_BINOP_LIST
 
-#define VISIT_SIMD_ADD(Type, PairwiseType, NeonWidth)             \
-  void InstructionSelector::Visit##Type##Add(Node* node) {        \
-    ArmOperandGenerator g(this);                                  \
-    Node* left = node->InputAt(0);                                \
-    Node* right = node->InputAt(1);                               \
-    if (left->opcode() ==                                         \
-            IrOpcode::k##Type##ExtAddPairwise##PairwiseType##S && \
-        CanCover(node, left)) {                                   \
-      Emit(kArmVpadal | MiscField::encode(NeonS##NeonWidth),      \
-           g.DefineSameAsFirst(node), g.UseRegister(right),       \
-           g.UseRegister(left->InputAt(0)));                      \
-      return;                                                     \
-    }                                                             \
-    if (left->opcode() ==                                         \
-            IrOpcode::k##Type##ExtAddPairwise##PairwiseType##U && \
-        CanCover(node, left)) {                                   \
-      Emit(kArmVpadal | MiscField::encode(NeonU##NeonWidth),      \
-           g.DefineSameAsFirst(node), g.UseRegister(right),       \
-           g.UseRegister(left->InputAt(0)));                      \
-      return;                                                     \
-    }                                                             \
-    if (right->opcode() ==                                        \
-            IrOpcode::k##Type##ExtAddPairwise##PairwiseType##S && \
-        CanCover(node, right)) {                                  \
-      Emit(kArmVpadal | MiscField::encode(NeonS##NeonWidth),      \
-           g.DefineSameAsFirst(node), g.UseRegister(left),        \
-           g.UseRegister(right->InputAt(0)));                     \
-      return;                                                     \
-    }                                                             \
-    if (right->opcode() ==                                        \
-            IrOpcode::k##Type##ExtAddPairwise##PairwiseType##U && \
-        CanCover(node, right)) {                                  \
-      Emit(kArmVpadal | MiscField::encode(NeonU##NeonWidth),      \
-           g.DefineSameAsFirst(node), g.UseRegister(left),        \
-           g.UseRegister(right->InputAt(0)));                     \
-      return;                                                     \
-    }                                                             \
-    VisitRRR(this, kArm##Type##Add, node);                        \
+#define VISIT_SIMD_ADD(Type, PairwiseType, NeonWidth)                \
+  template <typename Adapter>                                        \
+  void InstructionSelectorT<Adapter>::Visit##Type##Add(Node* node) { \
+    ArmOperandGeneratorT<Adapter> g(this);                           \
+    Node* left = node->InputAt(0);                                   \
+    Node* right = node->InputAt(1);                                  \
+    if (left->opcode() ==                                            \
+            IrOpcode::k##Type##ExtAddPairwise##PairwiseType##S &&    \
+        CanCover(node, left)) {                                      \
+      Emit(kArmVpadal | MiscField::encode(NeonS##NeonWidth),         \
+           g.DefineSameAsFirst(node), g.UseRegister(right),          \
+           g.UseRegister(left->InputAt(0)));                         \
+      return;                                                        \
+    }                                                                \
+    if (left->opcode() ==                                            \
+            IrOpcode::k##Type##ExtAddPairwise##PairwiseType##U &&    \
+        CanCover(node, left)) {                                      \
+      Emit(kArmVpadal | MiscField::encode(NeonU##NeonWidth),         \
+           g.DefineSameAsFirst(node), g.UseRegister(right),          \
+           g.UseRegister(left->InputAt(0)));                         \
+      return;                                                        \
+    }                                                                \
+    if (right->opcode() ==                                           \
+            IrOpcode::k##Type##ExtAddPairwise##PairwiseType##S &&    \
+        CanCover(node, right)) {                                     \
+      Emit(kArmVpadal | MiscField::encode(NeonS##NeonWidth),         \
+           g.DefineSameAsFirst(node), g.UseRegister(left),           \
+           g.UseRegister(right->InputAt(0)));                        \
+      return;                                                        \
+    }                                                                \
+    if (right->opcode() ==                                           \
+            IrOpcode::k##Type##ExtAddPairwise##PairwiseType##U &&    \
+        CanCover(node, right)) {                                     \
+      Emit(kArmVpadal | MiscField::encode(NeonU##NeonWidth),         \
+           g.DefineSameAsFirst(node), g.UseRegister(left),           \
+           g.UseRegister(right->InputAt(0)));                        \
+      return;                                                        \
+    }                                                                \
+    VisitRRR(this, kArm##Type##Add, node);                           \
   }
 
 VISIT_SIMD_ADD(I16x8, I8x16, 8)
 VISIT_SIMD_ADD(I32x4, I16x8, 16)
 #undef VISIT_SIMD_ADD
 
-void InstructionSelector::VisitI64x2SplatI32Pair(Node* node) {
-  ArmOperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI64x2SplatI32Pair(Node* node) {
+  ArmOperandGeneratorT<Adapter> g(this);
   InstructionOperand operand0 = g.UseRegister(node->InputAt(0));
   InstructionOperand operand1 = g.UseRegister(node->InputAt(1));
   Emit(kArmI64x2SplatI32Pair, g.DefineAsRegister(node), operand0, operand1);
 }
 
-void InstructionSelector::VisitI64x2ReplaceLaneI32Pair(Node* node) {
-  ArmOperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI64x2ReplaceLaneI32Pair(Node* node) {
+  ArmOperandGeneratorT<Adapter> g(this);
   InstructionOperand operand = g.UseRegister(node->InputAt(0));
   InstructionOperand lane = g.UseImmediate(OpParameter<int32_t>(node->op()));
   InstructionOperand low = g.UseRegister(node->InputAt(1));
@@ -2901,66 +3063,76 @@ void InstructionSelector::VisitI64x2ReplaceLaneI32Pair(Node* node) {
        low, high);
 }
 
-void InstructionSelector::VisitI64x2Neg(Node* node) {
-  ArmOperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI64x2Neg(Node* node) {
+  ArmOperandGeneratorT<Adapter> g(this);
   Emit(kArmI64x2Neg, g.DefineAsRegister(node),
        g.UseUniqueRegister(node->InputAt(0)));
 }
 
-void InstructionSelector::VisitI64x2Mul(Node* node) {
-  ArmOperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI64x2Mul(Node* node) {
+  ArmOperandGeneratorT<Adapter> g(this);
   InstructionOperand temps[] = {g.TempSimd128Register()};
   Emit(kArmI64x2Mul, g.DefineAsRegister(node),
        g.UseUniqueRegister(node->InputAt(0)),
        g.UseUniqueRegister(node->InputAt(1)), arraysize(temps), temps);
 }
 
-void InstructionSelector::VisitF32x4Sqrt(Node* node) {
-  ArmOperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF32x4Sqrt(Node* node) {
+  ArmOperandGeneratorT<Adapter> g(this);
   // Use fixed registers in the lower 8 Q-registers so we can directly access
   // mapped registers S0-S31.
   Emit(kArmF32x4Sqrt, g.DefineAsFixed(node, q0),
        g.UseFixed(node->InputAt(0), q0));
 }
 
-void InstructionSelector::VisitF32x4Div(Node* node) {
-  ArmOperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF32x4Div(Node* node) {
+  ArmOperandGeneratorT<Adapter> g(this);
   // Use fixed registers in the lower 8 Q-registers so we can directly access
   // mapped registers S0-S31.
   Emit(kArmF32x4Div, g.DefineAsFixed(node, q0),
        g.UseFixed(node->InputAt(0), q0), g.UseFixed(node->InputAt(1), q1));
 }
 
-void InstructionSelector::VisitS128Select(Node* node) {
-  ArmOperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitS128Select(Node* node) {
+  ArmOperandGeneratorT<Adapter> g(this);
   Emit(kArmS128Select, g.DefineSameAsFirst(node),
        g.UseRegister(node->InputAt(0)), g.UseRegister(node->InputAt(1)),
        g.UseRegister(node->InputAt(2)));
 }
 
-void InstructionSelector::VisitI8x16RelaxedLaneSelect(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI8x16RelaxedLaneSelect(Node* node) {
   VisitS128Select(node);
 }
 
-void InstructionSelector::VisitI16x8RelaxedLaneSelect(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI16x8RelaxedLaneSelect(Node* node) {
   VisitS128Select(node);
 }
 
-void InstructionSelector::VisitI32x4RelaxedLaneSelect(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI32x4RelaxedLaneSelect(Node* node) {
   VisitS128Select(node);
 }
 
-void InstructionSelector::VisitI64x2RelaxedLaneSelect(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI64x2RelaxedLaneSelect(Node* node) {
   VisitS128Select(node);
 }
 
-#define VISIT_SIMD_QFMOP(op)                        \
-  void InstructionSelector::Visit##op(Node* node) { \
-    ArmOperandGenerator g(this);                    \
-    Emit(kArm##op, g.DefineAsRegister(node),        \
-         g.UseUniqueRegister(node->InputAt(0)),     \
-         g.UseUniqueRegister(node->InputAt(1)),     \
-         g.UseUniqueRegister(node->InputAt(2)));    \
+#define VISIT_SIMD_QFMOP(op)                                  \
+  template <typename Adapter>                                 \
+  void InstructionSelectorT<Adapter>::Visit##op(Node* node) { \
+    ArmOperandGeneratorT<Adapter> g(this);                    \
+    Emit(kArm##op, g.DefineAsRegister(node),                  \
+         g.UseUniqueRegister(node->InputAt(0)),               \
+         g.UseUniqueRegister(node->InputAt(1)),               \
+         g.UseUniqueRegister(node->InputAt(2)));              \
   }
 VISIT_SIMD_QFMOP(F64x2Qfma)
 VISIT_SIMD_QFMOP(F64x2Qfms)
@@ -3042,8 +3214,10 @@ bool TryMatchArchShuffle(const uint8_t* shuffle, const ShuffleEntry* table,
   return false;
 }
 
-void ArrangeShuffleTable(ArmOperandGenerator* g, Node* input0, Node* input1,
-                         InstructionOperand* src0, InstructionOperand* src1) {
+template <typename Adapter>
+void ArrangeShuffleTable(ArmOperandGeneratorT<Adapter>* g, Node* input0,
+                         Node* input1, InstructionOperand* src0,
+                         InstructionOperand* src1) {
   if (input0 == input1) {
     // Unary, any q-register can be the table.
     *src0 = *src1 = g->UseRegister(input0);
@@ -3056,14 +3230,15 @@ void ArrangeShuffleTable(ArmOperandGenerator* g, Node* input0, Node* input1,
 
 }  // namespace
 
-void InstructionSelector::VisitI8x16Shuffle(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI8x16Shuffle(Node* node) {
   uint8_t shuffle[kSimd128Size];
   bool is_swizzle;
   CanonicalizeShuffle(node, shuffle, &is_swizzle);
   Node* input0 = node->InputAt(0);
   Node* input1 = node->InputAt(1);
   uint8_t shuffle32x4[4];
-  ArmOperandGenerator g(this);
+  ArmOperandGeneratorT<Adapter> g(this);
   int index = 0;
   if (wasm::SimdShuffle::TryMatch32x4Shuffle(shuffle, shuffle32x4)) {
     if (wasm::SimdShuffle::TryMatchSplat<4>(shuffle, &index)) {
@@ -3116,67 +3291,80 @@ void InstructionSelector::VisitI8x16Shuffle(Node* node) {
        g.UseImmediate(wasm::SimdShuffle::Pack4Lanes(shuffle + 12)));
 }
 #else
-void InstructionSelector::VisitI8x16Shuffle(Node* node) { UNREACHABLE(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI8x16Shuffle(Node* node) {
+  UNREACHABLE();
+}
 #endif  // V8_ENABLE_WEBASSEMBLY
 
-void InstructionSelector::VisitI8x16Swizzle(Node* node) {
-  ArmOperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI8x16Swizzle(Node* node) {
+  ArmOperandGeneratorT<Adapter> g(this);
   // We don't want input 0 (the table) to be the same as output, since we will
   // modify output twice (low and high), and need to keep the table the same.
   Emit(kArmI8x16Swizzle, g.DefineAsRegister(node),
        g.UseUniqueRegister(node->InputAt(0)), g.UseRegister(node->InputAt(1)));
 }
 
-void InstructionSelector::VisitSignExtendWord8ToInt32(Node* node) {
-  ArmOperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitSignExtendWord8ToInt32(Node* node) {
+  ArmOperandGeneratorT<Adapter> g(this);
   Emit(kArmSxtb, g.DefineAsRegister(node), g.UseRegister(node->InputAt(0)),
        g.TempImmediate(0));
 }
 
-void InstructionSelector::VisitSignExtendWord16ToInt32(Node* node) {
-  ArmOperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitSignExtendWord16ToInt32(Node* node) {
+  ArmOperandGeneratorT<Adapter> g(this);
   Emit(kArmSxth, g.DefineAsRegister(node), g.UseRegister(node->InputAt(0)),
        g.TempImmediate(0));
 }
 
-void InstructionSelector::VisitInt32AbsWithOverflow(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32AbsWithOverflow(Node* node) {
   UNREACHABLE();
 }
 
-void InstructionSelector::VisitInt64AbsWithOverflow(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt64AbsWithOverflow(Node* node) {
   UNREACHABLE();
 }
 
 namespace {
-template <ArchOpcode opcode>
-void VisitBitMask(InstructionSelector* selector, Node* node) {
-  ArmOperandGenerator g(selector);
+template <typename Adapter, ArchOpcode opcode>
+void VisitBitMask(InstructionSelectorT<Adapter>* selector, Node* node) {
+  ArmOperandGeneratorT<Adapter> g(selector);
   InstructionOperand temps[] = {g.TempSimd128Register()};
   selector->Emit(opcode, g.DefineAsRegister(node),
                  g.UseRegister(node->InputAt(0)), arraysize(temps), temps);
 }
 }  // namespace
 
-void InstructionSelector::VisitI8x16BitMask(Node* node) {
-  VisitBitMask<kArmI8x16BitMask>(this, node);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI8x16BitMask(Node* node) {
+  VisitBitMask<Adapter, kArmI8x16BitMask>(this, node);
 }
 
-void InstructionSelector::VisitI16x8BitMask(Node* node) {
-  VisitBitMask<kArmI16x8BitMask>(this, node);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI16x8BitMask(Node* node) {
+  VisitBitMask<Adapter, kArmI16x8BitMask>(this, node);
 }
 
-void InstructionSelector::VisitI32x4BitMask(Node* node) {
-  VisitBitMask<kArmI32x4BitMask>(this, node);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI32x4BitMask(Node* node) {
+  VisitBitMask<Adapter, kArmI32x4BitMask>(this, node);
 }
 
-void InstructionSelector::VisitI64x2BitMask(Node* node) {
-  VisitBitMask<kArmI64x2BitMask>(this, node);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI64x2BitMask(Node* node) {
+  VisitBitMask<Adapter, kArmI64x2BitMask>(this, node);
 }
 
 namespace {
-void VisitF32x4PminOrPmax(InstructionSelector* selector, ArchOpcode opcode,
-                          Node* node) {
-  ArmOperandGenerator g(selector);
+template <typename Adapter>
+void VisitF32x4PminOrPmax(InstructionSelectorT<Adapter>* selector,
+                          ArchOpcode opcode, Node* node) {
+  ArmOperandGeneratorT<Adapter> g(selector);
   // Need all unique registers because we first compare the two inputs, then we
   // need the inputs to remain unchanged for the bitselect later.
   selector->Emit(opcode, g.DefineAsRegister(node),
@@ -3184,36 +3372,43 @@ void VisitF32x4PminOrPmax(InstructionSelector* selector, ArchOpcode opcode,
                  g.UseUniqueRegister(node->InputAt(1)));
 }
 
-void VisitF64x2PminOrPMax(InstructionSelector* selector, ArchOpcode opcode,
-                          Node* node) {
-  ArmOperandGenerator g(selector);
+template <typename Adapter>
+void VisitF64x2PminOrPMax(InstructionSelectorT<Adapter>* selector,
+                          ArchOpcode opcode, Node* node) {
+  ArmOperandGeneratorT<Adapter> g(selector);
   selector->Emit(opcode, g.DefineSameAsFirst(node),
                  g.UseRegister(node->InputAt(0)),
                  g.UseRegister(node->InputAt(1)));
 }
 }  // namespace
 
-void InstructionSelector::VisitF32x4Pmin(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF32x4Pmin(Node* node) {
   VisitF32x4PminOrPmax(this, kArmF32x4Pmin, node);
 }
 
-void InstructionSelector::VisitF32x4Pmax(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF32x4Pmax(Node* node) {
   VisitF32x4PminOrPmax(this, kArmF32x4Pmax, node);
 }
 
-void InstructionSelector::VisitF64x2Pmin(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF64x2Pmin(Node* node) {
   VisitF64x2PminOrPMax(this, kArmF64x2Pmin, node);
 }
 
-void InstructionSelector::VisitF64x2Pmax(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF64x2Pmax(Node* node) {
   VisitF64x2PminOrPMax(this, kArmF64x2Pmax, node);
 }
 
-void InstructionSelector::VisitF64x2RelaxedMin(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF64x2RelaxedMin(Node* node) {
   VisitF64x2Pmin(node);
 }
 
-void InstructionSelector::VisitF64x2RelaxedMax(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF64x2RelaxedMax(Node* node) {
   VisitF64x2Pmax(node);
 }
 
@@ -3231,9 +3426,10 @@ void InstructionSelector::VisitF64x2RelaxedMax(Node* node) {
   V(I64x2ExtMulLowI32x4U, kArmVmullLow, NeonU32)   \
   V(I64x2ExtMulHighI32x4U, kArmVmullHigh, NeonU32)
 
-#define VISIT_EXT_MUL(OPCODE, VMULL, NEONSIZE)                 \
-  void InstructionSelector::Visit##OPCODE(Node* node) {        \
-    VisitRRR(this, VMULL | MiscField::encode(NEONSIZE), node); \
+#define VISIT_EXT_MUL(OPCODE, VMULL, NEONSIZE)                    \
+  template <typename Adapter>                                     \
+  void InstructionSelectorT<Adapter>::Visit##OPCODE(Node* node) { \
+    VisitRRR(this, VMULL | MiscField::encode(NEONSIZE), node);    \
   }
 
 EXT_MUL_LIST(VISIT_EXT_MUL)
@@ -3242,7 +3438,8 @@ EXT_MUL_LIST(VISIT_EXT_MUL)
 #undef EXT_MUL_LIST
 
 #define VISIT_EXTADD_PAIRWISE(OPCODE, NEONSIZE)                    \
-  void InstructionSelector::Visit##OPCODE(Node* node) {            \
+  template <typename Adapter>                                      \
+  void InstructionSelectorT<Adapter>::Visit##OPCODE(Node* node) {  \
     VisitRR(this, kArmVpaddl | MiscField::encode(NEONSIZE), node); \
   }
 VISIT_EXTADD_PAIRWISE(I16x8ExtAddPairwiseI8x16S, NeonS8)
@@ -3251,8 +3448,9 @@ VISIT_EXTADD_PAIRWISE(I32x4ExtAddPairwiseI16x8S, NeonS16)
 VISIT_EXTADD_PAIRWISE(I32x4ExtAddPairwiseI16x8U, NeonU16)
 #undef VISIT_EXTADD_PAIRWISE
 
-void InstructionSelector::VisitTruncateFloat32ToInt32(Node* node) {
-  ArmOperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitTruncateFloat32ToInt32(Node* node) {
+  ArmOperandGeneratorT<Adapter> g(this);
 
   InstructionCode opcode = kArmVcvtS32F32;
   TruncateKind kind = OpParameter<TruncateKind>(node->op());
@@ -3263,8 +3461,9 @@ void InstructionSelector::VisitTruncateFloat32ToInt32(Node* node) {
   Emit(opcode, g.DefineAsRegister(node), g.UseRegister(node->InputAt(0)));
 }
 
-void InstructionSelector::VisitTruncateFloat32ToUint32(Node* node) {
-  ArmOperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitTruncateFloat32ToUint32(Node* node) {
+  ArmOperandGeneratorT<Adapter> g(this);
 
   InstructionCode opcode = kArmVcvtU32F32;
   TruncateKind kind = OpParameter<TruncateKind>(node->op());
@@ -3278,59 +3477,70 @@ void InstructionSelector::VisitTruncateFloat32ToUint32(Node* node) {
 // TODO(v8:9780)
 // These double precision conversion instructions need a low Q register (q0-q7)
 // because the codegen accesses the S registers they overlap with.
-void InstructionSelector::VisitF64x2ConvertLowI32x4S(Node* node) {
-  ArmOperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF64x2ConvertLowI32x4S(Node* node) {
+  ArmOperandGeneratorT<Adapter> g(this);
   Emit(kArmF64x2ConvertLowI32x4S, g.DefineAsRegister(node),
        g.UseFixed(node->InputAt(0), q0));
 }
 
-void InstructionSelector::VisitF64x2ConvertLowI32x4U(Node* node) {
-  ArmOperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF64x2ConvertLowI32x4U(Node* node) {
+  ArmOperandGeneratorT<Adapter> g(this);
   Emit(kArmF64x2ConvertLowI32x4U, g.DefineAsRegister(node),
        g.UseFixed(node->InputAt(0), q0));
 }
 
-void InstructionSelector::VisitI32x4TruncSatF64x2SZero(Node* node) {
-  ArmOperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI32x4TruncSatF64x2SZero(Node* node) {
+  ArmOperandGeneratorT<Adapter> g(this);
   Emit(kArmI32x4TruncSatF64x2SZero, g.DefineAsFixed(node, q0),
        g.UseUniqueRegister(node->InputAt(0)));
 }
 
-void InstructionSelector::VisitI32x4TruncSatF64x2UZero(Node* node) {
-  ArmOperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI32x4TruncSatF64x2UZero(Node* node) {
+  ArmOperandGeneratorT<Adapter> g(this);
   Emit(kArmI32x4TruncSatF64x2UZero, g.DefineAsFixed(node, q0),
        g.UseUniqueRegister(node->InputAt(0)));
 }
 
-void InstructionSelector::VisitF32x4DemoteF64x2Zero(Node* node) {
-  ArmOperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF32x4DemoteF64x2Zero(Node* node) {
+  ArmOperandGeneratorT<Adapter> g(this);
   Emit(kArmF32x4DemoteF64x2Zero, g.DefineAsFixed(node, q0),
        g.UseUniqueRegister(node->InputAt(0)));
 }
 
-void InstructionSelector::VisitF64x2PromoteLowF32x4(Node* node) {
-  ArmOperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF64x2PromoteLowF32x4(Node* node) {
+  ArmOperandGeneratorT<Adapter> g(this);
   Emit(kArmF64x2PromoteLowF32x4, g.DefineAsRegister(node),
        g.UseFixed(node->InputAt(0), q0));
 }
 
-void InstructionSelector::VisitI32x4RelaxedTruncF64x2SZero(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI32x4RelaxedTruncF64x2SZero(
+    Node* node) {
   VisitI32x4TruncSatF64x2SZero(node);
 }
 
-void InstructionSelector::VisitI32x4RelaxedTruncF64x2UZero(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI32x4RelaxedTruncF64x2UZero(
+    Node* node) {
   VisitI32x4TruncSatF64x2UZero(node);
 }
 
-void InstructionSelector::AddOutputToSelectContinuation(OperandGenerator* g,
-                                                        int first_input_index,
-                                                        Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::AddOutputToSelectContinuation(
+    OperandGenerator* g, int first_input_index, Node* node) {
   UNREACHABLE();
 }
 
 // static
+template <typename Adapter>
 MachineOperatorBuilder::Flags
-InstructionSelector::SupportedMachineOperatorFlags() {
+InstructionSelectorT<Adapter>::SupportedMachineOperatorFlags() {
   MachineOperatorBuilder::Flags flags = MachineOperatorBuilder::kNoFlags;
   if (CpuFeatures::IsSupported(SUDIV)) {
     // The sdiv and udiv instructions correctly return 0 if the divisor is 0,
@@ -3357,8 +3567,9 @@ InstructionSelector::SupportedMachineOperatorFlags() {
 }
 
 // static
+template <typename Adapter>
 MachineOperatorBuilder::AlignmentRequirements
-InstructionSelector::AlignmentRequirements() {
+InstructionSelectorT<Adapter>::AlignmentRequirements() {
   base::EnumSet<MachineRepresentation> req_aligned;
   req_aligned.Add(MachineRepresentation::kFloat32);
   req_aligned.Add(MachineRepresentation::kFloat64);
@@ -3366,6 +3577,11 @@ InstructionSelector::AlignmentRequirements() {
       SomeUnalignedAccessUnsupported(req_aligned, req_aligned);
 }
 
+template class EXPORT_TEMPLATE_DEFINE(V8_EXPORT_PRIVATE)
+    InstructionSelectorT<TurbofanAdapter>;
+template class EXPORT_TEMPLATE_DEFINE(V8_EXPORT_PRIVATE)
+    InstructionSelectorT<TurboshaftAdapter>;
+
 }  // namespace compiler
 }  // namespace internal
 }  // namespace v8
diff --git a/src/compiler/backend/arm64/instruction-selector-arm64.cc b/src/compiler/backend/arm64/instruction-selector-arm64.cc
index 193c5a03525..5fda7e8b565 100644
--- a/src/compiler/backend/arm64/instruction-selector-arm64.cc
+++ b/src/compiler/backend/arm64/instruction-selector-arm64.cc
@@ -31,10 +31,13 @@ enum ImmediateMode {
 };
 
 // Adds Arm64-specific methods for generating operands.
-class Arm64OperandGenerator final : public OperandGenerator {
+template <typename Adapter>
+class Arm64OperandGeneratorT final : public OperandGeneratorT<Adapter> {
  public:
-  explicit Arm64OperandGenerator(InstructionSelector* selector)
-      : OperandGenerator(selector) {}
+  OPERAND_GENERATOR_T_BOILERPLATE(Adapter)
+
+  explicit Arm64OperandGeneratorT(InstructionSelectorT<Adapter>* selector)
+      : super(selector) {}
 
   InstructionOperand UseOperand(Node* node, ImmediateMode mode) {
     if (CanBeImmediate(node, mode)) {
@@ -162,37 +165,44 @@ class Arm64OperandGenerator final : public OperandGenerator {
 
 namespace {
 
-void VisitRR(InstructionSelector* selector, ArchOpcode opcode, Node* node) {
-  Arm64OperandGenerator g(selector);
+template <typename Adapter>
+void VisitRR(InstructionSelectorT<Adapter>* selector, ArchOpcode opcode,
+             Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(selector);
   selector->Emit(opcode, g.DefineAsRegister(node),
                  g.UseRegister(node->InputAt(0)));
 }
 
-void VisitRR(InstructionSelector* selector, InstructionCode opcode,
+template <typename Adapter>
+void VisitRR(InstructionSelectorT<Adapter>* selector, InstructionCode opcode,
              Node* node) {
-  Arm64OperandGenerator g(selector);
+  Arm64OperandGeneratorT<Adapter> g(selector);
   selector->Emit(opcode, g.DefineAsRegister(node),
                  g.UseRegister(node->InputAt(0)));
 }
 
-void VisitRRR(InstructionSelector* selector, ArchOpcode opcode, Node* node) {
-  Arm64OperandGenerator g(selector);
+template <typename Adapter>
+void VisitRRR(InstructionSelectorT<Adapter>* selector, ArchOpcode opcode,
+              Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(selector);
   selector->Emit(opcode, g.DefineAsRegister(node),
                  g.UseRegister(node->InputAt(0)),
                  g.UseRegister(node->InputAt(1)));
 }
 
-void VisitRRR(InstructionSelector* selector, InstructionCode opcode,
+template <typename Adapter>
+void VisitRRR(InstructionSelectorT<Adapter>* selector, InstructionCode opcode,
               Node* node) {
-  Arm64OperandGenerator g(selector);
+  Arm64OperandGeneratorT<Adapter> g(selector);
   selector->Emit(opcode, g.DefineAsRegister(node),
                  g.UseRegister(node->InputAt(0)),
                  g.UseRegister(node->InputAt(1)));
 }
 
-void VisitSimdShiftRRR(InstructionSelector* selector, ArchOpcode opcode,
-                       Node* node, int width) {
-  Arm64OperandGenerator g(selector);
+template <typename Adapter>
+void VisitSimdShiftRRR(InstructionSelectorT<Adapter>* selector,
+                       ArchOpcode opcode, Node* node, int width) {
+  Arm64OperandGeneratorT<Adapter> g(selector);
   if (g.IsIntegerConstant(node->InputAt(1))) {
     if (g.GetIntegerConstantValue(node->InputAt(1)) % width == 0) {
       selector->EmitIdentity(node);
@@ -208,33 +218,37 @@ void VisitSimdShiftRRR(InstructionSelector* selector, ArchOpcode opcode,
   }
 }
 
-void VisitRRI(InstructionSelector* selector, InstructionCode opcode,
+template <typename Adapter>
+void VisitRRI(InstructionSelectorT<Adapter>* selector, InstructionCode opcode,
               Node* node) {
-  Arm64OperandGenerator g(selector);
+  Arm64OperandGeneratorT<Adapter> g(selector);
   int32_t imm = OpParameter<int32_t>(node->op());
   selector->Emit(opcode, g.DefineAsRegister(node),
                  g.UseRegister(node->InputAt(0)), g.UseImmediate(imm));
 }
 
-void VisitRRO(InstructionSelector* selector, ArchOpcode opcode, Node* node,
-              ImmediateMode operand_mode) {
-  Arm64OperandGenerator g(selector);
+template <typename Adapter>
+void VisitRRO(InstructionSelectorT<Adapter>* selector, ArchOpcode opcode,
+              Node* node, ImmediateMode operand_mode) {
+  Arm64OperandGeneratorT<Adapter> g(selector);
   selector->Emit(opcode, g.DefineAsRegister(node),
                  g.UseRegister(node->InputAt(0)),
                  g.UseOperand(node->InputAt(1), operand_mode));
 }
 
-void VisitRRIR(InstructionSelector* selector, InstructionCode opcode,
+template <typename Adapter>
+void VisitRRIR(InstructionSelectorT<Adapter>* selector, InstructionCode opcode,
                Node* node) {
-  Arm64OperandGenerator g(selector);
+  Arm64OperandGeneratorT<Adapter> g(selector);
   int32_t imm = OpParameter<int32_t>(node->op());
   selector->Emit(opcode, g.DefineAsRegister(node),
                  g.UseRegister(node->InputAt(0)), g.UseImmediate(imm),
                  g.UseUniqueRegister(node->InputAt(1)));
 }
 
+template <typename Adapter>
 struct ExtendingLoadMatcher {
-  ExtendingLoadMatcher(Node* node, InstructionSelector* selector)
+  ExtendingLoadMatcher(Node* node, InstructionSelectorT<Adapter>* selector)
       : matches_(false), selector_(selector), base_(nullptr), immediate_(0) {
     Initialize(node);
   }
@@ -256,7 +270,7 @@ struct ExtendingLoadMatcher {
 
  private:
   bool matches_;
-  InstructionSelector* selector_;
+  InstructionSelectorT<Adapter>* selector_;
   Node* base_;
   int64_t immediate_;
   ArchOpcode opcode_;
@@ -269,7 +283,7 @@ struct ExtendingLoadMatcher {
     DCHECK(m.IsWord64Sar());
     if (m.left().IsLoad() && m.right().Is(32) &&
         selector_->CanCover(m.node(), m.left().node())) {
-      Arm64OperandGenerator g(selector_);
+      Arm64OperandGeneratorT<Adapter> g(selector_);
       Node* load = m.left().node();
       Node* offset = load->InputAt(1);
       base_ = load->InputAt(0);
@@ -282,14 +296,17 @@ struct ExtendingLoadMatcher {
   }
 };
 
-bool TryMatchExtendingLoad(InstructionSelector* selector, Node* node) {
-  ExtendingLoadMatcher m(node, selector);
+template <typename Adapter>
+bool TryMatchExtendingLoad(InstructionSelectorT<Adapter>* selector,
+                           Node* node) {
+  ExtendingLoadMatcher<Adapter> m(node, selector);
   return m.Matches();
 }
 
-bool TryEmitExtendingLoad(InstructionSelector* selector, Node* node) {
-  ExtendingLoadMatcher m(node, selector);
-  Arm64OperandGenerator g(selector);
+template <typename Adapter>
+bool TryEmitExtendingLoad(InstructionSelectorT<Adapter>* selector, Node* node) {
+  ExtendingLoadMatcher<Adapter> m(node, selector);
+  Arm64OperandGeneratorT<Adapter> g(selector);
   if (m.Matches()) {
     InstructionOperand inputs[2];
     inputs[0] = g.UseRegister(m.base());
@@ -305,10 +322,11 @@ bool TryEmitExtendingLoad(InstructionSelector* selector, Node* node) {
   return false;
 }
 
-bool TryMatchAnyShift(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+bool TryMatchAnyShift(InstructionSelectorT<Adapter>* selector, Node* node,
                       Node* input_node, InstructionCode* opcode, bool try_ror,
                       MachineRepresentation rep) {
-  Arm64OperandGenerator g(selector);
+  Arm64OperandGeneratorT<Adapter> g(selector);
 
   if (!selector->CanCover(node, input_node)) return false;
   if (input_node->InputCount() != 2) return false;
@@ -359,8 +377,10 @@ bool TryMatchAnyShift(InstructionSelector* selector, Node* node,
   }
 }
 
-bool TryMatchAnyExtend(Arm64OperandGenerator* g, InstructionSelector* selector,
-                       Node* node, Node* left_node, Node* right_node,
+template <typename Adapter>
+bool TryMatchAnyExtend(Arm64OperandGeneratorT<Adapter>* g,
+                       InstructionSelectorT<Adapter>* selector, Node* node,
+                       Node* left_node, Node* right_node,
                        InstructionOperand* left_op,
                        InstructionOperand* right_op, InstructionCode* opcode) {
   if (!selector->CanCover(node, right_node)) return false;
@@ -402,8 +422,9 @@ bool TryMatchAnyExtend(Arm64OperandGenerator* g, InstructionSelector* selector,
   return false;
 }
 
-bool TryMatchLoadStoreShift(Arm64OperandGenerator* g,
-                            InstructionSelector* selector,
+template <typename Adapter>
+bool TryMatchLoadStoreShift(Arm64OperandGeneratorT<Adapter>* g,
+                            InstructionSelectorT<Adapter>* selector,
                             MachineRepresentation rep, Node* node, Node* index,
                             InstructionOperand* index_op,
                             InstructionOperand* shift_immediate_op) {
@@ -491,11 +512,11 @@ uint8_t GetBinopProperties(InstructionCode opcode) {
 }
 
 // Shared routine for multiple binary operations.
-template <typename Matcher>
-void VisitBinop(InstructionSelector* selector, Node* node,
+template <typename Adapter, typename Matcher>
+void VisitBinop(InstructionSelectorT<Adapter>* selector, Node* node,
                 InstructionCode opcode, ImmediateMode operand_mode,
                 FlagsContinuation* cont) {
-  Arm64OperandGenerator g(selector);
+  Arm64OperandGeneratorT<Adapter> g(selector);
   InstructionOperand inputs[5];
   size_t input_count = 0;
   InstructionOperand outputs[1];
@@ -567,17 +588,17 @@ void VisitBinop(InstructionSelector* selector, Node* node,
 }
 
 // Shared routine for multiple binary operations.
-template <typename Matcher>
-void VisitBinop(InstructionSelector* selector, Node* node, ArchOpcode opcode,
-                ImmediateMode operand_mode) {
+template <typename Adapter, typename Matcher>
+void VisitBinop(InstructionSelectorT<Adapter>* selector, Node* node,
+                ArchOpcode opcode, ImmediateMode operand_mode) {
   FlagsContinuation cont;
-  VisitBinop<Matcher>(selector, node, opcode, operand_mode, &cont);
+  VisitBinop<Adapter, Matcher>(selector, node, opcode, operand_mode, &cont);
 }
 
-template <typename Matcher>
-void VisitAddSub(InstructionSelector* selector, Node* node, ArchOpcode opcode,
-                 ArchOpcode negate_opcode) {
-  Arm64OperandGenerator g(selector);
+template <typename Adapter, typename Matcher>
+void VisitAddSub(InstructionSelectorT<Adapter>* selector, Node* node,
+                 ArchOpcode opcode, ArchOpcode negate_opcode) {
+  Arm64OperandGeneratorT<Adapter> g(selector);
   Matcher m(node);
   if (m.right().HasResolvedValue() && (m.right().ResolvedValue() < 0) &&
       (m.right().ResolvedValue() > std::numeric_limits<int>::min()) &&
@@ -586,7 +607,7 @@ void VisitAddSub(InstructionSelector* selector, Node* node, ArchOpcode opcode,
         negate_opcode, g.DefineAsRegister(node), g.UseRegister(m.left().node()),
         g.TempImmediate(static_cast<int32_t>(-m.right().ResolvedValue())));
   } else {
-    VisitBinop<Matcher>(selector, node, opcode, kArithmeticImm);
+    VisitBinop<Adapter, Matcher>(selector, node, opcode, kArithmeticImm);
   }
 }
 
@@ -607,9 +628,11 @@ int32_t LeftShiftForReducedMultiply(Matcher* m) {
 
 }  // namespace
 
-void InstructionSelector::VisitTraceInstruction(Node* node) {}
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitTraceInstruction(Node* node) {}
 
-void InstructionSelector::VisitStackSlot(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitStackSlot(Node* node) {
   StackSlotRepresentation rep = StackSlotRepresentationOf(node->op());
   int slot = frame_->AllocateSpillSlot(rep.size(), rep.alignment());
   OperandGenerator g(this);
@@ -618,15 +641,17 @@ void InstructionSelector::VisitStackSlot(Node* node) {
        sequence()->AddImmediate(Constant(slot)), 0, nullptr);
 }
 
-void InstructionSelector::VisitAbortCSADcheck(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitAbortCSADcheck(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   Emit(kArchAbortCSADcheck, g.NoOutput(), g.UseFixed(node->InputAt(0), x1));
 }
 
-void EmitLoad(InstructionSelector* selector, Node* node, InstructionCode opcode,
-              ImmediateMode immediate_mode, MachineRepresentation rep,
-              Node* output = nullptr) {
-  Arm64OperandGenerator g(selector);
+template <typename Adapter>
+void EmitLoad(InstructionSelectorT<Adapter>* selector, Node* node,
+              InstructionCode opcode, ImmediateMode immediate_mode,
+              MachineRepresentation rep, Node* output = nullptr) {
+  Arm64OperandGeneratorT<Adapter> g(selector);
   Node* base = node->InputAt(0);
   Node* index = node->InputAt(1);
   InstructionOperand inputs[3];
@@ -686,10 +711,11 @@ namespace {
 // Manually add base and index into a register to get the actual address.
 // This should be used prior to instructions that only support
 // immediate/post-index addressing, like ld1 and st1.
-InstructionOperand EmitAddBeforeLoadOrStore(InstructionSelector* selector,
-                                            Node* node,
-                                            InstructionCode* opcode) {
-  Arm64OperandGenerator g(selector);
+template <typename Adapter>
+InstructionOperand EmitAddBeforeLoadOrStore(
+    InstructionSelectorT<Adapter>* selector, Node* node,
+    InstructionCode* opcode) {
+  Arm64OperandGeneratorT<Adapter> g(selector);
   InstructionOperand addr = g.TempRegister();
   selector->Emit(kArm64Add, addr, g.UseRegister(node->InputAt(0)),
                  g.UseRegister(node->InputAt(1)));
@@ -698,7 +724,8 @@ InstructionOperand EmitAddBeforeLoadOrStore(InstructionSelector* selector,
 }
 }  // namespace
 
-void InstructionSelector::VisitLoadLane(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitLoadLane(Node* node) {
   LoadLaneParameters params = LoadLaneParametersOf(node->op());
   DCHECK(
       params.rep == MachineType::Int8() || params.rep == MachineType::Int16() ||
@@ -710,13 +737,14 @@ void InstructionSelector::VisitLoadLane(Node* node) {
     opcode |= AccessModeField::encode(kMemoryAccessProtectedMemOutOfBounds);
   }
 
-  Arm64OperandGenerator g(this);
+  Arm64OperandGeneratorT<Adapter> g(this);
   InstructionOperand addr = EmitAddBeforeLoadOrStore(this, node, &opcode);
   Emit(opcode, g.DefineSameAsFirst(node), g.UseRegister(node->InputAt(2)),
        g.UseImmediate(params.laneidx), addr, g.TempImmediate(0));
 }
 
-void InstructionSelector::VisitStoreLane(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitStoreLane(Node* node) {
   StoreLaneParameters params = StoreLaneParametersOf(node->op());
   DCHECK_LE(MachineRepresentation::kWord8, params.rep);
   DCHECK_GE(MachineRepresentation::kWord64, params.rep);
@@ -728,7 +756,7 @@ void InstructionSelector::VisitStoreLane(Node* node) {
     opcode |= AccessModeField::encode(kMemoryAccessProtectedMemOutOfBounds);
   }
 
-  Arm64OperandGenerator g(this);
+  Arm64OperandGeneratorT<Adapter> g(this);
   InstructionOperand addr = EmitAddBeforeLoadOrStore(this, node, &opcode);
   InstructionOperand inputs[4] = {
       g.UseRegister(node->InputAt(2)),
@@ -740,7 +768,8 @@ void InstructionSelector::VisitStoreLane(Node* node) {
   Emit(opcode, 0, nullptr, 4, inputs);
 }
 
-void InstructionSelector::VisitLoadTransform(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitLoadTransform(Node* node) {
   LoadTransformParameters params = LoadTransformParametersOf(node->op());
   InstructionCode opcode = kArchNop;
   bool require_add = false;
@@ -795,7 +824,7 @@ void InstructionSelector::VisitLoadTransform(Node* node) {
   // ARM64 supports unaligned loads
   DCHECK_NE(params.kind, MemoryAccessKind::kUnaligned);
 
-  Arm64OperandGenerator g(this);
+  Arm64OperandGeneratorT<Adapter> g(this);
   Node* base = node->InputAt(0);
   Node* index = node->InputAt(1);
   InstructionOperand inputs[2];
@@ -820,7 +849,8 @@ void InstructionSelector::VisitLoadTransform(Node* node) {
   Emit(opcode, 1, outputs, 2, inputs);
 }
 
-void InstructionSelector::VisitLoad(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitLoad(Node* node) {
   InstructionCode opcode = kArchNop;
   ImmediateMode immediate_mode = kNoImmediate;
   LoadRepresentation load_rep = LoadRepresentationOf(node->op());
@@ -905,14 +935,21 @@ void InstructionSelector::VisitLoad(Node* node) {
   EmitLoad(this, node, opcode, immediate_mode, rep);
 }
 
-void InstructionSelector::VisitProtectedLoad(Node* node) { VisitLoad(node); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitProtectedLoad(Node* node) {
+  VisitLoad(node);
+}
 
-void InstructionSelector::VisitStorePair(Node* node) { VisitStore(node); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitStorePair(Node* node) {
+  VisitStore(node);
+}
 
-void InstructionSelector::VisitStore(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitStore(Node* node) {
   const bool kStorePair = node->opcode() == IrOpcode::kStorePair;
 
-  Arm64OperandGenerator g(this);
+  Arm64OperandGeneratorT<Adapter> g(this);
   Node* base = node->InputAt(0);
   Node* index = node->InputAt(1);
   Node* value = node->InputAt(2);
@@ -1143,23 +1180,33 @@ void InstructionSelector::VisitStore(Node* node) {
   Emit(opcode, 0, nullptr, input_count, inputs);
 }
 
-void InstructionSelector::VisitProtectedStore(Node* node) { VisitStore(node); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitProtectedStore(Node* node) {
+  VisitStore(node);
+}
 
-void InstructionSelector::VisitSimd128ReverseBytes(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitSimd128ReverseBytes(Node* node) {
   UNREACHABLE();
 }
 
 // Architecture supports unaligned access, therefore VisitLoad is used instead
-void InstructionSelector::VisitUnalignedLoad(Node* node) { UNREACHABLE(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitUnalignedLoad(Node* node) {
+  UNREACHABLE();
+}
 
 // Architecture supports unaligned access, therefore VisitStore is used instead
-void InstructionSelector::VisitUnalignedStore(Node* node) { UNREACHABLE(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitUnalignedStore(Node* node) {
+  UNREACHABLE();
+}
 
-template <typename Matcher>
-static void VisitLogical(InstructionSelector* selector, Node* node, Matcher* m,
-                         ArchOpcode opcode, bool left_can_cover,
+template <typename Adapter, typename Matcher>
+static void VisitLogical(InstructionSelectorT<Adapter>* selector, Node* node,
+                         Matcher* m, ArchOpcode opcode, bool left_can_cover,
                          bool right_can_cover, ImmediateMode imm_mode) {
-  Arm64OperandGenerator g(selector);
+  Arm64OperandGeneratorT<Adapter> g(selector);
 
   // Map instruction to equivalent operation with inverted right input.
   ArchOpcode inv_opcode = opcode;
@@ -1218,12 +1265,13 @@ static void VisitLogical(InstructionSelector* selector, Node* node, Matcher* m,
     selector->Emit(kArm64Not, g.DefineAsRegister(node),
                    g.UseRegister(m->left().node()));
   } else {
-    VisitBinop<Matcher>(selector, node, opcode, imm_mode);
+    VisitBinop<Adapter, Matcher>(selector, node, opcode, imm_mode);
   }
 }
 
-void InstructionSelector::VisitWord32And(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32And(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   Int32BinopMatcher m(node);
   if (m.left().IsWord32Shr() && CanCover(node, m.left().node()) &&
       m.right().HasResolvedValue()) {
@@ -1257,13 +1305,14 @@ void InstructionSelector::VisitWord32And(Node* node) {
       // Other cases fall through to the normal And operation.
     }
   }
-  VisitLogical<Int32BinopMatcher>(
+  VisitLogical<Adapter, Int32BinopMatcher>(
       this, node, &m, kArm64And32, CanCover(node, m.left().node()),
       CanCover(node, m.right().node()), kLogical32Imm);
 }
 
-void InstructionSelector::VisitWord64And(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64And(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   Int64BinopMatcher m(node);
   if (m.left().IsWord64Shr() && CanCover(node, m.left().node()) &&
       m.right().HasResolvedValue()) {
@@ -1298,44 +1347,49 @@ void InstructionSelector::VisitWord64And(Node* node) {
       // Other cases fall through to the normal And operation.
     }
   }
-  VisitLogical<Int64BinopMatcher>(
+  VisitLogical<Adapter, Int64BinopMatcher>(
       this, node, &m, kArm64And, CanCover(node, m.left().node()),
       CanCover(node, m.right().node()), kLogical64Imm);
 }
 
-void InstructionSelector::VisitWord32Or(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32Or(Node* node) {
   Int32BinopMatcher m(node);
-  VisitLogical<Int32BinopMatcher>(
+  VisitLogical<Adapter, Int32BinopMatcher>(
       this, node, &m, kArm64Or32, CanCover(node, m.left().node()),
       CanCover(node, m.right().node()), kLogical32Imm);
 }
 
-void InstructionSelector::VisitWord64Or(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64Or(Node* node) {
   Int64BinopMatcher m(node);
-  VisitLogical<Int64BinopMatcher>(
+  VisitLogical<Adapter, Int64BinopMatcher>(
       this, node, &m, kArm64Or, CanCover(node, m.left().node()),
       CanCover(node, m.right().node()), kLogical64Imm);
 }
 
-void InstructionSelector::VisitWord32Xor(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32Xor(Node* node) {
   Int32BinopMatcher m(node);
-  VisitLogical<Int32BinopMatcher>(
+  VisitLogical<Adapter, Int32BinopMatcher>(
       this, node, &m, kArm64Eor32, CanCover(node, m.left().node()),
       CanCover(node, m.right().node()), kLogical32Imm);
 }
 
-void InstructionSelector::VisitWord64Xor(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64Xor(Node* node) {
   Int64BinopMatcher m(node);
-  VisitLogical<Int64BinopMatcher>(
+  VisitLogical<Adapter, Int64BinopMatcher>(
       this, node, &m, kArm64Eor, CanCover(node, m.left().node()),
       CanCover(node, m.right().node()), kLogical64Imm);
 }
 
-void InstructionSelector::VisitWord32Shl(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32Shl(Node* node) {
   Int32BinopMatcher m(node);
   if (m.left().IsWord32And() && CanCover(node, m.left().node()) &&
       m.right().IsInRange(1, 31)) {
-    Arm64OperandGenerator g(this);
+    Arm64OperandGeneratorT<Adapter> g(this);
     Int32BinopMatcher mleft(m.left().node());
     if (mleft.right().HasResolvedValue()) {
       uint32_t mask = mleft.right().ResolvedValue();
@@ -1367,8 +1421,9 @@ void InstructionSelector::VisitWord32Shl(Node* node) {
   VisitRRO(this, kArm64Lsl32, node, kShift32Imm);
 }
 
-void InstructionSelector::VisitWord64Shl(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64Shl(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   Int64BinopMatcher m(node);
   if ((m.left().IsChangeInt32ToInt64() || m.left().IsChangeUint32ToUint64()) &&
       m.right().IsInRange(32, 63) && CanCover(node, m.left().node())) {
@@ -1382,13 +1437,14 @@ void InstructionSelector::VisitWord64Shl(Node* node) {
   VisitRRO(this, kArm64Lsl, node, kShift64Imm);
 }
 
-void InstructionSelector::VisitStackPointerGreaterThan(
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitStackPointerGreaterThan(
     Node* node, FlagsContinuation* cont) {
   StackCheckKind kind = StackCheckKindOf(node->op());
   InstructionCode opcode =
       kArchStackPointerGreaterThan | MiscField::encode(static_cast<int>(kind));
 
-  Arm64OperandGenerator g(this);
+  Arm64OperandGeneratorT<Adapter> g(this);
 
   // No outputs.
   InstructionOperand* const outputs = nullptr;
@@ -1413,8 +1469,10 @@ void InstructionSelector::VisitStackPointerGreaterThan(
 
 namespace {
 
-bool TryEmitBitfieldExtract32(InstructionSelector* selector, Node* node) {
-  Arm64OperandGenerator g(selector);
+template <typename Adapter>
+bool TryEmitBitfieldExtract32(InstructionSelectorT<Adapter>* selector,
+                              Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(selector);
   Int32BinopMatcher m(node);
   if (selector->CanCover(node, m.left().node()) && m.left().IsWord32Shl()) {
     // Select Ubfx or Sbfx for (x << (K & 0x1F)) OP (K & 0x1F), where
@@ -1441,7 +1499,8 @@ bool TryEmitBitfieldExtract32(InstructionSelector* selector, Node* node) {
 
 }  // namespace
 
-void InstructionSelector::VisitWord32Shr(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32Shr(Node* node) {
   Int32BinopMatcher m(node);
   if (m.left().IsWord32And() && m.right().HasResolvedValue()) {
     uint32_t lsb = m.right().ResolvedValue() & 0x1F;
@@ -1455,7 +1514,7 @@ void InstructionSelector::VisitWord32Shr(Node* node) {
       unsigned mask_width = base::bits::CountPopulation(mask);
       unsigned mask_msb = base::bits::CountLeadingZeros32(mask);
       if ((mask_msb + mask_width + lsb) == 32) {
-        Arm64OperandGenerator g(this);
+        Arm64OperandGeneratorT<Adapter> g(this);
         DCHECK_EQ(lsb, base::bits::CountTrailingZeros32(mask));
         Emit(kArm64Ubfx32, g.DefineAsRegister(node),
              g.UseRegister(mleft.left().node()),
@@ -1472,7 +1531,7 @@ void InstructionSelector::VisitWord32Shr(Node* node) {
       CanCover(node, node->InputAt(0))) {
     // Combine this shift with the multiply and shift that would be generated
     // by Uint32MulHigh.
-    Arm64OperandGenerator g(this);
+    Arm64OperandGeneratorT<Adapter> g(this);
     Node* left = m.left().node();
     int shift = m.right().ResolvedValue() & 0x1F;
     InstructionOperand const smull_operand = g.TempRegister();
@@ -1486,7 +1545,8 @@ void InstructionSelector::VisitWord32Shr(Node* node) {
   VisitRRO(this, kArm64Lsr32, node, kShift32Imm);
 }
 
-void InstructionSelector::VisitWord64Shr(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64Shr(Node* node) {
   Int64BinopMatcher m(node);
   if (m.left().IsWord64And() && m.right().HasResolvedValue()) {
     uint32_t lsb = m.right().ResolvedValue() & 0x3F;
@@ -1500,7 +1560,7 @@ void InstructionSelector::VisitWord64Shr(Node* node) {
       unsigned mask_width = base::bits::CountPopulation(mask);
       unsigned mask_msb = base::bits::CountLeadingZeros64(mask);
       if ((mask_msb + mask_width + lsb) == 64) {
-        Arm64OperandGenerator g(this);
+        Arm64OperandGeneratorT<Adapter> g(this);
         DCHECK_EQ(lsb, base::bits::CountTrailingZeros64(mask));
         Emit(kArm64Ubfx, g.DefineAsRegister(node),
              g.UseRegister(mleft.left().node()),
@@ -1513,7 +1573,8 @@ void InstructionSelector::VisitWord64Shr(Node* node) {
   VisitRRO(this, kArm64Lsr, node, kShift64Imm);
 }
 
-void InstructionSelector::VisitWord32Sar(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32Sar(Node* node) {
   if (TryEmitBitfieldExtract32(this, node)) {
     return;
   }
@@ -1523,7 +1584,7 @@ void InstructionSelector::VisitWord32Sar(Node* node) {
       CanCover(node, node->InputAt(0))) {
     // Combine this shift with the multiply and shift that would be generated
     // by Int32MulHigh.
-    Arm64OperandGenerator g(this);
+    Arm64OperandGeneratorT<Adapter> g(this);
     Node* left = m.left().node();
     int shift = m.right().ResolvedValue() & 0x1F;
     InstructionOperand const smull_operand = g.TempRegister();
@@ -1544,7 +1605,7 @@ void InstructionSelector::VisitWord32Sar(Node* node) {
       // on the left of this Sar operation. We do it here, as the result of the
       // add potentially has 33 bits, so we have to ensure the result is
       // truncated by being the input to this 32-bit Sar operation.
-      Arm64OperandGenerator g(this);
+      Arm64OperandGeneratorT<Adapter> g(this);
       Node* mul_node = madd_node.left().node();
 
       InstructionOperand const smull_operand = g.TempRegister();
@@ -1565,7 +1626,8 @@ void InstructionSelector::VisitWord32Sar(Node* node) {
   VisitRRO(this, kArm64Asr32, node, kShift32Imm);
 }
 
-void InstructionSelector::VisitWord64Sar(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64Sar(Node* node) {
   if (TryEmitExtendingLoad(this, node)) return;
 
   // Select Sbfx(x, imm, 32-imm) for Word64Sar(ChangeInt32ToInt64(x), imm)
@@ -1578,7 +1640,7 @@ void InstructionSelector::VisitWord64Sar(Node* node) {
     if ((m.left().InputAt(0)->opcode() != IrOpcode::kLoad &&
          m.left().InputAt(0)->opcode() != IrOpcode::kLoadImmutable) ||
         !CanCover(m.left().node(), m.left().InputAt(0))) {
-      Arm64OperandGenerator g(this);
+      Arm64OperandGeneratorT<Adapter> g(this);
       int right = static_cast<int>(m.right().ResolvedValue());
       Emit(kArm64Sbfx, g.DefineAsRegister(node),
            g.UseRegister(m.left().node()->InputAt(0)),
@@ -1590,15 +1652,23 @@ void InstructionSelector::VisitWord64Sar(Node* node) {
   VisitRRO(this, kArm64Asr, node, kShift64Imm);
 }
 
-void InstructionSelector::VisitWord32Rol(Node* node) { UNREACHABLE(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32Rol(Node* node) {
+  UNREACHABLE();
+}
 
-void InstructionSelector::VisitWord64Rol(Node* node) { UNREACHABLE(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64Rol(Node* node) {
+  UNREACHABLE();
+}
 
-void InstructionSelector::VisitWord32Ror(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32Ror(Node* node) {
   VisitRRO(this, kArm64Ror32, node, kShift32Imm);
 }
 
-void InstructionSelector::VisitWord64Ror(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64Ror(Node* node) {
   VisitRRO(this, kArm64Ror, node, kShift64Imm);
 }
 
@@ -1677,28 +1747,37 @@ void InstructionSelector::VisitWord64Ror(Node* node) {
   V(Float64Min, kArm64Float64Min) \
   V(I8x16Swizzle, kArm64I8x16Swizzle)
 
-#define RR_VISITOR(Name, opcode)                      \
-  void InstructionSelector::Visit##Name(Node* node) { \
-    VisitRR(this, opcode, node);                      \
+#define RR_VISITOR(Name, opcode)                                \
+  template <typename Adapter>                                   \
+  void InstructionSelectorT<Adapter>::Visit##Name(Node* node) { \
+    VisitRR(this, opcode, node);                                \
   }
 RR_OP_LIST(RR_VISITOR)
 #undef RR_VISITOR
 #undef RR_OP_LIST
 
-#define RRR_VISITOR(Name, opcode)                     \
-  void InstructionSelector::Visit##Name(Node* node) { \
-    VisitRRR(this, opcode, node);                     \
+#define RRR_VISITOR(Name, opcode)                               \
+  template <typename Adapter>                                   \
+  void InstructionSelectorT<Adapter>::Visit##Name(Node* node) { \
+    VisitRRR(this, opcode, node);                               \
   }
 RRR_OP_LIST(RRR_VISITOR)
 #undef RRR_VISITOR
 #undef RRR_OP_LIST
 
-void InstructionSelector::VisitWord32Ctz(Node* node) { UNREACHABLE(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32Ctz(Node* node) {
+  UNREACHABLE();
+}
 
-void InstructionSelector::VisitWord64Ctz(Node* node) { UNREACHABLE(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64Ctz(Node* node) {
+  UNREACHABLE();
+}
 
-void InstructionSelector::VisitInt32Add(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32Add(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   Int32BinopMatcher m(node);
   // Select Madd(x, y, z) for Add(Mul(x, y), z).
   if (m.left().IsInt32Mul() && CanCover(node, m.left().node())) {
@@ -1724,11 +1803,12 @@ void InstructionSelector::VisitInt32Add(Node* node) {
       return;
     }
   }
-  VisitAddSub<Int32BinopMatcher>(this, node, kArm64Add32, kArm64Sub32);
+  VisitAddSub<Adapter, Int32BinopMatcher>(this, node, kArm64Add32, kArm64Sub32);
 }
 
-void InstructionSelector::VisitInt64Add(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt64Add(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   Int64BinopMatcher m(node);
   // Select Madd(x, y, z) for Add(Mul(x, y), z).
   if (m.left().IsInt64Mul() && CanCover(node, m.left().node())) {
@@ -1754,11 +1834,12 @@ void InstructionSelector::VisitInt64Add(Node* node) {
       return;
     }
   }
-  VisitAddSub<Int64BinopMatcher>(this, node, kArm64Add, kArm64Sub);
+  VisitAddSub<Adapter, Int64BinopMatcher>(this, node, kArm64Add, kArm64Sub);
 }
 
-void InstructionSelector::VisitInt32Sub(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32Sub(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   Int32BinopMatcher m(node);
 
   // Select Msub(x, y, a) for Sub(a, Mul(x, y)).
@@ -1774,11 +1855,12 @@ void InstructionSelector::VisitInt32Sub(Node* node) {
     }
   }
 
-  VisitAddSub<Int32BinopMatcher>(this, node, kArm64Sub32, kArm64Add32);
+  VisitAddSub<Adapter, Int32BinopMatcher>(this, node, kArm64Sub32, kArm64Add32);
 }
 
-void InstructionSelector::VisitInt64Sub(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt64Sub(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   Int64BinopMatcher m(node);
 
   // Select Msub(x, y, a) for Sub(a, Mul(x, y)).
@@ -1794,14 +1876,15 @@ void InstructionSelector::VisitInt64Sub(Node* node) {
     }
   }
 
-  VisitAddSub<Int64BinopMatcher>(this, node, kArm64Sub, kArm64Add);
+  VisitAddSub<Adapter, Int64BinopMatcher>(this, node, kArm64Sub, kArm64Add);
 }
 
 namespace {
 
-void EmitInt32MulWithOverflow(InstructionSelector* selector, Node* node,
-                              FlagsContinuation* cont) {
-  Arm64OperandGenerator g(selector);
+template <typename Adapter>
+void EmitInt32MulWithOverflow(InstructionSelectorT<Adapter>* selector,
+                              Node* node, FlagsContinuation* cont) {
+  Arm64OperandGeneratorT<Adapter> g(selector);
   Int32BinopMatcher m(node);
   InstructionOperand result = g.DefineAsRegister(node);
   InstructionOperand left = g.UseRegister(m.left().node());
@@ -1822,9 +1905,10 @@ void EmitInt32MulWithOverflow(InstructionSelector* selector, Node* node,
   selector->EmitWithContinuation(opcode, result, result, cont);
 }
 
-void EmitInt64MulWithOverflow(InstructionSelector* selector, Node* node,
-                              FlagsContinuation* cont) {
-  Arm64OperandGenerator g(selector);
+template <typename Adapter>
+void EmitInt64MulWithOverflow(InstructionSelectorT<Adapter>* selector,
+                              Node* node, FlagsContinuation* cont) {
+  Arm64OperandGeneratorT<Adapter> g(selector);
   Int64BinopMatcher m(node);
   InstructionOperand result = g.DefineAsRegister(node);
   InstructionOperand left = g.UseRegister(m.left().node());
@@ -1843,8 +1927,9 @@ void EmitInt64MulWithOverflow(InstructionSelector* selector, Node* node,
 
 }  // namespace
 
-void InstructionSelector::VisitInt32Mul(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32Mul(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   Int32BinopMatcher m(node);
 
   // First, try to reduce the multiplication to addition with left shift.
@@ -1884,8 +1969,9 @@ void InstructionSelector::VisitInt32Mul(Node* node) {
   VisitRRR(this, kArm64Mul32, node);
 }
 
-void InstructionSelector::VisitInt64Mul(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt64Mul(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   Int64BinopMatcher m(node);
 
   // First, try to reduce the multiplication to addition with left shift.
@@ -1925,113 +2011,136 @@ void InstructionSelector::VisitInt64Mul(Node* node) {
 }
 
 namespace {
-void VisitExtMul(InstructionSelector* selector, ArchOpcode opcode, Node* node,
-                 int dst_lane_size) {
+template <typename Adapter>
+void VisitExtMul(InstructionSelectorT<Adapter>* selector, ArchOpcode opcode,
+                 Node* node, int dst_lane_size) {
   InstructionCode code = opcode;
   code |= LaneSizeField::encode(dst_lane_size);
   VisitRRR(selector, code, node);
 }
 }  // namespace
 
-void InstructionSelector::VisitI16x8ExtMulLowI8x16S(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI16x8ExtMulLowI8x16S(Node* node) {
   VisitExtMul(this, kArm64Smull, node, 16);
 }
 
-void InstructionSelector::VisitI16x8ExtMulHighI8x16S(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI16x8ExtMulHighI8x16S(Node* node) {
   VisitExtMul(this, kArm64Smull2, node, 16);
 }
 
-void InstructionSelector::VisitI16x8ExtMulLowI8x16U(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI16x8ExtMulLowI8x16U(Node* node) {
   VisitExtMul(this, kArm64Umull, node, 16);
 }
 
-void InstructionSelector::VisitI16x8ExtMulHighI8x16U(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI16x8ExtMulHighI8x16U(Node* node) {
   VisitExtMul(this, kArm64Umull2, node, 16);
 }
 
-void InstructionSelector::VisitI32x4ExtMulLowI16x8S(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI32x4ExtMulLowI16x8S(Node* node) {
   VisitExtMul(this, kArm64Smull, node, 32);
 }
 
-void InstructionSelector::VisitI32x4ExtMulHighI16x8S(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI32x4ExtMulHighI16x8S(Node* node) {
   VisitExtMul(this, kArm64Smull2, node, 32);
 }
 
-void InstructionSelector::VisitI32x4ExtMulLowI16x8U(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI32x4ExtMulLowI16x8U(Node* node) {
   VisitExtMul(this, kArm64Umull, node, 32);
 }
 
-void InstructionSelector::VisitI32x4ExtMulHighI16x8U(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI32x4ExtMulHighI16x8U(Node* node) {
   VisitExtMul(this, kArm64Umull2, node, 32);
 }
 
-void InstructionSelector::VisitI64x2ExtMulLowI32x4S(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI64x2ExtMulLowI32x4S(Node* node) {
   VisitExtMul(this, kArm64Smull, node, 64);
 }
 
-void InstructionSelector::VisitI64x2ExtMulHighI32x4S(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI64x2ExtMulHighI32x4S(Node* node) {
   VisitExtMul(this, kArm64Smull2, node, 64);
 }
 
-void InstructionSelector::VisitI64x2ExtMulLowI32x4U(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI64x2ExtMulLowI32x4U(Node* node) {
   VisitExtMul(this, kArm64Umull, node, 64);
 }
 
-void InstructionSelector::VisitI64x2ExtMulHighI32x4U(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI64x2ExtMulHighI32x4U(Node* node) {
   VisitExtMul(this, kArm64Umull2, node, 64);
 }
 
 namespace {
-void VisitExtAddPairwise(InstructionSelector* selector, ArchOpcode opcode,
-                         Node* node, int dst_lane_size) {
+template <typename Adapter>
+void VisitExtAddPairwise(InstructionSelectorT<Adapter>* selector,
+                         ArchOpcode opcode, Node* node, int dst_lane_size) {
   InstructionCode code = opcode;
   code |= LaneSizeField::encode(dst_lane_size);
   VisitRR(selector, code, node);
 }
 }  // namespace
 
-void InstructionSelector::VisitI32x4ExtAddPairwiseI16x8S(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI32x4ExtAddPairwiseI16x8S(Node* node) {
   VisitExtAddPairwise(this, kArm64Saddlp, node, 32);
 }
 
-void InstructionSelector::VisitI32x4ExtAddPairwiseI16x8U(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI32x4ExtAddPairwiseI16x8U(Node* node) {
   VisitExtAddPairwise(this, kArm64Uaddlp, node, 32);
 }
 
-void InstructionSelector::VisitI16x8ExtAddPairwiseI8x16S(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI16x8ExtAddPairwiseI8x16S(Node* node) {
   VisitExtAddPairwise(this, kArm64Saddlp, node, 16);
 }
 
-void InstructionSelector::VisitI16x8ExtAddPairwiseI8x16U(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI16x8ExtAddPairwiseI8x16U(Node* node) {
   VisitExtAddPairwise(this, kArm64Uaddlp, node, 16);
 }
 
-void InstructionSelector::VisitInt32MulHigh(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32MulHigh(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   InstructionOperand const smull_operand = g.TempRegister();
   Emit(kArm64Smull, smull_operand, g.UseRegister(node->InputAt(0)),
        g.UseRegister(node->InputAt(1)));
   Emit(kArm64Asr, g.DefineAsRegister(node), smull_operand, g.TempImmediate(32));
 }
 
-void InstructionSelector::VisitInt64MulHigh(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt64MulHigh(Node* node) {
   return VisitRRR(this, kArm64Smulh, node);
 }
 
-void InstructionSelector::VisitUint32MulHigh(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitUint32MulHigh(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   InstructionOperand const smull_operand = g.TempRegister();
   Emit(kArm64Umull, smull_operand, g.UseRegister(node->InputAt(0)),
        g.UseRegister(node->InputAt(1)));
   Emit(kArm64Lsr, g.DefineAsRegister(node), smull_operand, g.TempImmediate(32));
 }
 
-void InstructionSelector::VisitUint64MulHigh(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitUint64MulHigh(Node* node) {
   return VisitRRR(this, kArm64Umulh, node);
 }
 
-void InstructionSelector::VisitTruncateFloat32ToInt32(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitTruncateFloat32ToInt32(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
 
   InstructionCode opcode = kArm64Float32ToInt32;
   TruncateKind kind = OpParameter<TruncateKind>(node->op());
@@ -2040,8 +2149,9 @@ void InstructionSelector::VisitTruncateFloat32ToInt32(Node* node) {
   Emit(opcode, g.DefineAsRegister(node), g.UseRegister(node->InputAt(0)));
 }
 
-void InstructionSelector::VisitTruncateFloat32ToUint32(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitTruncateFloat32ToUint32(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
 
   InstructionCode opcode = kArm64Float32ToUint32;
   TruncateKind kind = OpParameter<TruncateKind>(node->op());
@@ -2052,8 +2162,9 @@ void InstructionSelector::VisitTruncateFloat32ToUint32(Node* node) {
   Emit(opcode, g.DefineAsRegister(node), g.UseRegister(node->InputAt(0)));
 }
 
-void InstructionSelector::VisitTryTruncateFloat32ToInt64(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitTryTruncateFloat32ToInt64(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
 
   InstructionOperand inputs[] = {g.UseRegister(node->InputAt(0))};
   InstructionOperand outputs[2];
@@ -2068,8 +2179,9 @@ void InstructionSelector::VisitTryTruncateFloat32ToInt64(Node* node) {
   Emit(kArm64Float32ToInt64, output_count, outputs, 1, inputs);
 }
 
-void InstructionSelector::VisitTruncateFloat64ToInt64(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitTruncateFloat64ToInt64(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
 
   InstructionCode opcode = kArm64Float64ToInt64;
   TruncateKind kind = OpParameter<TruncateKind>(node->op());
@@ -2080,8 +2192,9 @@ void InstructionSelector::VisitTruncateFloat64ToInt64(Node* node) {
   Emit(opcode, g.DefineAsRegister(node), g.UseRegister(node->InputAt(0)));
 }
 
-void InstructionSelector::VisitTryTruncateFloat64ToInt64(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitTryTruncateFloat64ToInt64(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
 
   InstructionOperand inputs[] = {g.UseRegister(node->InputAt(0))};
   InstructionOperand outputs[2];
@@ -2096,8 +2209,10 @@ void InstructionSelector::VisitTryTruncateFloat64ToInt64(Node* node) {
   Emit(kArm64Float64ToInt64, output_count, outputs, 1, inputs);
 }
 
-void InstructionSelector::VisitTryTruncateFloat32ToUint64(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitTryTruncateFloat32ToUint64(
+    Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
 
   InstructionOperand inputs[] = {g.UseRegister(node->InputAt(0))};
   InstructionOperand outputs[2];
@@ -2112,8 +2227,10 @@ void InstructionSelector::VisitTryTruncateFloat32ToUint64(Node* node) {
   Emit(kArm64Float32ToUint64, output_count, outputs, 1, inputs);
 }
 
-void InstructionSelector::VisitTryTruncateFloat64ToUint64(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitTryTruncateFloat64ToUint64(
+    Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
 
   InstructionOperand inputs[] = {g.UseRegister(node->InputAt(0))};
   InstructionOperand outputs[2];
@@ -2128,8 +2245,9 @@ void InstructionSelector::VisitTryTruncateFloat64ToUint64(Node* node) {
   Emit(kArm64Float64ToUint64, output_count, outputs, 1, inputs);
 }
 
-void InstructionSelector::VisitTryTruncateFloat64ToInt32(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitTryTruncateFloat64ToInt32(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   InstructionOperand inputs[] = {g.UseRegister(node->InputAt(0))};
   InstructionOperand outputs[2];
   size_t output_count = 0;
@@ -2143,8 +2261,10 @@ void InstructionSelector::VisitTryTruncateFloat64ToInt32(Node* node) {
   Emit(kArm64Float64ToInt32, output_count, outputs, 1, inputs);
 }
 
-void InstructionSelector::VisitTryTruncateFloat64ToUint32(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitTryTruncateFloat64ToUint32(
+    Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   InstructionOperand inputs[] = {g.UseRegister(node->InputAt(0))};
   InstructionOperand outputs[2];
   size_t output_count = 0;
@@ -2158,13 +2278,15 @@ void InstructionSelector::VisitTryTruncateFloat64ToUint32(Node* node) {
   Emit(kArm64Float64ToUint32, output_count, outputs, 1, inputs);
 }
 
-void InstructionSelector::VisitBitcastWord32ToWord64(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitBitcastWord32ToWord64(Node* node) {
   DCHECK(SmiValuesAre31Bits());
   DCHECK(COMPRESS_POINTERS_BOOL);
   EmitIdentity(node);
 }
 
-void InstructionSelector::VisitChangeInt32ToInt64(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitChangeInt32ToInt64(Node* node) {
   Node* value = node->InputAt(0);
   if ((value->opcode() == IrOpcode::kLoad ||
        value->opcode() == IrOpcode::kLoadImmutable) &&
@@ -2204,7 +2326,7 @@ void InstructionSelector::VisitChangeInt32ToInt64(Node* node) {
   if (value->opcode() == IrOpcode::kWord32Sar && CanCover(node, value)) {
     Int32BinopMatcher m(value);
     if (m.right().HasResolvedValue()) {
-      Arm64OperandGenerator g(this);
+      Arm64OperandGeneratorT<Adapter> g(this);
       // Mask the shift amount, to keep the same semantics as Word32Sar.
       int right = m.right().ResolvedValue() & 0x1F;
       Emit(kArm64Sbfx, g.DefineAsRegister(node), g.UseRegister(m.left().node()),
@@ -2216,7 +2338,9 @@ void InstructionSelector::VisitChangeInt32ToInt64(Node* node) {
   VisitRR(this, kArm64Sxtw, node);
 }
 
-bool InstructionSelector::ZeroExtendsWord32ToWord64NoPhis(Node* node) {
+template <typename Adapter>
+bool InstructionSelectorT<Adapter>::ZeroExtendsWord32ToWord64NoPhis(
+    Node* node) {
   DCHECK_NE(node->opcode(), IrOpcode::kPhi);
   switch (node->opcode()) {
     case IrOpcode::kWord32And:
@@ -2266,8 +2390,9 @@ bool InstructionSelector::ZeroExtendsWord32ToWord64NoPhis(Node* node) {
   }
 }
 
-void InstructionSelector::VisitChangeUint32ToUint64(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitChangeUint32ToUint64(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   Node* value = node->InputAt(0);
   if (ZeroExtendsWord32ToWord64(value)) {
     return EmitIdentity(node);
@@ -2275,44 +2400,51 @@ void InstructionSelector::VisitChangeUint32ToUint64(Node* node) {
   Emit(kArm64Mov32, g.DefineAsRegister(node), g.UseRegister(value));
 }
 
-void InstructionSelector::VisitTruncateInt64ToInt32(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitTruncateInt64ToInt32(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   // The top 32 bits in the 64-bit register will be undefined, and
   // must not be used by a dependent node.
   EmitIdentity(node);
 }
 
-void InstructionSelector::VisitFloat64Mod(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64Mod(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   Emit(kArm64Float64Mod, g.DefineAsFixed(node, d0),
        g.UseFixed(node->InputAt(0), d0), g.UseFixed(node->InputAt(1), d1))
       ->MarkAsCall();
 }
 
-void InstructionSelector::VisitFloat64Ieee754Binop(Node* node,
-                                                   InstructionCode opcode) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64Ieee754Binop(
+    Node* node, InstructionCode opcode) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   Emit(opcode, g.DefineAsFixed(node, d0), g.UseFixed(node->InputAt(0), d0),
        g.UseFixed(node->InputAt(1), d1))
       ->MarkAsCall();
 }
 
-void InstructionSelector::VisitFloat64Ieee754Unop(Node* node,
-                                                  InstructionCode opcode) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64Ieee754Unop(
+    Node* node, InstructionCode opcode) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   Emit(opcode, g.DefineAsFixed(node, d0), g.UseFixed(node->InputAt(0), d0))
       ->MarkAsCall();
 }
 
-void InstructionSelector::EmitMoveParamToFPR(Node* node, int index) {}
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::EmitMoveParamToFPR(Node* node, int index) {}
 
-void InstructionSelector::EmitMoveFPRToParam(InstructionOperand* op,
-                                             LinkageLocation location) {}
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::EmitMoveFPRToParam(
+    InstructionOperand* op, LinkageLocation location) {}
 
-void InstructionSelector::EmitPrepareArguments(
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::EmitPrepareArguments(
     ZoneVector<PushParameter>* arguments, const CallDescriptor* call_descriptor,
     Node* node) {
-  Arm64OperandGenerator g(this);
+  Arm64OperandGeneratorT<Adapter> g(this);
 
   // `arguments` includes alignment "holes". This means that slots bigger than
   // kSystemPointerSize, e.g. Simd128, will span across multiple arguments.
@@ -2358,10 +2490,11 @@ void InstructionSelector::EmitPrepareArguments(
   }
 }
 
-void InstructionSelector::EmitPrepareResults(
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::EmitPrepareResults(
     ZoneVector<PushParameter>* results, const CallDescriptor* call_descriptor,
     Node* node) {
-  Arm64OperandGenerator g(this);
+  Arm64OperandGeneratorT<Adapter> g(this);
 
   for (PushParameter output : *results) {
     if (!output.location.IsCallerFrameSlot()) continue;
@@ -2385,16 +2518,20 @@ void InstructionSelector::EmitPrepareResults(
   }
 }
 
-bool InstructionSelector::IsTailCallAddressImmediate() { return false; }
+template <typename Adapter>
+bool InstructionSelectorT<Adapter>::IsTailCallAddressImmediate() {
+  return false;
+}
 
 namespace {
 
 // Shared routine for multiple compare operations.
-void VisitCompare(InstructionSelector* selector, InstructionCode opcode,
-                  InstructionOperand left, InstructionOperand right,
-                  FlagsContinuation* cont) {
+template <typename Adapter>
+void VisitCompare(InstructionSelectorT<Adapter>* selector,
+                  InstructionCode opcode, InstructionOperand left,
+                  InstructionOperand right, FlagsContinuation* cont) {
   if (cont->IsSelect()) {
-    Arm64OperandGenerator g(selector);
+    Arm64OperandGeneratorT<Adapter> g(selector);
     InstructionOperand inputs[] = {left, right,
                                    g.UseRegister(cont->true_value()),
                                    g.UseRegister(cont->false_value())};
@@ -2456,12 +2593,11 @@ FlagsCondition MapForFlagSettingBinop(FlagsCondition cond) {
 // (a <ops> b), b.<cond'>
 // where <ops> is the flag setting version of <op>, and if so,
 // updates {node}, {opcode} and {cont} accordingly.
-void MaybeReplaceCmpZeroWithFlagSettingBinop(InstructionSelector* selector,
-                                             Node** node, Node* binop,
-                                             ArchOpcode* opcode,
-                                             FlagsCondition cond,
-                                             FlagsContinuation* cont,
-                                             ImmediateMode* immediate_mode) {
+template <typename Adapter>
+void MaybeReplaceCmpZeroWithFlagSettingBinop(
+    InstructionSelectorT<Adapter>* selector, Node** node, Node* binop,
+    ArchOpcode* opcode, FlagsCondition cond, FlagsContinuation* cont,
+    ImmediateMode* immediate_mode) {
   ArchOpcode binop_opcode;
   ArchOpcode no_output_opcode;
   ImmediateMode binop_immediate_mode;
@@ -2528,7 +2664,8 @@ FlagsCondition MapForCbz(FlagsCondition cond) {
   }
 }
 
-void EmitBranchOrDeoptimize(InstructionSelector* selector,
+template <typename Adapter>
+void EmitBranchOrDeoptimize(InstructionSelectorT<Adapter>* selector,
                             InstructionCode opcode, InstructionOperand value,
                             FlagsContinuation* cont) {
   DCHECK(cont->IsBranch() || cont->IsDeoptimize());
@@ -2561,8 +2698,8 @@ struct CbzOrTbzMatchTrait<64> {
 
 // Try to emit TBZ, TBNZ, CBZ or CBNZ for certain comparisons of {node}
 // against {value}, depending on the condition.
-template <int N>
-bool TryEmitCbzOrTbz(InstructionSelector* selector, Node* node,
+template <typename Adapter, int N>
+bool TryEmitCbzOrTbz(InstructionSelectorT<Adapter>* selector, Node* node,
                      typename CbzOrTbzMatchTrait<N>::IntegralType value,
                      Node* user, FlagsCondition cond, FlagsContinuation* cont) {
   // Only handle branches and deoptimisations.
@@ -2577,7 +2714,7 @@ bool TryEmitCbzOrTbz(InstructionSelector* selector, Node* node,
       // shorter range than conditional branches and generating them for
       // deoptimisations results in more veneers.
       if (cont->IsDeoptimize()) return false;
-      Arm64OperandGenerator g(selector);
+      Arm64OperandGeneratorT<Adapter> g(selector);
       cont->Overwrite(MapForTbz(cond));
 
       if (N == 32) {
@@ -2608,7 +2745,7 @@ bool TryEmitCbzOrTbz(InstructionSelector* selector, Node* node,
         typename CbzOrTbzMatchTrait<N>::BinopMatcher m_and(node);
         if (cont->IsBranch() && base::bits::IsPowerOfTwo(value) &&
             m_and.right().Is(value) && selector->CanCover(user, node)) {
-          Arm64OperandGenerator g(selector);
+          Arm64OperandGeneratorT<Adapter> g(selector);
           // In the code generator, Equal refers to a bit being cleared. We want
           // the opposite here so negate the condition.
           cont->Negate();
@@ -2624,7 +2761,7 @@ bool TryEmitCbzOrTbz(InstructionSelector* selector, Node* node,
     case kUnsignedLessThanOrEqual:
     case kUnsignedGreaterThan: {
       if (value != 0) return false;
-      Arm64OperandGenerator g(selector);
+      Arm64OperandGeneratorT<Adapter> g(selector);
       cont->Overwrite(MapForCbz(cond));
       EmitBranchOrDeoptimize(selector,
                              CbzOrTbzMatchTrait<N>::kCompareAndBranchOpcode,
@@ -2637,10 +2774,11 @@ bool TryEmitCbzOrTbz(InstructionSelector* selector, Node* node,
 }
 
 // Shared routine for multiple word compare operations.
-void VisitWordCompare(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitWordCompare(InstructionSelectorT<Adapter>* selector, Node* node,
                       InstructionCode opcode, FlagsContinuation* cont,
                       ImmediateMode immediate_mode) {
-  Arm64OperandGenerator g(selector);
+  Arm64OperandGeneratorT<Adapter> g(selector);
 
   Node* left = node->InputAt(0);
   Node* right = node->InputAt(1);
@@ -2655,8 +2793,8 @@ void VisitWordCompare(InstructionSelector* selector, Node* node,
   if (opcode == kArm64Cmp) {
     Int64Matcher m(right);
     if (m.HasResolvedValue()) {
-      if (TryEmitCbzOrTbz<64>(selector, left, m.ResolvedValue(), node,
-                              cont->condition(), cont)) {
+      if (TryEmitCbzOrTbz<Adapter, 64>(selector, left, m.ResolvedValue(), node,
+                                       cont->condition(), cont)) {
         return;
       }
     }
@@ -2666,20 +2804,22 @@ void VisitWordCompare(InstructionSelector* selector, Node* node,
                g.UseOperand(right, immediate_mode), cont);
 }
 
-void VisitWord32Compare(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitWord32Compare(InstructionSelectorT<Adapter>* selector, Node* node,
                         FlagsContinuation* cont) {
   Int32BinopMatcher m(node);
   FlagsCondition cond = cont->condition();
   if (m.right().HasResolvedValue()) {
-    if (TryEmitCbzOrTbz<32>(selector, m.left().node(),
-                            m.right().ResolvedValue(), node, cond, cont)) {
+    if (TryEmitCbzOrTbz<Adapter, 32>(selector, m.left().node(),
+                                     m.right().ResolvedValue(), node, cond,
+                                     cont)) {
       return;
     }
   } else if (m.left().HasResolvedValue()) {
     FlagsCondition commuted_cond = CommuteFlagsCondition(cond);
-    if (TryEmitCbzOrTbz<32>(selector, m.right().node(),
-                            m.left().ResolvedValue(), node, commuted_cond,
-                            cont)) {
+    if (TryEmitCbzOrTbz<Adapter, 32>(selector, m.right().node(),
+                                     m.left().ResolvedValue(), node,
+                                     commuted_cond, cont)) {
       return;
     }
   }
@@ -2725,22 +2865,26 @@ void VisitWord32Compare(InstructionSelector* selector, Node* node,
       opcode = kArm64Cmn32;
     }
   }
-  VisitBinop<Int32BinopMatcher>(selector, node, opcode, immediate_mode, cont);
+  VisitBinop<Adapter, Int32BinopMatcher>(selector, node, opcode, immediate_mode,
+                                         cont);
 }
 
-void VisitWordTest(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitWordTest(InstructionSelectorT<Adapter>* selector, Node* node,
                    InstructionCode opcode, FlagsContinuation* cont) {
-  Arm64OperandGenerator g(selector);
+  Arm64OperandGeneratorT<Adapter> g(selector);
   VisitCompare(selector, opcode, g.UseRegister(node), g.UseRegister(node),
                cont);
 }
 
-void VisitWord32Test(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitWord32Test(InstructionSelectorT<Adapter>* selector, Node* node,
                      FlagsContinuation* cont) {
   VisitWordTest(selector, node, kArm64Tst32, cont);
 }
 
-void VisitWord64Test(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitWord64Test(InstructionSelectorT<Adapter>* selector, Node* node,
                      FlagsContinuation* cont) {
   VisitWordTest(selector, node, kArm64Tst, cont);
 }
@@ -2782,9 +2926,10 @@ struct TestAndBranchMatcher {
 };
 
 // Shared routine for multiple float32 compare operations.
-void VisitFloat32Compare(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitFloat32Compare(InstructionSelectorT<Adapter>* selector, Node* node,
                          FlagsContinuation* cont) {
-  Arm64OperandGenerator g(selector);
+  Arm64OperandGeneratorT<Adapter> g(selector);
   Float32BinopMatcher m(node);
   if (m.right().Is(0.0f)) {
     VisitCompare(selector, kArm64Float32Cmp, g.UseRegister(m.left().node()),
@@ -2800,9 +2945,10 @@ void VisitFloat32Compare(InstructionSelector* selector, Node* node,
 }
 
 // Shared routine for multiple float64 compare operations.
-void VisitFloat64Compare(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitFloat64Compare(InstructionSelectorT<Adapter>* selector, Node* node,
                          FlagsContinuation* cont) {
-  Arm64OperandGenerator g(selector);
+  Arm64OperandGeneratorT<Adapter> g(selector);
   Float64BinopMatcher m(node);
   if (m.right().Is(0.0)) {
     VisitCompare(selector, kArm64Float64Cmp, g.UseRegister(m.left().node()),
@@ -2817,10 +2963,11 @@ void VisitFloat64Compare(InstructionSelector* selector, Node* node,
   }
 }
 
-void VisitAtomicExchange(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitAtomicExchange(InstructionSelectorT<Adapter>* selector, Node* node,
                          ArchOpcode opcode, AtomicWidth width,
                          MemoryAccessKind access_kind) {
-  Arm64OperandGenerator g(selector);
+  Arm64OperandGeneratorT<Adapter> g(selector);
   Node* base = node->InputAt(0);
   Node* index = node->InputAt(1);
   Node* value = node->InputAt(2);
@@ -2843,10 +2990,12 @@ void VisitAtomicExchange(InstructionSelector* selector, Node* node,
   }
 }
 
-void VisitAtomicCompareExchange(InstructionSelector* selector, Node* node,
-                                ArchOpcode opcode, AtomicWidth width,
+template <typename Adapter>
+void VisitAtomicCompareExchange(InstructionSelectorT<Adapter>* selector,
+                                Node* node, ArchOpcode opcode,
+                                AtomicWidth width,
                                 MemoryAccessKind access_kind) {
-  Arm64OperandGenerator g(selector);
+  Arm64OperandGeneratorT<Adapter> g(selector);
   Node* base = node->InputAt(0);
   Node* index = node->InputAt(1);
   Node* old_value = node->InputAt(2);
@@ -2873,9 +3022,10 @@ void VisitAtomicCompareExchange(InstructionSelector* selector, Node* node,
   }
 }
 
-void VisitAtomicLoad(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitAtomicLoad(InstructionSelectorT<Adapter>* selector, Node* node,
                      AtomicWidth width) {
-  Arm64OperandGenerator g(selector);
+  Arm64OperandGeneratorT<Adapter> g(selector);
   Node* base = node->InputAt(0);
   Node* index = node->InputAt(1);
   InstructionOperand inputs[] = {g.UseRegister(base), g.UseRegister(index)};
@@ -2943,9 +3093,10 @@ void VisitAtomicLoad(InstructionSelector* selector, Node* node,
                  arraysize(temps), temps);
 }
 
-void VisitAtomicStore(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitAtomicStore(InstructionSelectorT<Adapter>* selector, Node* node,
                       AtomicWidth width) {
-  Arm64OperandGenerator g(selector);
+  Arm64OperandGeneratorT<Adapter> g(selector);
   Node* base = node->InputAt(0);
   Node* index = node->InputAt(1);
   Node* value = node->InputAt(2);
@@ -3018,10 +3169,11 @@ void VisitAtomicStore(InstructionSelector* selector, Node* node,
                  temps);
 }
 
-void VisitAtomicBinop(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitAtomicBinop(InstructionSelectorT<Adapter>* selector, Node* node,
                       ArchOpcode opcode, AtomicWidth width,
                       MemoryAccessKind access_kind) {
-  Arm64OperandGenerator g(selector);
+  Arm64OperandGeneratorT<Adapter> g(selector);
   Node* base = node->InputAt(0);
   Node* index = node->InputAt(1);
   Node* value = node->InputAt(2);
@@ -3049,9 +3201,10 @@ void VisitAtomicBinop(InstructionSelector* selector, Node* node,
 
 }  // namespace
 
-void InstructionSelector::VisitWordCompareZero(Node* user, Node* value,
-                                               FlagsContinuation* cont) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWordCompareZero(
+    Node* user, Node* value, FlagsContinuation* cont) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   // Try to combine with comparisons against 0 by simply inverting the branch.
   while (value->opcode() == IrOpcode::kWord32Equal && CanCover(user, value)) {
     Int32BinopMatcher m(value);
@@ -3077,7 +3230,7 @@ void InstructionSelector::VisitWordCompareZero(Node* user, Node* value,
           // into a tbz/tbnz instruction.
           TestAndBranchMatcher<Uint64BinopMatcher> tbm(left, cont);
           if (tbm.Matches()) {
-            Arm64OperandGenerator gen(this);
+            Arm64OperandGeneratorT<Adapter> gen(this);
             cont->OverwriteAndNegateIfEqual(kEqual);
             this->EmitWithContinuation(kArm64TestAndBranch,
                                        gen.UseRegister(tbm.input()),
@@ -3091,7 +3244,7 @@ void InstructionSelector::VisitWordCompareZero(Node* user, Node* value,
     case IrOpcode::kWord32And: {
       TestAndBranchMatcher<Uint32BinopMatcher> tbm(value, cont);
       if (tbm.Matches()) {
-        Arm64OperandGenerator gen(this);
+        Arm64OperandGeneratorT<Adapter> gen(this);
         this->EmitWithContinuation(kArm64TestAndBranch32,
                                    gen.UseRegister(tbm.input()),
                                    gen.TempImmediate(tbm.bit()), cont);
@@ -3102,7 +3255,7 @@ void InstructionSelector::VisitWordCompareZero(Node* user, Node* value,
     case IrOpcode::kWord64And: {
       TestAndBranchMatcher<Uint64BinopMatcher> tbm(value, cont);
       if (tbm.Matches()) {
-        Arm64OperandGenerator gen(this);
+        Arm64OperandGeneratorT<Adapter> gen(this);
         this->EmitWithContinuation(kArm64TestAndBranch,
                                    gen.UseRegister(tbm.input()),
                                    gen.TempImmediate(tbm.bit()), cont);
@@ -3187,12 +3340,12 @@ void InstructionSelector::VisitWordCompareZero(Node* user, Node* value,
             switch (node->opcode()) {
               case IrOpcode::kInt32AddWithOverflow:
                 cont->OverwriteAndNegateIfEqual(kOverflow);
-                return VisitBinop<Int32BinopMatcher>(this, node, kArm64Add32,
-                                                     kArithmeticImm, cont);
+                return VisitBinop<Adapter, Int32BinopMatcher>(
+                    this, node, kArm64Add32, kArithmeticImm, cont);
               case IrOpcode::kInt32SubWithOverflow:
                 cont->OverwriteAndNegateIfEqual(kOverflow);
-                return VisitBinop<Int32BinopMatcher>(this, node, kArm64Sub32,
-                                                     kArithmeticImm, cont);
+                return VisitBinop<Adapter, Int32BinopMatcher>(
+                    this, node, kArm64Sub32, kArithmeticImm, cont);
               case IrOpcode::kInt32MulWithOverflow:
                 // ARM64 doesn't set the overflow flag for multiplication, so we
                 // need to test on kNotEqual. Here is the code sequence used:
@@ -3202,12 +3355,12 @@ void InstructionSelector::VisitWordCompareZero(Node* user, Node* value,
                 return EmitInt32MulWithOverflow(this, node, cont);
               case IrOpcode::kInt64AddWithOverflow:
                 cont->OverwriteAndNegateIfEqual(kOverflow);
-                return VisitBinop<Int64BinopMatcher>(this, node, kArm64Add,
-                                                     kArithmeticImm, cont);
+                return VisitBinop<Adapter, Int64BinopMatcher>(
+                    this, node, kArm64Add, kArithmeticImm, cont);
               case IrOpcode::kInt64SubWithOverflow:
                 cont->OverwriteAndNegateIfEqual(kOverflow);
-                return VisitBinop<Int64BinopMatcher>(this, node, kArm64Sub,
-                                                     kArithmeticImm, cont);
+                return VisitBinop<Adapter, Int64BinopMatcher>(
+                    this, node, kArm64Sub, kArithmeticImm, cont);
               case IrOpcode::kInt64MulWithOverflow:
                 // ARM64 doesn't set the overflow flag for multiplication, so we
                 // need to test on kNotEqual. Here is the code sequence used:
@@ -3249,8 +3402,10 @@ void InstructionSelector::VisitWordCompareZero(Node* user, Node* value,
   }
 }
 
-void InstructionSelector::VisitSwitch(Node* node, const SwitchInfo& sw) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitSwitch(Node* node,
+                                                const SwitchInfo& sw) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   InstructionOperand value_operand = g.UseRegister(node->InputAt(0));
 
   // Emit either ArchTableSwitch or ArchBinarySearchSwitch.
@@ -3280,7 +3435,8 @@ void InstructionSelector::VisitSwitch(Node* node, const SwitchInfo& sw) {
   return EmitBinarySearchSwitch(sw, value_operand);
 }
 
-void InstructionSelector::VisitWord32Equal(Node* const node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32Equal(Node* const node) {
   Node* const user = node;
   FlagsContinuation cont = FlagsContinuation::ForSet(kEqual, node);
   Int32BinopMatcher m(user);
@@ -3318,7 +3474,7 @@ void InstructionSelector::VisitWord32Equal(Node* const node) {
 
   if (isolate() && (V8_STATIC_ROOTS_BOOL ||
                     (COMPRESS_POINTERS_BOOL && !isolate()->bootstrapper()))) {
-    Arm64OperandGenerator g(this);
+    Arm64OperandGeneratorT<Adapter> g(this);
     const RootsTable& roots_table = isolate()->roots_table();
     RootIndex root_index;
     Node* left = nullptr;
@@ -3355,29 +3511,34 @@ void InstructionSelector::VisitWord32Equal(Node* const node) {
   VisitWord32Compare(this, node, &cont);
 }
 
-void InstructionSelector::VisitInt32LessThan(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32LessThan(Node* node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kSignedLessThan, node);
   VisitWord32Compare(this, node, &cont);
 }
 
-void InstructionSelector::VisitInt32LessThanOrEqual(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32LessThanOrEqual(Node* node) {
   FlagsContinuation cont =
       FlagsContinuation::ForSet(kSignedLessThanOrEqual, node);
   VisitWord32Compare(this, node, &cont);
 }
 
-void InstructionSelector::VisitUint32LessThan(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitUint32LessThan(Node* node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kUnsignedLessThan, node);
   VisitWord32Compare(this, node, &cont);
 }
 
-void InstructionSelector::VisitUint32LessThanOrEqual(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitUint32LessThanOrEqual(Node* node) {
   FlagsContinuation cont =
       FlagsContinuation::ForSet(kUnsignedLessThanOrEqual, node);
   VisitWord32Compare(this, node, &cont);
 }
 
-void InstructionSelector::VisitWord64Equal(Node* const node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64Equal(Node* const node) {
   Node* const user = node;
   FlagsContinuation cont = FlagsContinuation::ForSet(kEqual, node);
   Int64BinopMatcher m(user);
@@ -3396,27 +3557,32 @@ void InstructionSelector::VisitWord64Equal(Node* const node) {
   VisitWordCompare(this, node, kArm64Cmp, &cont, kArithmeticImm);
 }
 
-void InstructionSelector::VisitInt32AddWithOverflow(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32AddWithOverflow(Node* node) {
   if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
     FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
-    return VisitBinop<Int32BinopMatcher>(this, node, kArm64Add32,
-                                         kArithmeticImm, &cont);
+    return VisitBinop<Adapter, Int32BinopMatcher>(this, node, kArm64Add32,
+                                                  kArithmeticImm, &cont);
   }
   FlagsContinuation cont;
-  VisitBinop<Int32BinopMatcher>(this, node, kArm64Add32, kArithmeticImm, &cont);
+  VisitBinop<Adapter, Int32BinopMatcher>(this, node, kArm64Add32,
+                                         kArithmeticImm, &cont);
 }
 
-void InstructionSelector::VisitInt32SubWithOverflow(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32SubWithOverflow(Node* node) {
   if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
     FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
-    return VisitBinop<Int32BinopMatcher>(this, node, kArm64Sub32,
-                                         kArithmeticImm, &cont);
+    return VisitBinop<Adapter, Int32BinopMatcher>(this, node, kArm64Sub32,
+                                                  kArithmeticImm, &cont);
   }
   FlagsContinuation cont;
-  VisitBinop<Int32BinopMatcher>(this, node, kArm64Sub32, kArithmeticImm, &cont);
+  VisitBinop<Adapter, Int32BinopMatcher>(this, node, kArm64Sub32,
+                                         kArithmeticImm, &cont);
 }
 
-void InstructionSelector::VisitInt32MulWithOverflow(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32MulWithOverflow(Node* node) {
   if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
     // ARM64 doesn't set the overflow flag for multiplication, so we need to
     // test on kNotEqual. Here is the code sequence used:
@@ -3429,27 +3595,32 @@ void InstructionSelector::VisitInt32MulWithOverflow(Node* node) {
   EmitInt32MulWithOverflow(this, node, &cont);
 }
 
-void InstructionSelector::VisitInt64AddWithOverflow(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt64AddWithOverflow(Node* node) {
   if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
     FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
-    return VisitBinop<Int64BinopMatcher>(this, node, kArm64Add, kArithmeticImm,
-                                         &cont);
+    return VisitBinop<Adapter, Int64BinopMatcher>(this, node, kArm64Add,
+                                                  kArithmeticImm, &cont);
   }
   FlagsContinuation cont;
-  VisitBinop<Int64BinopMatcher>(this, node, kArm64Add, kArithmeticImm, &cont);
+  VisitBinop<Adapter, Int64BinopMatcher>(this, node, kArm64Add, kArithmeticImm,
+                                         &cont);
 }
 
-void InstructionSelector::VisitInt64SubWithOverflow(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt64SubWithOverflow(Node* node) {
   if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
     FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
-    return VisitBinop<Int64BinopMatcher>(this, node, kArm64Sub, kArithmeticImm,
-                                         &cont);
+    return VisitBinop<Adapter, Int64BinopMatcher>(this, node, kArm64Sub,
+                                                  kArithmeticImm, &cont);
   }
   FlagsContinuation cont;
-  VisitBinop<Int64BinopMatcher>(this, node, kArm64Sub, kArithmeticImm, &cont);
+  VisitBinop<Adapter, Int64BinopMatcher>(this, node, kArm64Sub, kArithmeticImm,
+                                         &cont);
 }
 
-void InstructionSelector::VisitInt64MulWithOverflow(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt64MulWithOverflow(Node* node) {
   if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
     // ARM64 doesn't set the overflow flag for multiplication, so we need to
     // test on kNotEqual. Here is the code sequence used:
@@ -3463,30 +3634,35 @@ void InstructionSelector::VisitInt64MulWithOverflow(Node* node) {
   EmitInt64MulWithOverflow(this, node, &cont);
 }
 
-void InstructionSelector::VisitInt64LessThan(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt64LessThan(Node* node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kSignedLessThan, node);
   VisitWordCompare(this, node, kArm64Cmp, &cont, kArithmeticImm);
 }
 
-void InstructionSelector::VisitInt64LessThanOrEqual(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt64LessThanOrEqual(Node* node) {
   FlagsContinuation cont =
       FlagsContinuation::ForSet(kSignedLessThanOrEqual, node);
   VisitWordCompare(this, node, kArm64Cmp, &cont, kArithmeticImm);
 }
 
-void InstructionSelector::VisitUint64LessThan(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitUint64LessThan(Node* node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kUnsignedLessThan, node);
   VisitWordCompare(this, node, kArm64Cmp, &cont, kArithmeticImm);
 }
 
-void InstructionSelector::VisitUint64LessThanOrEqual(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitUint64LessThanOrEqual(Node* node) {
   FlagsContinuation cont =
       FlagsContinuation::ForSet(kUnsignedLessThanOrEqual, node);
   VisitWordCompare(this, node, kArm64Cmp, &cont, kArithmeticImm);
 }
 
-void InstructionSelector::VisitFloat32Neg(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat32Neg(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   Node* in = node->InputAt(0);
   if (in->opcode() == IrOpcode::kFloat32Mul && CanCover(node, in)) {
     Float32BinopMatcher m(in);
@@ -3497,8 +3673,9 @@ void InstructionSelector::VisitFloat32Neg(Node* node) {
   VisitRR(this, kArm64Float32Neg, node);
 }
 
-void InstructionSelector::VisitFloat32Mul(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat32Mul(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   Float32BinopMatcher m(node);
 
   if (m.left().IsFloat32Neg() && CanCover(node, m.left().node())) {
@@ -3517,8 +3694,9 @@ void InstructionSelector::VisitFloat32Mul(Node* node) {
   return VisitRRR(this, kArm64Float32Mul, node);
 }
 
-void InstructionSelector::VisitFloat32Abs(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat32Abs(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   Node* in = node->InputAt(0);
   if (in->opcode() == IrOpcode::kFloat32Sub && CanCover(node, in)) {
     Emit(kArm64Float32Abd, g.DefineAsRegister(node),
@@ -3529,8 +3707,9 @@ void InstructionSelector::VisitFloat32Abs(Node* node) {
   return VisitRR(this, kArm64Float32Abs, node);
 }
 
-void InstructionSelector::VisitFloat64Abs(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64Abs(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   Node* in = node->InputAt(0);
   if (in->opcode() == IrOpcode::kFloat64Sub && CanCover(node, in)) {
     Emit(kArm64Float64Abd, g.DefineAsRegister(node),
@@ -3541,40 +3720,47 @@ void InstructionSelector::VisitFloat64Abs(Node* node) {
   return VisitRR(this, kArm64Float64Abs, node);
 }
 
-void InstructionSelector::VisitFloat32Equal(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat32Equal(Node* node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kEqual, node);
   VisitFloat32Compare(this, node, &cont);
 }
 
-void InstructionSelector::VisitFloat32LessThan(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat32LessThan(Node* node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kFloatLessThan, node);
   VisitFloat32Compare(this, node, &cont);
 }
 
-void InstructionSelector::VisitFloat32LessThanOrEqual(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat32LessThanOrEqual(Node* node) {
   FlagsContinuation cont =
       FlagsContinuation::ForSet(kFloatLessThanOrEqual, node);
   VisitFloat32Compare(this, node, &cont);
 }
 
-void InstructionSelector::VisitFloat64Equal(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64Equal(Node* node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kEqual, node);
   VisitFloat64Compare(this, node, &cont);
 }
 
-void InstructionSelector::VisitFloat64LessThan(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64LessThan(Node* node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kFloatLessThan, node);
   VisitFloat64Compare(this, node, &cont);
 }
 
-void InstructionSelector::VisitFloat64LessThanOrEqual(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64LessThanOrEqual(Node* node) {
   FlagsContinuation cont =
       FlagsContinuation::ForSet(kFloatLessThanOrEqual, node);
   VisitFloat64Compare(this, node, &cont);
 }
 
-void InstructionSelector::VisitFloat64InsertLowWord32(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64InsertLowWord32(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   Node* left = node->InputAt(0);
   Node* right = node->InputAt(1);
   if (left->opcode() == IrOpcode::kFloat64InsertHighWord32 &&
@@ -3590,8 +3776,9 @@ void InstructionSelector::VisitFloat64InsertLowWord32(Node* node) {
        g.UseRegister(left), g.UseRegister(right));
 }
 
-void InstructionSelector::VisitFloat64InsertHighWord32(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64InsertHighWord32(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   Node* left = node->InputAt(0);
   Node* right = node->InputAt(1);
   if (left->opcode() == IrOpcode::kFloat64InsertLowWord32 &&
@@ -3606,8 +3793,9 @@ void InstructionSelector::VisitFloat64InsertHighWord32(Node* node) {
        g.UseRegister(left), g.UseRegister(right));
 }
 
-void InstructionSelector::VisitFloat64Neg(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64Neg(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   Node* in = node->InputAt(0);
   if (in->opcode() == IrOpcode::kFloat64Mul && CanCover(node, in)) {
     Float64BinopMatcher m(in);
@@ -3618,8 +3806,9 @@ void InstructionSelector::VisitFloat64Neg(Node* node) {
   VisitRR(this, kArm64Float64Neg, node);
 }
 
-void InstructionSelector::VisitFloat64Mul(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64Mul(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   Float64BinopMatcher m(node);
 
   if (m.left().IsFloat64Neg() && CanCover(node, m.left().node())) {
@@ -3638,29 +3827,35 @@ void InstructionSelector::VisitFloat64Mul(Node* node) {
   return VisitRRR(this, kArm64Float64Mul, node);
 }
 
-void InstructionSelector::VisitMemoryBarrier(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitMemoryBarrier(Node* node) {
   // Use DMB ISH for both acquire-release and sequentially consistent barriers.
-  Arm64OperandGenerator g(this);
+  Arm64OperandGeneratorT<Adapter> g(this);
   Emit(kArm64DmbIsh, g.NoOutput());
 }
 
-void InstructionSelector::VisitWord32AtomicLoad(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32AtomicLoad(Node* node) {
   VisitAtomicLoad(this, node, AtomicWidth::kWord32);
 }
 
-void InstructionSelector::VisitWord64AtomicLoad(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64AtomicLoad(Node* node) {
   VisitAtomicLoad(this, node, AtomicWidth::kWord64);
 }
 
-void InstructionSelector::VisitWord32AtomicStore(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32AtomicStore(Node* node) {
   VisitAtomicStore(this, node, AtomicWidth::kWord32);
 }
 
-void InstructionSelector::VisitWord64AtomicStore(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64AtomicStore(Node* node) {
   VisitAtomicStore(this, node, AtomicWidth::kWord64);
 }
 
-void InstructionSelector::VisitWord32AtomicExchange(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32AtomicExchange(Node* node) {
   ArchOpcode opcode;
   AtomicOpParameters params = AtomicOpParametersOf(node->op());
   if (params.type() == MachineType::Int8()) {
@@ -3680,7 +3875,8 @@ void InstructionSelector::VisitWord32AtomicExchange(Node* node) {
   VisitAtomicExchange(this, node, opcode, AtomicWidth::kWord32, params.kind());
 }
 
-void InstructionSelector::VisitWord64AtomicExchange(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64AtomicExchange(Node* node) {
   ArchOpcode opcode;
   AtomicOpParameters params = AtomicOpParametersOf(node->op());
   if (params.type() == MachineType::Uint8()) {
@@ -3697,7 +3893,9 @@ void InstructionSelector::VisitWord64AtomicExchange(Node* node) {
   VisitAtomicExchange(this, node, opcode, AtomicWidth::kWord64, params.kind());
 }
 
-void InstructionSelector::VisitWord32AtomicCompareExchange(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32AtomicCompareExchange(
+    Node* node) {
   ArchOpcode opcode;
   AtomicOpParameters params = AtomicOpParametersOf(node->op());
   if (params.type() == MachineType::Int8()) {
@@ -3718,7 +3916,9 @@ void InstructionSelector::VisitWord32AtomicCompareExchange(Node* node) {
                              params.kind());
 }
 
-void InstructionSelector::VisitWord64AtomicCompareExchange(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64AtomicCompareExchange(
+    Node* node) {
   ArchOpcode opcode;
   AtomicOpParameters params = AtomicOpParametersOf(node->op());
   if (params.type() == MachineType::Uint8()) {
@@ -3736,7 +3936,8 @@ void InstructionSelector::VisitWord64AtomicCompareExchange(Node* node) {
                              params.kind());
 }
 
-void InstructionSelector::VisitWord32AtomicBinaryOperation(
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32AtomicBinaryOperation(
     Node* node, ArchOpcode int8_op, ArchOpcode uint8_op, ArchOpcode int16_op,
     ArchOpcode uint16_op, ArchOpcode word32_op) {
   ArchOpcode opcode;
@@ -3758,11 +3959,12 @@ void InstructionSelector::VisitWord32AtomicBinaryOperation(
   VisitAtomicBinop(this, node, opcode, AtomicWidth::kWord32, params.kind());
 }
 
-#define VISIT_ATOMIC_BINOP(op)                                           \
-  void InstructionSelector::VisitWord32Atomic##op(Node* node) {          \
-    VisitWord32AtomicBinaryOperation(                                    \
-        node, kAtomic##op##Int8, kAtomic##op##Uint8, kAtomic##op##Int16, \
-        kAtomic##op##Uint16, kAtomic##op##Word32);                       \
+#define VISIT_ATOMIC_BINOP(op)                                            \
+  template <typename Adapter>                                             \
+  void InstructionSelectorT<Adapter>::VisitWord32Atomic##op(Node* node) { \
+    VisitWord32AtomicBinaryOperation(                                     \
+        node, kAtomic##op##Int8, kAtomic##op##Uint8, kAtomic##op##Int16,  \
+        kAtomic##op##Uint16, kAtomic##op##Word32);                        \
   }
 VISIT_ATOMIC_BINOP(Add)
 VISIT_ATOMIC_BINOP(Sub)
@@ -3771,7 +3973,8 @@ VISIT_ATOMIC_BINOP(Or)
 VISIT_ATOMIC_BINOP(Xor)
 #undef VISIT_ATOMIC_BINOP
 
-void InstructionSelector::VisitWord64AtomicBinaryOperation(
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64AtomicBinaryOperation(
     Node* node, ArchOpcode uint8_op, ArchOpcode uint16_op, ArchOpcode uint32_op,
     ArchOpcode uint64_op) {
   ArchOpcode opcode;
@@ -3791,7 +3994,8 @@ void InstructionSelector::VisitWord64AtomicBinaryOperation(
 }
 
 #define VISIT_ATOMIC_BINOP(op)                                                 \
-  void InstructionSelector::VisitWord64Atomic##op(Node* node) {                \
+  template <typename Adapter>                                                  \
+  void InstructionSelectorT<Adapter>::VisitWord64Atomic##op(Node* node) {      \
     VisitWord64AtomicBinaryOperation(node, kAtomic##op##Uint8,                 \
                                      kAtomic##op##Uint16, kAtomic##op##Word32, \
                                      kArm64Word64Atomic##op##Uint64);          \
@@ -3803,11 +4007,13 @@ VISIT_ATOMIC_BINOP(Or)
 VISIT_ATOMIC_BINOP(Xor)
 #undef VISIT_ATOMIC_BINOP
 
-void InstructionSelector::VisitInt32AbsWithOverflow(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32AbsWithOverflow(Node* node) {
   UNREACHABLE();
 }
 
-void InstructionSelector::VisitInt64AbsWithOverflow(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt64AbsWithOverflow(Node* node) {
   UNREACHABLE();
 }
 
@@ -3933,8 +4139,9 @@ void InstructionSelector::VisitInt64AbsWithOverflow(Node* node) {
   V(I8x16MinU, kArm64IMinU, 8)                         \
   V(I8x16MaxU, kArm64IMaxU, 8)
 
-void InstructionSelector::VisitS128Const(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitS128Const(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   static const int kUint32Immediates = 4;
   uint32_t val[kUint32Immediates];
   static_assert(sizeof(val) == kSimd128Size);
@@ -4017,9 +4224,10 @@ base::Optional<BicImmResult> BicImmHelper(Node* and_node, bool not_imm) {
   return base::nullopt;
 }
 
-bool TryEmitS128AndNotImm(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+bool TryEmitS128AndNotImm(InstructionSelectorT<Adapter>* selector, Node* node,
                           bool not_imm) {
-  Arm64OperandGenerator g(selector);
+  Arm64OperandGeneratorT<Adapter> g(selector);
   base::Optional<BicImmResult> result = BicImmHelper(node, not_imm);
   if (!result.has_value()) return false;
   base::Optional<BicImmParam> param = result->param;
@@ -4037,27 +4245,31 @@ bool TryEmitS128AndNotImm(InstructionSelector* selector, Node* node,
 
 }  // namespace
 
-void InstructionSelector::VisitS128AndNot(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitS128AndNot(Node* node) {
   if (!TryEmitS128AndNotImm(this, node, false)) {
     VisitRRR(this, kArm64S128AndNot, node);
   }
 }
 
-void InstructionSelector::VisitS128And(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitS128And(Node* node) {
   // AndNot can be used if we negate the immediate input of And.
   if (!TryEmitS128AndNotImm(this, node, true)) {
     VisitRRR(this, kArm64S128And, node);
   }
 }
 
-void InstructionSelector::VisitS128Zero(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitS128Zero(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   Emit(kArm64S128Const, g.DefineAsRegister(node), g.UseImmediate(0),
        g.UseImmediate(0), g.UseImmediate(0), g.UseImmediate(0));
 }
 
-void InstructionSelector::VisitI32x4DotI8x16I7x16AddS(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI32x4DotI8x16I7x16AddS(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   InstructionOperand output = CpuFeatures::IsSupported(DOTPROD)
                                   ? g.DefineSameAsInput(node, 2)
                                   : g.DefineAsRegister(node);
@@ -4066,7 +4278,9 @@ void InstructionSelector::VisitI32x4DotI8x16I7x16AddS(Node* node) {
 }
 
 #define SIMD_VISIT_EXTRACT_LANE(Type, T, Sign, LaneSize)                     \
-  void InstructionSelector::Visit##Type##ExtractLane##Sign(Node* node) {     \
+  template <typename Adapter>                                                \
+  void InstructionSelectorT<Adapter>::Visit##Type##ExtractLane##Sign(        \
+      Node* node) {                                                          \
     VisitRRI(this,                                                           \
              kArm64##T##ExtractLane##Sign | LaneSizeField::encode(LaneSize), \
              node);                                                          \
@@ -4082,7 +4296,8 @@ SIMD_VISIT_EXTRACT_LANE(I8x16, I, S, 8)
 #undef SIMD_VISIT_EXTRACT_LANE
 
 #define SIMD_VISIT_REPLACE_LANE(Type, T, LaneSize)                            \
-  void InstructionSelector::Visit##Type##ReplaceLane(Node* node) {            \
+  template <typename Adapter>                                                 \
+  void InstructionSelectorT<Adapter>::Visit##Type##ReplaceLane(Node* node) {  \
     VisitRRIR(this, kArm64##T##ReplaceLane | LaneSizeField::encode(LaneSize), \
               node);                                                          \
   }
@@ -4094,32 +4309,36 @@ SIMD_VISIT_REPLACE_LANE(I16x8, I, 16)
 SIMD_VISIT_REPLACE_LANE(I8x16, I, 8)
 #undef SIMD_VISIT_REPLACE_LANE
 
-#define SIMD_VISIT_UNOP(Name, instruction)            \
-  void InstructionSelector::Visit##Name(Node* node) { \
-    VisitRR(this, instruction, node);                 \
+#define SIMD_VISIT_UNOP(Name, instruction)                      \
+  template <typename Adapter>                                   \
+  void InstructionSelectorT<Adapter>::Visit##Name(Node* node) { \
+    VisitRR(this, instruction, node);                           \
   }
 SIMD_UNOP_LIST(SIMD_VISIT_UNOP)
 #undef SIMD_VISIT_UNOP
 #undef SIMD_UNOP_LIST
 
-#define SIMD_VISIT_SHIFT_OP(Name, width)                \
-  void InstructionSelector::Visit##Name(Node* node) {   \
-    VisitSimdShiftRRR(this, kArm64##Name, node, width); \
+#define SIMD_VISIT_SHIFT_OP(Name, width)                        \
+  template <typename Adapter>                                   \
+  void InstructionSelectorT<Adapter>::Visit##Name(Node* node) { \
+    VisitSimdShiftRRR(this, kArm64##Name, node, width);         \
   }
 SIMD_SHIFT_OP_LIST(SIMD_VISIT_SHIFT_OP)
 #undef SIMD_VISIT_SHIFT_OP
 #undef SIMD_SHIFT_OP_LIST
 
-#define SIMD_VISIT_BINOP(Name, instruction)           \
-  void InstructionSelector::Visit##Name(Node* node) { \
-    VisitRRR(this, instruction, node);                \
+#define SIMD_VISIT_BINOP(Name, instruction)                     \
+  template <typename Adapter>                                   \
+  void InstructionSelectorT<Adapter>::Visit##Name(Node* node) { \
+    VisitRRR(this, instruction, node);                          \
   }
 SIMD_BINOP_LIST(SIMD_VISIT_BINOP)
 #undef SIMD_VISIT_BINOP
 #undef SIMD_BINOP_LIST
 
 #define SIMD_VISIT_BINOP_LANE_SIZE(Name, instruction, LaneSize)          \
-  void InstructionSelector::Visit##Name(Node* node) {                    \
+  template <typename Adapter>                                            \
+  void InstructionSelectorT<Adapter>::Visit##Name(Node* node) {          \
     VisitRRR(this, instruction | LaneSizeField::encode(LaneSize), node); \
   }
 SIMD_BINOP_LANE_SIZE_LIST(SIMD_VISIT_BINOP_LANE_SIZE)
@@ -4127,7 +4346,8 @@ SIMD_BINOP_LANE_SIZE_LIST(SIMD_VISIT_BINOP_LANE_SIZE)
 #undef SIMD_BINOP_LANE_SIZE_LIST
 
 #define SIMD_VISIT_UNOP_LANE_SIZE(Name, instruction, LaneSize)          \
-  void InstructionSelector::Visit##Name(Node* node) {                   \
+  template <typename Adapter>                                           \
+  void InstructionSelectorT<Adapter>::Visit##Name(Node* node) {         \
     VisitRR(this, instruction | LaneSizeField::encode(LaneSize), node); \
   }
 SIMD_UNOP_LANE_SIZE_LIST(SIMD_VISIT_UNOP_LANE_SIZE)
@@ -4192,9 +4412,10 @@ MulWithDupResult TryMatchMulWithDup(Node* node) {
 }
 }  // namespace
 
-void InstructionSelector::VisitF32x4Mul(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF32x4Mul(Node* node) {
   if (MulWithDupResult result = TryMatchMulWithDup<4>(node)) {
-    Arm64OperandGenerator g(this);
+    Arm64OperandGeneratorT<Adapter> g(this);
     Emit(kArm64FMulElement | LaneSizeField::encode(32),
          g.DefineAsRegister(node), g.UseRegister(result.input),
          g.UseRegister(result.dup_node), g.UseImmediate(result.index));
@@ -4203,9 +4424,10 @@ void InstructionSelector::VisitF32x4Mul(Node* node) {
   }
 }
 
-void InstructionSelector::VisitF64x2Mul(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF64x2Mul(Node* node) {
   if (MulWithDupResult result = TryMatchMulWithDup<2>(node)) {
-    Arm64OperandGenerator g(this);
+    Arm64OperandGeneratorT<Adapter> g(this);
     Emit(kArm64FMulElement | LaneSizeField::encode(64),
          g.DefineAsRegister(node), g.UseRegister(result.input),
          g.UseRegister(result.dup_node), g.UseImmediate(result.index));
@@ -4214,8 +4436,9 @@ void InstructionSelector::VisitF64x2Mul(Node* node) {
   }
 }
 
-void InstructionSelector::VisitI64x2Mul(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI64x2Mul(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   InstructionOperand temps[] = {g.TempSimd128Register()};
   Emit(kArm64I64x2Mul, g.DefineAsRegister(node),
        g.UseRegister(node->InputAt(0)), g.UseRegister(node->InputAt(1)),
@@ -4253,10 +4476,11 @@ struct SimdAddOpMatcher : public NodeMatcher {
   Node* right_;
 };
 
-bool ShraHelper(InstructionSelector* selector, Node* node, int lane_size,
-                InstructionCode shra_code, InstructionCode add_code,
-                IrOpcode::Value shift_op) {
-  Arm64OperandGenerator g(selector);
+template <typename Adapter>
+bool ShraHelper(InstructionSelectorT<Adapter>* selector, Node* node,
+                int lane_size, InstructionCode shra_code,
+                InstructionCode add_code, IrOpcode::Value shift_op) {
+  Arm64OperandGeneratorT<Adapter> g(selector);
   SimdAddOpMatcher m(node, shift_op);
   if (!m.Matches() || !selector->CanCover(node, m.left())) return false;
   if (!g.IsIntegerConstant(m.left()->InputAt(1))) return false;
@@ -4275,9 +4499,11 @@ bool ShraHelper(InstructionSelector* selector, Node* node, int lane_size,
   return true;
 }
 
-bool AdalpHelper(InstructionSelector* selector, Node* node, int lane_size,
-                 InstructionCode adalp_code, IrOpcode::Value ext_op) {
-  Arm64OperandGenerator g(selector);
+template <typename Adapter>
+bool AdalpHelper(InstructionSelectorT<Adapter>* selector, Node* node,
+                 int lane_size, InstructionCode adalp_code,
+                 IrOpcode::Value ext_op) {
+  Arm64OperandGeneratorT<Adapter> g(selector);
   SimdAddOpMatcher m(node, ext_op);
   if (!m.Matches() || !selector->CanCover(node, m.left())) return false;
   selector->Emit(adalp_code | LaneSizeField::encode(lane_size),
@@ -4286,9 +4512,10 @@ bool AdalpHelper(InstructionSelector* selector, Node* node, int lane_size,
   return true;
 }
 
-bool MlaHelper(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+bool MlaHelper(InstructionSelectorT<Adapter>* selector, Node* node,
                InstructionCode mla_code, IrOpcode::Value mul_op) {
-  Arm64OperandGenerator g(selector);
+  Arm64OperandGeneratorT<Adapter> g(selector);
   SimdAddOpMatcher m(node, mul_op);
   if (!m.Matches() || !selector->CanCover(node, m.left())) return false;
   selector->Emit(mla_code, g.DefineSameAsFirst(node), g.UseRegister(m.right()),
@@ -4297,9 +4524,11 @@ bool MlaHelper(InstructionSelector* selector, Node* node,
   return true;
 }
 
-bool SmlalHelper(InstructionSelector* selector, Node* node, int lane_size,
-                 InstructionCode smlal_code, IrOpcode::Value ext_mul_op) {
-  Arm64OperandGenerator g(selector);
+template <typename Adapter>
+bool SmlalHelper(InstructionSelectorT<Adapter>* selector, Node* node,
+                 int lane_size, InstructionCode smlal_code,
+                 IrOpcode::Value ext_mul_op) {
+  Arm64OperandGeneratorT<Adapter> g(selector);
   SimdAddOpMatcher m(node, ext_mul_op);
   if (!m.Matches() || !selector->CanCover(node, m.left())) return false;
 
@@ -4312,7 +4541,8 @@ bool SmlalHelper(InstructionSelector* selector, Node* node, int lane_size,
 
 }  // namespace
 
-void InstructionSelector::VisitI64x2Add(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI64x2Add(Node* node) {
   if (!ShraHelper(this, node, 64, kArm64Ssra,
                   kArm64IAdd | LaneSizeField::encode(64),
                   IrOpcode::kI64x2ShrS) &&
@@ -4323,7 +4553,8 @@ void InstructionSelector::VisitI64x2Add(Node* node) {
   }
 }
 
-void InstructionSelector::VisitI8x16Add(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI8x16Add(Node* node) {
   if (!ShraHelper(this, node, 8, kArm64Ssra,
                   kArm64IAdd | LaneSizeField::encode(8),
                   IrOpcode::kI8x16ShrS) &&
@@ -4335,7 +4566,8 @@ void InstructionSelector::VisitI8x16Add(Node* node) {
 }
 
 #define VISIT_SIMD_ADD(Type, PairwiseType, LaneSize)                       \
-  void InstructionSelector::Visit##Type##Add(Node* node) {                 \
+  template <typename Adapter>                                              \
+  void InstructionSelectorT<Adapter>::Visit##Type##Add(Node* node) {       \
     /* Select Mla(z, x, y) for Add(x, Mul(y, z)). */                       \
     if (MlaHelper(this, node, kArm64Mla | LaneSizeField::encode(LaneSize), \
                   IrOpcode::k##Type##Mul)) {                               \
@@ -4377,8 +4609,9 @@ VISIT_SIMD_ADD(I16x8, I8x16, 16)
 #undef VISIT_SIMD_ADD
 
 #define VISIT_SIMD_SUB(Type, LaneSize)                                        \
-  void InstructionSelector::Visit##Type##Sub(Node* node) {                    \
-    Arm64OperandGenerator g(this);                                            \
+  template <typename Adapter>                                                 \
+  void InstructionSelectorT<Adapter>::Visit##Type##Sub(Node* node) {          \
+    Arm64OperandGeneratorT<Adapter> g(this);                                  \
     Node* left = node->InputAt(0);                                            \
     Node* right = node->InputAt(1);                                           \
     /* Select Mls(z, x, y) for Sub(z, Mul(x, y)). */                          \
@@ -4397,7 +4630,8 @@ VISIT_SIMD_SUB(I16x8, 16)
 #undef VISIT_SIMD_SUB
 
 namespace {
-bool isSimdZero(Arm64OperandGenerator& g, Node* node) {
+template <typename Adapter>
+bool isSimdZero(Arm64OperandGeneratorT<Adapter>& g, Node* node) {
   auto m = V128ConstMatcher(node);
   if (m.HasResolvedValue()) {
     auto imms = m.ResolvedValue().immediate();
@@ -4408,8 +4642,9 @@ bool isSimdZero(Arm64OperandGenerator& g, Node* node) {
 }  // namespace
 
 #define VISIT_SIMD_CM(Type, T, CmOp, CmOpposite, LaneSize)                   \
-  void InstructionSelector::Visit##Type##CmOp(Node* node) {                  \
-    Arm64OperandGenerator g(this);                                           \
+  template <typename Adapter>                                                \
+  void InstructionSelectorT<Adapter>::Visit##Type##CmOp(Node* node) {        \
+    Arm64OperandGeneratorT<Adapter> g(this);                                 \
     Node* left = node->InputAt(0);                                           \
     Node* right = node->InputAt(1);                                          \
     if (isSimdZero(g, left)) {                                               \
@@ -4451,32 +4686,38 @@ VISIT_SIMD_CM(I8x16, I, GtS, LtS, 8)
 VISIT_SIMD_CM(I8x16, I, GeS, LeS, 8)
 #undef VISIT_SIMD_CM
 
-void InstructionSelector::VisitS128Select(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitS128Select(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   Emit(kArm64S128Select, g.DefineSameAsFirst(node),
        g.UseRegister(node->InputAt(0)), g.UseRegister(node->InputAt(1)),
        g.UseRegister(node->InputAt(2)));
 }
 
-void InstructionSelector::VisitI8x16RelaxedLaneSelect(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI8x16RelaxedLaneSelect(Node* node) {
   VisitS128Select(node);
 }
 
-void InstructionSelector::VisitI16x8RelaxedLaneSelect(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI16x8RelaxedLaneSelect(Node* node) {
   VisitS128Select(node);
 }
 
-void InstructionSelector::VisitI32x4RelaxedLaneSelect(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI32x4RelaxedLaneSelect(Node* node) {
   VisitS128Select(node);
 }
 
-void InstructionSelector::VisitI64x2RelaxedLaneSelect(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI64x2RelaxedLaneSelect(Node* node) {
   VisitS128Select(node);
 }
 
 #define VISIT_SIMD_QFMOP(op)                                               \
-  void InstructionSelector::Visit##op(Node* node) {                        \
-    Arm64OperandGenerator g(this);                                         \
+  template <typename Adapter>                                              \
+  void InstructionSelectorT<Adapter>::Visit##op(Node* node) {              \
+    Arm64OperandGeneratorT<Adapter> g(this);                               \
     Emit(kArm64##op, g.DefineSameAsInput(node, 2),                         \
          g.UseRegister(node->InputAt(0)), g.UseRegister(node->InputAt(1)), \
          g.UseRegister(node->InputAt(2)));                                 \
@@ -4565,8 +4806,10 @@ bool TryMatchArchShuffle(const uint8_t* shuffle, const ShuffleEntry* table,
   return false;
 }
 
-void ArrangeShuffleTable(Arm64OperandGenerator* g, Node* input0, Node* input1,
-                         InstructionOperand* src0, InstructionOperand* src1) {
+template <typename Adapter>
+void ArrangeShuffleTable(Arm64OperandGeneratorT<Adapter>* g, Node* input0,
+                         Node* input1, InstructionOperand* src0,
+                         InstructionOperand* src1) {
   if (input0 == input1) {
     // Unary, any q-register can be the table.
     *src0 = *src1 = g->UseRegister(input0);
@@ -4579,12 +4822,13 @@ void ArrangeShuffleTable(Arm64OperandGenerator* g, Node* input0, Node* input1,
 
 }  // namespace
 
-void InstructionSelector::VisitI8x16Shuffle(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI8x16Shuffle(Node* node) {
   uint8_t shuffle[kSimd128Size];
   bool is_swizzle;
   CanonicalizeShuffle(node, shuffle, &is_swizzle);
   uint8_t shuffle32x4[4];
-  Arm64OperandGenerator g(this);
+  Arm64OperandGeneratorT<Adapter> g(this);
   ArchOpcode opcode;
   if (TryMatchArchShuffle(shuffle, arch_shuffles, arraysize(arch_shuffles),
                           is_swizzle, &opcode)) {
@@ -4636,33 +4880,42 @@ void InstructionSelector::VisitI8x16Shuffle(Node* node) {
        g.UseImmediate(wasm::SimdShuffle::Pack4Lanes(shuffle + 12)));
 }
 #else
-void InstructionSelector::VisitI8x16Shuffle(Node* node) { UNREACHABLE(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI8x16Shuffle(Node* node) {
+  UNREACHABLE();
+}
 #endif  // V8_ENABLE_WEBASSEMBLY
 
-void InstructionSelector::VisitSignExtendWord8ToInt32(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitSignExtendWord8ToInt32(Node* node) {
   VisitRR(this, kArm64Sxtb32, node);
 }
 
-void InstructionSelector::VisitSignExtendWord16ToInt32(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitSignExtendWord16ToInt32(Node* node) {
   VisitRR(this, kArm64Sxth32, node);
 }
 
-void InstructionSelector::VisitSignExtendWord8ToInt64(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitSignExtendWord8ToInt64(Node* node) {
   VisitRR(this, kArm64Sxtb, node);
 }
 
-void InstructionSelector::VisitSignExtendWord16ToInt64(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitSignExtendWord16ToInt64(Node* node) {
   VisitRR(this, kArm64Sxth, node);
 }
 
-void InstructionSelector::VisitSignExtendWord32ToInt64(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitSignExtendWord32ToInt64(Node* node) {
   VisitRR(this, kArm64Sxtw, node);
 }
 
 namespace {
-void VisitPminOrPmax(InstructionSelector* selector, ArchOpcode opcode,
+template <typename Adapter>
+void VisitPminOrPmax(InstructionSelectorT<Adapter>* selector, ArchOpcode opcode,
                      Node* node) {
-  Arm64OperandGenerator g(selector);
+  Arm64OperandGeneratorT<Adapter> g(selector);
   // Need all unique registers because we first compare the two inputs, then we
   // need the inputs to remain unchanged for the bitselect later.
   selector->Emit(opcode, g.DefineAsRegister(node),
@@ -4671,94 +4924,113 @@ void VisitPminOrPmax(InstructionSelector* selector, ArchOpcode opcode,
 }
 }  // namespace
 
-void InstructionSelector::VisitF32x4Pmin(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF32x4Pmin(Node* node) {
   VisitPminOrPmax(this, kArm64F32x4Pmin, node);
 }
 
-void InstructionSelector::VisitF32x4Pmax(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF32x4Pmax(Node* node) {
   VisitPminOrPmax(this, kArm64F32x4Pmax, node);
 }
 
-void InstructionSelector::VisitF64x2Pmin(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF64x2Pmin(Node* node) {
   VisitPminOrPmax(this, kArm64F64x2Pmin, node);
 }
 
-void InstructionSelector::VisitF64x2Pmax(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF64x2Pmax(Node* node) {
   VisitPminOrPmax(this, kArm64F64x2Pmax, node);
 }
 
 namespace {
-void VisitSignExtendLong(InstructionSelector* selector, ArchOpcode opcode,
-                         Node* node, int lane_size) {
+template <typename Adapter>
+void VisitSignExtendLong(InstructionSelectorT<Adapter>* selector,
+                         ArchOpcode opcode, Node* node, int lane_size) {
   InstructionCode code = opcode;
   code |= LaneSizeField::encode(lane_size);
   VisitRR(selector, code, node);
 }
 }  // namespace
 
-void InstructionSelector::VisitI64x2SConvertI32x4Low(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI64x2SConvertI32x4Low(Node* node) {
   VisitSignExtendLong(this, kArm64Sxtl, node, 64);
 }
 
-void InstructionSelector::VisitI64x2SConvertI32x4High(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI64x2SConvertI32x4High(Node* node) {
   VisitSignExtendLong(this, kArm64Sxtl2, node, 64);
 }
 
-void InstructionSelector::VisitI64x2UConvertI32x4Low(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI64x2UConvertI32x4Low(Node* node) {
   VisitSignExtendLong(this, kArm64Uxtl, node, 64);
 }
 
-void InstructionSelector::VisitI64x2UConvertI32x4High(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI64x2UConvertI32x4High(Node* node) {
   VisitSignExtendLong(this, kArm64Uxtl2, node, 64);
 }
 
-void InstructionSelector::VisitI32x4SConvertI16x8Low(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI32x4SConvertI16x8Low(Node* node) {
   VisitSignExtendLong(this, kArm64Sxtl, node, 32);
 }
 
-void InstructionSelector::VisitI32x4SConvertI16x8High(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI32x4SConvertI16x8High(Node* node) {
   VisitSignExtendLong(this, kArm64Sxtl2, node, 32);
 }
 
-void InstructionSelector::VisitI32x4UConvertI16x8Low(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI32x4UConvertI16x8Low(Node* node) {
   VisitSignExtendLong(this, kArm64Uxtl, node, 32);
 }
 
-void InstructionSelector::VisitI32x4UConvertI16x8High(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI32x4UConvertI16x8High(Node* node) {
   VisitSignExtendLong(this, kArm64Uxtl2, node, 32);
 }
 
-void InstructionSelector::VisitI16x8SConvertI8x16Low(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI16x8SConvertI8x16Low(Node* node) {
   VisitSignExtendLong(this, kArm64Sxtl, node, 16);
 }
 
-void InstructionSelector::VisitI16x8SConvertI8x16High(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI16x8SConvertI8x16High(Node* node) {
   VisitSignExtendLong(this, kArm64Sxtl2, node, 16);
 }
 
-void InstructionSelector::VisitI16x8UConvertI8x16Low(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI16x8UConvertI8x16Low(Node* node) {
   VisitSignExtendLong(this, kArm64Uxtl, node, 16);
 }
 
-void InstructionSelector::VisitI16x8UConvertI8x16High(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI16x8UConvertI8x16High(Node* node) {
   VisitSignExtendLong(this, kArm64Uxtl2, node, 16);
 }
 
-void InstructionSelector::VisitI8x16Popcnt(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI8x16Popcnt(Node* node) {
   InstructionCode code = kArm64Cnt;
   code |= LaneSizeField::encode(8);
   VisitRR(this, code, node);
 }
 
-void InstructionSelector::AddOutputToSelectContinuation(OperandGenerator* g,
-                                                        int first_input_index,
-                                                        Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::AddOutputToSelectContinuation(
+    OperandGenerator* g, int first_input_index, Node* node) {
   continuation_outputs_.push_back(g->DefineAsRegister(node));
 }
 
 // static
+template <typename Adapter>
 MachineOperatorBuilder::Flags
-InstructionSelector::SupportedMachineOperatorFlags() {
+InstructionSelectorT<Adapter>::SupportedMachineOperatorFlags() {
   return MachineOperatorBuilder::kFloat32RoundDown |
          MachineOperatorBuilder::kFloat64RoundDown |
          MachineOperatorBuilder::kFloat32RoundUp |
@@ -4782,12 +5054,18 @@ InstructionSelector::SupportedMachineOperatorFlags() {
 }
 
 // static
+template <typename Adapter>
 MachineOperatorBuilder::AlignmentRequirements
-InstructionSelector::AlignmentRequirements() {
+InstructionSelectorT<Adapter>::AlignmentRequirements() {
   return MachineOperatorBuilder::AlignmentRequirements::
       FullUnalignedAccessSupport();
 }
 
+template class EXPORT_TEMPLATE_DEFINE(V8_EXPORT_PRIVATE)
+    InstructionSelectorT<TurbofanAdapter>;
+template class EXPORT_TEMPLATE_DEFINE(V8_EXPORT_PRIVATE)
+    InstructionSelectorT<TurboshaftAdapter>;
+
 }  // namespace compiler
 }  // namespace internal
 }  // namespace v8
diff --git a/src/compiler/backend/ia32/instruction-selector-ia32.cc b/src/compiler/backend/ia32/instruction-selector-ia32.cc
index 36256d40d18..9ac7f2ee31d 100644
--- a/src/compiler/backend/ia32/instruction-selector-ia32.cc
+++ b/src/compiler/backend/ia32/instruction-selector-ia32.cc
@@ -48,10 +48,13 @@ namespace internal {
 namespace compiler {
 
 // Adds IA32-specific methods for generating operands.
-class IA32OperandGenerator final : public OperandGenerator {
+template <typename Adapter>
+class IA32OperandGeneratorT final : public OperandGeneratorT<Adapter> {
  public:
-  explicit IA32OperandGenerator(InstructionSelector* selector)
-      : OperandGenerator(selector) {}
+  OPERAND_GENERATOR_T_BOILERPLATE(Adapter)
+
+  explicit IA32OperandGeneratorT(InstructionSelectorT<Adapter>* selector)
+      : super(selector) {}
 
   InstructionOperand UseByteRegister(Node* node) {
     // TODO(titzer): encode byte register use constraints.
@@ -129,7 +132,8 @@ class IA32OperandGenerator final : public OperandGenerator {
   AddressingMode GenerateMemoryOperandInputs(
       Node* index, int scale, Node* base, int32_t displacement,
       DisplacementMode displacement_mode, InstructionOperand inputs[],
-      size_t* input_count, RegisterMode register_mode = kRegister) {
+      size_t* input_count,
+      RegisterMode register_mode = RegisterMode::kRegister) {
     AddressingMode mode = kMode_MRI;
     if (displacement_mode == kNegativeDisplacement) {
       displacement = base::bits::WraparoundNeg32(displacement);
@@ -189,7 +193,8 @@ class IA32OperandGenerator final : public OperandGenerator {
   AddressingMode GenerateMemoryOperandInputs(
       Node* index, int scale, Node* base, Node* displacement_node,
       DisplacementMode displacement_mode, InstructionOperand inputs[],
-      size_t* input_count, RegisterMode register_mode = kRegister) {
+      size_t* input_count,
+      RegisterMode register_mode = RegisterMode::kRegister) {
     int32_t displacement = (displacement_node == nullptr)
                                ? 0
                                : OpParameter<int32_t>(displacement_node->op());
@@ -200,7 +205,7 @@ class IA32OperandGenerator final : public OperandGenerator {
 
   AddressingMode GetEffectiveAddressMemoryOperand(
       Node* node, InstructionOperand inputs[], size_t* input_count,
-      RegisterMode register_mode = kRegister) {
+      RegisterMode register_mode = RegisterMode::kRegister) {
     {
       LoadMatcher<ExternalReferenceMatcher> m(node);
       if (m.index().HasResolvedValue() && m.object().HasResolvedValue() &&
@@ -294,8 +299,10 @@ ArchOpcode GetLoadOpcode(LoadRepresentation load_rep) {
   return opcode;
 }
 
-void VisitRO(InstructionSelector* selector, Node* node, ArchOpcode opcode) {
-  IA32OperandGenerator g(selector);
+template <typename Adapter>
+void VisitRO(InstructionSelectorT<Adapter>* selector, Node* node,
+             ArchOpcode opcode) {
+  IA32OperandGeneratorT<Adapter> g(selector);
   Node* input = node->InputAt(0);
   // We have to use a byte register as input to movsxb.
   InstructionOperand input_op =
@@ -303,33 +310,37 @@ void VisitRO(InstructionSelector* selector, Node* node, ArchOpcode opcode) {
   selector->Emit(opcode, g.DefineAsRegister(node), input_op);
 }
 
-void VisitROWithTemp(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitROWithTemp(InstructionSelectorT<Adapter>* selector, Node* node,
                      ArchOpcode opcode) {
-  IA32OperandGenerator g(selector);
+  IA32OperandGeneratorT<Adapter> g(selector);
   InstructionOperand temps[] = {g.TempRegister()};
   selector->Emit(opcode, g.DefineAsRegister(node), g.Use(node->InputAt(0)),
                  arraysize(temps), temps);
 }
 
-void VisitROWithTempSimd(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitROWithTempSimd(InstructionSelectorT<Adapter>* selector, Node* node,
                          ArchOpcode opcode) {
-  IA32OperandGenerator g(selector);
+  IA32OperandGeneratorT<Adapter> g(selector);
   InstructionOperand temps[] = {g.TempSimd128Register()};
   selector->Emit(opcode, g.DefineAsRegister(node),
                  g.UseUniqueRegister(node->InputAt(0)), arraysize(temps),
                  temps);
 }
 
-void VisitRR(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitRR(InstructionSelectorT<Adapter>* selector, Node* node,
              InstructionCode opcode) {
-  IA32OperandGenerator g(selector);
+  IA32OperandGeneratorT<Adapter> g(selector);
   selector->Emit(opcode, g.DefineAsRegister(node),
                  g.UseRegister(node->InputAt(0)));
 }
 
-void VisitRROFloat(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitRROFloat(InstructionSelectorT<Adapter>* selector, Node* node,
                    ArchOpcode opcode) {
-  IA32OperandGenerator g(selector);
+  IA32OperandGeneratorT<Adapter> g(selector);
   InstructionOperand operand0 = g.UseRegister(node->InputAt(0));
   InstructionOperand operand1 = g.Use(node->InputAt(1));
   if (selector->IsSupported(AVX)) {
@@ -342,9 +353,10 @@ void VisitRROFloat(InstructionSelector* selector, Node* node,
 // For float unary operations. Also allocates a temporary general register for
 // used in external operands. If a temp is not required, use VisitRRSimd (since
 // float and SIMD registers are the same on IA32.
-void VisitFloatUnop(InstructionSelector* selector, Node* node, Node* input,
-                    ArchOpcode opcode) {
-  IA32OperandGenerator g(selector);
+template <typename Adapter>
+void VisitFloatUnop(InstructionSelectorT<Adapter>* selector, Node* node,
+                    Node* input, ArchOpcode opcode) {
+  IA32OperandGeneratorT<Adapter> g(selector);
   InstructionOperand temps[] = {g.TempRegister()};
   // No need for unique because inputs are float but temp is general.
   if (selector->IsSupported(AVX)) {
@@ -356,9 +368,10 @@ void VisitFloatUnop(InstructionSelector* selector, Node* node, Node* input,
   }
 }
 
-void VisitRRSimd(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitRRSimd(InstructionSelectorT<Adapter>* selector, Node* node,
                  ArchOpcode avx_opcode, ArchOpcode sse_opcode) {
-  IA32OperandGenerator g(selector);
+  IA32OperandGeneratorT<Adapter> g(selector);
   InstructionOperand operand0 = g.UseRegister(node->InputAt(0));
   if (selector->IsSupported(AVX)) {
     selector->Emit(avx_opcode, g.DefineAsRegister(node), operand0);
@@ -367,7 +380,9 @@ void VisitRRSimd(InstructionSelector* selector, Node* node,
   }
 }
 
-void VisitRRSimd(InstructionSelector* selector, Node* node, ArchOpcode opcode) {
+template <typename Adapter>
+void VisitRRSimd(InstructionSelectorT<Adapter>* selector, Node* node,
+                 ArchOpcode opcode) {
   VisitRRSimd(selector, node, opcode, opcode);
 }
 
@@ -375,9 +390,10 @@ void VisitRRSimd(InstructionSelector* selector, Node* node, ArchOpcode opcode) {
 // a register as we don't have memory alignment yet. For AVX, memory operands
 // are fine, but can have performance issues if not aligned to 16/32 bytes
 // (based on load size), see SDM Vol 1, chapter 14.9
-void VisitRROSimd(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitRROSimd(InstructionSelectorT<Adapter>* selector, Node* node,
                   ArchOpcode avx_opcode, ArchOpcode sse_opcode) {
-  IA32OperandGenerator g(selector);
+  IA32OperandGeneratorT<Adapter> g(selector);
   InstructionOperand operand0 = g.UseRegister(node->InputAt(0));
   if (selector->IsSupported(AVX)) {
     selector->Emit(avx_opcode, g.DefineAsRegister(node), operand0,
@@ -387,9 +403,11 @@ void VisitRROSimd(InstructionSelector* selector, Node* node,
                    g.UseRegister(node->InputAt(1)));
   }
 }
-void VisitRRRSimd(InstructionSelector* selector, Node* node,
+
+template <typename Adapter>
+void VisitRRRSimd(InstructionSelectorT<Adapter>* selector, Node* node,
                   ArchOpcode opcode) {
-  IA32OperandGenerator g(selector);
+  IA32OperandGeneratorT<Adapter> g(selector);
   InstructionOperand dst = selector->IsSupported(AVX)
                                ? g.DefineAsRegister(node)
                                : g.DefineSameAsFirst(node);
@@ -398,9 +416,10 @@ void VisitRRRSimd(InstructionSelector* selector, Node* node,
   selector->Emit(opcode, dst, operand0, operand1);
 }
 
-void VisitRRISimd(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitRRISimd(InstructionSelectorT<Adapter>* selector, Node* node,
                   ArchOpcode opcode) {
-  IA32OperandGenerator g(selector);
+  IA32OperandGeneratorT<Adapter> g(selector);
   InstructionOperand operand0 = g.UseRegister(node->InputAt(0));
   InstructionOperand operand1 =
       g.UseImmediate(OpParameter<int32_t>(node->op()));
@@ -412,9 +431,10 @@ void VisitRRISimd(InstructionSelector* selector, Node* node,
   selector->Emit(opcode, dest, operand0, operand1);
 }
 
-void VisitRRISimd(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitRRISimd(InstructionSelectorT<Adapter>* selector, Node* node,
                   ArchOpcode avx_opcode, ArchOpcode sse_opcode) {
-  IA32OperandGenerator g(selector);
+  IA32OperandGeneratorT<Adapter> g(selector);
   InstructionOperand operand0 = g.UseRegister(node->InputAt(0));
   InstructionOperand operand1 =
       g.UseImmediate(OpParameter<int32_t>(node->op()));
@@ -425,9 +445,10 @@ void VisitRRISimd(InstructionSelector* selector, Node* node,
   }
 }
 
-void VisitRROSimdShift(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitRROSimdShift(InstructionSelectorT<Adapter>* selector, Node* node,
                        ArchOpcode opcode) {
-  IA32OperandGenerator g(selector);
+  IA32OperandGeneratorT<Adapter> g(selector);
   if (g.CanBeImmediate(node->InputAt(1))) {
     selector->Emit(opcode, g.DefineSameAsFirst(node),
                    g.UseRegister(node->InputAt(0)),
@@ -441,17 +462,19 @@ void VisitRROSimdShift(InstructionSelector* selector, Node* node,
   }
 }
 
-void VisitRRRR(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitRRRR(InstructionSelectorT<Adapter>* selector, Node* node,
                InstructionCode opcode) {
-  IA32OperandGenerator g(selector);
+  IA32OperandGeneratorT<Adapter> g(selector);
   selector->Emit(
       opcode, g.DefineAsRegister(node), g.UseRegister(node->InputAt(0)),
       g.UseRegister(node->InputAt(1)), g.UseRegister(node->InputAt(2)));
 }
 
-void VisitI8x16Shift(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitI8x16Shift(InstructionSelectorT<Adapter>* selector, Node* node,
                      ArchOpcode opcode) {
-  IA32OperandGenerator g(selector);
+  IA32OperandGeneratorT<Adapter> g(selector);
   InstructionOperand output = CpuFeatures::IsSupported(AVX)
                                   ? g.UseRegister(node)
                                   : g.DefineSameAsFirst(node);
@@ -474,7 +497,8 @@ void VisitI8x16Shift(InstructionSelector* selector, Node* node,
 }
 }  // namespace
 
-void InstructionSelector::VisitStackSlot(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitStackSlot(Node* node) {
   StackSlotRepresentation rep = StackSlotRepresentationOf(node->op());
   int slot = frame_->AllocateSpillSlot(rep.size(), rep.alignment());
   OperandGenerator g(this);
@@ -483,12 +507,14 @@ void InstructionSelector::VisitStackSlot(Node* node) {
        sequence()->AddImmediate(Constant(slot)), 0, nullptr);
 }
 
-void InstructionSelector::VisitAbortCSADcheck(Node* node) {
-  IA32OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitAbortCSADcheck(Node* node) {
+  IA32OperandGeneratorT<Adapter> g(this);
   Emit(kArchAbortCSADcheck, g.NoOutput(), g.UseFixed(node->InputAt(0), edx));
 }
 
-void InstructionSelector::VisitLoadLane(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitLoadLane(Node* node) {
   LoadLaneParameters params = LoadLaneParametersOf(node->op());
   InstructionCode opcode = kArchNop;
   if (params.rep == MachineType::Int8()) {
@@ -509,7 +535,7 @@ void InstructionSelector::VisitLoadLane(Node* node) {
     UNREACHABLE();
   }
 
-  IA32OperandGenerator g(this);
+  IA32OperandGeneratorT<Adapter> g(this);
   InstructionOperand outputs[] = {IsSupported(AVX) ? g.DefineAsRegister(node)
                                                    : g.DefineSameAsFirst(node)};
   // Input 0 is value node, 1 is lane idx, and GetEffectiveAddressMemoryOperand
@@ -535,7 +561,8 @@ void InstructionSelector::VisitLoadLane(Node* node) {
   Emit(opcode, 1, outputs, input_count, inputs);
 }
 
-void InstructionSelector::VisitLoadTransform(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitLoadTransform(Node* node) {
   LoadTransformParameters params = LoadTransformParametersOf(node->op());
   InstructionCode opcode;
   switch (params.transformation) {
@@ -584,7 +611,7 @@ void InstructionSelector::VisitLoadTransform(Node* node) {
   // Trap handler is not supported on IA32.
   DCHECK_NE(params.kind, MemoryAccessKind::kProtected);
 
-  IA32OperandGenerator g(this);
+  IA32OperandGeneratorT<Adapter> g(this);
   InstructionOperand outputs[1];
   outputs[0] = g.DefineAsRegister(node);
   InstructionOperand inputs[3];
@@ -595,9 +622,10 @@ void InstructionSelector::VisitLoadTransform(Node* node) {
   Emit(code, 1, outputs, input_count, inputs);
 }
 
-void InstructionSelector::VisitLoad(Node* node, Node* value,
-                                    InstructionCode opcode) {
-  IA32OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitLoad(Node* node, Node* value,
+                                              InstructionCode opcode) {
+  IA32OperandGeneratorT<Adapter> g(this);
   InstructionOperand outputs[1];
   outputs[0] = g.DefineAsRegister(node);
   InstructionOperand inputs[3];
@@ -608,13 +636,15 @@ void InstructionSelector::VisitLoad(Node* node, Node* value,
   Emit(code, 1, outputs, input_count, inputs);
 }
 
-void InstructionSelector::VisitLoad(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitLoad(Node* node) {
   LoadRepresentation load_rep = LoadRepresentationOf(node->op());
   DCHECK(!load_rep.IsMapWord());
   VisitLoad(node, node, GetLoadOpcode(load_rep));
 }
 
-void InstructionSelector::VisitProtectedLoad(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitProtectedLoad(Node* node) {
   // TODO(eholk)
   UNIMPLEMENTED();
 }
@@ -666,9 +696,10 @@ ArchOpcode GetSeqCstStoreOpcode(MachineRepresentation rep) {
   }
 }
 
-void VisitAtomicExchange(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitAtomicExchange(InstructionSelectorT<Adapter>* selector, Node* node,
                          ArchOpcode opcode, MachineRepresentation rep) {
-  IA32OperandGenerator g(selector);
+  IA32OperandGeneratorT<Adapter> g(selector);
   Node* base = node->InputAt(0);
   Node* index = node->InputAt(1);
   Node* value = node->InputAt(2);
@@ -689,10 +720,11 @@ void VisitAtomicExchange(InstructionSelector* selector, Node* node,
   selector->Emit(code, 1, outputs, arraysize(inputs), inputs);
 }
 
-void VisitStoreCommon(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitStoreCommon(InstructionSelectorT<Adapter>* selector, Node* node,
                       StoreRepresentation store_rep,
                       base::Optional<AtomicMemoryOrder> atomic_order) {
-  IA32OperandGenerator g(selector);
+  IA32OperandGeneratorT<Adapter> g(selector);
   Node* base = node->InputAt(0);
   Node* index = node->InputAt(1);
   Node* value = node->InputAt(2);
@@ -754,20 +786,26 @@ void VisitStoreCommon(InstructionSelector* selector, Node* node,
 
 }  // namespace
 
-void InstructionSelector::VisitStorePair(Node* node) { UNREACHABLE(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitStorePair(Node* node) {
+  UNREACHABLE();
+}
 
-void InstructionSelector::VisitStore(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitStore(Node* node) {
   VisitStoreCommon(this, node, StoreRepresentationOf(node->op()),
                    base::nullopt);
 }
 
-void InstructionSelector::VisitProtectedStore(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitProtectedStore(Node* node) {
   // TODO(eholk)
   UNIMPLEMENTED();
 }
 
-void InstructionSelector::VisitStoreLane(Node* node) {
-  IA32OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitStoreLane(Node* node) {
+  IA32OperandGeneratorT<Adapter> g(this);
 
   StoreLaneParameters params = StoreLaneParametersOf(node->op());
   InstructionCode opcode = kArchNop;
@@ -802,17 +840,24 @@ void InstructionSelector::VisitStoreLane(Node* node) {
 }
 
 // Architecture supports unaligned access, therefore VisitLoad is used instead
-void InstructionSelector::VisitUnalignedLoad(Node* node) { UNREACHABLE(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitUnalignedLoad(Node* node) {
+  UNREACHABLE();
+}
 
 // Architecture supports unaligned access, therefore VisitStore is used instead
-void InstructionSelector::VisitUnalignedStore(Node* node) { UNREACHABLE(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitUnalignedStore(Node* node) {
+  UNREACHABLE();
+}
 
 namespace {
 
 // Shared routine for multiple binary operations.
-void VisitBinop(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitBinop(InstructionSelectorT<Adapter>* selector, Node* node,
                 InstructionCode opcode, FlagsContinuation* cont) {
-  IA32OperandGenerator g(selector);
+  IA32OperandGeneratorT<Adapter> g(selector);
   Int32BinopMatcher m(node);
   Node* left = m.left().node();
   Node* right = m.right().node();
@@ -867,7 +912,8 @@ void VisitBinop(InstructionSelector* selector, Node* node,
 }
 
 // Shared routine for multiple binary operations.
-void VisitBinop(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitBinop(InstructionSelectorT<Adapter>* selector, Node* node,
                 InstructionCode opcode) {
   FlagsContinuation cont;
   VisitBinop(selector, node, opcode, &cont);
@@ -875,16 +921,19 @@ void VisitBinop(InstructionSelector* selector, Node* node,
 
 }  // namespace
 
-void InstructionSelector::VisitWord32And(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32And(Node* node) {
   VisitBinop(this, node, kIA32And);
 }
 
-void InstructionSelector::VisitWord32Or(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32Or(Node* node) {
   VisitBinop(this, node, kIA32Or);
 }
 
-void InstructionSelector::VisitWord32Xor(Node* node) {
-  IA32OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32Xor(Node* node) {
+  IA32OperandGeneratorT<Adapter> g(this);
   Int32BinopMatcher m(node);
   if (m.right().Is(-1)) {
     Emit(kIA32Not, g.DefineSameAsFirst(node), g.UseRegister(m.left().node()));
@@ -893,7 +942,8 @@ void InstructionSelector::VisitWord32Xor(Node* node) {
   }
 }
 
-void InstructionSelector::VisitStackPointerGreaterThan(
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitStackPointerGreaterThan(
     Node* node, FlagsContinuation* cont) {
   StackCheckKind kind = StackCheckKindOf(node->op());
   InstructionCode opcode =
@@ -901,7 +951,7 @@ void InstructionSelector::VisitStackPointerGreaterThan(
 
   int effect_level = GetEffectLevel(node, cont);
 
-  IA32OperandGenerator g(this);
+  IA32OperandGeneratorT<Adapter> g(this);
 
   // No outputs.
   InstructionOperand* const outputs = nullptr;
@@ -942,9 +992,10 @@ void InstructionSelector::VisitStackPointerGreaterThan(
 }
 
 // Shared routine for multiple shift operations.
-static inline void VisitShift(InstructionSelector* selector, Node* node,
-                              ArchOpcode opcode) {
-  IA32OperandGenerator g(selector);
+template <typename Adapter>
+static inline void VisitShift(InstructionSelectorT<Adapter>* selector,
+                              Node* node, ArchOpcode opcode) {
+  IA32OperandGeneratorT<Adapter> g(selector);
   Node* left = node->InputAt(0);
   Node* right = node->InputAt(1);
 
@@ -959,35 +1010,41 @@ static inline void VisitShift(InstructionSelector* selector, Node* node,
 
 namespace {
 
-void VisitMulHigh(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitMulHigh(InstructionSelectorT<Adapter>* selector, Node* node,
                   ArchOpcode opcode) {
-  IA32OperandGenerator g(selector);
+  IA32OperandGeneratorT<Adapter> g(selector);
   InstructionOperand temps[] = {g.TempRegister(eax)};
   selector->Emit(
       opcode, g.DefineAsFixed(node, edx), g.UseFixed(node->InputAt(0), eax),
       g.UseUniqueRegister(node->InputAt(1)), arraysize(temps), temps);
 }
 
-void VisitDiv(InstructionSelector* selector, Node* node, ArchOpcode opcode) {
-  IA32OperandGenerator g(selector);
+template <typename Adapter>
+void VisitDiv(InstructionSelectorT<Adapter>* selector, Node* node,
+              ArchOpcode opcode) {
+  IA32OperandGeneratorT<Adapter> g(selector);
   InstructionOperand temps[] = {g.TempRegister(edx)};
   selector->Emit(opcode, g.DefineAsFixed(node, eax),
                  g.UseFixed(node->InputAt(0), eax),
                  g.UseUnique(node->InputAt(1)), arraysize(temps), temps);
 }
 
-void VisitMod(InstructionSelector* selector, Node* node, ArchOpcode opcode) {
-  IA32OperandGenerator g(selector);
+template <typename Adapter>
+void VisitMod(InstructionSelectorT<Adapter>* selector, Node* node,
+              ArchOpcode opcode) {
+  IA32OperandGeneratorT<Adapter> g(selector);
   InstructionOperand temps[] = {g.TempRegister(eax)};
   selector->Emit(opcode, g.DefineAsFixed(node, edx),
                  g.UseFixed(node->InputAt(0), eax),
                  g.UseUnique(node->InputAt(1)), arraysize(temps), temps);
 }
 
-void EmitLea(InstructionSelector* selector, Node* result, Node* index,
+template <typename Adapter>
+void EmitLea(InstructionSelectorT<Adapter>* selector, Node* result, Node* index,
              int scale, Node* base, Node* displacement,
              DisplacementMode displacement_mode) {
-  IA32OperandGenerator g(selector);
+  IA32OperandGeneratorT<Adapter> g(selector);
   InstructionOperand inputs[4];
   size_t input_count = 0;
   AddressingMode mode =
@@ -1007,7 +1064,8 @@ void EmitLea(InstructionSelector* selector, Node* result, Node* index,
 
 }  // namespace
 
-void InstructionSelector::VisitWord32Shl(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32Shl(Node* node) {
   Int32ScaleMatcher m(node, true);
   if (m.matches()) {
     Node* index = node->InputAt(0);
@@ -1018,16 +1076,19 @@ void InstructionSelector::VisitWord32Shl(Node* node) {
   VisitShift(this, node, kIA32Shl);
 }
 
-void InstructionSelector::VisitWord32Shr(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32Shr(Node* node) {
   VisitShift(this, node, kIA32Shr);
 }
 
-void InstructionSelector::VisitWord32Sar(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32Sar(Node* node) {
   VisitShift(this, node, kIA32Sar);
 }
 
-void InstructionSelector::VisitInt32PairAdd(Node* node) {
-  IA32OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32PairAdd(Node* node) {
+  IA32OperandGeneratorT<Adapter> g(this);
 
   Node* projection1 = NodeProperties::FindProjection(node, 1);
   if (projection1) {
@@ -1052,8 +1113,9 @@ void InstructionSelector::VisitInt32PairAdd(Node* node) {
   }
 }
 
-void InstructionSelector::VisitInt32PairSub(Node* node) {
-  IA32OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32PairSub(Node* node) {
+  IA32OperandGeneratorT<Adapter> g(this);
 
   Node* projection1 = NodeProperties::FindProjection(node, 1);
   if (projection1) {
@@ -1078,8 +1140,9 @@ void InstructionSelector::VisitInt32PairSub(Node* node) {
   }
 }
 
-void InstructionSelector::VisitInt32PairMul(Node* node) {
-  IA32OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32PairMul(Node* node) {
+  IA32OperandGeneratorT<Adapter> g(this);
 
   Node* projection1 = NodeProperties::FindProjection(node, 1);
   if (projection1) {
@@ -1106,9 +1169,10 @@ void InstructionSelector::VisitInt32PairMul(Node* node) {
   }
 }
 
-void VisitWord32PairShift(InstructionSelector* selector, InstructionCode opcode,
-                          Node* node) {
-  IA32OperandGenerator g(selector);
+template <typename Adapter>
+void VisitWord32PairShift(InstructionSelectorT<Adapter>* selector,
+                          InstructionCode opcode, Node* node) {
+  IA32OperandGeneratorT<Adapter> g(selector);
 
   Node* shift = node->InputAt(2);
   InstructionOperand shift_operand;
@@ -1136,23 +1200,28 @@ void VisitWord32PairShift(InstructionSelector* selector, InstructionCode opcode,
   selector->Emit(opcode, output_count, outputs, 3, inputs, temp_count, temps);
 }
 
-void InstructionSelector::VisitWord32PairShl(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32PairShl(Node* node) {
   VisitWord32PairShift(this, kIA32ShlPair, node);
 }
 
-void InstructionSelector::VisitWord32PairShr(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32PairShr(Node* node) {
   VisitWord32PairShift(this, kIA32ShrPair, node);
 }
 
-void InstructionSelector::VisitWord32PairSar(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32PairSar(Node* node) {
   VisitWord32PairShift(this, kIA32SarPair, node);
 }
 
-void InstructionSelector::VisitWord32Rol(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32Rol(Node* node) {
   VisitShift(this, node, kIA32Rol);
 }
 
-void InstructionSelector::VisitWord32Ror(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32Ror(Node* node) {
   VisitShift(this, node, kIA32Ror);
 }
 
@@ -1233,69 +1302,84 @@ void InstructionSelector::VisitWord32Ror(Node* node) {
   V(F64x2Abs, kFloat64Abs)   \
   V(F64x2Neg, kFloat64Neg)
 
-#define RO_VISITOR(Name, opcode)                      \
-  void InstructionSelector::Visit##Name(Node* node) { \
-    VisitRO(this, node, opcode);                      \
+#define RO_VISITOR(Name, opcode)                                \
+  template <typename Adapter>                                   \
+  void InstructionSelectorT<Adapter>::Visit##Name(Node* node) { \
+    VisitRO(this, node, opcode);                                \
   }
 RO_OP_LIST(RO_VISITOR)
 #undef RO_VISITOR
 #undef RO_OP_LIST
 
-#define RO_WITH_TEMP_VISITOR(Name, opcode)            \
-  void InstructionSelector::Visit##Name(Node* node) { \
-    VisitROWithTemp(this, node, opcode);              \
+#define RO_WITH_TEMP_VISITOR(Name, opcode)                      \
+  template <typename Adapter>                                   \
+  void InstructionSelectorT<Adapter>::Visit##Name(Node* node) { \
+    VisitROWithTemp(this, node, opcode);                        \
   }
 RO_WITH_TEMP_OP_LIST(RO_WITH_TEMP_VISITOR)
 #undef RO_WITH_TEMP_VISITOR
 #undef RO_WITH_TEMP_OP_LIST
 
-#define RO_WITH_TEMP_SIMD_VISITOR(Name, opcode)       \
-  void InstructionSelector::Visit##Name(Node* node) { \
-    VisitROWithTempSimd(this, node, opcode);          \
+#define RO_WITH_TEMP_SIMD_VISITOR(Name, opcode)                 \
+  template <typename Adapter>                                   \
+  void InstructionSelectorT<Adapter>::Visit##Name(Node* node) { \
+    VisitROWithTempSimd(this, node, opcode);                    \
   }
 RO_WITH_TEMP_SIMD_OP_LIST(RO_WITH_TEMP_SIMD_VISITOR)
 #undef RO_WITH_TEMP_SIMD_VISITOR
 #undef RO_WITH_TEMP_SIMD_OP_LIST
 
-#define RR_VISITOR(Name, opcode)                      \
-  void InstructionSelector::Visit##Name(Node* node) { \
-    VisitRR(this, node, opcode);                      \
+#define RR_VISITOR(Name, opcode)                                \
+  template <typename Adapter>                                   \
+  void InstructionSelectorT<Adapter>::Visit##Name(Node* node) { \
+    VisitRR(this, node, opcode);                                \
   }
 RR_OP_LIST(RR_VISITOR)
 #undef RR_VISITOR
 #undef RR_OP_LIST
 
-#define RRO_FLOAT_VISITOR(Name, opcode)               \
-  void InstructionSelector::Visit##Name(Node* node) { \
-    VisitRROFloat(this, node, opcode);                \
+#define RRO_FLOAT_VISITOR(Name, opcode)                         \
+  template <typename Adapter>                                   \
+  void InstructionSelectorT<Adapter>::Visit##Name(Node* node) { \
+    VisitRROFloat(this, node, opcode);                          \
   }
 RRO_FLOAT_OP_LIST(RRO_FLOAT_VISITOR)
 #undef RRO_FLOAT_VISITOR
 #undef RRO_FLOAT_OP_LIST
 
-#define FLOAT_UNOP_VISITOR(Name, opcode)                  \
-  void InstructionSelector::Visit##Name(Node* node) {     \
-    VisitFloatUnop(this, node, node->InputAt(0), opcode); \
+#define FLOAT_UNOP_VISITOR(Name, opcode)                        \
+  template <typename Adapter>                                   \
+  void InstructionSelectorT<Adapter>::Visit##Name(Node* node) { \
+    VisitFloatUnop(this, node, node->InputAt(0), opcode);       \
   }
 FLOAT_UNOP_LIST(FLOAT_UNOP_VISITOR)
 #undef FLOAT_UNOP_VISITOR
 #undef FLOAT_UNOP_LIST
 
-void InstructionSelector::VisitWord32ReverseBits(Node* node) { UNREACHABLE(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32ReverseBits(Node* node) {
+  UNREACHABLE();
+}
 
-void InstructionSelector::VisitWord64ReverseBytes(Node* node) { UNREACHABLE(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64ReverseBytes(Node* node) {
+  UNREACHABLE();
+}
 
-void InstructionSelector::VisitWord32ReverseBytes(Node* node) {
-  IA32OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32ReverseBytes(Node* node) {
+  IA32OperandGeneratorT<Adapter> g(this);
   Emit(kIA32Bswap, g.DefineSameAsFirst(node), g.UseRegister(node->InputAt(0)));
 }
 
-void InstructionSelector::VisitSimd128ReverseBytes(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitSimd128ReverseBytes(Node* node) {
   UNREACHABLE();
 }
 
-void InstructionSelector::VisitInt32Add(Node* node) {
-  IA32OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32Add(Node* node) {
+  IA32OperandGeneratorT<Adapter> g(this);
 
   // Try to match the Add to a lea pattern
   BaseWithIndexAndDisplacement32Matcher m(node);
@@ -1322,8 +1406,9 @@ void InstructionSelector::VisitInt32Add(Node* node) {
   VisitBinop(this, node, kIA32Add);
 }
 
-void InstructionSelector::VisitInt32Sub(Node* node) {
-  IA32OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32Sub(Node* node) {
+  IA32OperandGeneratorT<Adapter> g(this);
   Int32BinopMatcher m(node);
   if (m.left().Is(0)) {
     Emit(kIA32Neg, g.DefineSameAsFirst(node), g.Use(m.right().node()));
@@ -1332,7 +1417,8 @@ void InstructionSelector::VisitInt32Sub(Node* node) {
   }
 }
 
-void InstructionSelector::VisitInt32Mul(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32Mul(Node* node) {
   Int32ScaleMatcher m(node, true);
   if (m.matches()) {
     Node* index = node->InputAt(0);
@@ -1340,7 +1426,7 @@ void InstructionSelector::VisitInt32Mul(Node* node) {
     EmitLea(this, node, index, m.scale(), base, nullptr, kPositiveDisplacement);
     return;
   }
-  IA32OperandGenerator g(this);
+  IA32OperandGeneratorT<Adapter> g(this);
   Node* left = node->InputAt(0);
   Node* right = node->InputAt(1);
   if (g.CanBeImmediate(right)) {
@@ -1355,105 +1441,123 @@ void InstructionSelector::VisitInt32Mul(Node* node) {
   }
 }
 
-void InstructionSelector::VisitInt32MulHigh(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32MulHigh(Node* node) {
   VisitMulHigh(this, node, kIA32ImulHigh);
 }
 
-void InstructionSelector::VisitUint32MulHigh(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitUint32MulHigh(Node* node) {
   VisitMulHigh(this, node, kIA32UmulHigh);
 }
 
-void InstructionSelector::VisitInt32Div(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32Div(Node* node) {
   VisitDiv(this, node, kIA32Idiv);
 }
 
-void InstructionSelector::VisitUint32Div(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitUint32Div(Node* node) {
   VisitDiv(this, node, kIA32Udiv);
 }
 
-void InstructionSelector::VisitInt32Mod(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32Mod(Node* node) {
   VisitMod(this, node, kIA32Idiv);
 }
 
-void InstructionSelector::VisitUint32Mod(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitUint32Mod(Node* node) {
   VisitMod(this, node, kIA32Udiv);
 }
 
-void InstructionSelector::VisitRoundUint32ToFloat32(Node* node) {
-  IA32OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitRoundUint32ToFloat32(Node* node) {
+  IA32OperandGeneratorT<Adapter> g(this);
   InstructionOperand temps[] = {g.TempRegister()};
   Emit(kIA32Uint32ToFloat32, g.DefineAsRegister(node), g.Use(node->InputAt(0)),
        arraysize(temps), temps);
 }
 
-void InstructionSelector::VisitFloat64Mod(Node* node) {
-  IA32OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64Mod(Node* node) {
+  IA32OperandGeneratorT<Adapter> g(this);
   InstructionOperand temps[] = {g.TempRegister(eax), g.TempRegister()};
   Emit(kIA32Float64Mod, g.DefineSameAsFirst(node),
        g.UseRegister(node->InputAt(0)), g.UseRegister(node->InputAt(1)),
        arraysize(temps), temps);
 }
 
-void InstructionSelector::VisitFloat32Max(Node* node) {
-  IA32OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat32Max(Node* node) {
+  IA32OperandGeneratorT<Adapter> g(this);
   InstructionOperand temps[] = {g.TempRegister()};
   Emit(kIA32Float32Max, g.DefineSameAsFirst(node),
        g.UseRegister(node->InputAt(0)), g.Use(node->InputAt(1)),
        arraysize(temps), temps);
 }
 
-void InstructionSelector::VisitFloat64Max(Node* node) {
-  IA32OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64Max(Node* node) {
+  IA32OperandGeneratorT<Adapter> g(this);
   InstructionOperand temps[] = {g.TempRegister()};
   Emit(kIA32Float64Max, g.DefineSameAsFirst(node),
        g.UseRegister(node->InputAt(0)), g.Use(node->InputAt(1)),
        arraysize(temps), temps);
 }
 
-void InstructionSelector::VisitFloat32Min(Node* node) {
-  IA32OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat32Min(Node* node) {
+  IA32OperandGeneratorT<Adapter> g(this);
   InstructionOperand temps[] = {g.TempRegister()};
   Emit(kIA32Float32Min, g.DefineSameAsFirst(node),
        g.UseRegister(node->InputAt(0)), g.Use(node->InputAt(1)),
        arraysize(temps), temps);
 }
 
-void InstructionSelector::VisitFloat64Min(Node* node) {
-  IA32OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64Min(Node* node) {
+  IA32OperandGeneratorT<Adapter> g(this);
   InstructionOperand temps[] = {g.TempRegister()};
   Emit(kIA32Float64Min, g.DefineSameAsFirst(node),
        g.UseRegister(node->InputAt(0)), g.Use(node->InputAt(1)),
        arraysize(temps), temps);
 }
 
-void InstructionSelector::VisitFloat64RoundTiesAway(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64RoundTiesAway(Node* node) {
   UNREACHABLE();
 }
 
-void InstructionSelector::VisitFloat64Ieee754Binop(Node* node,
-                                                   InstructionCode opcode) {
-  IA32OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64Ieee754Binop(
+    Node* node, InstructionCode opcode) {
+  IA32OperandGeneratorT<Adapter> g(this);
   Emit(opcode, g.DefineSameAsFirst(node), g.UseRegister(node->InputAt(0)),
        g.UseRegister(node->InputAt(1)))
       ->MarkAsCall();
 }
 
-void InstructionSelector::VisitFloat64Ieee754Unop(Node* node,
-                                                  InstructionCode opcode) {
-  IA32OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64Ieee754Unop(
+    Node* node, InstructionCode opcode) {
+  IA32OperandGeneratorT<Adapter> g(this);
   Emit(opcode, g.DefineSameAsFirst(node), g.UseRegister(node->InputAt(0)))
       ->MarkAsCall();
 }
 
-void InstructionSelector::EmitMoveParamToFPR(Node* node, int index) {}
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::EmitMoveParamToFPR(Node* node, int index) {}
 
-void InstructionSelector::EmitMoveFPRToParam(InstructionOperand* op,
-                                             LinkageLocation location) {}
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::EmitMoveFPRToParam(
+    InstructionOperand* op, LinkageLocation location) {}
 
-void InstructionSelector::EmitPrepareArguments(
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::EmitPrepareArguments(
     ZoneVector<PushParameter>* arguments, const CallDescriptor* call_descriptor,
     Node* node) {
-  IA32OperandGenerator g(this);
+  IA32OperandGeneratorT<Adapter> g(this);
 
   // Prepare for C function call.
   if (call_descriptor->IsCFunctionCall()) {
@@ -1509,10 +1613,11 @@ void InstructionSelector::EmitPrepareArguments(
   }
 }
 
-void InstructionSelector::EmitPrepareResults(
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::EmitPrepareResults(
     ZoneVector<PushParameter>* results, const CallDescriptor* call_descriptor,
     Node* node) {
-  IA32OperandGenerator g(this);
+  IA32OperandGeneratorT<Adapter> g(this);
 
   for (PushParameter output : *results) {
     if (!output.location.IsCallerFrameSlot()) continue;
@@ -1534,17 +1639,21 @@ void InstructionSelector::EmitPrepareResults(
   }
 }
 
-bool InstructionSelector::IsTailCallAddressImmediate() { return true; }
+template <typename Adapter>
+bool InstructionSelectorT<Adapter>::IsTailCallAddressImmediate() {
+  return true;
+}
 
 namespace {
 
-void VisitCompareWithMemoryOperand(InstructionSelector* selector,
+template <typename Adapter>
+void VisitCompareWithMemoryOperand(InstructionSelectorT<Adapter>* selector,
                                    InstructionCode opcode, Node* left,
                                    InstructionOperand right,
                                    FlagsContinuation* cont) {
   DCHECK(left->opcode() == IrOpcode::kLoad ||
          left->opcode() == IrOpcode::kLoadImmutable);
-  IA32OperandGenerator g(selector);
+  IA32OperandGeneratorT<Adapter> g(selector);
   size_t input_count = 0;
   InstructionOperand inputs[4];
   AddressingMode addressing_mode =
@@ -1556,17 +1665,19 @@ void VisitCompareWithMemoryOperand(InstructionSelector* selector,
 }
 
 // Shared routine for multiple compare operations.
-void VisitCompare(InstructionSelector* selector, InstructionCode opcode,
-                  InstructionOperand left, InstructionOperand right,
-                  FlagsContinuation* cont) {
+template <typename Adapter>
+void VisitCompare(InstructionSelectorT<Adapter>* selector,
+                  InstructionCode opcode, InstructionOperand left,
+                  InstructionOperand right, FlagsContinuation* cont) {
   selector->EmitWithContinuation(opcode, left, right, cont);
 }
 
 // Shared routine for multiple compare operations.
-void VisitCompare(InstructionSelector* selector, InstructionCode opcode,
-                  Node* left, Node* right, FlagsContinuation* cont,
-                  bool commutative) {
-  IA32OperandGenerator g(selector);
+template <typename Adapter>
+void VisitCompare(InstructionSelectorT<Adapter>* selector,
+                  InstructionCode opcode, Node* left, Node* right,
+                  FlagsContinuation* cont, bool commutative) {
+  IA32OperandGeneratorT<Adapter> g(selector);
   if (commutative && g.CanBeBetterLeftOperand(right)) {
     std::swap(left, right);
   }
@@ -1658,7 +1769,8 @@ InstructionCode TryNarrowOpcodeSize(InstructionCode opcode, Node* left,
 }
 
 // Shared routine for multiple float32 compare operations (inputs commuted).
-void VisitFloat32Compare(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitFloat32Compare(InstructionSelectorT<Adapter>* selector, Node* node,
                          FlagsContinuation* cont) {
   Node* const left = node->InputAt(0);
   Node* const right = node->InputAt(1);
@@ -1666,7 +1778,8 @@ void VisitFloat32Compare(InstructionSelector* selector, Node* node,
 }
 
 // Shared routine for multiple float64 compare operations (inputs commuted).
-void VisitFloat64Compare(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitFloat64Compare(InstructionSelectorT<Adapter>* selector, Node* node,
                          FlagsContinuation* cont) {
   Node* const left = node->InputAt(0);
   Node* const right = node->InputAt(1);
@@ -1674,9 +1787,10 @@ void VisitFloat64Compare(InstructionSelector* selector, Node* node,
 }
 
 // Shared routine for multiple word compare operations.
-void VisitWordCompare(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitWordCompare(InstructionSelectorT<Adapter>* selector, Node* node,
                       InstructionCode opcode, FlagsContinuation* cont) {
-  IA32OperandGenerator g(selector);
+  IA32OperandGeneratorT<Adapter> g(selector);
   Node* left = node->InputAt(0);
   Node* right = node->InputAt(1);
 
@@ -1718,15 +1832,17 @@ void VisitWordCompare(InstructionSelector* selector, Node* node,
                       node->op()->HasProperty(Operator::kCommutative));
 }
 
-void VisitWordCompare(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitWordCompare(InstructionSelectorT<Adapter>* selector, Node* node,
                       FlagsContinuation* cont) {
   VisitWordCompare(selector, node, kIA32Cmp, cont);
 }
 
-void VisitAtomicBinOp(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitAtomicBinOp(InstructionSelectorT<Adapter>* selector, Node* node,
                       ArchOpcode opcode, MachineRepresentation rep) {
   AddressingMode addressing_mode;
-  IA32OperandGenerator g(selector);
+  IA32OperandGeneratorT<Adapter> g(selector);
   Node* base = node->InputAt(0);
   Node* index = node->InputAt(1);
   Node* value = node->InputAt(2);
@@ -1742,9 +1858,10 @@ void VisitAtomicBinOp(InstructionSelector* selector, Node* node,
                  arraysize(temp), temp);
 }
 
-void VisitPairAtomicBinOp(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitPairAtomicBinOp(InstructionSelectorT<Adapter>* selector, Node* node,
                           ArchOpcode opcode) {
-  IA32OperandGenerator g(selector);
+  IA32OperandGeneratorT<Adapter> g(selector);
   Node* base = node->InputAt(0);
   Node* index = node->InputAt(1);
   Node* value = node->InputAt(2);
@@ -1783,8 +1900,9 @@ void VisitPairAtomicBinOp(InstructionSelector* selector, Node* node,
 }  // namespace
 
 // Shared routine for word comparison with zero.
-void InstructionSelector::VisitWordCompareZero(Node* user, Node* value,
-                                               FlagsContinuation* cont) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWordCompareZero(
+    Node* user, Node* value, FlagsContinuation* cont) {
   // Try to combine with comparisons against 0 by simply inverting the branch.
   while (value->opcode() == IrOpcode::kWord32Equal && CanCover(user, value)) {
     Int32BinopMatcher m(value);
@@ -1871,12 +1989,14 @@ void InstructionSelector::VisitWordCompareZero(Node* user, Node* value,
   }
 
   // Continuation could not be combined with a compare, emit compare against 0.
-  IA32OperandGenerator g(this);
+  IA32OperandGeneratorT<Adapter> g(this);
   VisitCompare(this, kIA32Cmp, g.Use(value), g.TempImmediate(0), cont);
 }
 
-void InstructionSelector::VisitSwitch(Node* node, const SwitchInfo& sw) {
-  IA32OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitSwitch(Node* node,
+                                                const SwitchInfo& sw) {
+  IA32OperandGeneratorT<Adapter> g(this);
   InstructionOperand value_operand = g.UseRegister(node->InputAt(0));
 
   // Emit either ArchTableSwitch or ArchBinarySearchSwitch.
@@ -1906,7 +2026,8 @@ void InstructionSelector::VisitSwitch(Node* node, const SwitchInfo& sw) {
   return EmitBinarySearchSwitch(sw, value_operand);
 }
 
-void InstructionSelector::VisitWord32Equal(Node* const node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32Equal(Node* const node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kEqual, node);
   Int32BinopMatcher m(node);
   if (m.right().Is(0)) {
@@ -1915,29 +2036,34 @@ void InstructionSelector::VisitWord32Equal(Node* const node) {
   VisitWordCompare(this, node, &cont);
 }
 
-void InstructionSelector::VisitInt32LessThan(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32LessThan(Node* node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kSignedLessThan, node);
   VisitWordCompare(this, node, &cont);
 }
 
-void InstructionSelector::VisitInt32LessThanOrEqual(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32LessThanOrEqual(Node* node) {
   FlagsContinuation cont =
       FlagsContinuation::ForSet(kSignedLessThanOrEqual, node);
   VisitWordCompare(this, node, &cont);
 }
 
-void InstructionSelector::VisitUint32LessThan(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitUint32LessThan(Node* node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kUnsignedLessThan, node);
   VisitWordCompare(this, node, &cont);
 }
 
-void InstructionSelector::VisitUint32LessThanOrEqual(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitUint32LessThanOrEqual(Node* node) {
   FlagsContinuation cont =
       FlagsContinuation::ForSet(kUnsignedLessThanOrEqual, node);
   VisitWordCompare(this, node, &cont);
 }
 
-void InstructionSelector::VisitInt32AddWithOverflow(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32AddWithOverflow(Node* node) {
   if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
     FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
     return VisitBinop(this, node, kIA32Add, &cont);
@@ -1946,7 +2072,8 @@ void InstructionSelector::VisitInt32AddWithOverflow(Node* node) {
   VisitBinop(this, node, kIA32Add, &cont);
 }
 
-void InstructionSelector::VisitInt32SubWithOverflow(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32SubWithOverflow(Node* node) {
   if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
     FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
     return VisitBinop(this, node, kIA32Sub, &cont);
@@ -1955,7 +2082,8 @@ void InstructionSelector::VisitInt32SubWithOverflow(Node* node) {
   VisitBinop(this, node, kIA32Sub, &cont);
 }
 
-void InstructionSelector::VisitInt32MulWithOverflow(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32MulWithOverflow(Node* node) {
   if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
     FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
     return VisitBinop(this, node, kIA32Imul, &cont);
@@ -1964,42 +2092,49 @@ void InstructionSelector::VisitInt32MulWithOverflow(Node* node) {
   VisitBinop(this, node, kIA32Imul, &cont);
 }
 
-void InstructionSelector::VisitFloat32Equal(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat32Equal(Node* node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kUnorderedEqual, node);
   VisitFloat32Compare(this, node, &cont);
 }
 
-void InstructionSelector::VisitFloat32LessThan(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat32LessThan(Node* node) {
   FlagsContinuation cont =
       FlagsContinuation::ForSet(kUnsignedGreaterThan, node);
   VisitFloat32Compare(this, node, &cont);
 }
 
-void InstructionSelector::VisitFloat32LessThanOrEqual(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat32LessThanOrEqual(Node* node) {
   FlagsContinuation cont =
       FlagsContinuation::ForSet(kUnsignedGreaterThanOrEqual, node);
   VisitFloat32Compare(this, node, &cont);
 }
 
-void InstructionSelector::VisitFloat64Equal(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64Equal(Node* node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kUnorderedEqual, node);
   VisitFloat64Compare(this, node, &cont);
 }
 
-void InstructionSelector::VisitFloat64LessThan(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64LessThan(Node* node) {
   FlagsContinuation cont =
       FlagsContinuation::ForSet(kUnsignedGreaterThan, node);
   VisitFloat64Compare(this, node, &cont);
 }
 
-void InstructionSelector::VisitFloat64LessThanOrEqual(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64LessThanOrEqual(Node* node) {
   FlagsContinuation cont =
       FlagsContinuation::ForSet(kUnsignedGreaterThanOrEqual, node);
   VisitFloat64Compare(this, node, &cont);
 }
 
-void InstructionSelector::VisitFloat64InsertLowWord32(Node* node) {
-  IA32OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64InsertLowWord32(Node* node) {
+  IA32OperandGeneratorT<Adapter> g(this);
   Node* left = node->InputAt(0);
   Node* right = node->InputAt(1);
   Float64Matcher mleft(left);
@@ -2012,33 +2147,37 @@ void InstructionSelector::VisitFloat64InsertLowWord32(Node* node) {
        g.UseRegister(left), g.Use(right));
 }
 
-void InstructionSelector::VisitFloat64InsertHighWord32(Node* node) {
-  IA32OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64InsertHighWord32(Node* node) {
+  IA32OperandGeneratorT<Adapter> g(this);
   Node* left = node->InputAt(0);
   Node* right = node->InputAt(1);
   Emit(kIA32Float64InsertHighWord32, g.DefineSameAsFirst(node),
        g.UseRegister(left), g.Use(right));
 }
 
-void InstructionSelector::VisitFloat64SilenceNaN(Node* node) {
-  IA32OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64SilenceNaN(Node* node) {
+  IA32OperandGeneratorT<Adapter> g(this);
   Emit(kIA32Float64SilenceNaN, g.DefineSameAsFirst(node),
        g.UseRegister(node->InputAt(0)));
 }
 
-void InstructionSelector::VisitMemoryBarrier(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitMemoryBarrier(Node* node) {
   // ia32 is no weaker than release-acquire and only needs to emit an
   // instruction for SeqCst memory barriers.
   AtomicMemoryOrder order = OpParameter<AtomicMemoryOrder>(node->op());
   if (order == AtomicMemoryOrder::kSeqCst) {
-    IA32OperandGenerator g(this);
+    IA32OperandGeneratorT<Adapter> g(this);
     Emit(kIA32MFence, g.NoOutput());
     return;
   }
   DCHECK_EQ(AtomicMemoryOrder::kAcqRel, order);
 }
 
-void InstructionSelector::VisitWord32AtomicLoad(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32AtomicLoad(Node* node) {
   AtomicLoadParameters atomic_load_params = AtomicLoadParametersOf(node->op());
   LoadRepresentation load_rep = atomic_load_params.representation();
   DCHECK(load_rep.representation() == MachineRepresentation::kWord8 ||
@@ -2054,14 +2193,16 @@ void InstructionSelector::VisitWord32AtomicLoad(Node* node) {
   VisitLoad(node, node, GetLoadOpcode(load_rep));
 }
 
-void InstructionSelector::VisitWord32AtomicStore(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32AtomicStore(Node* node) {
   AtomicStoreParameters store_params = AtomicStoreParametersOf(node->op());
   VisitStoreCommon(this, node, store_params.store_representation(),
                    store_params.order());
 }
 
-void InstructionSelector::VisitWord32AtomicExchange(Node* node) {
-  IA32OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32AtomicExchange(Node* node) {
+  IA32OperandGeneratorT<Adapter> g(this);
   MachineType type = AtomicOpType(node->op());
   ArchOpcode opcode;
   if (type == MachineType::Int8()) {
@@ -2080,8 +2221,10 @@ void InstructionSelector::VisitWord32AtomicExchange(Node* node) {
   VisitAtomicExchange(this, node, opcode, type.representation());
 }
 
-void InstructionSelector::VisitWord32AtomicCompareExchange(Node* node) {
-  IA32OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32AtomicCompareExchange(
+    Node* node) {
+  IA32OperandGeneratorT<Adapter> g(this);
   Node* base = node->InputAt(0);
   Node* index = node->InputAt(1);
   Node* old_value = node->InputAt(2);
@@ -2115,7 +2258,8 @@ void InstructionSelector::VisitWord32AtomicCompareExchange(Node* node) {
   Emit(code, 1, outputs, arraysize(inputs), inputs);
 }
 
-void InstructionSelector::VisitWord32AtomicBinaryOperation(
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32AtomicBinaryOperation(
     Node* node, ArchOpcode int8_op, ArchOpcode uint8_op, ArchOpcode int16_op,
     ArchOpcode uint16_op, ArchOpcode word32_op) {
   MachineType type = AtomicOpType(node->op());
@@ -2136,11 +2280,12 @@ void InstructionSelector::VisitWord32AtomicBinaryOperation(
   VisitAtomicBinOp(this, node, opcode, type.representation());
 }
 
-#define VISIT_ATOMIC_BINOP(op)                                           \
-  void InstructionSelector::VisitWord32Atomic##op(Node* node) {          \
-    VisitWord32AtomicBinaryOperation(                                    \
-        node, kAtomic##op##Int8, kAtomic##op##Uint8, kAtomic##op##Int16, \
-        kAtomic##op##Uint16, kAtomic##op##Word32);                       \
+#define VISIT_ATOMIC_BINOP(op)                                            \
+  template <typename Adapter>                                             \
+  void InstructionSelectorT<Adapter>::VisitWord32Atomic##op(Node* node) { \
+    VisitWord32AtomicBinaryOperation(                                     \
+        node, kAtomic##op##Int8, kAtomic##op##Uint8, kAtomic##op##Int16,  \
+        kAtomic##op##Uint16, kAtomic##op##Word32);                        \
   }
 VISIT_ATOMIC_BINOP(Add)
 VISIT_ATOMIC_BINOP(Sub)
@@ -2149,10 +2294,11 @@ VISIT_ATOMIC_BINOP(Or)
 VISIT_ATOMIC_BINOP(Xor)
 #undef VISIT_ATOMIC_BINOP
 
-void InstructionSelector::VisitWord32AtomicPairLoad(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32AtomicPairLoad(Node* node) {
   // Both acquire and sequentially consistent loads can emit MOV.
   // https://www.cl.cam.ac.uk/~pes20/cpp/cpp0xmappings.html
-  IA32OperandGenerator g(this);
+  IA32OperandGeneratorT<Adapter> g(this);
   AddressingMode mode;
   Node* base = node->InputAt(0);
   Node* index = node->InputAt(1);
@@ -2185,12 +2331,13 @@ void InstructionSelector::VisitWord32AtomicPairLoad(Node* node) {
   }
 }
 
-void InstructionSelector::VisitWord32AtomicPairStore(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32AtomicPairStore(Node* node) {
   // Release pair stores emit a MOVQ via a double register, and sequentially
   // consistent stores emit CMPXCHG8B.
   // https://www.cl.cam.ac.uk/~pes20/cpp/cpp0xmappings.html
 
-  IA32OperandGenerator g(this);
+  IA32OperandGeneratorT<Adapter> g(this);
   Node* base = node->InputAt(0);
   Node* index = node->InputAt(1);
   Node* value = node->InputAt(2);
@@ -2227,32 +2374,40 @@ void InstructionSelector::VisitWord32AtomicPairStore(Node* node) {
   }
 }
 
-void InstructionSelector::VisitWord32AtomicPairAdd(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32AtomicPairAdd(Node* node) {
   VisitPairAtomicBinOp(this, node, kIA32Word32AtomicPairAdd);
 }
 
-void InstructionSelector::VisitWord32AtomicPairSub(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32AtomicPairSub(Node* node) {
   VisitPairAtomicBinOp(this, node, kIA32Word32AtomicPairSub);
 }
 
-void InstructionSelector::VisitWord32AtomicPairAnd(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32AtomicPairAnd(Node* node) {
   VisitPairAtomicBinOp(this, node, kIA32Word32AtomicPairAnd);
 }
 
-void InstructionSelector::VisitWord32AtomicPairOr(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32AtomicPairOr(Node* node) {
   VisitPairAtomicBinOp(this, node, kIA32Word32AtomicPairOr);
 }
 
-void InstructionSelector::VisitWord32AtomicPairXor(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32AtomicPairXor(Node* node) {
   VisitPairAtomicBinOp(this, node, kIA32Word32AtomicPairXor);
 }
 
-void InstructionSelector::VisitWord32AtomicPairExchange(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32AtomicPairExchange(Node* node) {
   VisitPairAtomicBinOp(this, node, kIA32Word32AtomicPairExchange);
 }
 
-void InstructionSelector::VisitWord32AtomicPairCompareExchange(Node* node) {
-  IA32OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32AtomicPairCompareExchange(
+    Node* node) {
+  IA32OperandGeneratorT<Adapter> g(this);
   Node* index = node->InputAt(1);
   AddressingMode addressing_mode;
 
@@ -2429,8 +2584,9 @@ void InstructionSelector::VisitWord32AtomicPairCompareExchange(Node* node) {
   V(I16x8ShrS)                               \
   V(I16x8ShrU)
 
-void InstructionSelector::VisitS128Const(Node* node) {
-  IA32OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitS128Const(Node* node) {
+  IA32OperandGeneratorT<Adapter> g(this);
   static const int kUint32Immediates = kSimd128Size / sizeof(uint32_t);
   uint32_t val[kUint32Immediates];
   memcpy(val, S128ImmediateParameterOf(node->op()).data(), kSimd128Size);
@@ -2453,8 +2609,9 @@ void InstructionSelector::VisitS128Const(Node* node) {
   }
 }
 
-void InstructionSelector::VisitF64x2Min(Node* node) {
-  IA32OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF64x2Min(Node* node) {
+  IA32OperandGeneratorT<Adapter> g(this);
   InstructionOperand operand0 = g.UseRegister(node->InputAt(0));
   InstructionOperand operand1 = g.UseRegister(node->InputAt(1));
 
@@ -2465,8 +2622,9 @@ void InstructionSelector::VisitF64x2Min(Node* node) {
   }
 }
 
-void InstructionSelector::VisitF64x2Max(Node* node) {
-  IA32OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF64x2Max(Node* node) {
+  IA32OperandGeneratorT<Adapter> g(this);
   InstructionOperand operand0 = g.UseRegister(node->InputAt(0));
   InstructionOperand operand1 = g.UseRegister(node->InputAt(1));
   if (IsSupported(AVX)) {
@@ -2476,16 +2634,19 @@ void InstructionSelector::VisitF64x2Max(Node* node) {
   }
 }
 
-void InstructionSelector::VisitF64x2Splat(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF64x2Splat(Node* node) {
   VisitRRSimd(this, node, kIA32F64x2Splat);
 }
 
-void InstructionSelector::VisitF64x2ExtractLane(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF64x2ExtractLane(Node* node) {
   VisitRRISimd(this, node, kIA32F64x2ExtractLane, kIA32F64x2ExtractLane);
 }
 
-void InstructionSelector::VisitI64x2SplatI32Pair(Node* node) {
-  IA32OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI64x2SplatI32Pair(Node* node) {
+  IA32OperandGeneratorT<Adapter> g(this);
   Int32Matcher match_left(node->InputAt(0));
   Int32Matcher match_right(node->InputAt(1));
   if (match_left.Is(0) && match_right.Is(0)) {
@@ -2497,8 +2658,9 @@ void InstructionSelector::VisitI64x2SplatI32Pair(Node* node) {
   }
 }
 
-void InstructionSelector::VisitI64x2ReplaceLaneI32Pair(Node* node) {
-  IA32OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI64x2ReplaceLaneI32Pair(Node* node) {
+  IA32OperandGeneratorT<Adapter> g(this);
   InstructionOperand operand = g.UseRegister(node->InputAt(0));
   InstructionOperand lane = g.UseImmediate(OpParameter<int32_t>(node->op()));
   InstructionOperand low = g.Use(node->InputAt(1));
@@ -2507,8 +2669,9 @@ void InstructionSelector::VisitI64x2ReplaceLaneI32Pair(Node* node) {
        low, high);
 }
 
-void InstructionSelector::VisitI64x2Neg(Node* node) {
-  IA32OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI64x2Neg(Node* node) {
+  IA32OperandGeneratorT<Adapter> g(this);
   // If AVX unsupported, make sure dst != src to avoid a move.
   InstructionOperand operand0 = IsSupported(AVX)
                                     ? g.UseRegister(node->InputAt(0))
@@ -2516,8 +2679,9 @@ void InstructionSelector::VisitI64x2Neg(Node* node) {
   Emit(kIA32I64x2Neg, g.DefineAsRegister(node), operand0);
 }
 
-void InstructionSelector::VisitI64x2ShrS(Node* node) {
-  IA32OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI64x2ShrS(Node* node) {
+  IA32OperandGeneratorT<Adapter> g(this);
   InstructionOperand dst =
       IsSupported(AVX) ? g.DefineAsRegister(node) : g.DefineSameAsFirst(node);
 
@@ -2531,8 +2695,9 @@ void InstructionSelector::VisitI64x2ShrS(Node* node) {
   }
 }
 
-void InstructionSelector::VisitI64x2Mul(Node* node) {
-  IA32OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI64x2Mul(Node* node) {
+  IA32OperandGeneratorT<Adapter> g(this);
   InstructionOperand temps[] = {g.TempSimd128Register(),
                                 g.TempSimd128Register()};
   Emit(kIA32I64x2Mul, g.DefineAsRegister(node),
@@ -2540,24 +2705,28 @@ void InstructionSelector::VisitI64x2Mul(Node* node) {
        g.UseUniqueRegister(node->InputAt(1)), arraysize(temps), temps);
 }
 
-void InstructionSelector::VisitF32x4Splat(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF32x4Splat(Node* node) {
   VisitRRSimd(this, node, kIA32F32x4Splat);
 }
 
-void InstructionSelector::VisitF32x4ExtractLane(Node* node) {
-  IA32OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF32x4ExtractLane(Node* node) {
+  IA32OperandGeneratorT<Adapter> g(this);
   InstructionOperand operand0 = g.UseRegister(node->InputAt(0));
   InstructionOperand operand1 =
       g.UseImmediate(OpParameter<int32_t>(node->op()));
   Emit(kIA32F32x4ExtractLane, g.DefineAsRegister(node), operand0, operand1);
 }
 
-void InstructionSelector::VisitF32x4UConvertI32x4(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF32x4UConvertI32x4(Node* node) {
   VisitRRSimd(this, node, kIA32F32x4UConvertI32x4);
 }
 
-void InstructionSelector::VisitI32x4SConvertF32x4(Node* node) {
-  IA32OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI32x4SConvertF32x4(Node* node) {
+  IA32OperandGeneratorT<Adapter> g(this);
   InstructionOperand temps[] = {g.TempRegister()};
   InstructionOperand dst =
       IsSupported(AVX) ? g.DefineAsRegister(node) : g.DefineSameAsFirst(node);
@@ -2565,8 +2734,9 @@ void InstructionSelector::VisitI32x4SConvertF32x4(Node* node) {
        arraysize(temps), temps);
 }
 
-void InstructionSelector::VisitI32x4UConvertF32x4(Node* node) {
-  IA32OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI32x4UConvertF32x4(Node* node) {
+  IA32OperandGeneratorT<Adapter> g(this);
   InstructionOperand temps[] = {g.TempSimd128Register()};
   InstructionCode opcode =
       IsSupported(AVX) ? kAVXI32x4UConvertF32x4 : kSSEI32x4UConvertF32x4;
@@ -2574,21 +2744,24 @@ void InstructionSelector::VisitI32x4UConvertF32x4(Node* node) {
        arraysize(temps), temps);
 }
 
-void InstructionSelector::VisitS128Zero(Node* node) {
-  IA32OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitS128Zero(Node* node) {
+  IA32OperandGeneratorT<Adapter> g(this);
   Emit(kIA32S128Zero, g.DefineAsRegister(node));
 }
 
-void InstructionSelector::VisitS128Select(Node* node) {
-  IA32OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitS128Select(Node* node) {
+  IA32OperandGeneratorT<Adapter> g(this);
   InstructionOperand dst =
       IsSupported(AVX) ? g.DefineAsRegister(node) : g.DefineSameAsFirst(node);
   Emit(kIA32S128Select, dst, g.UseRegister(node->InputAt(0)),
        g.UseRegister(node->InputAt(1)), g.UseRegister(node->InputAt(2)));
 }
 
-void InstructionSelector::VisitS128AndNot(Node* node) {
-  IA32OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitS128AndNot(Node* node) {
+  IA32OperandGeneratorT<Adapter> g(this);
   // andnps a b does ~a & b, but we want a & !b, so flip the input.
   InstructionOperand dst =
       IsSupported(AVX) ? g.DefineAsRegister(node) : g.DefineSameAsFirst(node);
@@ -2596,37 +2769,43 @@ void InstructionSelector::VisitS128AndNot(Node* node) {
        g.UseRegister(node->InputAt(0)));
 }
 
-#define VISIT_SIMD_SPLAT(Type)                               \
-  void InstructionSelector::Visit##Type##Splat(Node* node) { \
-    Int32Matcher int32_matcher(node->InputAt(0));            \
-    if (int32_matcher.Is(0)) {                               \
-      IA32OperandGenerator g(this);                          \
-      Emit(kIA32S128Zero, g.DefineAsRegister(node));         \
-    } else {                                                 \
-      VisitRO(this, node, kIA32##Type##Splat);               \
-    }                                                        \
+#define VISIT_SIMD_SPLAT(Type)                                         \
+  template <typename Adapter>                                          \
+  void InstructionSelectorT<Adapter>::Visit##Type##Splat(Node* node) { \
+    Int32Matcher int32_matcher(node->InputAt(0));                      \
+    if (int32_matcher.Is(0)) {                                         \
+      IA32OperandGeneratorT<Adapter> g(this);                          \
+      Emit(kIA32S128Zero, g.DefineAsRegister(node));                   \
+    } else {                                                           \
+      VisitRO(this, node, kIA32##Type##Splat);                         \
+    }                                                                  \
   }
 SIMD_INT_TYPES(VISIT_SIMD_SPLAT)
 #undef SIMD_INT_TYPES
 #undef VISIT_SIMD_SPLAT
 
-void InstructionSelector::VisitI8x16ExtractLaneU(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI8x16ExtractLaneU(Node* node) {
   VisitRRISimd(this, node, kIA32Pextrb);
 }
 
-void InstructionSelector::VisitI8x16ExtractLaneS(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI8x16ExtractLaneS(Node* node) {
   VisitRRISimd(this, node, kIA32I8x16ExtractLaneS);
 }
 
-void InstructionSelector::VisitI16x8ExtractLaneU(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI16x8ExtractLaneU(Node* node) {
   VisitRRISimd(this, node, kIA32Pextrw);
 }
 
-void InstructionSelector::VisitI16x8ExtractLaneS(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI16x8ExtractLaneS(Node* node) {
   VisitRRISimd(this, node, kIA32I16x8ExtractLaneS);
 }
 
-void InstructionSelector::VisitI32x4ExtractLane(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI32x4ExtractLane(Node* node) {
   VisitRRISimd(this, node, kIA32I32x4ExtractLane);
 }
 
@@ -2636,23 +2815,25 @@ void InstructionSelector::VisitI32x4ExtractLane(Node* node) {
   V(I8x16, kIA32Pinsrb)              \
   V(F32x4, kIA32Insertps)
 
-#define VISIT_SIMD_REPLACE_LANE(TYPE, OPCODE)                              \
-  void InstructionSelector::Visit##TYPE##ReplaceLane(Node* node) {         \
-    IA32OperandGenerator g(this);                                          \
-    InstructionOperand operand0 = g.UseRegister(node->InputAt(0));         \
-    InstructionOperand operand1 =                                          \
-        g.UseImmediate(OpParameter<int32_t>(node->op()));                  \
-    InstructionOperand operand2 = g.Use(node->InputAt(1));                 \
-    InstructionOperand dst = IsSupported(AVX) ? g.DefineAsRegister(node)   \
-                                              : g.DefineSameAsFirst(node); \
-    Emit(OPCODE, dst, operand0, operand1, operand2);                       \
+#define VISIT_SIMD_REPLACE_LANE(TYPE, OPCODE)                                \
+  template <typename Adapter>                                                \
+  void InstructionSelectorT<Adapter>::Visit##TYPE##ReplaceLane(Node* node) { \
+    IA32OperandGeneratorT<Adapter> g(this);                                  \
+    InstructionOperand operand0 = g.UseRegister(node->InputAt(0));           \
+    InstructionOperand operand1 =                                            \
+        g.UseImmediate(OpParameter<int32_t>(node->op()));                    \
+    InstructionOperand operand2 = g.Use(node->InputAt(1));                   \
+    InstructionOperand dst = IsSupported(AVX) ? g.DefineAsRegister(node)     \
+                                              : g.DefineSameAsFirst(node);   \
+    Emit(OPCODE, dst, operand0, operand1, operand2);                         \
   }
 SIMD_REPLACE_LANE_TYPE_OP(VISIT_SIMD_REPLACE_LANE)
 #undef VISIT_SIMD_REPLACE_LANE
 #undef SIMD_REPLACE_LANE_TYPE_OP
 
-void InstructionSelector::VisitF64x2ReplaceLane(Node* node) {
-  IA32OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF64x2ReplaceLane(Node* node) {
+  IA32OperandGeneratorT<Adapter> g(this);
   int32_t lane = OpParameter<int32_t>(node->op());
   // When no-AVX, define dst == src to save a move.
   InstructionOperand dst =
@@ -2661,9 +2842,10 @@ void InstructionSelector::VisitF64x2ReplaceLane(Node* node) {
        g.UseImmediate(lane), g.UseRegister(node->InputAt(1)));
 }
 
-#define VISIT_SIMD_SHIFT_UNIFIED_SSE_AVX(Opcode)        \
-  void InstructionSelector::Visit##Opcode(Node* node) { \
-    VisitRROSimdShift(this, node, kIA32##Opcode);       \
+#define VISIT_SIMD_SHIFT_UNIFIED_SSE_AVX(Opcode)                  \
+  template <typename Adapter>                                     \
+  void InstructionSelectorT<Adapter>::Visit##Opcode(Node* node) { \
+    VisitRROSimdShift(this, node, kIA32##Opcode);                 \
   }
 SIMD_SHIFT_OPCODES_UNIFED_SSE_AVX(VISIT_SIMD_SHIFT_UNIFIED_SSE_AVX)
 #undef VISIT_SIMD_SHIFT_UNIFIED_SSE_AVX
@@ -2673,26 +2855,29 @@ SIMD_SHIFT_OPCODES_UNIFED_SSE_AVX(VISIT_SIMD_SHIFT_UNIFIED_SSE_AVX)
 // alignment yet. For AVX, memory operands are fine, but can have performance
 // issues if not aligned to 16/32 bytes (based on load size), see SDM Vol 1,
 // chapter 14.9
-#define VISIT_SIMD_UNOP(Opcode)                         \
-  void InstructionSelector::Visit##Opcode(Node* node) { \
-    IA32OperandGenerator g(this);                       \
-    Emit(kIA32##Opcode, g.DefineAsRegister(node),       \
-         g.UseRegister(node->InputAt(0)));              \
+#define VISIT_SIMD_UNOP(Opcode)                                   \
+  template <typename Adapter>                                     \
+  void InstructionSelectorT<Adapter>::Visit##Opcode(Node* node) { \
+    IA32OperandGeneratorT<Adapter> g(this);                       \
+    Emit(kIA32##Opcode, g.DefineAsRegister(node),                 \
+         g.UseRegister(node->InputAt(0)));                        \
   }
 SIMD_UNOP_LIST(VISIT_SIMD_UNOP)
 #undef VISIT_SIMD_UNOP
 #undef SIMD_UNOP_LIST
 
-void InstructionSelector::VisitV128AnyTrue(Node* node) {
-  IA32OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitV128AnyTrue(Node* node) {
+  IA32OperandGeneratorT<Adapter> g(this);
   InstructionOperand temps[] = {g.TempRegister()};
   Emit(kIA32S128AnyTrue, g.DefineAsRegister(node),
        g.UseRegister(node->InputAt(0)), arraysize(temps), temps);
 }
 
 #define VISIT_SIMD_ALLTRUE(Opcode)                                            \
-  void InstructionSelector::Visit##Opcode(Node* node) {                       \
-    IA32OperandGenerator g(this);                                             \
+  template <typename Adapter>                                                 \
+  void InstructionSelectorT<Adapter>::Visit##Opcode(Node* node) {             \
+    IA32OperandGeneratorT<Adapter> g(this);                                   \
     InstructionOperand temps[] = {g.TempRegister(), g.TempSimd128Register()}; \
     Emit(kIA32##Opcode, g.DefineAsRegister(node),                             \
          g.UseUniqueRegister(node->InputAt(0)), arraysize(temps), temps);     \
@@ -2701,54 +2886,63 @@ SIMD_ALLTRUE_LIST(VISIT_SIMD_ALLTRUE)
 #undef VISIT_SIMD_ALLTRUE
 #undef SIMD_ALLTRUE_LIST
 
-#define VISIT_SIMD_BINOP(Opcode)                          \
-  void InstructionSelector::Visit##Opcode(Node* node) {   \
-    VisitRROSimd(this, node, kAVX##Opcode, kSSE##Opcode); \
+#define VISIT_SIMD_BINOP(Opcode)                                  \
+  template <typename Adapter>                                     \
+  void InstructionSelectorT<Adapter>::Visit##Opcode(Node* node) { \
+    VisitRROSimd(this, node, kAVX##Opcode, kSSE##Opcode);         \
   }
 SIMD_BINOP_LIST(VISIT_SIMD_BINOP)
 #undef VISIT_SIMD_BINOP
 #undef SIMD_BINOP_LIST
 
-#define VISIT_SIMD_BINOP_UNIFIED_SSE_AVX(Opcode)            \
-  void InstructionSelector::Visit##Opcode(Node* node) {     \
-    VisitRROSimd(this, node, kIA32##Opcode, kIA32##Opcode); \
+#define VISIT_SIMD_BINOP_UNIFIED_SSE_AVX(Opcode)                  \
+  template <typename Adapter>                                     \
+  void InstructionSelectorT<Adapter>::Visit##Opcode(Node* node) { \
+    VisitRROSimd(this, node, kIA32##Opcode, kIA32##Opcode);       \
   }
 SIMD_BINOP_UNIFIED_SSE_AVX_LIST(VISIT_SIMD_BINOP_UNIFIED_SSE_AVX)
 #undef VISIT_SIMD_BINOP_UNIFIED_SSE_AVX
 #undef SIMD_BINOP_UNIFIED_SSE_AVX_LIST
 
-#define VISIT_SIMD_BINOP_RRR(OPCODE)                    \
-  void InstructionSelector::Visit##OPCODE(Node* node) { \
-    VisitRRRSimd(this, node, kIA32##OPCODE);            \
+#define VISIT_SIMD_BINOP_RRR(OPCODE)                              \
+  template <typename Adapter>                                     \
+  void InstructionSelectorT<Adapter>::Visit##OPCODE(Node* node) { \
+    VisitRRRSimd(this, node, kIA32##OPCODE);                      \
   }
 SIMD_BINOP_RRR(VISIT_SIMD_BINOP_RRR)
 #undef VISIT_SIMD_BINOP_RRR
 #undef SIMD_BINOP_RRR
 
-void InstructionSelector::VisitI16x8BitMask(Node* node) {
-  IA32OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI16x8BitMask(Node* node) {
+  IA32OperandGeneratorT<Adapter> g(this);
   InstructionOperand temps[] = {g.TempSimd128Register()};
   Emit(kIA32I16x8BitMask, g.DefineAsRegister(node),
        g.UseUniqueRegister(node->InputAt(0)), arraysize(temps), temps);
 }
 
-void InstructionSelector::VisitI8x16Shl(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI8x16Shl(Node* node) {
   VisitI8x16Shift(this, node, kIA32I8x16Shl);
 }
 
-void InstructionSelector::VisitI8x16ShrS(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI8x16ShrS(Node* node) {
   VisitI8x16Shift(this, node, kIA32I8x16ShrS);
 }
 
-void InstructionSelector::VisitI8x16ShrU(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI8x16ShrU(Node* node) {
   VisitI8x16Shift(this, node, kIA32I8x16ShrU);
 }
 
-void InstructionSelector::VisitInt32AbsWithOverflow(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32AbsWithOverflow(Node* node) {
   UNREACHABLE();
 }
 
-void InstructionSelector::VisitInt64AbsWithOverflow(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt64AbsWithOverflow(Node* node) {
   UNREACHABLE();
 }
 
@@ -2891,7 +3085,8 @@ bool TryMatchArchShuffle(const uint8_t* shuffle, const ShuffleEntry* table,
 
 }  // namespace
 
-void InstructionSelector::VisitI8x16Shuffle(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI8x16Shuffle(Node* node) {
   uint8_t shuffle[kSimd128Size];
   bool is_swizzle;
   CanonicalizeShuffle(node, shuffle, &is_swizzle);
@@ -2903,7 +3098,7 @@ void InstructionSelector::VisitI8x16Shuffle(Node* node) {
   static const int kMaxTemps = 2;
   InstructionOperand temps[kMaxTemps];
 
-  IA32OperandGenerator g(this);
+  IA32OperandGeneratorT<Adapter> g(this);
   bool use_avx = CpuFeatures::IsSupported(AVX);
   // AVX and swizzles don't generally need DefineSameAsFirst to avoid a move.
   bool no_same_as_first = use_avx || is_swizzle;
@@ -3043,7 +3238,8 @@ void InstructionSelector::VisitI8x16Shuffle(Node* node) {
   Emit(opcode, 1, &dst, input_count, inputs, temp_count, temps);
 }
 
-void InstructionSelector::VisitI8x16Swizzle(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI8x16Swizzle(Node* node) {
   InstructionCode op = kIA32I8x16Swizzle;
 
   bool relaxed = OpParameter<bool>(node->op());
@@ -3060,7 +3256,7 @@ void InstructionSelector::VisitI8x16Swizzle(Node* node) {
     }
   }
 
-  IA32OperandGenerator g(this);
+  IA32OperandGeneratorT<Adapter> g(this);
   InstructionOperand temps[] = {g.TempRegister()};
   Emit(op,
        IsSupported(AVX) ? g.DefineAsRegister(node) : g.DefineSameAsFirst(node),
@@ -3068,16 +3264,21 @@ void InstructionSelector::VisitI8x16Swizzle(Node* node) {
        arraysize(temps), temps);
 }
 #else
-void InstructionSelector::VisitI8x16Shuffle(Node* node) { UNREACHABLE(); }
-void InstructionSelector::VisitI8x16Swizzle(Node* node) { UNREACHABLE(); }
+void InstructionSelectorT<Adapter>::VisitI8x16Shuffle(Node* node) {
+  UNREACHABLE();
+}
+void InstructionSelectorT<Adapter>::VisitI8x16Swizzle(Node* node) {
+  UNREACHABLE();
+}
 #endif  // V8_ENABLE_WEBASSEMBLY
 
 namespace {
-void VisitMinOrMax(InstructionSelector* selector, Node* node, ArchOpcode opcode,
-                   bool flip_inputs) {
+template <typename Adapter>
+void VisitMinOrMax(InstructionSelectorT<Adapter>* selector, Node* node,
+                   ArchOpcode opcode, bool flip_inputs) {
   // Due to the way minps/minpd work, we want the dst to be same as the second
   // input: b = pmin(a, b) directly maps to minps b a.
-  IA32OperandGenerator g(selector);
+  IA32OperandGeneratorT<Adapter> g(selector);
   InstructionOperand dst = selector->IsSupported(AVX)
                                ? g.DefineAsRegister(node)
                                : g.DefineSameAsFirst(node);
@@ -3093,42 +3294,51 @@ void VisitMinOrMax(InstructionSelector* selector, Node* node, ArchOpcode opcode,
 }
 }  // namespace
 
-void InstructionSelector::VisitF32x4Pmin(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF32x4Pmin(Node* node) {
   VisitMinOrMax(this, node, kIA32Minps, true);
 }
 
-void InstructionSelector::VisitF32x4Pmax(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF32x4Pmax(Node* node) {
   VisitMinOrMax(this, node, kIA32Maxps, true);
 }
 
-void InstructionSelector::VisitF64x2Pmin(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF64x2Pmin(Node* node) {
   VisitMinOrMax(this, node, kIA32Minpd, true);
 }
 
-void InstructionSelector::VisitF64x2Pmax(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF64x2Pmax(Node* node) {
   VisitMinOrMax(this, node, kIA32Maxpd, true);
 }
 
-void InstructionSelector::VisitF32x4RelaxedMin(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF32x4RelaxedMin(Node* node) {
   VisitMinOrMax(this, node, kIA32Minps, false);
 }
 
-void InstructionSelector::VisitF32x4RelaxedMax(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF32x4RelaxedMax(Node* node) {
   VisitMinOrMax(this, node, kIA32Maxps, false);
 }
 
-void InstructionSelector::VisitF64x2RelaxedMin(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF64x2RelaxedMin(Node* node) {
   VisitMinOrMax(this, node, kIA32Minpd, false);
 }
 
-void InstructionSelector::VisitF64x2RelaxedMax(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF64x2RelaxedMax(Node* node) {
   VisitMinOrMax(this, node, kIA32Maxpd, false);
 }
 
 namespace {
-void VisitExtAddPairwise(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitExtAddPairwise(InstructionSelectorT<Adapter>* selector, Node* node,
                          ArchOpcode opcode, bool need_temp) {
-  IA32OperandGenerator g(selector);
+  IA32OperandGeneratorT<Adapter> g(selector);
   InstructionOperand operand0 = g.UseRegister(node->InputAt(0));
   InstructionOperand dst = (selector->IsSupported(AVX))
                                ? g.DefineAsRegister(node)
@@ -3142,24 +3352,29 @@ void VisitExtAddPairwise(InstructionSelector* selector, Node* node,
 }
 }  // namespace
 
-void InstructionSelector::VisitI32x4ExtAddPairwiseI16x8S(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI32x4ExtAddPairwiseI16x8S(Node* node) {
   VisitExtAddPairwise(this, node, kIA32I32x4ExtAddPairwiseI16x8S, true);
 }
 
-void InstructionSelector::VisitI32x4ExtAddPairwiseI16x8U(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI32x4ExtAddPairwiseI16x8U(Node* node) {
   VisitExtAddPairwise(this, node, kIA32I32x4ExtAddPairwiseI16x8U, false);
 }
 
-void InstructionSelector::VisitI16x8ExtAddPairwiseI8x16S(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI16x8ExtAddPairwiseI8x16S(Node* node) {
   VisitExtAddPairwise(this, node, kIA32I16x8ExtAddPairwiseI8x16S, true);
 }
 
-void InstructionSelector::VisitI16x8ExtAddPairwiseI8x16U(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI16x8ExtAddPairwiseI8x16U(Node* node) {
   VisitExtAddPairwise(this, node, kIA32I16x8ExtAddPairwiseI8x16U, true);
 }
 
-void InstructionSelector::VisitI8x16Popcnt(Node* node) {
-  IA32OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI8x16Popcnt(Node* node) {
+  IA32OperandGeneratorT<Adapter> g(this);
   InstructionOperand dst = CpuFeatures::IsSupported(AVX)
                                ? g.DefineAsRegister(node)
                                : g.DefineAsRegister(node);
@@ -3168,8 +3383,9 @@ void InstructionSelector::VisitI8x16Popcnt(Node* node) {
        arraysize(temps), temps);
 }
 
-void InstructionSelector::VisitF64x2ConvertLowI32x4U(Node* node) {
-  IA32OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF64x2ConvertLowI32x4U(Node* node) {
+  IA32OperandGeneratorT<Adapter> g(this);
   InstructionOperand temps[] = {g.TempRegister()};
   InstructionOperand dst =
       IsSupported(AVX) ? g.DefineAsRegister(node) : g.DefineSameAsFirst(node);
@@ -3177,8 +3393,9 @@ void InstructionSelector::VisitF64x2ConvertLowI32x4U(Node* node) {
        arraysize(temps), temps);
 }
 
-void InstructionSelector::VisitI32x4TruncSatF64x2SZero(Node* node) {
-  IA32OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI32x4TruncSatF64x2SZero(Node* node) {
+  IA32OperandGeneratorT<Adapter> g(this);
   InstructionOperand temps[] = {g.TempRegister()};
   if (IsSupported(AVX)) {
     // Requires dst != src.
@@ -3190,8 +3407,9 @@ void InstructionSelector::VisitI32x4TruncSatF64x2SZero(Node* node) {
   }
 }
 
-void InstructionSelector::VisitI32x4TruncSatF64x2UZero(Node* node) {
-  IA32OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI32x4TruncSatF64x2UZero(Node* node) {
+  IA32OperandGeneratorT<Adapter> g(this);
   InstructionOperand temps[] = {g.TempRegister()};
   InstructionOperand dst =
       IsSupported(AVX) ? g.DefineAsRegister(node) : g.DefineSameAsFirst(node);
@@ -3199,24 +3417,31 @@ void InstructionSelector::VisitI32x4TruncSatF64x2UZero(Node* node) {
        arraysize(temps), temps);
 }
 
-void InstructionSelector::VisitI32x4RelaxedTruncF64x2SZero(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI32x4RelaxedTruncF64x2SZero(
+    Node* node) {
   VisitRRSimd(this, node, kIA32Cvttpd2dq);
 }
 
-void InstructionSelector::VisitI32x4RelaxedTruncF64x2UZero(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI32x4RelaxedTruncF64x2UZero(
+    Node* node) {
   VisitFloatUnop(this, node, node->InputAt(0), kIA32I32x4TruncF64x2UZero);
 }
 
-void InstructionSelector::VisitI32x4RelaxedTruncF32x4S(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI32x4RelaxedTruncF32x4S(Node* node) {
   VisitRRSimd(this, node, kIA32Cvttps2dq);
 }
 
-void InstructionSelector::VisitI32x4RelaxedTruncF32x4U(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI32x4RelaxedTruncF32x4U(Node* node) {
   VisitFloatUnop(this, node, node->InputAt(0), kIA32I32x4TruncF32x4U);
 }
 
-void InstructionSelector::VisitI64x2GtS(Node* node) {
-  IA32OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI64x2GtS(Node* node) {
+  IA32OperandGeneratorT<Adapter> g(this);
   if (CpuFeatures::IsSupported(AVX)) {
     Emit(kIA32I64x2GtS, g.DefineAsRegister(node),
          g.UseRegister(node->InputAt(0)), g.UseRegister(node->InputAt(1)));
@@ -3230,8 +3455,9 @@ void InstructionSelector::VisitI64x2GtS(Node* node) {
   }
 }
 
-void InstructionSelector::VisitI64x2GeS(Node* node) {
-  IA32OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI64x2GeS(Node* node) {
+  IA32OperandGeneratorT<Adapter> g(this);
   if (CpuFeatures::IsSupported(AVX)) {
     Emit(kIA32I64x2GeS, g.DefineAsRegister(node),
          g.UseRegister(node->InputAt(0)), g.UseRegister(node->InputAt(1)));
@@ -3246,12 +3472,14 @@ void InstructionSelector::VisitI64x2GeS(Node* node) {
   }
 }
 
-void InstructionSelector::VisitI64x2Abs(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI64x2Abs(Node* node) {
   VisitRRSimd(this, node, kIA32I64x2Abs, kIA32I64x2Abs);
 }
 
-void InstructionSelector::VisitF64x2PromoteLowF32x4(Node* node) {
-  IA32OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF64x2PromoteLowF32x4(Node* node) {
+  IA32OperandGeneratorT<Adapter> g(this);
   InstructionCode code = kIA32F64x2PromoteLowF32x4;
   Node* input = node->InputAt(0);
   LoadTransformMatcher m(input);
@@ -3270,9 +3498,10 @@ void InstructionSelector::VisitF64x2PromoteLowF32x4(Node* node) {
 }
 
 namespace {
-void VisitRelaxedLaneSelect(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitRelaxedLaneSelect(InstructionSelectorT<Adapter>* selector, Node* node,
                             InstructionCode code = kIA32Pblendvb) {
-  IA32OperandGenerator g(selector);
+  IA32OperandGeneratorT<Adapter> g(selector);
   // pblendvb/blendvps/blendvpd copies src2 when mask is set, opposite from Wasm
   // semantics. node's inputs are: mask, lhs, rhs (determined in
   // wasm-compiler.cc).
@@ -3290,43 +3519,53 @@ void VisitRelaxedLaneSelect(InstructionSelector* selector, Node* node,
 }
 }  // namespace
 
-void InstructionSelector::VisitI8x16RelaxedLaneSelect(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI8x16RelaxedLaneSelect(Node* node) {
   VisitRelaxedLaneSelect(this, node);
 }
-void InstructionSelector::VisitI16x8RelaxedLaneSelect(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI16x8RelaxedLaneSelect(Node* node) {
   VisitRelaxedLaneSelect(this, node);
 }
-void InstructionSelector::VisitI32x4RelaxedLaneSelect(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI32x4RelaxedLaneSelect(Node* node) {
   VisitRelaxedLaneSelect(this, node, kIA32Blendvps);
 }
-void InstructionSelector::VisitI64x2RelaxedLaneSelect(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI64x2RelaxedLaneSelect(Node* node) {
   VisitRelaxedLaneSelect(this, node, kIA32Blendvpd);
 }
 
-void InstructionSelector::VisitF64x2Qfma(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF64x2Qfma(Node* node) {
   VisitRRRR(this, node, kIA32F64x2Qfma);
 }
 
-void InstructionSelector::VisitF64x2Qfms(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF64x2Qfms(Node* node) {
   VisitRRRR(this, node, kIA32F64x2Qfms);
 }
 
-void InstructionSelector::VisitF32x4Qfma(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF32x4Qfma(Node* node) {
   VisitRRRR(this, node, kIA32F32x4Qfma);
 }
 
-void InstructionSelector::VisitF32x4Qfms(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF32x4Qfms(Node* node) {
   VisitRRRR(this, node, kIA32F32x4Qfms);
 }
 
-void InstructionSelector::VisitI16x8DotI8x16I7x16S(Node* node) {
-  IA32OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI16x8DotI8x16I7x16S(Node* node) {
+  IA32OperandGeneratorT<Adapter> g(this);
   Emit(kIA32I16x8DotI8x16I7x16S, g.DefineAsRegister(node),
        g.UseUniqueRegister(node->InputAt(0)), g.UseRegister(node->InputAt(1)));
 }
 
-void InstructionSelector::VisitI32x4DotI8x16I7x16AddS(Node* node) {
-  IA32OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI32x4DotI8x16I7x16AddS(Node* node) {
+  IA32OperandGeneratorT<Adapter> g(this);
   InstructionOperand temps[] = {g.TempSimd128Register()};
   Emit(kIA32I32x4DotI8x16I7x16AddS, g.DefineSameAsInput(node, 2),
        g.UseUniqueRegister(node->InputAt(0)),
@@ -3334,15 +3573,16 @@ void InstructionSelector::VisitI32x4DotI8x16I7x16AddS(Node* node) {
        g.UseUniqueRegister(node->InputAt(2)), arraysize(temps), temps);
 }
 
-void InstructionSelector::AddOutputToSelectContinuation(OperandGenerator* g,
-                                                        int first_input_index,
-                                                        Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::AddOutputToSelectContinuation(
+    OperandGenerator* g, int first_input_index, Node* node) {
   UNREACHABLE();
 }
 
 // static
+template <typename Adapter>
 MachineOperatorBuilder::Flags
-InstructionSelector::SupportedMachineOperatorFlags() {
+InstructionSelectorT<Adapter>::SupportedMachineOperatorFlags() {
   MachineOperatorBuilder::Flags flags =
       MachineOperatorBuilder::kWord32ShiftIsSafe |
       MachineOperatorBuilder::kWord32Ctz | MachineOperatorBuilder::kWord32Rol;
@@ -3363,12 +3603,18 @@ InstructionSelector::SupportedMachineOperatorFlags() {
 }
 
 // static
+template <typename Adapter>
 MachineOperatorBuilder::AlignmentRequirements
-InstructionSelector::AlignmentRequirements() {
+InstructionSelectorT<Adapter>::AlignmentRequirements() {
   return MachineOperatorBuilder::AlignmentRequirements::
       FullUnalignedAccessSupport();
 }
 
+template class EXPORT_TEMPLATE_DEFINE(V8_EXPORT_PRIVATE)
+    InstructionSelectorT<TurbofanAdapter>;
+template class EXPORT_TEMPLATE_DEFINE(V8_EXPORT_PRIVATE)
+    InstructionSelectorT<TurboshaftAdapter>;
+
 }  // namespace compiler
 }  // namespace internal
 }  // namespace v8
diff --git a/src/compiler/backend/instruction-selector-impl.h b/src/compiler/backend/instruction-selector-impl.h
index cb8f9a00c30..3741ead2b2f 100644
--- a/src/compiler/backend/instruction-selector-impl.h
+++ b/src/compiler/backend/instruction-selector-impl.h
@@ -68,11 +68,26 @@ class SwitchInfo {
   BasicBlock* default_branch_;
 };
 
+#define OPERAND_GENERATOR_T_BOILERPLATE(adapter)           \
+  using super = OperandGeneratorT<adapter>;                \
+  using RegisterMode = typename super::RegisterMode;       \
+  using RegisterUseKind = typename super::RegisterUseKind; \
+  using super::selector;                                   \
+  using super::DefineAsRegister;                           \
+  using super::TempImmediate;                              \
+  using super::UseFixed;                                   \
+  using super::UseImmediate;                               \
+  using super::UseNegatedImmediate;                        \
+  using super::UseRegister;                                \
+  using super::UseRegisterWithMode;                        \
+  using super::UseUniqueRegister;
+
 // A helper class for the instruction selector that simplifies construction of
 // Operands. This class implements a base for architecture-specific helpers.
-class OperandGenerator {
+template <typename Adapter>
+class OperandGeneratorT {
  public:
-  explicit OperandGenerator(InstructionSelector* selector)
+  explicit OperandGeneratorT(InstructionSelectorT<Adapter>* selector)
       : selector_(selector) {}
 
   InstructionOperand NoOutput() {
@@ -326,7 +341,7 @@ class OperandGenerator {
   }
 
  protected:
-  InstructionSelector* selector() const { return selector_; }
+  InstructionSelectorT<Adapter>* selector() const { return selector_; }
   InstructionSequence* sequence() const { return selector()->sequence(); }
   Zone* zone() const { return selector()->instruction_zone(); }
 
@@ -465,7 +480,7 @@ class OperandGenerator {
                               location.AsRegister(), virtual_register);
   }
 
-  InstructionSelector* selector_;
+  InstructionSelectorT<Adapter>* selector_;
 };
 
 }  // namespace compiler
diff --git a/src/compiler/backend/instruction-selector.cc b/src/compiler/backend/instruction-selector.cc
index 68dd7dd7290..b0b76cbe482 100644
--- a/src/compiler/backend/instruction-selector.cc
+++ b/src/compiler/backend/instruction-selector.cc
@@ -35,7 +35,8 @@ Smi NumberConstantToSmi(Node* node) {
   return smi;
 }
 
-InstructionSelector::InstructionSelector(
+template <typename Adapter>
+InstructionSelectorT<Adapter>::InstructionSelectorT(
     Zone* zone, size_t node_count, Linkage* linkage,
     InstructionSequence* sequence, Schedule* schedule,
     SourcePositionTable* source_positions, Frame* frame,
@@ -92,7 +93,9 @@ InstructionSelector::InstructionSelector(
   }
 }
 
-base::Optional<BailoutReason> InstructionSelector::SelectInstructions() {
+template <typename Adapter>
+base::Optional<BailoutReason>
+InstructionSelectorT<Adapter>::SelectInstructions() {
   // Mark the inputs of all phis in loop headers as used.
   BasicBlockVector* blocks = schedule()->rpo_order();
   for (auto const block : *blocks) {
@@ -117,7 +120,7 @@ base::Optional<BailoutReason> InstructionSelector::SelectInstructions() {
 
   // Schedule the selected instructions.
   if (UseInstructionScheduling()) {
-    scheduler_ = zone()->New<InstructionScheduler>(zone(), sequence());
+    scheduler_ = zone()->template New<InstructionScheduler>(zone(), sequence());
   }
 
   for (auto const block : *blocks) {
@@ -146,7 +149,8 @@ base::Optional<BailoutReason> InstructionSelector::SelectInstructions() {
   return base::nullopt;
 }
 
-void InstructionSelector::StartBlock(RpoNumber rpo) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::StartBlock(RpoNumber rpo) {
   if (UseInstructionScheduling()) {
     DCHECK_NOT_NULL(scheduler_);
     scheduler_->StartBlock(rpo);
@@ -155,7 +159,8 @@ void InstructionSelector::StartBlock(RpoNumber rpo) {
   }
 }
 
-void InstructionSelector::EndBlock(RpoNumber rpo) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::EndBlock(RpoNumber rpo) {
   if (UseInstructionScheduling()) {
     DCHECK_NOT_NULL(scheduler_);
     scheduler_->EndBlock(rpo);
@@ -164,7 +169,8 @@ void InstructionSelector::EndBlock(RpoNumber rpo) {
   }
 }
 
-void InstructionSelector::AddTerminator(Instruction* instr) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::AddTerminator(Instruction* instr) {
   if (UseInstructionScheduling()) {
     DCHECK_NOT_NULL(scheduler_);
     scheduler_->AddTerminator(instr);
@@ -173,7 +179,8 @@ void InstructionSelector::AddTerminator(Instruction* instr) {
   }
 }
 
-void InstructionSelector::AddInstruction(Instruction* instr) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::AddInstruction(Instruction* instr) {
   if (UseInstructionScheduling()) {
     DCHECK_NOT_NULL(scheduler_);
     scheduler_->AddInstruction(instr);
@@ -182,27 +189,29 @@ void InstructionSelector::AddInstruction(Instruction* instr) {
   }
 }
 
-Instruction* InstructionSelector::Emit(InstructionCode opcode,
-                                       InstructionOperand output,
-                                       size_t temp_count,
-                                       InstructionOperand* temps) {
+template <typename Adapter>
+Instruction* InstructionSelectorT<Adapter>::Emit(InstructionCode opcode,
+                                                 InstructionOperand output,
+                                                 size_t temp_count,
+                                                 InstructionOperand* temps) {
   size_t output_count = output.IsInvalid() ? 0 : 1;
   return Emit(opcode, output_count, &output, 0, nullptr, temp_count, temps);
 }
 
-Instruction* InstructionSelector::Emit(InstructionCode opcode,
-                                       InstructionOperand output,
-                                       InstructionOperand a, size_t temp_count,
-                                       InstructionOperand* temps) {
+template <typename Adapter>
+Instruction* InstructionSelectorT<Adapter>::Emit(InstructionCode opcode,
+                                                 InstructionOperand output,
+                                                 InstructionOperand a,
+                                                 size_t temp_count,
+                                                 InstructionOperand* temps) {
   size_t output_count = output.IsInvalid() ? 0 : 1;
   return Emit(opcode, output_count, &output, 1, &a, temp_count, temps);
 }
 
-Instruction* InstructionSelector::Emit(InstructionCode opcode,
-                                       InstructionOperand output,
-                                       InstructionOperand a,
-                                       InstructionOperand b, size_t temp_count,
-                                       InstructionOperand* temps) {
+template <typename Adapter>
+Instruction* InstructionSelectorT<Adapter>::Emit(
+    InstructionCode opcode, InstructionOperand output, InstructionOperand a,
+    InstructionOperand b, size_t temp_count, InstructionOperand* temps) {
   size_t output_count = output.IsInvalid() ? 0 : 1;
   InstructionOperand inputs[] = {a, b};
   size_t input_count = arraysize(inputs);
@@ -210,12 +219,11 @@ Instruction* InstructionSelector::Emit(InstructionCode opcode,
               temps);
 }
 
-Instruction* InstructionSelector::Emit(InstructionCode opcode,
-                                       InstructionOperand output,
-                                       InstructionOperand a,
-                                       InstructionOperand b,
-                                       InstructionOperand c, size_t temp_count,
-                                       InstructionOperand* temps) {
+template <typename Adapter>
+Instruction* InstructionSelectorT<Adapter>::Emit(
+    InstructionCode opcode, InstructionOperand output, InstructionOperand a,
+    InstructionOperand b, InstructionOperand c, size_t temp_count,
+    InstructionOperand* temps) {
   size_t output_count = output.IsInvalid() ? 0 : 1;
   InstructionOperand inputs[] = {a, b, c};
   size_t input_count = arraysize(inputs);
@@ -223,7 +231,8 @@ Instruction* InstructionSelector::Emit(InstructionCode opcode,
               temps);
 }
 
-Instruction* InstructionSelector::Emit(
+template <typename Adapter>
+Instruction* InstructionSelectorT<Adapter>::Emit(
     InstructionCode opcode, InstructionOperand output, InstructionOperand a,
     InstructionOperand b, InstructionOperand c, InstructionOperand d,
     size_t temp_count, InstructionOperand* temps) {
@@ -234,7 +243,8 @@ Instruction* InstructionSelector::Emit(
               temps);
 }
 
-Instruction* InstructionSelector::Emit(
+template <typename Adapter>
+Instruction* InstructionSelectorT<Adapter>::Emit(
     InstructionCode opcode, InstructionOperand output, InstructionOperand a,
     InstructionOperand b, InstructionOperand c, InstructionOperand d,
     InstructionOperand e, size_t temp_count, InstructionOperand* temps) {
@@ -245,7 +255,8 @@ Instruction* InstructionSelector::Emit(
               temps);
 }
 
-Instruction* InstructionSelector::Emit(
+template <typename Adapter>
+Instruction* InstructionSelectorT<Adapter>::Emit(
     InstructionCode opcode, InstructionOperand output, InstructionOperand a,
     InstructionOperand b, InstructionOperand c, InstructionOperand d,
     InstructionOperand e, InstructionOperand f, size_t temp_count,
@@ -257,7 +268,8 @@ Instruction* InstructionSelector::Emit(
               temps);
 }
 
-Instruction* InstructionSelector::Emit(
+template <typename Adapter>
+Instruction* InstructionSelectorT<Adapter>::Emit(
     InstructionCode opcode, size_t output_count, InstructionOperand* outputs,
     size_t input_count, InstructionOperand* inputs, size_t temp_count,
     InstructionOperand* temps) {
@@ -274,12 +286,14 @@ Instruction* InstructionSelector::Emit(
   return Emit(instr);
 }
 
-Instruction* InstructionSelector::Emit(Instruction* instr) {
+template <typename Adapter>
+Instruction* InstructionSelectorT<Adapter>::Emit(Instruction* instr) {
   instructions_.push_back(instr);
   return instr;
 }
 
-bool InstructionSelector::CanCover(Node* user, Node* node) const {
+template <typename Adapter>
+bool InstructionSelectorT<Adapter>::CanCover(Node* user, Node* node) const {
   // 1. Both {user} and {node} must be in the same basic block.
   if (schedule()->block(node) != current_block_) {
     return false;
@@ -301,8 +315,9 @@ bool InstructionSelector::CanCover(Node* user, Node* node) const {
   return true;
 }
 
-bool InstructionSelector::IsOnlyUserOfNodeInSameBlock(Node* user,
-                                                      Node* node) const {
+template <typename Adapter>
+bool InstructionSelectorT<Adapter>::IsOnlyUserOfNodeInSameBlock(
+    Node* user, Node* node) const {
   BasicBlock* bb_user = schedule()->block(user);
   BasicBlock* bb_node = schedule()->block(node);
   if (bb_user != bb_node) return false;
@@ -315,13 +330,15 @@ bool InstructionSelector::IsOnlyUserOfNodeInSameBlock(Node* user,
   return true;
 }
 
-void InstructionSelector::UpdateRenames(Instruction* instruction) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::UpdateRenames(Instruction* instruction) {
   for (size_t i = 0; i < instruction->InputCount(); i++) {
     TryRename(instruction->InputAt(i));
   }
 }
 
-void InstructionSelector::UpdateRenamesInPhi(PhiInstruction* phi) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::UpdateRenamesInPhi(PhiInstruction* phi) {
   for (size_t i = 0; i < phi->operands().size(); i++) {
     int vreg = phi->operands()[i];
     int renamed = GetRename(vreg);
@@ -331,7 +348,8 @@ void InstructionSelector::UpdateRenamesInPhi(PhiInstruction* phi) {
   }
 }
 
-int InstructionSelector::GetRename(int virtual_register) {
+template <typename Adapter>
+int InstructionSelectorT<Adapter>::GetRename(int virtual_register) {
   int rename = virtual_register;
   while (true) {
     if (static_cast<size_t>(rename) >= virtual_register_rename_.size()) break;
@@ -344,7 +362,8 @@ int InstructionSelector::GetRename(int virtual_register) {
   return rename;
 }
 
-void InstructionSelector::TryRename(InstructionOperand* op) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::TryRename(InstructionOperand* op) {
   if (!op->IsUnallocated()) return;
   UnallocatedOperand* unalloc = UnallocatedOperand::cast(op);
   int vreg = unalloc->virtual_register();
@@ -354,7 +373,9 @@ void InstructionSelector::TryRename(InstructionOperand* op) {
   }
 }
 
-void InstructionSelector::SetRename(const Node* node, const Node* rename) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::SetRename(const Node* node,
+                                              const Node* rename) {
   int vreg = GetVirtualRegister(node);
   if (static_cast<size_t>(vreg) >= virtual_register_rename_.size()) {
     int invalid = InstructionOperand::kInvalidVirtualRegister;
@@ -363,7 +384,8 @@ void InstructionSelector::SetRename(const Node* node, const Node* rename) {
   virtual_register_rename_[vreg] = GetVirtualRegister(rename);
 }
 
-int InstructionSelector::GetVirtualRegister(const Node* node) {
+template <typename Adapter>
+int InstructionSelectorT<Adapter>::GetVirtualRegister(const Node* node) {
   DCHECK_NOT_NULL(node);
   size_t const id = node->id();
   DCHECK_LT(id, virtual_registers_.size());
@@ -375,8 +397,9 @@ int InstructionSelector::GetVirtualRegister(const Node* node) {
   return virtual_register;
 }
 
-const std::map<NodeId, int> InstructionSelector::GetVirtualRegistersForTesting()
-    const {
+template <typename Adapter>
+const std::map<NodeId, int>
+InstructionSelectorT<Adapter>::GetVirtualRegistersForTesting() const {
   std::map<NodeId, int> virtual_registers;
   for (size_t n = 0; n < virtual_registers_.size(); ++n) {
     if (virtual_registers_[n] != InstructionOperand::kInvalidVirtualRegister) {
@@ -387,17 +410,20 @@ const std::map<NodeId, int> InstructionSelector::GetVirtualRegistersForTesting()
   return virtual_registers;
 }
 
-bool InstructionSelector::IsDefined(Node* node) const {
+template <typename Adapter>
+bool InstructionSelectorT<Adapter>::IsDefined(Node* node) const {
   DCHECK_NOT_NULL(node);
   return defined_.Contains(node->id());
 }
 
-void InstructionSelector::MarkAsDefined(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::MarkAsDefined(Node* node) {
   DCHECK_NOT_NULL(node);
   defined_.Add(node->id());
 }
 
-bool InstructionSelector::IsUsed(Node* node) const {
+template <typename Adapter>
+bool InstructionSelectorT<Adapter>::IsUsed(Node* node) const {
   DCHECK_NOT_NULL(node);
   // TODO(bmeurer): This is a terrible monster hack, but we have to make sure
   // that the Retain is actually emitted, otherwise the GC will mess up.
@@ -406,34 +432,40 @@ bool InstructionSelector::IsUsed(Node* node) const {
   return used_.Contains(node->id());
 }
 
-void InstructionSelector::MarkAsUsed(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::MarkAsUsed(Node* node) {
   DCHECK_NOT_NULL(node);
   used_.Add(node->id());
 }
 
-int InstructionSelector::GetEffectLevel(Node* node) const {
+template <typename Adapter>
+int InstructionSelectorT<Adapter>::GetEffectLevel(Node* node) const {
   DCHECK_NOT_NULL(node);
   size_t const id = node->id();
   DCHECK_LT(id, effect_level_.size());
   return effect_level_[id];
 }
 
-int InstructionSelector::GetEffectLevel(Node* node,
-                                        FlagsContinuation* cont) const {
+template <typename Adapter>
+int InstructionSelectorT<Adapter>::GetEffectLevel(
+    Node* node, FlagsContinuation* cont) const {
   return cont->IsBranch()
              ? GetEffectLevel(
                    cont->true_block()->PredecessorAt(0)->control_input())
              : GetEffectLevel(node);
 }
 
-void InstructionSelector::SetEffectLevel(Node* node, int effect_level) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::SetEffectLevel(Node* node,
+                                                   int effect_level) {
   DCHECK_NOT_NULL(node);
   size_t const id = node->id();
   DCHECK_LT(id, effect_level_.size());
   effect_level_[id] = effect_level;
 }
 
-bool InstructionSelector::CanAddressRelativeToRootsRegister(
+template <typename Adapter>
+bool InstructionSelectorT<Adapter>::CanAddressRelativeToRootsRegister(
     const ExternalReference& reference) const {
   // There are three things to consider here:
   // 1. CanUseRootsRegister: Is kRootRegister initialized?
@@ -456,26 +488,31 @@ bool InstructionSelector::CanAddressRelativeToRootsRegister(
   return this_root_relative_offset_is_constant;
 }
 
-bool InstructionSelector::CanUseRootsRegister() const {
+template <typename Adapter>
+bool InstructionSelectorT<Adapter>::CanUseRootsRegister() const {
   return linkage()->GetIncomingDescriptor()->flags() &
          CallDescriptor::kCanUseRoots;
 }
 
-void InstructionSelector::MarkAsRepresentation(MachineRepresentation rep,
-                                               const InstructionOperand& op) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::MarkAsRepresentation(
+    MachineRepresentation rep, const InstructionOperand& op) {
   UnallocatedOperand unalloc = UnallocatedOperand::cast(op);
   sequence()->MarkAsRepresentation(rep, unalloc.virtual_register());
 }
 
-void InstructionSelector::MarkAsRepresentation(MachineRepresentation rep,
-                                               Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::MarkAsRepresentation(
+    MachineRepresentation rep, Node* node) {
   sequence()->MarkAsRepresentation(rep, GetVirtualRegister(node));
 }
 
 namespace {
 
-InstructionOperand OperandForDeopt(Isolate* isolate, OperandGenerator* g,
-                                   Node* input, FrameStateInputKind kind,
+template <typename Adapter>
+InstructionOperand OperandForDeopt(Isolate* isolate,
+                                   OperandGeneratorT<Adapter>* g, Node* input,
+                                   FrameStateInputKind kind,
                                    MachineRepresentation rep) {
   if (rep == MachineRepresentation::kNone) {
     return g->TempImmediate(FrameStateDescriptor::kImpossibleValue);
@@ -583,7 +620,8 @@ class StateObjectDeduplicator {
 };
 
 // Returns the number of instruction operands added to inputs.
-size_t InstructionSelector::AddOperandToStateValueDescriptor(
+template <typename Adapter>
+size_t InstructionSelectorT<Adapter>::AddOperandToStateValueDescriptor(
     StateValueList* values, InstructionOperandVector* inputs,
     OperandGenerator* g, StateObjectDeduplicator* deduplicator, Node* input,
     MachineType type, FrameStateInputKind kind, Zone* zone) {
@@ -644,7 +682,8 @@ size_t InstructionSelector::AddOperandToStateValueDescriptor(
   }
 }
 
-struct InstructionSelector::CachedStateValues : public ZoneObject {
+template <typename Adapter>
+struct InstructionSelectorT<Adapter>::CachedStateValues : public ZoneObject {
  public:
   CachedStateValues(Zone* zone, StateValueList* values, size_t values_start,
                     InstructionOperandVector* inputs, size_t inputs_start)
@@ -662,7 +701,8 @@ struct InstructionSelector::CachedStateValues : public ZoneObject {
   StateValueList::Slice values_;
 };
 
-class InstructionSelector::CachedStateValuesBuilder {
+template <typename Adapter>
+class InstructionSelectorT<Adapter>::CachedStateValuesBuilder {
  public:
   explicit CachedStateValuesBuilder(StateValueList* values,
                                     InstructionOperandVector* inputs,
@@ -679,10 +719,10 @@ class InstructionSelector::CachedStateValuesBuilder {
   // any of the ids in the deduplicator.
   bool CanCache() const { return deduplicator_->size() == deduplicator_start_; }
 
-  InstructionSelector::CachedStateValues* Build(Zone* zone) {
+  InstructionSelectorT<Adapter>::CachedStateValues* Build(Zone* zone) {
     DCHECK(CanCache());
     DCHECK(values_->nested_count() == nested_start_);
-    return zone->New<InstructionSelector::CachedStateValues>(
+    return zone->New<InstructionSelectorT<Adapter>::CachedStateValues>(
         zone, values_, values_start_, inputs_, inputs_start_);
   }
 
@@ -696,7 +736,8 @@ class InstructionSelector::CachedStateValuesBuilder {
   size_t deduplicator_start_;
 };
 
-size_t InstructionSelector::AddInputsToFrameStateDescriptor(
+template <typename Adapter>
+size_t InstructionSelectorT<Adapter>::AddInputsToFrameStateDescriptor(
     StateValueList* values, InstructionOperandVector* inputs,
     OperandGenerator* g, StateObjectDeduplicator* deduplicator, Node* node,
     FrameStateInputKind kind, Zone* zone) {
@@ -735,7 +776,8 @@ size_t InstructionSelector::AddInputsToFrameStateDescriptor(
 }
 
 // Returns the number of instruction operands added to inputs.
-size_t InstructionSelector::AddInputsToFrameStateDescriptor(
+template <typename Adapter>
+size_t InstructionSelectorT<Adapter>::AddInputsToFrameStateDescriptor(
     FrameStateDescriptor* descriptor, FrameState state, OperandGenerator* g,
     StateObjectDeduplicator* deduplicator, InstructionOperandVector* inputs,
     FrameStateInputKind kind, Zone* zone) {
@@ -788,12 +830,14 @@ size_t InstructionSelector::AddInputsToFrameStateDescriptor(
   return entries;
 }
 
-Instruction* InstructionSelector::EmitWithContinuation(
+template <typename Adapter>
+Instruction* InstructionSelectorT<Adapter>::EmitWithContinuation(
     InstructionCode opcode, InstructionOperand a, FlagsContinuation* cont) {
   return EmitWithContinuation(opcode, 0, nullptr, 1, &a, cont);
 }
 
-Instruction* InstructionSelector::EmitWithContinuation(
+template <typename Adapter>
+Instruction* InstructionSelectorT<Adapter>::EmitWithContinuation(
     InstructionCode opcode, InstructionOperand a, InstructionOperand b,
     FlagsContinuation* cont) {
   InstructionOperand inputs[] = {a, b};
@@ -801,7 +845,8 @@ Instruction* InstructionSelector::EmitWithContinuation(
                               cont);
 }
 
-Instruction* InstructionSelector::EmitWithContinuation(
+template <typename Adapter>
+Instruction* InstructionSelectorT<Adapter>::EmitWithContinuation(
     InstructionCode opcode, InstructionOperand a, InstructionOperand b,
     InstructionOperand c, FlagsContinuation* cont) {
   InstructionOperand inputs[] = {a, b, c};
@@ -809,14 +854,16 @@ Instruction* InstructionSelector::EmitWithContinuation(
                               cont);
 }
 
-Instruction* InstructionSelector::EmitWithContinuation(
+template <typename Adapter>
+Instruction* InstructionSelectorT<Adapter>::EmitWithContinuation(
     InstructionCode opcode, size_t output_count, InstructionOperand* outputs,
     size_t input_count, InstructionOperand* inputs, FlagsContinuation* cont) {
   return EmitWithContinuation(opcode, output_count, outputs, input_count,
                               inputs, 0, nullptr, cont);
 }
 
-Instruction* InstructionSelector::EmitWithContinuation(
+template <typename Adapter>
+Instruction* InstructionSelectorT<Adapter>::EmitWithContinuation(
     InstructionCode opcode, size_t output_count, InstructionOperand* outputs,
     size_t input_count, InstructionOperand* inputs, size_t temp_count,
     InstructionOperand* temps, FlagsContinuation* cont) {
@@ -878,7 +925,8 @@ Instruction* InstructionSelector::EmitWithContinuation(
               emit_inputs, emit_temps_size, emit_temps);
 }
 
-void InstructionSelector::AppendDeoptimizeArguments(
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::AppendDeoptimizeArguments(
     InstructionOperandVector* args, DeoptimizeReason reason, NodeId node_id,
     FeedbackSource const& feedback, FrameState frame_state,
     DeoptimizeKind kind) {
@@ -932,9 +980,10 @@ struct CallBuffer {
 
 // TODO(bmeurer): Get rid of the CallBuffer business and make
 // InstructionSelector::VisitCall platform independent instead.
-void InstructionSelector::InitializeCallBuffer(Node* call, CallBuffer* buffer,
-                                               CallBufferFlags flags,
-                                               int stack_param_delta) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::InitializeCallBuffer(
+    Node* call, CallBuffer* buffer, CallBufferFlags flags,
+    int stack_param_delta) {
   OperandGenerator g(this);
   size_t ret_count = buffer->descriptor->ReturnCount();
   bool is_tail_call = (flags & kCallTail) != 0;
@@ -1160,7 +1209,8 @@ void InstructionSelector::InitializeCallBuffer(Node* call, CallBuffer* buffer,
   }
 }
 
-bool InstructionSelector::IsSourcePositionUsed(Node* node) {
+template <typename Adapter>
+bool InstructionSelectorT<Adapter>::IsSourcePositionUsed(Node* node) {
   return (source_position_mode_ == kAllSourcePositions ||
           node->opcode() == IrOpcode::kCall ||
           node->opcode() == IrOpcode::kTrapIf ||
@@ -1171,7 +1221,8 @@ bool InstructionSelector::IsSourcePositionUsed(Node* node) {
           node->opcode() == IrOpcode::kStoreTrapOnNull);
 }
 
-void InstructionSelector::VisitBlock(BasicBlock* block) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitBlock(BasicBlock* block) {
   DCHECK(!current_block_);
   current_block_ = block;
   auto current_num_instructions = [&] {
@@ -1255,7 +1306,8 @@ void InstructionSelector::VisitBlock(BasicBlock* block) {
   current_block_ = nullptr;
 }
 
-void InstructionSelector::VisitControl(BasicBlock* block) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitControl(BasicBlock* block) {
 #ifdef DEBUG
   // SSA deconstruction requires targets of branches not to have phis.
   // Edge split form guarantees this property, but is more strict.
@@ -1359,7 +1411,8 @@ void InstructionSelector::VisitControl(BasicBlock* block) {
   }
 }
 
-void InstructionSelector::MarkPairProjectionsAsWord32(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::MarkPairProjectionsAsWord32(Node* node) {
   Node* projection0 = NodeProperties::FindProjection(node, 0);
   if (projection0) {
     MarkAsWord32(projection0);
@@ -1370,7 +1423,8 @@ void InstructionSelector::MarkPairProjectionsAsWord32(Node* node) {
   }
 }
 
-void InstructionSelector::VisitNode(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitNode(Node* node) {
   tick_counter_->TickAndMaybeEnterSafepoint();
   DCHECK_NOT_NULL(schedule()->block(node));  // should only use scheduled nodes.
   switch (node->opcode()) {
@@ -2661,122 +2715,149 @@ void InstructionSelector::VisitNode(Node* node) {
   }
 }
 
-void InstructionSelector::VisitStackPointerGreaterThan(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitStackPointerGreaterThan(Node* node) {
   FlagsContinuation cont =
       FlagsContinuation::ForSet(kStackPointerGreaterThanCondition, node);
   VisitStackPointerGreaterThan(node, &cont);
 }
 
-void InstructionSelector::VisitLoadStackCheckOffset(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitLoadStackCheckOffset(Node* node) {
   OperandGenerator g(this);
   Emit(kArchStackCheckOffset, g.DefineAsRegister(node));
 }
 
-void InstructionSelector::VisitLoadFramePointer(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitLoadFramePointer(Node* node) {
   OperandGenerator g(this);
   Emit(kArchFramePointer, g.DefineAsRegister(node));
 }
 
-void InstructionSelector::VisitLoadParentFramePointer(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitLoadParentFramePointer(Node* node) {
   OperandGenerator g(this);
   Emit(kArchParentFramePointer, g.DefineAsRegister(node));
 }
 
-void InstructionSelector::VisitLoadRootRegister(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitLoadRootRegister(Node* node) {
   // Do nothing. Following loads/stores from this operator will use kMode_Root
   // to load/store from an offset of the root register.
 }
 
-void InstructionSelector::VisitFloat64Acos(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64Acos(Node* node) {
   VisitFloat64Ieee754Unop(node, kIeee754Float64Acos);
 }
 
-void InstructionSelector::VisitFloat64Acosh(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64Acosh(Node* node) {
   VisitFloat64Ieee754Unop(node, kIeee754Float64Acosh);
 }
 
-void InstructionSelector::VisitFloat64Asin(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64Asin(Node* node) {
   VisitFloat64Ieee754Unop(node, kIeee754Float64Asin);
 }
 
-void InstructionSelector::VisitFloat64Asinh(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64Asinh(Node* node) {
   VisitFloat64Ieee754Unop(node, kIeee754Float64Asinh);
 }
 
-void InstructionSelector::VisitFloat64Atan(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64Atan(Node* node) {
   VisitFloat64Ieee754Unop(node, kIeee754Float64Atan);
 }
 
-void InstructionSelector::VisitFloat64Atanh(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64Atanh(Node* node) {
   VisitFloat64Ieee754Unop(node, kIeee754Float64Atanh);
 }
 
-void InstructionSelector::VisitFloat64Atan2(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64Atan2(Node* node) {
   VisitFloat64Ieee754Binop(node, kIeee754Float64Atan2);
 }
 
-void InstructionSelector::VisitFloat64Cbrt(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64Cbrt(Node* node) {
   VisitFloat64Ieee754Unop(node, kIeee754Float64Cbrt);
 }
 
-void InstructionSelector::VisitFloat64Cos(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64Cos(Node* node) {
   VisitFloat64Ieee754Unop(node, kIeee754Float64Cos);
 }
 
-void InstructionSelector::VisitFloat64Cosh(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64Cosh(Node* node) {
   VisitFloat64Ieee754Unop(node, kIeee754Float64Cosh);
 }
 
-void InstructionSelector::VisitFloat64Exp(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64Exp(Node* node) {
   VisitFloat64Ieee754Unop(node, kIeee754Float64Exp);
 }
 
-void InstructionSelector::VisitFloat64Expm1(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64Expm1(Node* node) {
   VisitFloat64Ieee754Unop(node, kIeee754Float64Expm1);
 }
 
-void InstructionSelector::VisitFloat64Log(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64Log(Node* node) {
   VisitFloat64Ieee754Unop(node, kIeee754Float64Log);
 }
 
-void InstructionSelector::VisitFloat64Log1p(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64Log1p(Node* node) {
   VisitFloat64Ieee754Unop(node, kIeee754Float64Log1p);
 }
 
-void InstructionSelector::VisitFloat64Log2(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64Log2(Node* node) {
   VisitFloat64Ieee754Unop(node, kIeee754Float64Log2);
 }
 
-void InstructionSelector::VisitFloat64Log10(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64Log10(Node* node) {
   VisitFloat64Ieee754Unop(node, kIeee754Float64Log10);
 }
 
-void InstructionSelector::VisitFloat64Pow(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64Pow(Node* node) {
   VisitFloat64Ieee754Binop(node, kIeee754Float64Pow);
 }
 
-void InstructionSelector::VisitFloat64Sin(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64Sin(Node* node) {
   VisitFloat64Ieee754Unop(node, kIeee754Float64Sin);
 }
 
-void InstructionSelector::VisitFloat64Sinh(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64Sinh(Node* node) {
   VisitFloat64Ieee754Unop(node, kIeee754Float64Sinh);
 }
 
-void InstructionSelector::VisitFloat64Tan(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64Tan(Node* node) {
   VisitFloat64Ieee754Unop(node, kIeee754Float64Tan);
 }
 
-void InstructionSelector::VisitFloat64Tanh(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64Tanh(Node* node) {
   VisitFloat64Ieee754Unop(node, kIeee754Float64Tanh);
 }
 
-void InstructionSelector::EmitTableSwitch(
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::EmitTableSwitch(
     const SwitchInfo& sw, InstructionOperand const& index_operand) {
   OperandGenerator g(this);
   size_t input_count = 2 + sw.value_range();
   DCHECK_LE(sw.value_range(), std::numeric_limits<size_t>::max() - 2);
-  auto* inputs = zone()->NewArray<InstructionOperand>(input_count);
+  auto* inputs = zone()->template NewArray<InstructionOperand>(input_count);
   inputs[0] = index_operand;
   InstructionOperand default_operand = g.Label(sw.default_branch());
   std::fill(&inputs[1], &inputs[input_count], default_operand);
@@ -2789,12 +2870,13 @@ void InstructionSelector::EmitTableSwitch(
   Emit(kArchTableSwitch, 0, nullptr, input_count, inputs, 0, nullptr);
 }
 
-void InstructionSelector::EmitBinarySearchSwitch(
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::EmitBinarySearchSwitch(
     const SwitchInfo& sw, InstructionOperand const& value_operand) {
   OperandGenerator g(this);
   size_t input_count = 2 + sw.case_count() * 2;
   DCHECK_LE(sw.case_count(), (std::numeric_limits<size_t>::max() - 2) / 2);
-  auto* inputs = zone()->NewArray<InstructionOperand>(input_count);
+  auto* inputs = zone()->template NewArray<InstructionOperand>(input_count);
   inputs[0] = value_operand;
   inputs[1] = g.Label(sw.default_branch());
   std::vector<CaseInfo> cases = sw.CasesSortedByValue();
@@ -2806,11 +2888,13 @@ void InstructionSelector::EmitBinarySearchSwitch(
   Emit(kArchBinarySearchSwitch, 0, nullptr, input_count, inputs, 0, nullptr);
 }
 
-void InstructionSelector::VisitBitcastTaggedToWord(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitBitcastTaggedToWord(Node* node) {
   EmitIdentity(node);
 }
 
-void InstructionSelector::VisitBitcastWordToTagged(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitBitcastWordToTagged(Node* node) {
   OperandGenerator g(this);
   Emit(kArchNop, g.DefineSameAsFirst(node), g.Use(node->InputAt(0)));
 }
@@ -2818,218 +2902,347 @@ void InstructionSelector::VisitBitcastWordToTagged(Node* node) {
 // 32 bit targets do not implement the following instructions.
 #if V8_TARGET_ARCH_32_BIT
 
-void InstructionSelector::VisitWord64And(Node* node) { UNIMPLEMENTED(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64And(Node* node) {
+  UNIMPLEMENTED();
+}
 
-void InstructionSelector::VisitWord64Or(Node* node) { UNIMPLEMENTED(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64Or(Node* node) {
+  UNIMPLEMENTED();
+}
 
-void InstructionSelector::VisitWord64Xor(Node* node) { UNIMPLEMENTED(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64Xor(Node* node) {
+  UNIMPLEMENTED();
+}
 
-void InstructionSelector::VisitWord64Shl(Node* node) { UNIMPLEMENTED(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64Shl(Node* node) {
+  UNIMPLEMENTED();
+}
 
-void InstructionSelector::VisitWord64Shr(Node* node) { UNIMPLEMENTED(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64Shr(Node* node) {
+  UNIMPLEMENTED();
+}
 
-void InstructionSelector::VisitWord64Sar(Node* node) { UNIMPLEMENTED(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64Sar(Node* node) {
+  UNIMPLEMENTED();
+}
 
-void InstructionSelector::VisitWord64Rol(Node* node) { UNIMPLEMENTED(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64Rol(Node* node) {
+  UNIMPLEMENTED();
+}
 
-void InstructionSelector::VisitWord64Ror(Node* node) { UNIMPLEMENTED(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64Ror(Node* node) {
+  UNIMPLEMENTED();
+}
 
-void InstructionSelector::VisitWord64Clz(Node* node) { UNIMPLEMENTED(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64Clz(Node* node) {
+  UNIMPLEMENTED();
+}
 
-void InstructionSelector::VisitWord64Ctz(Node* node) { UNIMPLEMENTED(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64Ctz(Node* node) {
+  UNIMPLEMENTED();
+}
 
-void InstructionSelector::VisitWord64ReverseBits(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64ReverseBits(Node* node) {
   UNIMPLEMENTED();
 }
 
-void InstructionSelector::VisitWord64Popcnt(Node* node) { UNIMPLEMENTED(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64Popcnt(Node* node) {
+  UNIMPLEMENTED();
+}
 
-void InstructionSelector::VisitWord64Equal(Node* node) { UNIMPLEMENTED(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64Equal(Node* node) {
+  UNIMPLEMENTED();
+}
 
-void InstructionSelector::VisitInt64Add(Node* node) { UNIMPLEMENTED(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt64Add(Node* node) {
+  UNIMPLEMENTED();
+}
 
-void InstructionSelector::VisitInt64AddWithOverflow(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt64AddWithOverflow(Node* node) {
   UNIMPLEMENTED();
 }
 
-void InstructionSelector::VisitInt64Sub(Node* node) { UNIMPLEMENTED(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt64Sub(Node* node) {
+  UNIMPLEMENTED();
+}
 
-void InstructionSelector::VisitInt64SubWithOverflow(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt64SubWithOverflow(Node* node) {
   UNIMPLEMENTED();
 }
 
-void InstructionSelector::VisitInt64Mul(Node* node) { UNIMPLEMENTED(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt64Mul(Node* node) {
+  UNIMPLEMENTED();
+}
 
-void InstructionSelector::VisitInt64MulHigh(Node* node) { UNIMPLEMENTED(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt64MulHigh(Node* node) {
+  UNIMPLEMENTED();
+}
 
-void InstructionSelector::VisitUint64MulHigh(Node* node) { UNIMPLEMENTED(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitUint64MulHigh(Node* node) {
+  UNIMPLEMENTED();
+}
 
-void InstructionSelector::VisitInt64MulWithOverflow(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt64MulWithOverflow(Node* node) {
   UNIMPLEMENTED();
 }
 
-void InstructionSelector::VisitInt64Div(Node* node) { UNIMPLEMENTED(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt64Div(Node* node) {
+  UNIMPLEMENTED();
+}
 
-void InstructionSelector::VisitInt64LessThan(Node* node) { UNIMPLEMENTED(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt64LessThan(Node* node) {
+  UNIMPLEMENTED();
+}
 
-void InstructionSelector::VisitInt64LessThanOrEqual(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt64LessThanOrEqual(Node* node) {
   UNIMPLEMENTED();
 }
 
-void InstructionSelector::VisitUint64Div(Node* node) { UNIMPLEMENTED(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitUint64Div(Node* node) {
+  UNIMPLEMENTED();
+}
 
-void InstructionSelector::VisitInt64Mod(Node* node) { UNIMPLEMENTED(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt64Mod(Node* node) {
+  UNIMPLEMENTED();
+}
 
-void InstructionSelector::VisitUint64LessThan(Node* node) { UNIMPLEMENTED(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitUint64LessThan(Node* node) {
+  UNIMPLEMENTED();
+}
 
-void InstructionSelector::VisitUint64LessThanOrEqual(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitUint64LessThanOrEqual(Node* node) {
   UNIMPLEMENTED();
 }
 
-void InstructionSelector::VisitUint64Mod(Node* node) { UNIMPLEMENTED(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitUint64Mod(Node* node) {
+  UNIMPLEMENTED();
+}
 
-void InstructionSelector::VisitBitcastWord32ToWord64(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitBitcastWord32ToWord64(Node* node) {
   UNIMPLEMENTED();
 }
 
-void InstructionSelector::VisitChangeInt32ToInt64(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitChangeInt32ToInt64(Node* node) {
   UNIMPLEMENTED();
 }
 
-void InstructionSelector::VisitChangeInt64ToFloat64(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitChangeInt64ToFloat64(Node* node) {
   UNIMPLEMENTED();
 }
 
-void InstructionSelector::VisitChangeUint32ToUint64(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitChangeUint32ToUint64(Node* node) {
   UNIMPLEMENTED();
 }
 
-void InstructionSelector::VisitChangeFloat64ToInt64(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitChangeFloat64ToInt64(Node* node) {
   UNIMPLEMENTED();
 }
 
-void InstructionSelector::VisitChangeFloat64ToUint64(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitChangeFloat64ToUint64(Node* node) {
   UNIMPLEMENTED();
 }
 
-void InstructionSelector::VisitTruncateFloat64ToInt64(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitTruncateFloat64ToInt64(Node* node) {
   UNIMPLEMENTED();
 }
 
-void InstructionSelector::VisitTryTruncateFloat32ToInt64(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitTryTruncateFloat32ToInt64(Node* node) {
   UNIMPLEMENTED();
 }
 
-void InstructionSelector::VisitTryTruncateFloat64ToInt64(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitTryTruncateFloat64ToInt64(Node* node) {
   UNIMPLEMENTED();
 }
 
-void InstructionSelector::VisitTryTruncateFloat32ToUint64(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitTryTruncateFloat32ToUint64(
+    Node* node) {
   UNIMPLEMENTED();
 }
 
-void InstructionSelector::VisitTryTruncateFloat64ToUint64(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitTryTruncateFloat64ToUint64(
+    Node* node) {
   UNIMPLEMENTED();
 }
 
-void InstructionSelector::VisitTryTruncateFloat64ToInt32(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitTryTruncateFloat64ToInt32(Node* node) {
   UNIMPLEMENTED();
 }
 
-void InstructionSelector::VisitTryTruncateFloat64ToUint32(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitTryTruncateFloat64ToUint32(
+    Node* node) {
   UNIMPLEMENTED();
 }
 
-void InstructionSelector::VisitTruncateInt64ToInt32(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitTruncateInt64ToInt32(Node* node) {
   UNIMPLEMENTED();
 }
 
-void InstructionSelector::VisitRoundInt64ToFloat32(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitRoundInt64ToFloat32(Node* node) {
   UNIMPLEMENTED();
 }
 
-void InstructionSelector::VisitRoundInt64ToFloat64(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitRoundInt64ToFloat64(Node* node) {
   UNIMPLEMENTED();
 }
 
-void InstructionSelector::VisitRoundUint64ToFloat32(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitRoundUint64ToFloat32(Node* node) {
   UNIMPLEMENTED();
 }
 
-void InstructionSelector::VisitRoundUint64ToFloat64(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitRoundUint64ToFloat64(Node* node) {
   UNIMPLEMENTED();
 }
 
-void InstructionSelector::VisitBitcastFloat64ToInt64(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitBitcastFloat64ToInt64(Node* node) {
   UNIMPLEMENTED();
 }
 
-void InstructionSelector::VisitBitcastInt64ToFloat64(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitBitcastInt64ToFloat64(Node* node) {
   UNIMPLEMENTED();
 }
 
-void InstructionSelector::VisitSignExtendWord8ToInt64(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitSignExtendWord8ToInt64(Node* node) {
   UNIMPLEMENTED();
 }
 
-void InstructionSelector::VisitSignExtendWord16ToInt64(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitSignExtendWord16ToInt64(Node* node) {
   UNIMPLEMENTED();
 }
 
-void InstructionSelector::VisitSignExtendWord32ToInt64(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitSignExtendWord32ToInt64(Node* node) {
   UNIMPLEMENTED();
 }
 #endif  // V8_TARGET_ARCH_32_BIT
 
 // 64 bit targets do not implement the following instructions.
 #if V8_TARGET_ARCH_64_BIT
-void InstructionSelector::VisitInt32PairAdd(Node* node) { UNIMPLEMENTED(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32PairAdd(Node* node) {
+  UNIMPLEMENTED();
+}
 
-void InstructionSelector::VisitInt32PairSub(Node* node) { UNIMPLEMENTED(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32PairSub(Node* node) {
+  UNIMPLEMENTED();
+}
 
-void InstructionSelector::VisitInt32PairMul(Node* node) { UNIMPLEMENTED(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32PairMul(Node* node) {
+  UNIMPLEMENTED();
+}
 
-void InstructionSelector::VisitWord32PairShl(Node* node) { UNIMPLEMENTED(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32PairShl(Node* node) {
+  UNIMPLEMENTED();
+}
 
-void InstructionSelector::VisitWord32PairShr(Node* node) { UNIMPLEMENTED(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32PairShr(Node* node) {
+  UNIMPLEMENTED();
+}
 
-void InstructionSelector::VisitWord32PairSar(Node* node) { UNIMPLEMENTED(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32PairSar(Node* node) {
+  UNIMPLEMENTED();
+}
 #endif  // V8_TARGET_ARCH_64_BIT
 
 #if !V8_TARGET_ARCH_IA32 && !V8_TARGET_ARCH_ARM && !V8_TARGET_ARCH_RISCV32
-void InstructionSelector::VisitWord32AtomicPairLoad(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32AtomicPairLoad(Node* node) {
   UNIMPLEMENTED();
 }
 
-void InstructionSelector::VisitWord32AtomicPairStore(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32AtomicPairStore(Node* node) {
   UNIMPLEMENTED();
 }
 
-void InstructionSelector::VisitWord32AtomicPairAdd(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32AtomicPairAdd(Node* node) {
   UNIMPLEMENTED();
 }
 
-void InstructionSelector::VisitWord32AtomicPairSub(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32AtomicPairSub(Node* node) {
   UNIMPLEMENTED();
 }
 
-void InstructionSelector::VisitWord32AtomicPairAnd(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32AtomicPairAnd(Node* node) {
   UNIMPLEMENTED();
 }
 
-void InstructionSelector::VisitWord32AtomicPairOr(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32AtomicPairOr(Node* node) {
   UNIMPLEMENTED();
 }
 
-void InstructionSelector::VisitWord32AtomicPairXor(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32AtomicPairXor(Node* node) {
   UNIMPLEMENTED();
 }
 
-void InstructionSelector::VisitWord32AtomicPairExchange(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32AtomicPairExchange(Node* node) {
   UNIMPLEMENTED();
 }
 
-void InstructionSelector::VisitWord32AtomicPairCompareExchange(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32AtomicPairCompareExchange(
+    Node* node) {
   UNIMPLEMENTED();
 }
 #endif  // !V8_TARGET_ARCH_IA32 && !V8_TARGET_ARCH_ARM
@@ -3038,27 +3251,49 @@ void InstructionSelector::VisitWord32AtomicPairCompareExchange(Node* node) {
 #if !V8_TARGET_ARCH_X64 && !V8_TARGET_ARCH_ARM64 && !V8_TARGET_ARCH_MIPS64 && \
     !V8_TARGET_ARCH_S390 && !V8_TARGET_ARCH_PPC64 &&                          \
     !V8_TARGET_ARCH_RISCV64 && !V8_TARGET_ARCH_LOONG64
-void InstructionSelector::VisitWord64AtomicLoad(Node* node) { UNIMPLEMENTED(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64AtomicLoad(Node* node) {
+  UNIMPLEMENTED();
+}
 
-void InstructionSelector::VisitWord64AtomicStore(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64AtomicStore(Node* node) {
   UNIMPLEMENTED();
 }
 
-void InstructionSelector::VisitWord64AtomicAdd(Node* node) { UNIMPLEMENTED(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64AtomicAdd(Node* node) {
+  UNIMPLEMENTED();
+}
 
-void InstructionSelector::VisitWord64AtomicSub(Node* node) { UNIMPLEMENTED(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64AtomicSub(Node* node) {
+  UNIMPLEMENTED();
+}
 
-void InstructionSelector::VisitWord64AtomicAnd(Node* node) { UNIMPLEMENTED(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64AtomicAnd(Node* node) {
+  UNIMPLEMENTED();
+}
 
-void InstructionSelector::VisitWord64AtomicOr(Node* node) { UNIMPLEMENTED(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64AtomicOr(Node* node) {
+  UNIMPLEMENTED();
+}
 
-void InstructionSelector::VisitWord64AtomicXor(Node* node) { UNIMPLEMENTED(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64AtomicXor(Node* node) {
+  UNIMPLEMENTED();
+}
 
-void InstructionSelector::VisitWord64AtomicExchange(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64AtomicExchange(Node* node) {
   UNIMPLEMENTED();
 }
 
-void InstructionSelector::VisitWord64AtomicCompareExchange(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64AtomicCompareExchange(
+    Node* node) {
   UNIMPLEMENTED();
 }
 #endif  // !V8_TARGET_ARCH_X64 && !V8_TARGET_ARCH_ARM64 && !V8_TARGET_ARCH_PPC64
@@ -3067,10 +3302,12 @@ void InstructionSelector::VisitWord64AtomicCompareExchange(Node* node) {
 
 #if !V8_TARGET_ARCH_IA32 && !V8_TARGET_ARCH_ARM && !V8_TARGET_ARCH_RISCV32
 // This is only needed on 32-bit to split the 64-bit value into two operands.
-void InstructionSelector::VisitI64x2SplatI32Pair(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI64x2SplatI32Pair(Node* node) {
   UNIMPLEMENTED();
 }
-void InstructionSelector::VisitI64x2ReplaceLaneI32Pair(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI64x2ReplaceLaneI32Pair(Node* node) {
   UNIMPLEMENTED();
 }
 #endif  // !V8_TARGET_ARCH_IA32 && !V8_TARGET_ARCH_ARM &&
@@ -3080,17 +3317,30 @@ void InstructionSelector::VisitI64x2ReplaceLaneI32Pair(Node* node) {
 #if !V8_TARGET_ARCH_ARM64
 #if !V8_TARGET_ARCH_MIPS64 && !V8_TARGET_ARCH_LOONG64 && \
     !V8_TARGET_ARCH_RISCV32 && !V8_TARGET_ARCH_RISCV64
-void InstructionSelector::VisitI64x2Splat(Node* node) { UNIMPLEMENTED(); }
-void InstructionSelector::VisitI64x2ExtractLane(Node* node) { UNIMPLEMENTED(); }
-void InstructionSelector::VisitI64x2ReplaceLane(Node* node) { UNIMPLEMENTED(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI64x2Splat(Node* node) {
+  UNIMPLEMENTED();
+}
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI64x2ExtractLane(Node* node) {
+  UNIMPLEMENTED();
+}
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI64x2ReplaceLane(Node* node) {
+  UNIMPLEMENTED();
+}
 #endif  // !V8_TARGET_ARCH_MIPS64 && !V8_TARGET_ARCH_LOONG64 &&
         // !V8_TARGET_ARCH_RISCV64 && !V8_TARGET_ARCH_RISCV32
 #endif  // !V8_TARGET_ARCH_ARM64
 #endif  // !V8_TARGET_ARCH_X64 && !V8_TARGET_ARCH_S390X && !V8_TARGET_ARCH_PPC64
 
-void InstructionSelector::VisitFinishRegion(Node* node) { EmitIdentity(node); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFinishRegion(Node* node) {
+  EmitIdentity(node);
+}
 
-void InstructionSelector::VisitParameter(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitParameter(Node* node) {
   OperandGenerator g(this);
   int index = ParameterIndexOf(node->op());
 
@@ -3125,23 +3375,26 @@ constexpr InstructionCode EncodeCallDescriptorFlags(
 
 }  // namespace
 
-void InstructionSelector::VisitIfException(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitIfException(Node* node) {
   OperandGenerator g(this);
   DCHECK_EQ(IrOpcode::kCall, node->InputAt(1)->opcode());
   Emit(kArchNop, g.DefineAsLocation(node, ExceptionLocation()));
 }
 
-void InstructionSelector::VisitOsrValue(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitOsrValue(Node* node) {
   OperandGenerator g(this);
   int index = OsrValueIndexOf(node->op());
   Emit(kArchNop,
        g.DefineAsLocation(node, linkage()->GetOsrValueLocation(index)));
 }
 
-void InstructionSelector::VisitPhi(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitPhi(Node* node) {
   const int input_count = node->op()->ValueInputCount();
   DCHECK_EQ(input_count, current_block_->PredecessorCount());
-  PhiInstruction* phi = instruction_zone()->New<PhiInstruction>(
+  PhiInstruction* phi = instruction_zone()->template New<PhiInstruction>(
       instruction_zone(), GetVirtualRegister(node),
       static_cast<size_t>(input_count));
   sequence()
@@ -3154,7 +3407,8 @@ void InstructionSelector::VisitPhi(Node* node) {
   }
 }
 
-void InstructionSelector::VisitProjection(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitProjection(Node* node) {
   OperandGenerator g(this);
   Node* value = node->InputAt(0);
   switch (value->opcode()) {
@@ -3190,18 +3444,21 @@ void InstructionSelector::VisitProjection(Node* node) {
   }
 }
 
-void InstructionSelector::VisitConstant(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitConstant(Node* node) {
   // We must emit a NOP here because every live range needs a defining
   // instruction in the register allocator.
   OperandGenerator g(this);
   Emit(kArchNop, g.DefineAsConstant(node));
 }
 
-void InstructionSelector::UpdateMaxPushedArgumentCount(size_t count) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::UpdateMaxPushedArgumentCount(size_t count) {
   *max_pushed_argument_count_ = std::max(count, *max_pushed_argument_count_);
 }
 
-void InstructionSelector::VisitCall(Node* node, BasicBlock* handler) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitCall(Node* node, BasicBlock* handler) {
   OperandGenerator g(this);
   auto call_descriptor = CallDescriptorOf(node->op());
   SaveFPRegsMode mode = call_descriptor->NeedsCallerSavedFPRegisters()
@@ -3299,7 +3556,8 @@ void InstructionSelector::VisitCall(Node* node, BasicBlock* handler) {
   }
 }
 
-void InstructionSelector::VisitTailCall(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitTailCall(Node* node) {
   OperandGenerator g(this);
 
   auto caller = linkage()->GetIncomingDescriptor();
@@ -3361,19 +3619,22 @@ void InstructionSelector::VisitTailCall(Node* node) {
        temps.empty() ? nullptr : &temps.front());
 }
 
-void InstructionSelector::VisitGoto(BasicBlock* target) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitGoto(BasicBlock* target) {
   // jump to the next block.
   OperandGenerator g(this);
   Emit(kArchJmp, g.NoOutput(), g.Label(target));
 }
 
-void InstructionSelector::VisitReturn(Node* ret) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitReturn(Node* ret) {
   OperandGenerator g(this);
   const int input_count = linkage()->GetIncomingDescriptor()->ReturnCount() == 0
                               ? 1
                               : ret->op()->ValueInputCount();
   DCHECK_GE(input_count, 1);
-  auto value_locations = zone()->NewArray<InstructionOperand>(input_count);
+  auto value_locations =
+      zone()->template NewArray<InstructionOperand>(input_count);
   Node* pop_count = ret->InputAt(0);
   value_locations[0] = (pop_count->opcode() == IrOpcode::kInt32Constant ||
                         pop_count->opcode() == IrOpcode::kInt64Constant)
@@ -3386,8 +3647,10 @@ void InstructionSelector::VisitReturn(Node* ret) {
   Emit(kArchRet, 0, nullptr, input_count, value_locations);
 }
 
-void InstructionSelector::VisitBranch(Node* branch, BasicBlock* tbranch,
-                                      BasicBlock* fbranch) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitBranch(Node* branch,
+                                                BasicBlock* tbranch,
+                                                BasicBlock* fbranch) {
   TryPrepareScheduleFirstProjection(branch->InputAt(0));
 
   FlagsContinuation cont =
@@ -3417,7 +3680,8 @@ void InstructionSelector::VisitBranch(Node* branch, BasicBlock* tbranch,
 // TryPrepareScheduleFirstProjection is thus called from
 // VisitDeoptimizeIf/VisitDeoptimizeUnless/VisitBranch and detects if the 1st
 // projection could be scheduled now, and, if so, defines it.
-void InstructionSelector::TryPrepareScheduleFirstProjection(
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::TryPrepareScheduleFirstProjection(
     Node* const maybe_projection) {
   if (maybe_projection->opcode() != IrOpcode::kProjection) {
     // The DeoptimizeIf/DeoptimizeUnless/Branch condition is not a projection.
@@ -3491,7 +3755,8 @@ void InstructionSelector::TryPrepareScheduleFirstProjection(
   }
 }
 
-void InstructionSelector::VisitDeoptimizeIf(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitDeoptimizeIf(Node* node) {
   TryPrepareScheduleFirstProjection(node->InputAt(0));
 
   DeoptimizeParameters p = DeoptimizeParametersOf(node->op());
@@ -3501,7 +3766,8 @@ void InstructionSelector::VisitDeoptimizeIf(Node* node) {
   VisitWordCompareZero(node, node->InputAt(0), &cont);
 }
 
-void InstructionSelector::VisitDeoptimizeUnless(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitDeoptimizeUnless(Node* node) {
   TryPrepareScheduleFirstProjection(node->InputAt(0));
 
   DeoptimizeParameters p = DeoptimizeParametersOf(node->op());
@@ -3511,14 +3777,16 @@ void InstructionSelector::VisitDeoptimizeUnless(Node* node) {
   VisitWordCompareZero(node, node->InputAt(0), &cont);
 }
 
-void InstructionSelector::VisitSelect(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitSelect(Node* node) {
   FlagsContinuation cont =
       FlagsContinuation::ForSelect(kNotEqual, node,
                                    node->InputAt(1), node->InputAt(2));
   VisitWordCompareZero(node, node->InputAt(0), &cont);
 }
 
-void InstructionSelector::VisitTrapIf(Node* node, TrapId trap_id) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitTrapIf(Node* node, TrapId trap_id) {
   // FrameStates are only used for wasm traps inlined in JS. In that case the
   // trap node will be lowered (replaced) before instruction selection.
   // Therefore any TrapIf node has only one input.
@@ -3527,7 +3795,9 @@ void InstructionSelector::VisitTrapIf(Node* node, TrapId trap_id) {
   VisitWordCompareZero(node, node->InputAt(0), &cont);
 }
 
-void InstructionSelector::VisitTrapUnless(Node* node, TrapId trap_id) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitTrapUnless(Node* node,
+                                                    TrapId trap_id) {
   // FrameStates are only used for wasm traps inlined in JS. In that case the
   // trap node will be lowered (replaced) before instruction selection.
   // Therefore any TrapUnless node has only one input.
@@ -3536,37 +3806,42 @@ void InstructionSelector::VisitTrapUnless(Node* node, TrapId trap_id) {
   VisitWordCompareZero(node, node->InputAt(0), &cont);
 }
 
-void InstructionSelector::EmitIdentity(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::EmitIdentity(Node* node) {
   MarkAsUsed(node->InputAt(0));
   MarkAsDefined(node);
   SetRename(node, node->InputAt(0));
 }
 
-void InstructionSelector::VisitDeoptimize(DeoptimizeReason reason,
-                                          NodeId node_id,
-                                          FeedbackSource const& feedback,
-                                          FrameState frame_state) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitDeoptimize(
+    DeoptimizeReason reason, NodeId node_id, FeedbackSource const& feedback,
+    FrameState frame_state) {
   InstructionOperandVector args(instruction_zone());
   AppendDeoptimizeArguments(&args, reason, node_id, feedback, frame_state);
   Emit(kArchDeoptimize, 0, nullptr, args.size(), &args.front(), 0, nullptr);
 }
 
-void InstructionSelector::VisitThrow(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitThrow(Node* node) {
   OperandGenerator g(this);
   Emit(kArchThrowTerminator, g.NoOutput());
 }
 
-void InstructionSelector::VisitDebugBreak(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitDebugBreak(Node* node) {
   OperandGenerator g(this);
   Emit(kArchDebugBreak, g.NoOutput());
 }
 
-void InstructionSelector::VisitUnreachable(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitUnreachable(Node* node) {
   OperandGenerator g(this);
   Emit(kArchDebugBreak, g.NoOutput());
 }
 
-void InstructionSelector::VisitStaticAssert(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitStaticAssert(Node* node) {
   Node* asserted = node->InputAt(0);
   UnparkedScopeIfNeeded scope(broker_);
   AllowHandleDereference allow_handle_dereference;
@@ -3576,24 +3851,28 @@ void InstructionSelector::VisitStaticAssert(Node* node) {
       StaticAssertSourceOf(node->op()));
 }
 
-void InstructionSelector::VisitDeadValue(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitDeadValue(Node* node) {
   OperandGenerator g(this);
   MarkAsRepresentation(DeadValueRepresentationOf(node->op()), node);
   Emit(kArchDebugBreak, g.DefineAsConstant(node));
 }
 
-void InstructionSelector::VisitComment(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitComment(Node* node) {
   OperandGenerator g(this);
   InstructionOperand operand(g.UseImmediate(node));
   Emit(kArchComment, 0, nullptr, 1, &operand);
 }
 
-void InstructionSelector::VisitRetain(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitRetain(Node* node) {
   OperandGenerator g(this);
   Emit(kArchNop, g.NoOutput(), g.UseAny(node->InputAt(0)));
 }
 
-bool InstructionSelector::CanProduceSignalingNaN(Node* node) {
+template <typename Adapter>
+bool InstructionSelectorT<Adapter>::CanProduceSignalingNaN(Node* node) {
   // TODO(jarin) Improve the heuristic here.
   if (node->opcode() == IrOpcode::kFloat64Add ||
       node->opcode() == IrOpcode::kFloat64Sub ||
@@ -3604,8 +3883,9 @@ bool InstructionSelector::CanProduceSignalingNaN(Node* node) {
 }
 
 #if V8_TARGET_ARCH_64_BIT
-bool InstructionSelector::ZeroExtendsWord32ToWord64(Node* node,
-                                                    int recursion_depth) {
+template <typename Adapter>
+bool InstructionSelectorT<Adapter>::ZeroExtendsWord32ToWord64(
+    Node* node, int recursion_depth) {
   // To compute whether a Node sets its upper 32 bits to zero, there are three
   // cases.
   // 1. Phi node, with a computed result already available in phi_states_:
@@ -3687,7 +3967,8 @@ FrameStateDescriptor* GetFrameStateDescriptorInternal(Zone* zone,
 
 }  // namespace
 
-FrameStateDescriptor* InstructionSelector::GetFrameStateDescriptor(
+template <typename Adapter>
+FrameStateDescriptor* InstructionSelectorT<Adapter>::GetFrameStateDescriptor(
     FrameState state) {
   auto* desc = GetFrameStateDescriptorInternal(instruction_zone(), state);
   *max_unoptimized_frame_height_ =
@@ -3697,8 +3978,10 @@ FrameStateDescriptor* InstructionSelector::GetFrameStateDescriptor(
 }
 
 #if V8_ENABLE_WEBASSEMBLY
-void InstructionSelector::CanonicalizeShuffle(Node* node, uint8_t* shuffle,
-                                              bool* is_swizzle) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::CanonicalizeShuffle(Node* node,
+                                                        uint8_t* shuffle,
+                                                        bool* is_swizzle) {
   // Get raw shuffle indices.
   memcpy(shuffle, S128ImmediateParameterOf(node->op()).data(), kSimd128Size);
   bool needs_swap;
@@ -3717,7 +4000,8 @@ void InstructionSelector::CanonicalizeShuffle(Node* node, uint8_t* shuffle,
 }
 
 // static
-void InstructionSelector::SwapShuffleInputs(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::SwapShuffleInputs(Node* node) {
   Node* input0 = node->InputAt(0);
   Node* input1 = node->InputAt(1);
   node->ReplaceInput(0, input1);
@@ -3725,6 +4009,11 @@ void InstructionSelector::SwapShuffleInputs(Node* node) {
 }
 #endif  // V8_ENABLE_WEBASSEMBLY
 
+template class EXPORT_TEMPLATE_DEFINE(V8_EXPORT_PRIVATE)
+    InstructionSelectorT<TurbofanAdapter>;
+template class EXPORT_TEMPLATE_DEFINE(V8_EXPORT_PRIVATE)
+    InstructionSelectorT<TurboshaftAdapter>;
+
 }  // namespace compiler
 }  // namespace internal
 }  // namespace v8
diff --git a/src/compiler/backend/instruction-selector.h b/src/compiler/backend/instruction-selector.h
index 7f3e92a956a..11ba995dccd 100644
--- a/src/compiler/backend/instruction-selector.h
+++ b/src/compiler/backend/instruction-selector.h
@@ -33,10 +33,15 @@ namespace compiler {
 class BasicBlock;
 struct CallBuffer;  // TODO(bmeurer): Remove this.
 class Linkage;
-class OperandGenerator;
+template <typename Adapter>
+class OperandGeneratorT;
 class SwitchInfo;
 class StateObjectDeduplicator;
 
+struct TurbofanAdapter {};
+
+struct TurboshaftAdapter {};
+
 // The flags continuation is a way to combine a branch or a materialization
 // of a boolean value with an instruction that sets the flags register.
 // The whole instruction is treated as a unit by the register allocator, and
@@ -260,8 +265,10 @@ struct PushParameter {
 enum class FrameStateInputKind { kAny, kStackSlot };
 
 // Instruction selection generates an InstructionSequence for a given Schedule.
-class V8_EXPORT_PRIVATE InstructionSelector final {
+template <typename Adapter>
+class EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE) InstructionSelectorT final {
  public:
+  using OperandGenerator = OperandGeneratorT<Adapter>;
   // Forward declarations.
   class Features;
 
@@ -277,7 +284,7 @@ class V8_EXPORT_PRIVATE InstructionSelector final {
   };
   enum EnableTraceTurboJson { kDisableTraceTurboJson, kEnableTraceTurboJson };
 
-  InstructionSelector(
+  InstructionSelectorT(
       Zone* zone, size_t node_count, Linkage* linkage,
       InstructionSequence* sequence, Schedule* schedule,
       SourcePositionTable* source_positions, Frame* frame,
@@ -468,7 +475,7 @@ class V8_EXPORT_PRIVATE InstructionSelector final {
   }
 
  private:
-  friend class OperandGenerator;
+  friend class OperandGeneratorT<Adapter>;
 
   bool UseInstructionScheduling() const {
     return (enable_scheduling_ == kEnableScheduling) &&
@@ -754,8 +761,9 @@ class V8_EXPORT_PRIVATE InstructionSelector final {
   EnableScheduling enable_scheduling_;
   EnableRootsRelativeAddressing enable_roots_relative_addressing_;
   EnableSwitchJumpTable enable_switch_jump_table_;
-  ZoneUnorderedMap<FrameStateInput, CachedStateValues*, FrameStateInput::Hash,
-                   FrameStateInput::Equal>
+  ZoneUnorderedMap<FrameStateInput, CachedStateValues*,
+                   typename FrameStateInput::Hash,
+                   typename FrameStateInput::Equal>
       state_values_cache_;
 
   Frame* frame_;
@@ -780,6 +788,13 @@ class V8_EXPORT_PRIVATE InstructionSelector final {
 #endif
 };
 
+using InstructionSelector = InstructionSelectorT<TurbofanAdapter>;
+
+extern template class EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE)
+    InstructionSelectorT<TurbofanAdapter>;
+extern template class EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE)
+    InstructionSelectorT<TurboshaftAdapter>;
+
 }  // namespace compiler
 }  // namespace internal
 }  // namespace v8
diff --git a/src/compiler/backend/x64/instruction-selector-x64.cc b/src/compiler/backend/x64/instruction-selector-x64.cc
index 17a1e2d1e34..f501996cf7d 100644
--- a/src/compiler/backend/x64/instruction-selector-x64.cc
+++ b/src/compiler/backend/x64/instruction-selector-x64.cc
@@ -48,10 +48,13 @@ bool IsCompressed(Node* const node) {
 }  // namespace
 
 // Adds X64-specific methods for generating operands.
-class X64OperandGenerator final : public OperandGenerator {
+template <typename Adapter>
+class X64OperandGeneratorT final : public OperandGeneratorT<Adapter> {
  public:
-  explicit X64OperandGenerator(InstructionSelector* selector)
-      : OperandGenerator(selector) {}
+  OPERAND_GENERATOR_T_BOILERPLATE(Adapter)
+
+  explicit X64OperandGeneratorT(InstructionSelectorT<Adapter>* selector)
+      : super(selector) {}
 
   bool CanBeImmediate(Node* node) {
     switch (node->opcode()) {
@@ -466,13 +469,15 @@ ArchOpcode GetSeqCstStoreOpcode(StoreRepresentation store_rep) {
 
 }  // namespace
 
-void InstructionSelector::VisitTraceInstruction(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitTraceInstruction(Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   uint32_t markid = OpParameter<uint32_t>(node->op());
   Emit(kX64TraceInstruction, g.Use(node), g.UseImmediate(markid));
 }
 
-void InstructionSelector::VisitStackSlot(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitStackSlot(Node* node) {
   StackSlotRepresentation rep = StackSlotRepresentationOf(node->op());
   int slot = frame_->AllocateSpillSlot(rep.size(), rep.alignment());
   OperandGenerator g(this);
@@ -481,12 +486,14 @@ void InstructionSelector::VisitStackSlot(Node* node) {
        sequence()->AddImmediate(Constant(slot)), 0, nullptr);
 }
 
-void InstructionSelector::VisitAbortCSADcheck(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitAbortCSADcheck(Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   Emit(kArchAbortCSADcheck, g.NoOutput(), g.UseFixed(node->InputAt(0), rdx));
 }
 
-void InstructionSelector::VisitLoadLane(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitLoadLane(Node* node) {
   LoadLaneParameters params = LoadLaneParametersOf(node->op());
   InstructionCode opcode = kArchNop;
   if (params.rep == MachineType::Int8()) {
@@ -501,7 +508,7 @@ void InstructionSelector::VisitLoadLane(Node* node) {
     UNREACHABLE();
   }
 
-  X64OperandGenerator g(this);
+  X64OperandGeneratorT<Adapter> g(this);
   InstructionOperand outputs[] = {g.DefineAsRegister(node)};
   // Input 0 is value node, 1 is lane idx, and GetEffectiveAddressMemoryOperand
   // uses up to 3 inputs. This ordering is consistent with other operations that
@@ -526,7 +533,8 @@ void InstructionSelector::VisitLoadLane(Node* node) {
   Emit(opcode, 1, outputs, input_count, inputs);
 }
 
-void InstructionSelector::VisitLoadTransform(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitLoadTransform(Node* node) {
   LoadTransformParameters params = LoadTransformParametersOf(node->op());
   ArchOpcode opcode;
   switch (params.transformation) {
@@ -585,9 +593,10 @@ void InstructionSelector::VisitLoadTransform(Node* node) {
   VisitLoad(node, node, code);
 }
 
-void InstructionSelector::VisitLoad(Node* node, Node* value,
-                                    InstructionCode opcode) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitLoad(Node* node, Node* value,
+                                              InstructionCode opcode) {
+  X64OperandGeneratorT<Adapter> g(this);
 #ifdef V8_IS_TSAN
   // On TSAN builds we require one scratch register. Because of this we also
   // have to modify the inputs to take into account possible aliasing and use
@@ -618,21 +627,26 @@ void InstructionSelector::VisitLoad(Node* node, Node* value,
   Emit(code, 1, outputs, input_count, inputs, temp_count, temps);
 }
 
-void InstructionSelector::VisitLoad(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitLoad(Node* node) {
   LoadRepresentation load_rep = LoadRepresentationOf(node->op());
   DCHECK(!load_rep.IsMapWord());
   VisitLoad(node, node, GetLoadOpcode(load_rep));
 }
 
-void InstructionSelector::VisitProtectedLoad(Node* node) { VisitLoad(node); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitProtectedLoad(Node* node) {
+  VisitLoad(node);
+}
 
 namespace {
 
 // Shared routine for Word32/Word64 Atomic Exchange
-void VisitAtomicExchange(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitAtomicExchange(InstructionSelectorT<Adapter>* selector, Node* node,
                          ArchOpcode opcode, AtomicWidth width,
                          MemoryAccessKind access_kind) {
-  X64OperandGenerator g(selector);
+  X64OperandGeneratorT<Adapter> g(selector);
   Node* base = node->InputAt(0);
   Node* index = node->InputAt(1);
   Node* value = node->InputAt(2);
@@ -649,11 +663,12 @@ void VisitAtomicExchange(InstructionSelector* selector, Node* node,
   selector->Emit(code, arraysize(outputs), outputs, arraysize(inputs), inputs);
 }
 
-void VisitStoreCommon(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitStoreCommon(InstructionSelectorT<Adapter>* selector, Node* node,
                       StoreRepresentation store_rep,
                       base::Optional<AtomicMemoryOrder> atomic_order,
                       MemoryAccessKind acs_kind = MemoryAccessKind::kNormal) {
-  X64OperandGenerator g(selector);
+  X64OperandGeneratorT<Adapter> g(selector);
   Node* base = node->InputAt(0);
   Node* index = node->InputAt(1);
   Node* value = node->InputAt(2);
@@ -700,11 +715,12 @@ void VisitStoreCommon(InstructionSelector* selector, Node* node,
     // UseUniqueRegister which is not required for non-TSAN builds.
     InstructionOperand temps[] = {g.TempRegister(), g.TempRegister()};
     size_t temp_count = arraysize(temps);
-    auto reg_kind = OperandGenerator::RegisterUseKind::kUseUniqueRegister;
+    auto reg_kind =
+        OperandGeneratorT<Adapter>::RegisterUseKind::kUseUniqueRegister;
 #else
     InstructionOperand* temps = nullptr;
     size_t temp_count = 0;
-    auto reg_kind = OperandGenerator::RegisterUseKind::kUseRegister;
+    auto reg_kind = OperandGeneratorT<Adapter>::RegisterUseKind::kUseRegister;
 #endif  // V8_IS_TSAN
 
     // Release and non-atomic stores emit MOV and sequentially consistent stores
@@ -751,26 +767,38 @@ void VisitStoreCommon(InstructionSelector* selector, Node* node,
 
 }  // namespace
 
-void InstructionSelector::VisitStorePair(Node* node) { UNREACHABLE(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitStorePair(Node* node) {
+  UNREACHABLE();
+}
 
-void InstructionSelector::VisitStore(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitStore(Node* node) {
   return VisitStoreCommon(this, node, StoreRepresentationOf(node->op()),
                           base::nullopt);
 }
 
-void InstructionSelector::VisitProtectedStore(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitProtectedStore(Node* node) {
   return VisitStoreCommon(this, node, StoreRepresentationOf(node->op()),
                           base::nullopt, MemoryAccessKind::kProtected);
 }
 
 // Architecture supports unaligned access, therefore VisitLoad is used instead
-void InstructionSelector::VisitUnalignedLoad(Node* node) { UNREACHABLE(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitUnalignedLoad(Node* node) {
+  UNREACHABLE();
+}
 
 // Architecture supports unaligned access, therefore VisitStore is used instead
-void InstructionSelector::VisitUnalignedStore(Node* node) { UNREACHABLE(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitUnalignedStore(Node* node) {
+  UNREACHABLE();
+}
 
-void InstructionSelector::VisitStoreLane(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitStoreLane(Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
 
   StoreLaneParameters params = StoreLaneParametersOf(node->op());
   InstructionCode opcode = kArchNop;
@@ -804,9 +832,10 @@ void InstructionSelector::VisitStoreLane(Node* node) {
 }
 
 // Shared routine for multiple binary operations.
-static void VisitBinop(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+static void VisitBinop(InstructionSelectorT<Adapter>* selector, Node* node,
                        InstructionCode opcode, FlagsContinuation* cont) {
-  X64OperandGenerator g(selector);
+  X64OperandGeneratorT<Adapter> g(selector);
   Int32BinopMatcher m(node);
   Node* left = m.left().node();
   Node* right = m.right().node();
@@ -866,14 +895,16 @@ static void VisitBinop(InstructionSelector* selector, Node* node,
 }
 
 // Shared routine for multiple binary operations.
-static void VisitBinop(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+static void VisitBinop(InstructionSelectorT<Adapter>* selector, Node* node,
                        InstructionCode opcode) {
   FlagsContinuation cont;
   VisitBinop(selector, node, opcode, &cont);
 }
 
-void InstructionSelector::VisitWord32And(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32And(Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   Uint32BinopMatcher m(node);
   if (m.right().Is(0xFF)) {
     Emit(kX64Movzxbl, g.DefineAsRegister(node), g.Use(m.left().node()));
@@ -884,8 +915,9 @@ void InstructionSelector::VisitWord32And(Node* node) {
   }
 }
 
-void InstructionSelector::VisitWord64And(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64And(Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   Uint64BinopMatcher m(node);
   if (m.right().Is(0xFF)) {
     Emit(kX64Movzxbq, g.DefineAsRegister(node), g.Use(m.left().node()));
@@ -902,16 +934,19 @@ void InstructionSelector::VisitWord64And(Node* node) {
   }
 }
 
-void InstructionSelector::VisitWord32Or(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32Or(Node* node) {
   VisitBinop(this, node, kX64Or32);
 }
 
-void InstructionSelector::VisitWord64Or(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64Or(Node* node) {
   VisitBinop(this, node, kX64Or);
 }
 
-void InstructionSelector::VisitWord32Xor(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32Xor(Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   Uint32BinopMatcher m(node);
   if (m.right().Is(-1)) {
     Emit(kX64Not32, g.DefineSameAsFirst(node), g.UseRegister(m.left().node()));
@@ -920,8 +955,9 @@ void InstructionSelector::VisitWord32Xor(Node* node) {
   }
 }
 
-void InstructionSelector::VisitWord64Xor(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64Xor(Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   Uint64BinopMatcher m(node);
   if (m.right().Is(-1)) {
     Emit(kX64Not, g.DefineSameAsFirst(node), g.UseRegister(m.left().node()));
@@ -930,7 +966,8 @@ void InstructionSelector::VisitWord64Xor(Node* node) {
   }
 }
 
-void InstructionSelector::VisitStackPointerGreaterThan(
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitStackPointerGreaterThan(
     Node* node, FlagsContinuation* cont) {
   StackCheckKind kind = StackCheckKindOf(node->op());
   InstructionCode opcode =
@@ -938,7 +975,7 @@ void InstructionSelector::VisitStackPointerGreaterThan(
 
   int effect_level = GetEffectLevel(node, cont);
 
-  X64OperandGenerator g(this);
+  X64OperandGeneratorT<Adapter> g(this);
   Node* const value = node->InputAt(0);
   if (g.CanBeMemoryOperand(kX64Cmp, node, value, effect_level)) {
     DCHECK(IrOpcode::kLoad == value->opcode() ||
@@ -962,8 +999,9 @@ void InstructionSelector::VisitStackPointerGreaterThan(
 
 namespace {
 
-void TryMergeTruncateInt64ToInt32IntoLoad(InstructionSelector* selector,
-                                          Node* node, Node* load) {
+template <typename Adapter>
+void TryMergeTruncateInt64ToInt32IntoLoad(
+    InstructionSelectorT<Adapter>* selector, Node* node, Node* load) {
   LoadRepresentation load_rep = LoadRepresentationOf(load->op());
   MachineRepresentation rep = load_rep.representation();
   InstructionCode opcode;
@@ -985,18 +1023,19 @@ void TryMergeTruncateInt64ToInt32IntoLoad(InstructionSelector* selector,
     default:
       UNREACHABLE();
   }
-  X64OperandGenerator g(selector);
+  X64OperandGeneratorT<Adapter> g(selector);
 #ifdef V8_IS_TSAN
   // On TSAN builds we require one scratch register. Because of this we also
   // have to modify the inputs to take into account possible aliasing and use
   // UseUniqueRegister which is not required for non-TSAN builds.
   InstructionOperand temps[] = {g.TempRegister()};
   size_t temp_count = arraysize(temps);
-  auto reg_kind = OperandGenerator::RegisterUseKind::kUseUniqueRegister;
+  auto reg_kind =
+      OperandGeneratorT<Adapter>::RegisterUseKind::kUseUniqueRegister;
 #else
   InstructionOperand* temps = nullptr;
   size_t temp_count = 0;
-  auto reg_kind = OperandGenerator::RegisterUseKind::kUseRegister;
+  auto reg_kind = OperandGeneratorT<Adapter>::RegisterUseKind::kUseRegister;
 #endif  // V8_IS_TSAN
   InstructionOperand outputs[] = {g.DefineAsRegister(node)};
   size_t input_count = 0;
@@ -1010,9 +1049,10 @@ void TryMergeTruncateInt64ToInt32IntoLoad(InstructionSelector* selector,
 
 // Shared routine for multiple 32-bit shift operations.
 // TODO(bmeurer): Merge this with VisitWord64Shift using template magic?
-void VisitWord32Shift(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitWord32Shift(InstructionSelectorT<Adapter>* selector, Node* node,
                       ArchOpcode opcode) {
-  X64OperandGenerator g(selector);
+  X64OperandGeneratorT<Adapter> g(selector);
   Int32BinopMatcher m(node);
   Node* left = m.left().node();
   Node* right = m.right().node();
@@ -1032,9 +1072,10 @@ void VisitWord32Shift(InstructionSelector* selector, Node* node,
 
 // Shared routine for multiple 64-bit shift operations.
 // TODO(bmeurer): Merge this with VisitWord32Shift using template magic?
-void VisitWord64Shift(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitWord64Shift(InstructionSelectorT<Adapter>* selector, Node* node,
                       ArchOpcode opcode) {
-  X64OperandGenerator g(selector);
+  X64OperandGeneratorT<Adapter> g(selector);
   Int64BinopMatcher m(node);
   Node* left = m.left().node();
   Node* right = m.right().node();
@@ -1061,10 +1102,10 @@ void VisitWord64Shift(InstructionSelector* selector, Node* node,
 }
 
 // Shared routine for multiple shift operations with continuation.
-template <typename BinopMatcher, int Bits>
-bool TryVisitWordShift(InstructionSelector* selector, Node* node,
+template <typename Adapter, typename BinopMatcher, int Bits>
+bool TryVisitWordShift(InstructionSelectorT<Adapter>* selector, Node* node,
                        ArchOpcode opcode, FlagsContinuation* cont) {
-  X64OperandGenerator g(selector);
+  X64OperandGeneratorT<Adapter> g(selector);
   BinopMatcher m(node);
   Node* left = m.left().node();
   Node* right = m.right().node();
@@ -1082,10 +1123,11 @@ bool TryVisitWordShift(InstructionSelector* selector, Node* node,
   return true;
 }
 
-void EmitLea(InstructionSelector* selector, InstructionCode opcode,
+template <typename Adapter>
+void EmitLea(InstructionSelectorT<Adapter>* selector, InstructionCode opcode,
              Node* result, Node* index, int scale, Node* base,
              Node* displacement, DisplacementMode displacement_mode) {
-  X64OperandGenerator g(selector);
+  X64OperandGeneratorT<Adapter> g(selector);
 
   InstructionOperand inputs[4];
   size_t input_count = 0;
@@ -1106,7 +1148,8 @@ void EmitLea(InstructionSelector* selector, InstructionCode opcode,
 
 }  // namespace
 
-void InstructionSelector::VisitWord32Shl(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32Shl(Node* node) {
   Int32ScaleMatcher m(node, true);
   if (m.matches()) {
     Node* index = node->InputAt(0);
@@ -1118,8 +1161,9 @@ void InstructionSelector::VisitWord32Shl(Node* node) {
   VisitWord32Shift(this, node, kX64Shl32);
 }
 
-void InstructionSelector::VisitWord64Shl(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64Shl(Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   Int64ScaleMatcher m(node, true);
   if (m.matches()) {
     Node* index = node->InputAt(0);
@@ -1143,7 +1187,8 @@ void InstructionSelector::VisitWord64Shl(Node* node) {
   VisitWord64Shift(this, node, kX64Shl);
 }
 
-void InstructionSelector::VisitWord32Shr(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32Shr(Node* node) {
   VisitWord32Shift(this, node, kX64Shr32);
 }
 
@@ -1187,11 +1232,12 @@ inline AddressingMode AddDisplacementToAddressingMode(AddressingMode mode) {
   UNREACHABLE();
 }
 
-bool TryMatchLoadWord64AndShiftRight(InstructionSelector* selector, Node* node,
-                                     InstructionCode opcode) {
+template <typename Adapter>
+bool TryMatchLoadWord64AndShiftRight(InstructionSelectorT<Adapter>* selector,
+                                     Node* node, InstructionCode opcode) {
   DCHECK(IrOpcode::kWord64Sar == node->opcode() ||
          IrOpcode::kWord64Shr == node->opcode());
-  X64OperandGenerator g(selector);
+  X64OperandGeneratorT<Adapter> g(selector);
   Int64BinopMatcher m(node);
   if (selector->CanCover(m.node(), m.left().node()) && m.left().IsLoad() &&
       m.right().Is(32)) {
@@ -1209,11 +1255,12 @@ bool TryMatchLoadWord64AndShiftRight(InstructionSelector* selector, Node* node,
       // use UseUniqueRegister which is not required for non-TSAN builds.
       InstructionOperand temps[] = {g.TempRegister()};
       size_t temp_count = arraysize(temps);
-      auto reg_kind = OperandGenerator::RegisterUseKind::kUseUniqueRegister;
+      auto reg_kind =
+          OperandGeneratorT<Adapter>::RegisterUseKind::kUseUniqueRegister;
 #else
       InstructionOperand* temps = nullptr;
       size_t temp_count = 0;
-      auto reg_kind = OperandGenerator::RegisterUseKind::kUseRegister;
+      auto reg_kind = OperandGeneratorT<Adapter>::RegisterUseKind::kUseRegister;
 #endif  // V8_IS_TSAN
       size_t input_count = 0;
       InstructionOperand inputs[3];
@@ -1246,13 +1293,15 @@ bool TryMatchLoadWord64AndShiftRight(InstructionSelector* selector, Node* node,
 
 }  // namespace
 
-void InstructionSelector::VisitWord64Shr(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64Shr(Node* node) {
   if (TryMatchLoadWord64AndShiftRight(this, node, kX64Movl)) return;
   VisitWord64Shift(this, node, kX64Shr);
 }
 
-void InstructionSelector::VisitWord32Sar(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32Sar(Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   Int32BinopMatcher m(node);
   if (CanCover(m.node(), m.left().node()) && m.left().IsWord32Shl()) {
     Int32BinopMatcher mleft(m.left().node());
@@ -1267,47 +1316,62 @@ void InstructionSelector::VisitWord32Sar(Node* node) {
   VisitWord32Shift(this, node, kX64Sar32);
 }
 
-void InstructionSelector::VisitWord64Sar(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64Sar(Node* node) {
   if (TryMatchLoadWord64AndShiftRight(this, node, kX64Movsxlq)) return;
   VisitWord64Shift(this, node, kX64Sar);
 }
 
-void InstructionSelector::VisitWord32Rol(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32Rol(Node* node) {
   VisitWord32Shift(this, node, kX64Rol32);
 }
 
-void InstructionSelector::VisitWord64Rol(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64Rol(Node* node) {
   VisitWord64Shift(this, node, kX64Rol);
 }
 
-void InstructionSelector::VisitWord32Ror(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32Ror(Node* node) {
   VisitWord32Shift(this, node, kX64Ror32);
 }
 
-void InstructionSelector::VisitWord64Ror(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64Ror(Node* node) {
   VisitWord64Shift(this, node, kX64Ror);
 }
 
-void InstructionSelector::VisitWord32ReverseBits(Node* node) { UNREACHABLE(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32ReverseBits(Node* node) {
+  UNREACHABLE();
+}
 
-void InstructionSelector::VisitWord64ReverseBits(Node* node) { UNREACHABLE(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64ReverseBits(Node* node) {
+  UNREACHABLE();
+}
 
-void InstructionSelector::VisitWord64ReverseBytes(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64ReverseBytes(Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   Emit(kX64Bswap, g.DefineSameAsFirst(node), g.UseRegister(node->InputAt(0)));
 }
 
-void InstructionSelector::VisitWord32ReverseBytes(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32ReverseBytes(Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   Emit(kX64Bswap32, g.DefineSameAsFirst(node), g.UseRegister(node->InputAt(0)));
 }
 
-void InstructionSelector::VisitSimd128ReverseBytes(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitSimd128ReverseBytes(Node* node) {
   UNREACHABLE();
 }
 
-void InstructionSelector::VisitInt32Add(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32Add(Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
 
   // No need to truncate the values before Int32Add.
   DCHECK_EQ(node->InputCount(), 2);
@@ -1333,8 +1397,9 @@ void InstructionSelector::VisitInt32Add(Node* node) {
   VisitBinop(this, node, kX64Add32);
 }
 
-void InstructionSelector::VisitInt64Add(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt64Add(Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
 
   // Try to match the Add to a leaq pattern
   BaseWithIndexAndDisplacement64Matcher m(node);
@@ -1349,7 +1414,8 @@ void InstructionSelector::VisitInt64Add(Node* node) {
   VisitBinop(this, node, kX64Add);
 }
 
-void InstructionSelector::VisitInt64AddWithOverflow(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt64AddWithOverflow(Node* node) {
   if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
     FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
     return VisitBinop(this, node, kX64Add, &cont);
@@ -1358,8 +1424,9 @@ void InstructionSelector::VisitInt64AddWithOverflow(Node* node) {
   VisitBinop(this, node, kX64Add, &cont);
 }
 
-void InstructionSelector::VisitInt32Sub(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32Sub(Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   DCHECK_EQ(node->InputCount(), 2);
   Node* input1 = node->InputAt(0);
   Node* input2 = node->InputAt(1);
@@ -1400,8 +1467,9 @@ void InstructionSelector::VisitInt32Sub(Node* node) {
   }
 }
 
-void InstructionSelector::VisitInt64Sub(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt64Sub(Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   Int64BinopMatcher m(node);
   if (m.left().Is(0)) {
     Emit(kX64Neg, g.DefineSameAsFirst(node), g.UseRegister(m.right().node()));
@@ -1418,7 +1486,8 @@ void InstructionSelector::VisitInt64Sub(Node* node) {
   }
 }
 
-void InstructionSelector::VisitInt64SubWithOverflow(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt64SubWithOverflow(Node* node) {
   if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
     FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
     return VisitBinop(this, node, kX64Sub, &cont);
@@ -1429,8 +1498,10 @@ void InstructionSelector::VisitInt64SubWithOverflow(Node* node) {
 
 namespace {
 
-void VisitMul(InstructionSelector* selector, Node* node, ArchOpcode opcode) {
-  X64OperandGenerator g(selector);
+template <typename Adapter>
+void VisitMul(InstructionSelectorT<Adapter>* selector, Node* node,
+              ArchOpcode opcode) {
+  X64OperandGeneratorT<Adapter> g(selector);
   Int32BinopMatcher m(node);
   Node* left = m.left().node();
   Node* right = m.right().node();
@@ -1446,9 +1517,10 @@ void VisitMul(InstructionSelector* selector, Node* node, ArchOpcode opcode) {
   }
 }
 
-void VisitMulHigh(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitMulHigh(InstructionSelectorT<Adapter>* selector, Node* node,
                   ArchOpcode opcode) {
-  X64OperandGenerator g(selector);
+  X64OperandGeneratorT<Adapter> g(selector);
   Node* left = node->InputAt(0);
   Node* right = node->InputAt(1);
   if (selector->IsLive(left) && !selector->IsLive(right)) {
@@ -1461,16 +1533,20 @@ void VisitMulHigh(InstructionSelector* selector, Node* node,
                  g.UseUniqueRegister(right), arraysize(temps), temps);
 }
 
-void VisitDiv(InstructionSelector* selector, Node* node, ArchOpcode opcode) {
-  X64OperandGenerator g(selector);
+template <typename Adapter>
+void VisitDiv(InstructionSelectorT<Adapter>* selector, Node* node,
+              ArchOpcode opcode) {
+  X64OperandGeneratorT<Adapter> g(selector);
   InstructionOperand temps[] = {g.TempRegister(rdx)};
   selector->Emit(
       opcode, g.DefineAsFixed(node, rax), g.UseFixed(node->InputAt(0), rax),
       g.UseUniqueRegister(node->InputAt(1)), arraysize(temps), temps);
 }
 
-void VisitMod(InstructionSelector* selector, Node* node, ArchOpcode opcode) {
-  X64OperandGenerator g(selector);
+template <typename Adapter>
+void VisitMod(InstructionSelectorT<Adapter>* selector, Node* node,
+              ArchOpcode opcode) {
+  X64OperandGeneratorT<Adapter> g(selector);
   InstructionOperand temps[] = {g.TempRegister(rax)};
   selector->Emit(
       opcode, g.DefineAsFixed(node, rdx), g.UseFixed(node->InputAt(0), rax),
@@ -1479,7 +1555,8 @@ void VisitMod(InstructionSelector* selector, Node* node, ArchOpcode opcode) {
 
 }  // namespace
 
-void InstructionSelector::VisitInt32Mul(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32Mul(Node* node) {
   Int32ScaleMatcher m(node, true);
   if (m.matches()) {
     Node* index = node->InputAt(0);
@@ -1491,7 +1568,8 @@ void InstructionSelector::VisitInt32Mul(Node* node) {
   VisitMul(this, node, kX64Imul32);
 }
 
-void InstructionSelector::VisitInt32MulWithOverflow(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32MulWithOverflow(Node* node) {
   // TODO(mvstanton): Use Int32ScaleMatcher somehow.
   if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
     FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
@@ -1501,7 +1579,8 @@ void InstructionSelector::VisitInt32MulWithOverflow(Node* node) {
   VisitBinop(this, node, kX64Imul32, &cont);
 }
 
-void InstructionSelector::VisitInt64Mul(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt64Mul(Node* node) {
   Int64ScaleMatcher m(node, true);
   if (m.matches()) {
     Node* index = node->InputAt(0);
@@ -1513,7 +1592,8 @@ void InstructionSelector::VisitInt64Mul(Node* node) {
   VisitMul(this, node, kX64Imul);
 }
 
-void InstructionSelector::VisitInt64MulWithOverflow(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt64MulWithOverflow(Node* node) {
   if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
     FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
     return VisitBinop(this, node, kX64Imul, &cont);
@@ -1522,51 +1602,63 @@ void InstructionSelector::VisitInt64MulWithOverflow(Node* node) {
   VisitBinop(this, node, kX64Imul, &cont);
 }
 
-void InstructionSelector::VisitInt32MulHigh(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32MulHigh(Node* node) {
   VisitMulHigh(this, node, kX64ImulHigh32);
 }
 
-void InstructionSelector::VisitInt64MulHigh(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt64MulHigh(Node* node) {
   VisitMulHigh(this, node, kX64ImulHigh64);
 }
 
-void InstructionSelector::VisitInt32Div(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32Div(Node* node) {
   VisitDiv(this, node, kX64Idiv32);
 }
 
-void InstructionSelector::VisitInt64Div(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt64Div(Node* node) {
   VisitDiv(this, node, kX64Idiv);
 }
 
-void InstructionSelector::VisitUint32Div(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitUint32Div(Node* node) {
   VisitDiv(this, node, kX64Udiv32);
 }
 
-void InstructionSelector::VisitUint64Div(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitUint64Div(Node* node) {
   VisitDiv(this, node, kX64Udiv);
 }
 
-void InstructionSelector::VisitInt32Mod(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32Mod(Node* node) {
   VisitMod(this, node, kX64Idiv32);
 }
 
-void InstructionSelector::VisitInt64Mod(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt64Mod(Node* node) {
   VisitMod(this, node, kX64Idiv);
 }
 
-void InstructionSelector::VisitUint32Mod(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitUint32Mod(Node* node) {
   VisitMod(this, node, kX64Udiv32);
 }
 
-void InstructionSelector::VisitUint64Mod(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitUint64Mod(Node* node) {
   VisitMod(this, node, kX64Udiv);
 }
 
-void InstructionSelector::VisitUint32MulHigh(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitUint32MulHigh(Node* node) {
   VisitMulHigh(this, node, kX64UmulHigh32);
 }
 
-void InstructionSelector::VisitUint64MulHigh(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitUint64MulHigh(Node* node) {
   VisitMulHigh(this, node, kX64UmulHigh64);
 }
 
@@ -1581,8 +1673,9 @@ void InstructionSelector::VisitUint64MulHigh(Node* node) {
 //      Set Projection(1) := 1;   -- the value was in range
 //    Else:
 //      Set Projection(1) := 0;   -- the value was out of range
-void InstructionSelector::VisitTryTruncateFloat32ToInt64(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitTryTruncateFloat32ToInt64(Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   InstructionOperand inputs[] = {g.UseRegister(node->InputAt(0))};
   InstructionOperand outputs[2];
   InstructionOperand temps[1];
@@ -1607,8 +1700,10 @@ void InstructionSelector::VisitTryTruncateFloat32ToInt64(Node* node) {
 // additional subtraction, conversion and (in case the value was originally
 // negative, but still within range) we restore it and set Projection(1) := 1.
 // In all other cases we set Projection(1) := 0, denoting value out of range.
-void InstructionSelector::VisitTryTruncateFloat64ToUint32(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitTryTruncateFloat64ToUint32(
+    Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   InstructionOperand inputs[] = {g.UseRegister(node->InputAt(0))};
   InstructionOperand outputs[2];
   size_t output_count = 0;
@@ -1622,8 +1717,10 @@ void InstructionSelector::VisitTryTruncateFloat64ToUint32(Node* node) {
   Emit(kSSEFloat64ToUint32, output_count, outputs, 1, inputs);
 }
 
-void InstructionSelector::VisitTryTruncateFloat32ToUint64(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitTryTruncateFloat32ToUint64(
+    Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   InstructionOperand inputs[] = {g.UseRegister(node->InputAt(0))};
   InstructionOperand outputs[2];
   size_t output_count = 0;
@@ -1637,8 +1734,10 @@ void InstructionSelector::VisitTryTruncateFloat32ToUint64(Node* node) {
   Emit(kSSEFloat32ToUint64, output_count, outputs, 1, inputs);
 }
 
-void InstructionSelector::VisitTryTruncateFloat64ToUint64(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitTryTruncateFloat64ToUint64(
+    Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   InstructionOperand inputs[] = {g.UseRegister(node->InputAt(0))};
   InstructionOperand outputs[2];
   size_t output_count = 0;
@@ -1652,8 +1751,9 @@ void InstructionSelector::VisitTryTruncateFloat64ToUint64(Node* node) {
   Emit(kSSEFloat64ToUint64, output_count, outputs, 1, inputs);
 }
 
-void InstructionSelector::VisitTryTruncateFloat64ToInt64(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitTryTruncateFloat64ToInt64(Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   InstructionOperand inputs[] = {g.UseRegister(node->InputAt(0))};
   InstructionOperand outputs[2];
   InstructionOperand temps[1];
@@ -1670,8 +1770,9 @@ void InstructionSelector::VisitTryTruncateFloat64ToInt64(Node* node) {
   Emit(kSSEFloat64ToInt64, output_count, outputs, 1, inputs, temp_count, temps);
 }
 
-void InstructionSelector::VisitTryTruncateFloat64ToInt32(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitTryTruncateFloat64ToInt32(Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   InstructionOperand inputs[] = {g.UseRegister(node->InputAt(0))};
   InstructionOperand outputs[2];
   InstructionOperand temps[1];
@@ -1688,16 +1789,18 @@ void InstructionSelector::VisitTryTruncateFloat64ToInt32(Node* node) {
   Emit(kSSEFloat64ToInt32, output_count, outputs, 1, inputs, temp_count, temps);
 }
 
-void InstructionSelector::VisitBitcastWord32ToWord64(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitBitcastWord32ToWord64(Node* node) {
   DCHECK(SmiValuesAre31Bits());
   DCHECK(COMPRESS_POINTERS_BOOL);
   EmitIdentity(node);
 }
 
-void InstructionSelector::VisitChangeInt32ToInt64(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitChangeInt32ToInt64(Node* node) {
   DCHECK_EQ(node->InputCount(), 1);
 
-  X64OperandGenerator g(this);
+  X64OperandGeneratorT<Adapter> g(this);
   Node* const value = node->InputAt(0);
   if ((value->opcode() == IrOpcode::kLoad ||
        value->opcode() == IrOpcode::kLoadImmutable) &&
@@ -1739,8 +1842,10 @@ void InstructionSelector::VisitChangeInt32ToInt64(Node* node) {
   }
 }
 
-bool InstructionSelector::ZeroExtendsWord32ToWord64NoPhis(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+bool InstructionSelectorT<Adapter>::ZeroExtendsWord32ToWord64NoPhis(
+    Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   DCHECK_NE(node->opcode(), IrOpcode::kPhi);
   switch (node->opcode()) {
     case IrOpcode::kWord32And:
@@ -1810,8 +1915,9 @@ bool InstructionSelector::ZeroExtendsWord32ToWord64NoPhis(Node* node) {
   }
 }
 
-void InstructionSelector::VisitChangeUint32ToUint64(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitChangeUint32ToUint64(Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   Node* value = node->InputAt(0);
   if (ZeroExtendsWord32ToWord64(value)) {
     // These 32-bit operations implicitly zero-extend to 64-bit on x64, so the
@@ -1823,29 +1929,33 @@ void InstructionSelector::VisitChangeUint32ToUint64(Node* node) {
 
 namespace {
 
-void VisitRO(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitRO(InstructionSelectorT<Adapter>* selector, Node* node,
              InstructionCode opcode) {
-  X64OperandGenerator g(selector);
+  X64OperandGeneratorT<Adapter> g(selector);
   selector->Emit(opcode, g.DefineAsRegister(node), g.Use(node->InputAt(0)));
 }
 
-void VisitRR(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitRR(InstructionSelectorT<Adapter>* selector, Node* node,
              InstructionCode opcode) {
-  X64OperandGenerator g(selector);
+  X64OperandGeneratorT<Adapter> g(selector);
   selector->Emit(opcode, g.DefineAsRegister(node),
                  g.UseRegister(node->InputAt(0)));
 }
 
-void VisitRRO(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitRRO(InstructionSelectorT<Adapter>* selector, Node* node,
               InstructionCode opcode) {
-  X64OperandGenerator g(selector);
+  X64OperandGeneratorT<Adapter> g(selector);
   selector->Emit(opcode, g.DefineSameAsFirst(node),
                  g.UseRegister(node->InputAt(0)), g.Use(node->InputAt(1)));
 }
 
-void VisitFloatBinop(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitFloatBinop(InstructionSelectorT<Adapter>* selector, Node* node,
                      InstructionCode avx_opcode, InstructionCode sse_opcode) {
-  X64OperandGenerator g(selector);
+  X64OperandGeneratorT<Adapter> g(selector);
   Node* left = node->InputAt(0);
   Node* right = node->InputAt(1);
   InstructionOperand inputs[8];
@@ -1901,9 +2011,10 @@ void VisitFloatBinop(InstructionSelector* selector, Node* node,
   }
 }
 
-void VisitFloatUnop(InstructionSelector* selector, Node* node, Node* input,
-                    InstructionCode opcode) {
-  X64OperandGenerator g(selector);
+template <typename Adapter>
+void VisitFloatUnop(InstructionSelectorT<Adapter>* selector, Node* node,
+                    Node* input, InstructionCode opcode) {
+  X64OperandGeneratorT<Adapter> g(selector);
   if (selector->IsSupported(AVX)) {
     selector->Emit(opcode, g.DefineAsRegister(node), g.UseRegister(input));
   } else {
@@ -1974,31 +2085,35 @@ void VisitFloatUnop(InstructionSelector* selector, Node* node, Node* input,
   V(F64x2Trunc, kX64F64x2Round | MiscField::encode(kRoundToZero))             \
   V(F64x2NearestInt, kX64F64x2Round | MiscField::encode(kRoundToNearest))
 
-#define RO_VISITOR(Name, opcode)                      \
-  void InstructionSelector::Visit##Name(Node* node) { \
-    VisitRO(this, node, opcode);                      \
+#define RO_VISITOR(Name, opcode)                                \
+  template <typename Adapter>                                   \
+  void InstructionSelectorT<Adapter>::Visit##Name(Node* node) { \
+    VisitRO(this, node, opcode);                                \
   }
 RO_OP_LIST(RO_VISITOR)
 #undef RO_VISITOR
 #undef RO_OP_LIST
 
-#define RR_VISITOR(Name, opcode)                      \
-  void InstructionSelector::Visit##Name(Node* node) { \
-    VisitRR(this, node, opcode);                      \
+#define RR_VISITOR(Name, opcode)                                \
+  template <typename Adapter>                                   \
+  void InstructionSelectorT<Adapter>::Visit##Name(Node* node) { \
+    VisitRR(this, node, opcode);                                \
   }
 RR_OP_LIST(RR_VISITOR)
 #undef RR_VISITOR
 #undef RR_OP_LIST
 
-void InstructionSelector::VisitTruncateFloat64ToWord32(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitTruncateFloat64ToWord32(Node* node) {
   VisitRR(this, node, kArchTruncateDoubleToI);
 }
 
-void InstructionSelector::VisitTruncateInt64ToInt32(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitTruncateInt64ToInt32(Node* node) {
   // We rely on the fact that TruncateInt64ToInt32 zero extends the
   // value (see ZeroExtendsWord32ToWord64). So all code paths here
   // have to satisfy that condition.
-  X64OperandGenerator g(this);
+  X64OperandGeneratorT<Adapter> g(this);
   Node* value = node->InputAt(0);
   bool can_cover = false;
   if (value->opcode() == IrOpcode::kBitcastTaggedToWordForTagAndSmiBits) {
@@ -2035,106 +2150,129 @@ void InstructionSelector::VisitTruncateInt64ToInt32(Node* node) {
   Emit(kX64Movl, g.DefineAsRegister(node), g.Use(value));
 }
 
-void InstructionSelector::VisitFloat32Add(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat32Add(Node* node) {
   VisitFloatBinop(this, node, kAVXFloat32Add, kSSEFloat32Add);
 }
 
-void InstructionSelector::VisitFloat32Sub(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat32Sub(Node* node) {
   VisitFloatBinop(this, node, kAVXFloat32Sub, kSSEFloat32Sub);
 }
 
-void InstructionSelector::VisitFloat32Mul(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat32Mul(Node* node) {
   VisitFloatBinop(this, node, kAVXFloat32Mul, kSSEFloat32Mul);
 }
 
-void InstructionSelector::VisitFloat32Div(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat32Div(Node* node) {
   VisitFloatBinop(this, node, kAVXFloat32Div, kSSEFloat32Div);
 }
 
-void InstructionSelector::VisitFloat32Abs(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat32Abs(Node* node) {
   VisitFloatUnop(this, node, node->InputAt(0), kX64Float32Abs);
 }
 
-void InstructionSelector::VisitFloat32Max(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat32Max(Node* node) {
   VisitRRO(this, node, kSSEFloat32Max);
 }
 
-void InstructionSelector::VisitFloat32Min(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat32Min(Node* node) {
   VisitRRO(this, node, kSSEFloat32Min);
 }
 
-void InstructionSelector::VisitFloat64Add(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64Add(Node* node) {
   VisitFloatBinop(this, node, kAVXFloat64Add, kSSEFloat64Add);
 }
 
-void InstructionSelector::VisitFloat64Sub(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64Sub(Node* node) {
   VisitFloatBinop(this, node, kAVXFloat64Sub, kSSEFloat64Sub);
 }
 
-void InstructionSelector::VisitFloat64Mul(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64Mul(Node* node) {
   VisitFloatBinop(this, node, kAVXFloat64Mul, kSSEFloat64Mul);
 }
 
-void InstructionSelector::VisitFloat64Div(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64Div(Node* node) {
   VisitFloatBinop(this, node, kAVXFloat64Div, kSSEFloat64Div);
 }
 
-void InstructionSelector::VisitFloat64Mod(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64Mod(Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   InstructionOperand temps[] = {g.TempRegister(rax)};
   Emit(kSSEFloat64Mod, g.DefineSameAsFirst(node),
        g.UseRegister(node->InputAt(0)), g.UseRegister(node->InputAt(1)), 1,
        temps);
 }
 
-void InstructionSelector::VisitFloat64Max(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64Max(Node* node) {
   VisitRRO(this, node, kSSEFloat64Max);
 }
 
-void InstructionSelector::VisitFloat64Min(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64Min(Node* node) {
   VisitRRO(this, node, kSSEFloat64Min);
 }
 
-void InstructionSelector::VisitFloat64Abs(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64Abs(Node* node) {
   VisitFloatUnop(this, node, node->InputAt(0), kX64Float64Abs);
 }
 
-void InstructionSelector::VisitFloat64RoundTiesAway(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64RoundTiesAway(Node* node) {
   UNREACHABLE();
 }
 
-void InstructionSelector::VisitFloat32Neg(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat32Neg(Node* node) {
   VisitFloatUnop(this, node, node->InputAt(0), kX64Float32Neg);
 }
 
-void InstructionSelector::VisitFloat64Neg(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64Neg(Node* node) {
   VisitFloatUnop(this, node, node->InputAt(0), kX64Float64Neg);
 }
 
-void InstructionSelector::VisitFloat64Ieee754Binop(Node* node,
-                                                   InstructionCode opcode) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64Ieee754Binop(
+    Node* node, InstructionCode opcode) {
+  X64OperandGeneratorT<Adapter> g(this);
   Emit(opcode, g.DefineAsFixed(node, xmm0), g.UseFixed(node->InputAt(0), xmm0),
        g.UseFixed(node->InputAt(1), xmm1))
       ->MarkAsCall();
 }
 
-void InstructionSelector::VisitFloat64Ieee754Unop(Node* node,
-                                                  InstructionCode opcode) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64Ieee754Unop(
+    Node* node, InstructionCode opcode) {
+  X64OperandGeneratorT<Adapter> g(this);
   Emit(opcode, g.DefineAsFixed(node, xmm0), g.UseFixed(node->InputAt(0), xmm0))
       ->MarkAsCall();
 }
 
-void InstructionSelector::EmitMoveParamToFPR(Node* node, int index) {}
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::EmitMoveParamToFPR(Node* node, int index) {}
 
-void InstructionSelector::EmitMoveFPRToParam(InstructionOperand* op,
-                                             LinkageLocation location) {}
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::EmitMoveFPRToParam(
+    InstructionOperand* op, LinkageLocation location) {}
 
-void InstructionSelector::EmitPrepareArguments(
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::EmitPrepareArguments(
     ZoneVector<PushParameter>* arguments, const CallDescriptor* call_descriptor,
     Node* node) {
-  X64OperandGenerator g(this);
+  X64OperandGeneratorT<Adapter> g(this);
 
   // Prepare for C function call.
   if (call_descriptor->IsCFunctionCall()) {
@@ -2188,10 +2326,11 @@ void InstructionSelector::EmitPrepareArguments(
   }
 }
 
-void InstructionSelector::EmitPrepareResults(
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::EmitPrepareResults(
     ZoneVector<PushParameter>* results, const CallDescriptor* call_descriptor,
     Node* node) {
-  X64OperandGenerator g(this);
+  X64OperandGeneratorT<Adapter> g(this);
   for (PushParameter output : *results) {
     if (!output.location.IsCallerFrameSlot()) continue;
     // Skip any alignment holes in nodes.
@@ -2213,17 +2352,21 @@ void InstructionSelector::EmitPrepareResults(
   }
 }
 
-bool InstructionSelector::IsTailCallAddressImmediate() { return true; }
+template <typename Adapter>
+bool InstructionSelectorT<Adapter>::IsTailCallAddressImmediate() {
+  return true;
+}
 
 namespace {
 
-void VisitCompareWithMemoryOperand(InstructionSelector* selector,
+template <typename Adapter>
+void VisitCompareWithMemoryOperand(InstructionSelectorT<Adapter>* selector,
                                    InstructionCode opcode, Node* left,
                                    InstructionOperand right,
                                    FlagsContinuation* cont) {
   DCHECK(IrOpcode::kLoad == left->opcode() ||
          IrOpcode::kLoadImmutable == left->opcode());
-  X64OperandGenerator g(selector);
+  X64OperandGeneratorT<Adapter> g(selector);
   size_t input_count = 0;
   InstructionOperand inputs[6];
   AddressingMode addressing_mode =
@@ -2245,11 +2388,12 @@ void VisitCompareWithMemoryOperand(InstructionSelector* selector,
 }
 
 // Shared routine for multiple compare operations.
-void VisitCompare(InstructionSelector* selector, InstructionCode opcode,
-                  InstructionOperand left, InstructionOperand right,
-                  FlagsContinuation* cont) {
+template <typename Adapter>
+void VisitCompare(InstructionSelectorT<Adapter>* selector,
+                  InstructionCode opcode, InstructionOperand left,
+                  InstructionOperand right, FlagsContinuation* cont) {
   if (cont->IsSelect()) {
-    X64OperandGenerator g(selector);
+    X64OperandGeneratorT<Adapter> g(selector);
     InstructionOperand inputs[4] = {left, right};
     if (cont->condition() == kUnorderedEqual) {
       cont->Negate();
@@ -2266,10 +2410,11 @@ void VisitCompare(InstructionSelector* selector, InstructionCode opcode,
 }
 
 // Shared routine for multiple compare operations.
-void VisitCompare(InstructionSelector* selector, InstructionCode opcode,
-                  Node* left, Node* right, FlagsContinuation* cont,
-                  bool commutative) {
-  X64OperandGenerator g(selector);
+template <typename Adapter>
+void VisitCompare(InstructionSelectorT<Adapter>* selector,
+                  InstructionCode opcode, Node* left, Node* right,
+                  FlagsContinuation* cont, bool commutative) {
+  X64OperandGeneratorT<Adapter> g(selector);
   if (commutative && g.CanBeBetterLeftOperand(right)) {
     std::swap(left, right);
   }
@@ -2495,9 +2640,10 @@ void RemoveUnnecessaryWordAnd(InstructionCode opcode, Node** and_node) {
 }
 
 // Shared routine for multiple word compare operations.
-void VisitWordCompare(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitWordCompare(InstructionSelectorT<Adapter>* selector, Node* node,
                       InstructionCode opcode, FlagsContinuation* cont) {
-  X64OperandGenerator g(selector);
+  X64OperandGeneratorT<Adapter> g(selector);
   Node* left = node->InputAt(0);
   Node* right = node->InputAt(1);
 
@@ -2550,10 +2696,11 @@ void VisitWordCompare(InstructionSelector* selector, Node* node,
                       node->op()->HasProperty(Operator::kCommutative));
 }
 
-void VisitWord64EqualImpl(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitWord64EqualImpl(InstructionSelectorT<Adapter>* selector, Node* node,
                           FlagsContinuation* cont) {
   if (selector->CanUseRootsRegister()) {
-    X64OperandGenerator g(selector);
+    X64OperandGeneratorT<Adapter> g(selector);
     const RootsTable& roots_table = selector->isolate()->roots_table();
     RootIndex root_index;
     HeapObjectBinopMatcher m(node);
@@ -2571,10 +2718,11 @@ void VisitWord64EqualImpl(InstructionSelector* selector, Node* node,
   VisitWordCompare(selector, node, kX64Cmp, cont);
 }
 
-void VisitWord32EqualImpl(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitWord32EqualImpl(InstructionSelectorT<Adapter>* selector, Node* node,
                           FlagsContinuation* cont) {
   if (COMPRESS_POINTERS_BOOL && selector->isolate()) {
-    X64OperandGenerator g(selector);
+    X64OperandGeneratorT<Adapter> g(selector);
     const RootsTable& roots_table = selector->isolate()->roots_table();
     RootIndex root_index;
     Node* left = nullptr;
@@ -2619,9 +2767,11 @@ void VisitWord32EqualImpl(InstructionSelector* selector, Node* node,
 }
 
 // Shared routine for comparison with zero.
-void VisitCompareZero(InstructionSelector* selector, Node* user, Node* node,
-                      InstructionCode opcode, FlagsContinuation* cont) {
-  X64OperandGenerator g(selector);
+template <typename Adapter>
+void VisitCompareZero(InstructionSelectorT<Adapter>* selector, Node* user,
+                      Node* node, InstructionCode opcode,
+                      FlagsContinuation* cont) {
+  X64OperandGeneratorT<Adapter> g(selector);
   if (cont->IsBranch() &&
       (cont->condition() == kNotEqual || cont->condition() == kEqual)) {
     switch (node->opcode()) {
@@ -2644,8 +2794,8 @@ void VisitCompareZero(InstructionSelector* selector, Node* user, Node* node,
 #undef FLAGS_SET_BINOP_LIST
 #undef FLAGS_SET_BINOP
 
-#define TRY_VISIT_WORD32_SHIFT TryVisitWordShift<Int32BinopMatcher, 32>
-#define TRY_VISIT_WORD64_SHIFT TryVisitWordShift<Int64BinopMatcher, 64>
+#define TRY_VISIT_WORD32_SHIFT TryVisitWordShift<Adapter, Int32BinopMatcher, 32>
+#define TRY_VISIT_WORD64_SHIFT TryVisitWordShift<Adapter, Int64BinopMatcher, 64>
 // Skip Word64Sar/Word32Sar since no instruction reduction in most cases.
 #define FLAGS_SET_SHIFT_LIST(V)                    \
   V(kWord32Shl, TRY_VISIT_WORD32_SHIFT, kX64Shl32) \
@@ -2698,7 +2848,8 @@ void VisitCompareZero(InstructionSelector* selector, Node* user, Node* node,
 }
 
 // Shared routine for multiple float32 compare operations (inputs commuted).
-void VisitFloat32Compare(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitFloat32Compare(InstructionSelectorT<Adapter>* selector, Node* node,
                          FlagsContinuation* cont) {
   Node* const left = node->InputAt(0);
   Node* const right = node->InputAt(1);
@@ -2708,7 +2859,8 @@ void VisitFloat32Compare(InstructionSelector* selector, Node* node,
 }
 
 // Shared routine for multiple float64 compare operations (inputs commuted).
-void VisitFloat64Compare(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitFloat64Compare(InstructionSelectorT<Adapter>* selector, Node* node,
                          FlagsContinuation* cont) {
   Node* const left = node->InputAt(0);
   Node* const right = node->InputAt(1);
@@ -2718,10 +2870,11 @@ void VisitFloat64Compare(InstructionSelector* selector, Node* node,
 }
 
 // Shared routine for Word32/Word64 Atomic Binops
-void VisitAtomicBinop(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitAtomicBinop(InstructionSelectorT<Adapter>* selector, Node* node,
                       ArchOpcode opcode, AtomicWidth width,
                       MemoryAccessKind access_kind) {
-  X64OperandGenerator g(selector);
+  X64OperandGeneratorT<Adapter> g(selector);
   Node* base = node->InputAt(0);
   Node* index = node->InputAt(1);
   Node* value = node->InputAt(2);
@@ -2741,10 +2894,12 @@ void VisitAtomicBinop(InstructionSelector* selector, Node* node,
 }
 
 // Shared routine for Word32/Word64 Atomic CmpExchg
-void VisitAtomicCompareExchange(InstructionSelector* selector, Node* node,
-                                ArchOpcode opcode, AtomicWidth width,
+template <typename Adapter>
+void VisitAtomicCompareExchange(InstructionSelectorT<Adapter>* selector,
+                                Node* node, ArchOpcode opcode,
+                                AtomicWidth width,
                                 MemoryAccessKind access_kind) {
-  X64OperandGenerator g(selector);
+  X64OperandGeneratorT<Adapter> g(selector);
   Node* base = node->InputAt(0);
   Node* index = node->InputAt(1);
   Node* old_value = node->InputAt(2);
@@ -2766,8 +2921,9 @@ void VisitAtomicCompareExchange(InstructionSelector* selector, Node* node,
 }  // namespace
 
 // Shared routine for word comparison against zero.
-void InstructionSelector::VisitWordCompareZero(Node* user, Node* value,
-                                               FlagsContinuation* cont) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWordCompareZero(
+    Node* user, Node* value, FlagsContinuation* cont) {
   // Try to combine with comparisons against 0 by simply inverting the branch.
   while (value->opcode() == IrOpcode::kWord32Equal && CanCover(user, value)) {
     Int32BinopMatcher m(value);
@@ -2916,8 +3072,10 @@ void InstructionSelector::VisitWordCompareZero(Node* user, Node* value,
   VisitCompareZero(this, user, value, kX64Cmp32, cont);
 }
 
-void InstructionSelector::VisitSwitch(Node* node, const SwitchInfo& sw) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitSwitch(Node* node,
+                                                const SwitchInfo& sw) {
+  X64OperandGeneratorT<Adapter> g(this);
   InstructionOperand value_operand = g.UseRegister(node->InputAt(0));
 
   // Emit either ArchTableSwitch or ArchBinarySearchSwitch.
@@ -2956,7 +3114,8 @@ void InstructionSelector::VisitSwitch(Node* node, const SwitchInfo& sw) {
   return EmitBinarySearchSwitch(sw, value_operand);
 }
 
-void InstructionSelector::VisitWord32Equal(Node* const node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32Equal(Node* const node) {
   Node* user = node;
   FlagsContinuation cont = FlagsContinuation::ForSet(kEqual, node);
   Int32BinopMatcher m(user);
@@ -2966,29 +3125,34 @@ void InstructionSelector::VisitWord32Equal(Node* const node) {
   VisitWord32EqualImpl(this, node, &cont);
 }
 
-void InstructionSelector::VisitInt32LessThan(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32LessThan(Node* node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kSignedLessThan, node);
   VisitWordCompare(this, node, kX64Cmp32, &cont);
 }
 
-void InstructionSelector::VisitInt32LessThanOrEqual(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32LessThanOrEqual(Node* node) {
   FlagsContinuation cont =
       FlagsContinuation::ForSet(kSignedLessThanOrEqual, node);
   VisitWordCompare(this, node, kX64Cmp32, &cont);
 }
 
-void InstructionSelector::VisitUint32LessThan(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitUint32LessThan(Node* node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kUnsignedLessThan, node);
   VisitWordCompare(this, node, kX64Cmp32, &cont);
 }
 
-void InstructionSelector::VisitUint32LessThanOrEqual(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitUint32LessThanOrEqual(Node* node) {
   FlagsContinuation cont =
       FlagsContinuation::ForSet(kUnsignedLessThanOrEqual, node);
   VisitWordCompare(this, node, kX64Cmp32, &cont);
 }
 
-void InstructionSelector::VisitWord64Equal(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64Equal(Node* node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kEqual, node);
   Int64BinopMatcher m(node);
   if (m.right().Is(0)) {
@@ -3009,7 +3173,8 @@ void InstructionSelector::VisitWord64Equal(Node* node) {
   VisitWord64EqualImpl(this, node, &cont);
 }
 
-void InstructionSelector::VisitInt32AddWithOverflow(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32AddWithOverflow(Node* node) {
   if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
     FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
     return VisitBinop(this, node, kX64Add32, &cont);
@@ -3018,7 +3183,8 @@ void InstructionSelector::VisitInt32AddWithOverflow(Node* node) {
   VisitBinop(this, node, kX64Add32, &cont);
 }
 
-void InstructionSelector::VisitInt32SubWithOverflow(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32SubWithOverflow(Node* node) {
   if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
     FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
     return VisitBinop(this, node, kX64Sub32, &cont);
@@ -3027,51 +3193,60 @@ void InstructionSelector::VisitInt32SubWithOverflow(Node* node) {
   VisitBinop(this, node, kX64Sub32, &cont);
 }
 
-void InstructionSelector::VisitInt64LessThan(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt64LessThan(Node* node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kSignedLessThan, node);
   VisitWordCompare(this, node, kX64Cmp, &cont);
 }
 
-void InstructionSelector::VisitInt64LessThanOrEqual(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt64LessThanOrEqual(Node* node) {
   FlagsContinuation cont =
       FlagsContinuation::ForSet(kSignedLessThanOrEqual, node);
   VisitWordCompare(this, node, kX64Cmp, &cont);
 }
 
-void InstructionSelector::VisitUint64LessThan(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitUint64LessThan(Node* node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kUnsignedLessThan, node);
   VisitWordCompare(this, node, kX64Cmp, &cont);
 }
 
-void InstructionSelector::VisitUint64LessThanOrEqual(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitUint64LessThanOrEqual(Node* node) {
   FlagsContinuation cont =
       FlagsContinuation::ForSet(kUnsignedLessThanOrEqual, node);
   VisitWordCompare(this, node, kX64Cmp, &cont);
 }
 
-void InstructionSelector::VisitFloat32Equal(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat32Equal(Node* node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kUnorderedEqual, node);
   VisitFloat32Compare(this, node, &cont);
 }
 
-void InstructionSelector::VisitFloat32LessThan(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat32LessThan(Node* node) {
   FlagsContinuation cont =
       FlagsContinuation::ForSet(kUnsignedGreaterThan, node);
   VisitFloat32Compare(this, node, &cont);
 }
 
-void InstructionSelector::VisitFloat32LessThanOrEqual(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat32LessThanOrEqual(Node* node) {
   FlagsContinuation cont =
       FlagsContinuation::ForSet(kUnsignedGreaterThanOrEqual, node);
   VisitFloat32Compare(this, node, &cont);
 }
 
-void InstructionSelector::VisitFloat64Equal(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64Equal(Node* node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kUnorderedEqual, node);
   VisitFloat64Compare(this, node, &cont);
 }
 
-void InstructionSelector::VisitFloat64LessThan(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64LessThan(Node* node) {
   Float64BinopMatcher m(node);
   if (m.left().Is(0.0) && m.right().IsFloat64Abs()) {
     // This matches the pattern
@@ -3093,14 +3268,16 @@ void InstructionSelector::VisitFloat64LessThan(Node* node) {
   VisitFloat64Compare(this, node, &cont);
 }
 
-void InstructionSelector::VisitFloat64LessThanOrEqual(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64LessThanOrEqual(Node* node) {
   FlagsContinuation cont =
       FlagsContinuation::ForSet(kUnsignedGreaterThanOrEqual, node);
   VisitFloat64Compare(this, node, &cont);
 }
 
-void InstructionSelector::VisitFloat64InsertLowWord32(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64InsertLowWord32(Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   Node* left = node->InputAt(0);
   Node* right = node->InputAt(1);
   Float64Matcher mleft(left);
@@ -3113,33 +3290,37 @@ void InstructionSelector::VisitFloat64InsertLowWord32(Node* node) {
        g.UseRegister(left), g.Use(right));
 }
 
-void InstructionSelector::VisitFloat64InsertHighWord32(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64InsertHighWord32(Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   Node* left = node->InputAt(0);
   Node* right = node->InputAt(1);
   Emit(kSSEFloat64InsertHighWord32, g.DefineSameAsFirst(node),
        g.UseRegister(left), g.Use(right));
 }
 
-void InstructionSelector::VisitFloat64SilenceNaN(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64SilenceNaN(Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   Emit(kSSEFloat64SilenceNaN, g.DefineSameAsFirst(node),
        g.UseRegister(node->InputAt(0)));
 }
 
-void InstructionSelector::VisitMemoryBarrier(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitMemoryBarrier(Node* node) {
   // x64 is no weaker than release-acquire and only needs to emit an instruction
   // for SeqCst memory barriers.
   AtomicMemoryOrder order = OpParameter<AtomicMemoryOrder>(node->op());
   if (order == AtomicMemoryOrder::kSeqCst) {
-    X64OperandGenerator g(this);
+    X64OperandGeneratorT<Adapter> g(this);
     Emit(kX64MFence, g.NoOutput());
     return;
   }
   DCHECK_EQ(AtomicMemoryOrder::kAcqRel, order);
 }
 
-void InstructionSelector::VisitWord32AtomicLoad(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32AtomicLoad(Node* node) {
   AtomicLoadParameters atomic_load_params = AtomicLoadParametersOf(node->op());
   LoadRepresentation load_rep = atomic_load_params.representation();
   DCHECK(IsIntegral(load_rep.representation()) ||
@@ -3154,7 +3335,8 @@ void InstructionSelector::VisitWord32AtomicLoad(Node* node) {
   VisitLoad(node, node, GetLoadOpcode(load_rep));
 }
 
-void InstructionSelector::VisitWord64AtomicLoad(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64AtomicLoad(Node* node) {
   AtomicLoadParameters atomic_load_params = AtomicLoadParametersOf(node->op());
   DCHECK(!atomic_load_params.representation().IsMapWord());
   // The memory order is ignored as both acquire and sequentially consistent
@@ -3163,7 +3345,8 @@ void InstructionSelector::VisitWord64AtomicLoad(Node* node) {
   VisitLoad(node, node, GetLoadOpcode(atomic_load_params.representation()));
 }
 
-void InstructionSelector::VisitWord32AtomicStore(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32AtomicStore(Node* node) {
   AtomicStoreParameters params = AtomicStoreParametersOf(node->op());
   DCHECK_NE(params.representation(), MachineRepresentation::kWord64);
   DCHECK_IMPLIES(CanBeTaggedOrCompressedPointer(params.representation()),
@@ -3172,7 +3355,8 @@ void InstructionSelector::VisitWord32AtomicStore(Node* node) {
                    params.kind());
 }
 
-void InstructionSelector::VisitWord64AtomicStore(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64AtomicStore(Node* node) {
   AtomicStoreParameters params = AtomicStoreParametersOf(node->op());
   DCHECK_IMPLIES(CanBeTaggedOrCompressedPointer(params.representation()),
                  kTaggedSize == 8);
@@ -3180,7 +3364,8 @@ void InstructionSelector::VisitWord64AtomicStore(Node* node) {
                    params.kind());
 }
 
-void InstructionSelector::VisitWord32AtomicExchange(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32AtomicExchange(Node* node) {
   AtomicOpParameters params = AtomicOpParametersOf(node->op());
   ArchOpcode opcode;
   if (params.type() == MachineType::Int8()) {
@@ -3200,7 +3385,8 @@ void InstructionSelector::VisitWord32AtomicExchange(Node* node) {
   VisitAtomicExchange(this, node, opcode, AtomicWidth::kWord32, params.kind());
 }
 
-void InstructionSelector::VisitWord64AtomicExchange(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64AtomicExchange(Node* node) {
   AtomicOpParameters params = AtomicOpParametersOf(node->op());
   ArchOpcode opcode;
   if (params.type() == MachineType::Uint8()) {
@@ -3217,7 +3403,9 @@ void InstructionSelector::VisitWord64AtomicExchange(Node* node) {
   VisitAtomicExchange(this, node, opcode, AtomicWidth::kWord64, params.kind());
 }
 
-void InstructionSelector::VisitWord32AtomicCompareExchange(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32AtomicCompareExchange(
+    Node* node) {
   AtomicOpParameters params = AtomicOpParametersOf(node->op());
   ArchOpcode opcode;
   if (params.type() == MachineType::Int8()) {
@@ -3238,7 +3426,9 @@ void InstructionSelector::VisitWord32AtomicCompareExchange(Node* node) {
                              params.kind());
 }
 
-void InstructionSelector::VisitWord64AtomicCompareExchange(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64AtomicCompareExchange(
+    Node* node) {
   AtomicOpParameters params = AtomicOpParametersOf(node->op());
   ArchOpcode opcode;
   if (params.type() == MachineType::Uint8()) {
@@ -3256,7 +3446,8 @@ void InstructionSelector::VisitWord64AtomicCompareExchange(Node* node) {
                              params.kind());
 }
 
-void InstructionSelector::VisitWord32AtomicBinaryOperation(
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32AtomicBinaryOperation(
     Node* node, ArchOpcode int8_op, ArchOpcode uint8_op, ArchOpcode int16_op,
     ArchOpcode uint16_op, ArchOpcode word32_op) {
   AtomicOpParameters params = AtomicOpParametersOf(node->op());
@@ -3278,11 +3469,12 @@ void InstructionSelector::VisitWord32AtomicBinaryOperation(
   VisitAtomicBinop(this, node, opcode, AtomicWidth::kWord32, params.kind());
 }
 
-#define VISIT_ATOMIC_BINOP(op)                                           \
-  void InstructionSelector::VisitWord32Atomic##op(Node* node) {          \
-    VisitWord32AtomicBinaryOperation(                                    \
-        node, kAtomic##op##Int8, kAtomic##op##Uint8, kAtomic##op##Int16, \
-        kAtomic##op##Uint16, kAtomic##op##Word32);                       \
+#define VISIT_ATOMIC_BINOP(op)                                            \
+  template <typename Adapter>                                             \
+  void InstructionSelectorT<Adapter>::VisitWord32Atomic##op(Node* node) { \
+    VisitWord32AtomicBinaryOperation(                                     \
+        node, kAtomic##op##Int8, kAtomic##op##Uint8, kAtomic##op##Int16,  \
+        kAtomic##op##Uint16, kAtomic##op##Word32);                        \
   }
 VISIT_ATOMIC_BINOP(Add)
 VISIT_ATOMIC_BINOP(Sub)
@@ -3291,7 +3483,8 @@ VISIT_ATOMIC_BINOP(Or)
 VISIT_ATOMIC_BINOP(Xor)
 #undef VISIT_ATOMIC_BINOP
 
-void InstructionSelector::VisitWord64AtomicBinaryOperation(
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64AtomicBinaryOperation(
     Node* node, ArchOpcode uint8_op, ArchOpcode uint16_op, ArchOpcode uint32_op,
     ArchOpcode word64_op) {
   AtomicOpParameters params = AtomicOpParametersOf(node->op());
@@ -3311,7 +3504,8 @@ void InstructionSelector::VisitWord64AtomicBinaryOperation(
 }
 
 #define VISIT_ATOMIC_BINOP(op)                                                 \
-  void InstructionSelector::VisitWord64Atomic##op(Node* node) {                \
+  template <typename Adapter>                                                  \
+  void InstructionSelectorT<Adapter>::VisitWord64Atomic##op(Node* node) {      \
     VisitWord64AtomicBinaryOperation(node, kAtomic##op##Uint8,                 \
                                      kAtomic##op##Uint16, kAtomic##op##Word32, \
                                      kX64Word64Atomic##op##Uint64);            \
@@ -3591,8 +3785,9 @@ VISIT_ATOMIC_BINOP(Xor)
   V(I8x16ShrS, IShrS, kL8, kV128)                            \
   V(I8x16ShrU, IShrU, kL8, kV128)
 
-void InstructionSelector::VisitS128Const(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitS128Const(Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   static const int kUint32Immediates = kSimd128Size / sizeof(uint32_t);
   uint32_t val[kUint32Immediates];
   memcpy(val, S128ImmediateParameterOf(node->op()).data(), kSimd128Size);
@@ -3611,13 +3806,15 @@ void InstructionSelector::VisitS128Const(Node* node) {
   }
 }
 
-void InstructionSelector::VisitS128Zero(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitS128Zero(Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   Emit(kX64SZero | VectorLengthField::encode(kV128), g.DefineAsRegister(node));
 }
 
-void InstructionSelector::VisitS256Zero(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitS256Zero(Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   Emit(kX64SZero | VectorLengthField::encode(kV256), g.DefineAsRegister(node));
 }
 // Name, LaneSize, VectorLength
@@ -3633,8 +3830,9 @@ void InstructionSelector::VisitS256Zero(Node* node) {
 
 // Splat with an optimization for const 0.
 #define VISIT_INT_SIMD_SPLAT(Type, LaneSize, VectorLength)                   \
-  void InstructionSelector::Visit##Type##Splat(Node* node) {                 \
-    X64OperandGenerator g(this);                                             \
+  template <typename Adapter>                                                \
+  void InstructionSelectorT<Adapter>::Visit##Type##Splat(Node* node) {       \
+    X64OperandGeneratorT<Adapter> g(this);                                   \
     Node* input = node->InputAt(0);                                          \
     if (g.CanBeImmediate(input) && g.GetImmediateIntegerValue(input) == 0) { \
       Emit(kX64SZero | VectorLengthField::encode(kV128),                     \
@@ -3649,23 +3847,27 @@ SIMD_INT_TYPES_FOR_SPLAT(VISIT_INT_SIMD_SPLAT)
 #undef VISIT_INT_SIMD_SPLAT
 #undef SIMD_INT_TYPES_FOR_SPLAT
 
-void InstructionSelector::VisitF64x2Splat(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF64x2Splat(Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   Emit(kX64FSplat | LaneSizeField::encode(kL64) |
            VectorLengthField::encode(kV128),
        g.DefineAsRegister(node), g.Use(node->InputAt(0)));
 }
 
-void InstructionSelector::VisitF32x4Splat(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF32x4Splat(Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   Emit(kX64FSplat | LaneSizeField::encode(kL32) |
            VectorLengthField::encode(kV128),
        g.DefineAsRegister(node), g.UseRegister(node->InputAt(0)));
 }
 
 #define SIMD_VISIT_EXTRACT_LANE(IF, Type, Sign, LaneSize, VectorLength)  \
-  void InstructionSelector::Visit##Type##ExtractLane##Sign(Node* node) { \
-    X64OperandGenerator g(this);                                         \
+  template <typename Adapter>                                            \
+  void InstructionSelectorT<Adapter>::Visit##Type##ExtractLane##Sign(    \
+      Node* node) {                                                      \
+    X64OperandGeneratorT<Adapter> g(this);                               \
     int32_t lane = OpParameter<int32_t>(node->op());                     \
     Emit(kX64##IF##ExtractLane##Sign | LaneSizeField::encode(LaneSize) | \
              VectorLengthField::encode(VectorLength),                    \
@@ -3680,22 +3882,25 @@ SIMD_VISIT_EXTRACT_LANE(I, I16x8, S, kL16, kV128)
 SIMD_VISIT_EXTRACT_LANE(I, I8x16, S, kL8, kV128)
 #undef SIMD_VISIT_EXTRACT_LANE
 
-void InstructionSelector::VisitI16x8ExtractLaneU(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI16x8ExtractLaneU(Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   int32_t lane = OpParameter<int32_t>(node->op());
   Emit(kX64Pextrw, g.DefineAsRegister(node), g.UseRegister(node->InputAt(0)),
        g.UseImmediate(lane));
 }
 
-void InstructionSelector::VisitI8x16ExtractLaneU(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI8x16ExtractLaneU(Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   int32_t lane = OpParameter<int32_t>(node->op());
   Emit(kX64Pextrb, g.DefineAsRegister(node), g.UseRegister(node->InputAt(0)),
        g.UseImmediate(lane));
 }
 
-void InstructionSelector::VisitF32x4ReplaceLane(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF32x4ReplaceLane(Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   int32_t lane = OpParameter<int32_t>(node->op());
   Emit(kX64FReplaceLane | LaneSizeField::encode(kL32) |
            VectorLengthField::encode(kV128),
@@ -3703,8 +3908,9 @@ void InstructionSelector::VisitF32x4ReplaceLane(Node* node) {
        g.UseImmediate(lane), g.Use(node->InputAt(1)));
 }
 
-void InstructionSelector::VisitF64x2ReplaceLane(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF64x2ReplaceLane(Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   int32_t lane = OpParameter<int32_t>(node->op());
   // When no-AVX, define dst == src to save a move.
   InstructionOperand dst =
@@ -3715,12 +3921,13 @@ void InstructionSelector::VisitF64x2ReplaceLane(Node* node) {
        g.UseRegister(node->InputAt(1)));
 }
 
-#define VISIT_SIMD_REPLACE_LANE(TYPE, OPCODE)                               \
-  void InstructionSelector::Visit##TYPE##ReplaceLane(Node* node) {          \
-    X64OperandGenerator g(this);                                            \
-    int32_t lane = OpParameter<int32_t>(node->op());                        \
-    Emit(OPCODE, g.DefineAsRegister(node), g.UseRegister(node->InputAt(0)), \
-         g.UseImmediate(lane), g.Use(node->InputAt(1)));                    \
+#define VISIT_SIMD_REPLACE_LANE(TYPE, OPCODE)                                \
+  template <typename Adapter>                                                \
+  void InstructionSelectorT<Adapter>::Visit##TYPE##ReplaceLane(Node* node) { \
+    X64OperandGeneratorT<Adapter> g(this);                                   \
+    int32_t lane = OpParameter<int32_t>(node->op());                         \
+    Emit(OPCODE, g.DefineAsRegister(node), g.UseRegister(node->InputAt(0)),  \
+         g.UseImmediate(lane), g.Use(node->InputAt(1)));                     \
   }
 
 #define SIMD_TYPES_FOR_REPLACE_LANE(V) \
@@ -3735,8 +3942,9 @@ SIMD_TYPES_FOR_REPLACE_LANE(VISIT_SIMD_REPLACE_LANE)
 
 #define VISIT_SIMD_SHIFT_LANE_SIZE_VECTOR_LENGTH_OPCODES(                  \
     Name, Opcode, LaneSize, VectorLength)                                  \
-  void InstructionSelector::Visit##Name(Node* node) {                      \
-    X64OperandGenerator g(this);                                           \
+  template <typename Adapter>                                              \
+  void InstructionSelectorT<Adapter>::Visit##Name(Node* node) {            \
+    X64OperandGeneratorT<Adapter> g(this);                                 \
     InstructionOperand dst = IsSupported(AVX) ? g.DefineAsRegister(node)   \
                                               : g.DefineSameAsFirst(node); \
     if (g.CanBeImmediate(node->InputAt(1))) {                              \
@@ -3759,8 +3967,9 @@ SIMD_SHIFT_LANE_SIZE_VECTOR_LENGTH_OPCODES(
 
 #define VISIT_SIMD_NARROW_SHIFT_LANE_SIZE_VECTOR_LENGTH_OPCODES(            \
     Name, Opcode, LaneSize, VectorLength)                                   \
-  void InstructionSelector::Visit##Name(Node* node) {                       \
-    X64OperandGenerator g(this);                                            \
+  template <typename Adapter>                                               \
+  void InstructionSelectorT<Adapter>::Visit##Name(Node* node) {             \
+    X64OperandGeneratorT<Adapter> g(this);                                  \
     InstructionOperand output =                                             \
         IsSupported(AVX) ? g.UseRegister(node) : g.DefineSameAsFirst(node); \
     if (g.CanBeImmediate(node->InputAt(1))) {                               \
@@ -3781,11 +3990,12 @@ SIMD_NARROW_SHIFT_LANE_SIZE_VECTOR_LENGTH_OPCODES(
 #undef VISIT_SIMD_NARROW_SHIFT_LANE_SIZE_VECTOR_LENGTH_OPCODES
 #undef SIMD_NARROW_SHIFT_LANE_SIZE_VECTOR_LENGTH_OPCODES
 
-#define VISIT_SIMD_UNOP(Opcode)                         \
-  void InstructionSelector::Visit##Opcode(Node* node) { \
-    X64OperandGenerator g(this);                        \
-    Emit(kX64##Opcode, g.DefineAsRegister(node),        \
-         g.UseRegister(node->InputAt(0)));              \
+#define VISIT_SIMD_UNOP(Opcode)                                   \
+  template <typename Adapter>                                     \
+  void InstructionSelectorT<Adapter>::Visit##Opcode(Node* node) { \
+    X64OperandGeneratorT<Adapter> g(this);                        \
+    Emit(kX64##Opcode, g.DefineAsRegister(node),                  \
+         g.UseRegister(node->InputAt(0)));                        \
   }
 SIMD_UNOP_LIST(VISIT_SIMD_UNOP)
 #undef VISIT_SIMD_UNOP
@@ -3793,8 +4003,9 @@ SIMD_UNOP_LIST(VISIT_SIMD_UNOP)
 
 #define VISIT_SIMD_UNOP_LANE_SIZE_VECTOR_LENGTH(Name, Opcode, LaneSize, \
                                                 VectorLength)           \
-  void InstructionSelector::Visit##Name(Node* node) {                   \
-    X64OperandGenerator g(this);                                        \
+  template <typename Adapter>                                           \
+  void InstructionSelectorT<Adapter>::Visit##Name(Node* node) {         \
+    X64OperandGeneratorT<Adapter> g(this);                              \
     Emit(kX64##Opcode | LaneSizeField::encode(LaneSize) |               \
              VectorLengthField::encode(VectorLength),                   \
          g.DefineAsRegister(node), g.UseRegister(node->InputAt(0)));    \
@@ -3807,8 +4018,9 @@ SIMD_UNOP_LANE_SIZE_VECTOR_LENGTH_LIST(VISIT_SIMD_UNOP_LANE_SIZE_VECTOR_LENGTH)
 
 #define VISIT_SIMD_BINOP_LANE_SIZE_VECTOR_LENGTH(Name, Opcode, LaneSize, \
                                                  VectorLength)           \
-  void InstructionSelector::Visit##Name(Node* node) {                    \
-    X64OperandGenerator g(this);                                         \
+  template <typename Adapter>                                            \
+  void InstructionSelectorT<Adapter>::Visit##Name(Node* node) {          \
+    X64OperandGeneratorT<Adapter> g(this);                               \
     Emit(kX64##Opcode | LaneSizeField::encode(LaneSize) |                \
              VectorLengthField::encode(VectorLength),                    \
          g.DefineSameAsFirst(node), g.UseRegister(node->InputAt(0)),     \
@@ -3822,8 +4034,9 @@ SIMD_BINOP_LANE_SIZE_VECTOR_LENGTH_LIST(
 #undef SIMD_BINOP_LANE_SIZE_VECTOR_LENGTH_LIST
 
 #define VISIT_SIMD_BINOP(Opcode)                                              \
-  void InstructionSelector::Visit##Opcode(Node* node) {                       \
-    X64OperandGenerator g(this);                                              \
+  template <typename Adapter>                                                 \
+  void InstructionSelectorT<Adapter>::Visit##Opcode(Node* node) {             \
+    X64OperandGeneratorT<Adapter> g(this);                                    \
     if (IsSupported(AVX)) {                                                   \
       Emit(kX64##Opcode, g.DefineAsRegister(node),                            \
            g.UseRegister(node->InputAt(0)), g.UseRegister(node->InputAt(1))); \
@@ -3839,8 +4052,9 @@ SIMD_BINOP_SSE_AVX_LIST(VISIT_SIMD_BINOP)
 
 #define VISIT_SIMD_BINOP_LANE_SIZE_VECTOR_LENGTH(Name, Opcode, LaneSize, \
                                                  VectorLength)           \
-  void InstructionSelector::Visit##Name(Node* node) {                    \
-    X64OperandGenerator g(this);                                         \
+  template <typename Adapter>                                            \
+  void InstructionSelectorT<Adapter>::Visit##Name(Node* node) {          \
+    X64OperandGeneratorT<Adapter> g(this);                               \
     if (IsSupported(AVX)) {                                              \
       Emit(kX64##Opcode | LaneSizeField::encode(LaneSize) |              \
                VectorLengthField::encode(VectorLength),                  \
@@ -3859,14 +4073,16 @@ SIMD_BINOP_SSE_AVX_LANE_SIZE_VECTOR_LENGTH_LIST(
 #undef VISIT_SIMD_BINOP_LANE_SIZE_VECTOR_LENGTH
 #undef SIMD_BINOP_SSE_AVX_LANE_SIZE_VECTOR_LENGTH_LIST
 
-void InstructionSelector::VisitV128AnyTrue(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitV128AnyTrue(Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   Emit(kX64V128AnyTrue, g.DefineAsRegister(node),
        g.UseUniqueRegister(node->InputAt(0)));
 }
 
-void InstructionSelector::VisitS128Select(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitS128Select(Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   InstructionOperand dst =
       IsSupported(AVX) ? g.DefineAsRegister(node) : g.DefineSameAsFirst(node);
   Emit(kX64SSelect | VectorLengthField::encode(kV128), dst,
@@ -3874,50 +4090,57 @@ void InstructionSelector::VisitS128Select(Node* node) {
        g.UseRegister(node->InputAt(2)));
 }
 
-void InstructionSelector::VisitS256Select(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitS256Select(Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   Emit(kX64SSelect | VectorLengthField::encode(kV256), g.DefineAsRegister(node),
        g.UseRegister(node->InputAt(0)), g.UseRegister(node->InputAt(1)),
        g.UseRegister(node->InputAt(2)));
 }
 
-void InstructionSelector::VisitS128AndNot(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitS128AndNot(Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   // andnps a b does ~a & b, but we want a & !b, so flip the input.
   Emit(kX64SAndNot | VectorLengthField::encode(kV128),
        g.DefineSameAsFirst(node), g.UseRegister(node->InputAt(1)),
        g.UseRegister(node->InputAt(0)));
 }
 
-void InstructionSelector::VisitS256AndNot(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitS256AndNot(Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   // andnps a b does ~a & b, but we want a & !b, so flip the input.
   Emit(kX64SAndNot | VectorLengthField::encode(kV256),
        g.DefineSameAsFirst(node), g.UseRegister(node->InputAt(1)),
        g.UseRegister(node->InputAt(0)));
 }
 
-void InstructionSelector::VisitF64x2Abs(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF64x2Abs(Node* node) {
   VisitFloatUnop(this, node, node->InputAt(0),
                  kX64FAbs | LaneSizeField::encode(kL64) |
                      VectorLengthField::encode(kV128));
 }
 
-void InstructionSelector::VisitF64x2Neg(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF64x2Neg(Node* node) {
   VisitFloatUnop(this, node, node->InputAt(0),
                  kX64FNeg | LaneSizeField::encode(kL64) |
                      VectorLengthField::encode(kV128));
 }
 
-void InstructionSelector::VisitF32x4UConvertI32x4(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF32x4UConvertI32x4(Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   Emit(kX64F32x4UConvertI32x4, g.DefineSameAsFirst(node),
        g.UseRegister(node->InputAt(0)));
 }
 
 #define VISIT_SIMD_QFMOP(Opcode)                                             \
-  void InstructionSelector::Visit##Opcode(Node* node) {                      \
-    X64OperandGenerator g(this);                                             \
+  template <typename Adapter>                                                \
+  void InstructionSelectorT<Adapter>::Visit##Opcode(Node* node) {            \
+    X64OperandGeneratorT<Adapter> g(this);                                   \
     Emit(kX64##Opcode, g.UseRegister(node), g.UseRegister(node->InputAt(0)), \
          g.UseRegister(node->InputAt(1)), g.UseRegister(node->InputAt(2)));  \
   }
@@ -3927,8 +4150,9 @@ VISIT_SIMD_QFMOP(F32x4Qfma)
 VISIT_SIMD_QFMOP(F32x4Qfms)
 #undef VISIT_SIMD_QFMOP
 
-void InstructionSelector::VisitI64x2Neg(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI64x2Neg(Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   // If AVX unsupported, make sure dst != src to avoid a move.
   InstructionOperand operand0 = IsSupported(AVX)
                                     ? g.UseRegister(node->InputAt(0))
@@ -3938,8 +4162,9 @@ void InstructionSelector::VisitI64x2Neg(Node* node) {
       g.DefineAsRegister(node), operand0);
 }
 
-void InstructionSelector::VisitI64x2ShrS(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI64x2ShrS(Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   InstructionOperand dst =
       IsSupported(AVX) ? g.DefineAsRegister(node) : g.DefineSameAsFirst(node);
 
@@ -3957,8 +4182,9 @@ void InstructionSelector::VisitI64x2ShrS(Node* node) {
   }
 }
 
-void InstructionSelector::VisitI64x2Mul(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI64x2Mul(Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   InstructionOperand temps[] = {g.TempSimd128Register()};
   Emit(
       kX64IMul | LaneSizeField::encode(kL64) | VectorLengthField::encode(kV128),
@@ -3966,8 +4192,9 @@ void InstructionSelector::VisitI64x2Mul(Node* node) {
       g.UseUniqueRegister(node->InputAt(1)), arraysize(temps), temps);
 }
 
-void InstructionSelector::VisitI64x4Mul(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI64x4Mul(Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   InstructionOperand temps[] = {g.TempSimd256Register()};
   Emit(
       kX64IMul | LaneSizeField::encode(kL64) | VectorLengthField::encode(kV256),
@@ -3975,26 +4202,30 @@ void InstructionSelector::VisitI64x4Mul(Node* node) {
       g.UseUniqueRegister(node->InputAt(1)), arraysize(temps), temps);
 }
 
-void InstructionSelector::VisitI32x4SConvertF32x4(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI32x4SConvertF32x4(Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   Emit(kX64I32x4SConvertF32x4,
        IsSupported(AVX) ? g.DefineAsRegister(node) : g.DefineSameAsFirst(node),
        g.UseRegister(node->InputAt(0)));
 }
 
-void InstructionSelector::VisitI32x4UConvertF32x4(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI32x4UConvertF32x4(Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   InstructionOperand temps[] = {g.TempSimd128Register(),
                                 g.TempSimd128Register()};
   Emit(kX64I32x4UConvertF32x4, g.DefineSameAsFirst(node),
        g.UseRegister(node->InputAt(0)), arraysize(temps), temps);
 }
 
-void InstructionSelector::VisitInt32AbsWithOverflow(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32AbsWithOverflow(Node* node) {
   UNREACHABLE();
 }
 
-void InstructionSelector::VisitInt64AbsWithOverflow(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt64AbsWithOverflow(Node* node) {
   UNREACHABLE();
 }
 
@@ -4167,7 +4398,8 @@ static bool TryMatchOneInputIsZeros(Node* node, uint8_t* shuffle,
 
 }  // namespace
 
-void InstructionSelector::VisitI8x16Shuffle(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI8x16Shuffle(Node* node) {
   uint8_t shuffle[kSimd128Size];
   bool is_swizzle;
   CanonicalizeShuffle(node, shuffle, &is_swizzle);
@@ -4179,7 +4411,7 @@ void InstructionSelector::VisitI8x16Shuffle(Node* node) {
   static const int kMaxTemps = 2;
   InstructionOperand temps[kMaxTemps];
 
-  X64OperandGenerator g(this);
+  X64OperandGeneratorT<Adapter> g(this);
   // Swizzles don't generally need DefineSameAsFirst to avoid a move.
   bool no_same_as_first = is_swizzle;
   // We generally need UseRegister for input0, Use for input1.
@@ -4359,11 +4591,15 @@ void InstructionSelector::VisitI8x16Shuffle(Node* node) {
   Emit(opcode, 1, &dst, input_count, inputs, temp_count, temps);
 }
 #else
-void InstructionSelector::VisitI8x16Shuffle(Node* node) { UNREACHABLE(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI8x16Shuffle(Node* node) {
+  UNREACHABLE();
+}
 #endif  // V8_ENABLE_WEBASSEMBLY
 
 #if V8_ENABLE_WEBASSEMBLY
-void InstructionSelector::VisitI8x16Swizzle(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI8x16Swizzle(Node* node) {
   InstructionCode op = kX64I8x16Swizzle;
 
   bool relaxed = OpParameter<bool>(node->op());
@@ -4380,16 +4616,17 @@ void InstructionSelector::VisitI8x16Swizzle(Node* node) {
     }
   }
 
-  X64OperandGenerator g(this);
+  X64OperandGeneratorT<Adapter> g(this);
   Emit(op,
        IsSupported(AVX) ? g.DefineAsRegister(node) : g.DefineSameAsFirst(node),
        g.UseRegister(node->InputAt(0)), g.UseRegister(node->InputAt(1)));
 }
 
 namespace {
-void VisitRelaxedLaneSelect(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitRelaxedLaneSelect(InstructionSelectorT<Adapter>* selector, Node* node,
                             InstructionCode code = kX64Pblendvb) {
-  X64OperandGenerator g(selector);
+  X64OperandGeneratorT<Adapter> g(selector);
   // pblendvb/blendvps/blendvpd copies src2 when mask is set, opposite from Wasm
   // semantics. Node's inputs are: mask, lhs, rhs (determined in
   // wasm-compiler.cc).
@@ -4407,40 +4644,52 @@ void VisitRelaxedLaneSelect(InstructionSelector* selector, Node* node,
 }
 }  // namespace
 
-void InstructionSelector::VisitI8x16RelaxedLaneSelect(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI8x16RelaxedLaneSelect(Node* node) {
   VisitRelaxedLaneSelect(this, node);
 }
-void InstructionSelector::VisitI16x8RelaxedLaneSelect(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI16x8RelaxedLaneSelect(Node* node) {
   VisitRelaxedLaneSelect(this, node);
 }
-void InstructionSelector::VisitI32x4RelaxedLaneSelect(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI32x4RelaxedLaneSelect(Node* node) {
   VisitRelaxedLaneSelect(this, node, kX64Blendvps);
 }
 
-void InstructionSelector::VisitI64x2RelaxedLaneSelect(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI64x2RelaxedLaneSelect(Node* node) {
   VisitRelaxedLaneSelect(this, node, kX64Blendvpd);
 }
 #else
-void InstructionSelector::VisitI8x16Swizzle(Node* node) { UNREACHABLE(); }
-void InstructionSelector::VisitI8x16RelaxedLaneSelect(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI8x16Swizzle(Node* node) {
   UNREACHABLE();
 }
-void InstructionSelector::VisitI16x8RelaxedLaneSelect(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI8x16RelaxedLaneSelect(Node* node) {
   UNREACHABLE();
 }
-void InstructionSelector::VisitI32x4RelaxedLaneSelect(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI16x8RelaxedLaneSelect(Node* node) {
   UNREACHABLE();
 }
-void InstructionSelector::VisitI64x2RelaxedLaneSelect(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI32x4RelaxedLaneSelect(Node* node) {
+  UNREACHABLE();
+}
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI64x2RelaxedLaneSelect(Node* node) {
   UNREACHABLE();
 }
 #endif  // V8_ENABLE_WEBASSEMBLY
 
 namespace {
 // Used for pmin/pmax and relaxed min/max.
-void VisitMinOrMax(InstructionSelector* selector, Node* node, ArchOpcode opcode,
-                   bool flip_inputs) {
-  X64OperandGenerator g(selector);
+template <typename Adapter>
+void VisitMinOrMax(InstructionSelectorT<Adapter>* selector, Node* node,
+                   ArchOpcode opcode, bool flip_inputs) {
+  X64OperandGeneratorT<Adapter> g(selector);
   InstructionOperand dst = selector->IsSupported(AVX)
                                ? g.DefineAsRegister(node)
                                : g.DefineSameAsFirst(node);
@@ -4456,109 +4705,132 @@ void VisitMinOrMax(InstructionSelector* selector, Node* node, ArchOpcode opcode,
 }
 }  // namespace
 
-void InstructionSelector::VisitF32x4Pmin(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF32x4Pmin(Node* node) {
   VisitMinOrMax(this, node, kX64Minps, true);
 }
 
-void InstructionSelector::VisitF32x4Pmax(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF32x4Pmax(Node* node) {
   VisitMinOrMax(this, node, kX64Maxps, true);
 }
 
-void InstructionSelector::VisitF64x2Pmin(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF64x2Pmin(Node* node) {
   VisitMinOrMax(this, node, kX64Minpd, true);
 }
 
-void InstructionSelector::VisitF64x2Pmax(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF64x2Pmax(Node* node) {
   VisitMinOrMax(this, node, kX64Maxpd, true);
 }
 
-void InstructionSelector::VisitF32x4RelaxedMin(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF32x4RelaxedMin(Node* node) {
   VisitMinOrMax(this, node, kX64Minps, false);
 }
 
-void InstructionSelector::VisitF32x4RelaxedMax(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF32x4RelaxedMax(Node* node) {
   VisitMinOrMax(this, node, kX64Maxps, false);
 }
 
-void InstructionSelector::VisitF64x2RelaxedMin(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF64x2RelaxedMin(Node* node) {
   VisitMinOrMax(this, node, kX64Minpd, false);
 }
 
-void InstructionSelector::VisitF64x2RelaxedMax(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF64x2RelaxedMax(Node* node) {
   VisitMinOrMax(this, node, kX64Maxpd, false);
 }
 
-void InstructionSelector::VisitI32x4ExtAddPairwiseI16x8S(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI32x4ExtAddPairwiseI16x8S(Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   InstructionOperand dst = CpuFeatures::IsSupported(AVX)
                                ? g.DefineAsRegister(node)
                                : g.DefineSameAsFirst(node);
   Emit(kX64I32x4ExtAddPairwiseI16x8S, dst, g.UseRegister(node->InputAt(0)));
 }
 
-void InstructionSelector::VisitI32x8ExtAddPairwiseI16x16S(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI32x8ExtAddPairwiseI16x16S(
+    Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   Emit(kX64I32x8ExtAddPairwiseI16x16S, g.DefineAsRegister(node),
        g.UseRegister(node->InputAt(0)));
 }
 
-void InstructionSelector::VisitI32x4ExtAddPairwiseI16x8U(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI32x4ExtAddPairwiseI16x8U(Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   InstructionOperand dst = CpuFeatures::IsSupported(AVX)
                                ? g.DefineAsRegister(node)
                                : g.DefineSameAsFirst(node);
   Emit(kX64I32x4ExtAddPairwiseI16x8U, dst, g.UseRegister(node->InputAt(0)));
 }
 
-void InstructionSelector::VisitI32x8ExtAddPairwiseI16x16U(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI32x8ExtAddPairwiseI16x16U(
+    Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   Emit(kX64I32x8ExtAddPairwiseI16x16U, g.DefineAsRegister(node),
        g.UseRegister(node->InputAt(0)));
 }
 
-void InstructionSelector::VisitI16x8ExtAddPairwiseI8x16S(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI16x8ExtAddPairwiseI8x16S(Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   // Codegen depends on dst != src.
   Emit(kX64I16x8ExtAddPairwiseI8x16S, g.DefineAsRegister(node),
        g.UseUniqueRegister(node->InputAt(0)));
 }
 
-void InstructionSelector::VisitI16x16ExtAddPairwiseI8x32S(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI16x16ExtAddPairwiseI8x32S(
+    Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   Emit(kX64I16x16ExtAddPairwiseI8x32S, g.DefineAsRegister(node),
        g.UseUniqueRegister(node->InputAt(0)));
 }
 
-void InstructionSelector::VisitI16x8ExtAddPairwiseI8x16U(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI16x8ExtAddPairwiseI8x16U(Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   InstructionOperand dst = CpuFeatures::IsSupported(AVX)
                                ? g.DefineAsRegister(node)
                                : g.DefineSameAsFirst(node);
   Emit(kX64I16x8ExtAddPairwiseI8x16U, dst, g.UseRegister(node->InputAt(0)));
 }
 
-void InstructionSelector::VisitI16x16ExtAddPairwiseI8x32U(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI16x16ExtAddPairwiseI8x32U(
+    Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   Emit(kX64I16x16ExtAddPairwiseI8x32U, g.DefineAsRegister(node),
        g.UseUniqueRegister(node->InputAt(0)));
 }
 
-void InstructionSelector::VisitI8x16Popcnt(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI8x16Popcnt(Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   InstructionOperand temps[] = {g.TempSimd128Register()};
   Emit(kX64I8x16Popcnt, g.DefineAsRegister(node),
        g.UseUniqueRegister(node->InputAt(0)), arraysize(temps), temps);
 }
 
-void InstructionSelector::VisitF64x2ConvertLowI32x4U(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF64x2ConvertLowI32x4U(Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   InstructionOperand dst =
       IsSupported(AVX) ? g.DefineAsRegister(node) : g.DefineSameAsFirst(node);
   Emit(kX64F64x2ConvertLowI32x4U, dst, g.UseRegister(node->InputAt(0)));
 }
 
-void InstructionSelector::VisitI32x4TruncSatF64x2SZero(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI32x4TruncSatF64x2SZero(Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   if (CpuFeatures::IsSupported(AVX)) {
     // Requires dst != src.
     Emit(kX64I32x4TruncSatF64x2SZero, g.DefineAsRegister(node),
@@ -4569,32 +4841,40 @@ void InstructionSelector::VisitI32x4TruncSatF64x2SZero(Node* node) {
   }
 }
 
-void InstructionSelector::VisitI32x4TruncSatF64x2UZero(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI32x4TruncSatF64x2UZero(Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   InstructionOperand dst = CpuFeatures::IsSupported(AVX)
                                ? g.DefineAsRegister(node)
                                : g.DefineSameAsFirst(node);
   Emit(kX64I32x4TruncSatF64x2UZero, dst, g.UseRegister(node->InputAt(0)));
 }
 
-void InstructionSelector::VisitI32x4RelaxedTruncF64x2SZero(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI32x4RelaxedTruncF64x2SZero(
+    Node* node) {
   VisitFloatUnop(this, node, node->InputAt(0), kX64Cvttpd2dq);
 }
 
-void InstructionSelector::VisitI32x4RelaxedTruncF64x2UZero(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI32x4RelaxedTruncF64x2UZero(
+    Node* node) {
   VisitFloatUnop(this, node, node->InputAt(0), kX64I32x4TruncF64x2UZero);
 }
 
-void InstructionSelector::VisitI32x4RelaxedTruncF32x4S(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI32x4RelaxedTruncF32x4S(Node* node) {
   VisitFloatUnop(this, node, node->InputAt(0), kX64Cvttps2dq);
 }
 
-void InstructionSelector::VisitI32x4RelaxedTruncF32x4U(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI32x4RelaxedTruncF32x4U(Node* node) {
   VisitFloatUnop(this, node, node->InputAt(0), kX64I32x4TruncF32x4U);
 }
 
-void InstructionSelector::VisitI64x2GtS(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI64x2GtS(Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   if (CpuFeatures::IsSupported(AVX)) {
     Emit(kX64IGtS | LaneSizeField::encode(kL64) |
              VectorLengthField::encode(kV128),
@@ -4613,8 +4893,9 @@ void InstructionSelector::VisitI64x2GtS(Node* node) {
   }
 }
 
-void InstructionSelector::VisitI64x2GeS(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI64x2GeS(Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   if (CpuFeatures::IsSupported(AVX)) {
     Emit(kX64IGeS | LaneSizeField::encode(kL64) |
              VectorLengthField::encode(kV128),
@@ -4633,8 +4914,9 @@ void InstructionSelector::VisitI64x2GeS(Node* node) {
   }
 }
 
-void InstructionSelector::VisitI64x2Abs(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI64x2Abs(Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   if (CpuFeatures::IsSupported(AVX)) {
     Emit(kX64IAbs | LaneSizeField::encode(kL64) |
              VectorLengthField::encode(kV128),
@@ -4646,8 +4928,9 @@ void InstructionSelector::VisitI64x2Abs(Node* node) {
   }
 }
 
-void InstructionSelector::VisitF64x2PromoteLowF32x4(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF64x2PromoteLowF32x4(Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   InstructionCode code = kX64F64x2PromoteLowF32x4;
   Node* input = node->InputAt(0);
   LoadTransformMatcher m(input);
@@ -4666,14 +4949,16 @@ void InstructionSelector::VisitF64x2PromoteLowF32x4(Node* node) {
   VisitRR(this, node, code);
 }
 
-void InstructionSelector::VisitI16x8DotI8x16I7x16S(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI16x8DotI8x16I7x16S(Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   Emit(kX64I16x8DotI8x16I7x16S, g.DefineAsRegister(node),
        g.UseUniqueRegister(node->InputAt(0)), g.UseRegister(node->InputAt(1)));
 }
 
-void InstructionSelector::VisitI32x4DotI8x16I7x16AddS(Node* node) {
-  X64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI32x4DotI8x16I7x16AddS(Node* node) {
+  X64OperandGeneratorT<Adapter> g(this);
   InstructionOperand temps[] = {g.TempSimd128Register()};
   Emit(kX64I32x4DotI8x16I7x16AddS, g.DefineSameAsInput(node, 2),
        g.UseUniqueRegister(node->InputAt(0)),
@@ -4681,16 +4966,17 @@ void InstructionSelector::VisitI32x4DotI8x16I7x16AddS(Node* node) {
        g.UseUniqueRegister(node->InputAt(2)), arraysize(temps), temps);
 }
 
-void InstructionSelector::AddOutputToSelectContinuation(OperandGenerator* g,
-                                                        int first_input_index,
-                                                        Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::AddOutputToSelectContinuation(
+    OperandGenerator* g, int first_input_index, Node* node) {
   continuation_outputs_.push_back(
       g->DefineSameAsInput(node, first_input_index));
 }
 
 // static
+template <typename Adapter>
 MachineOperatorBuilder::Flags
-InstructionSelector::SupportedMachineOperatorFlags() {
+InstructionSelectorT<Adapter>::SupportedMachineOperatorFlags() {
   MachineOperatorBuilder::Flags flags =
       MachineOperatorBuilder::kWord32ShiftIsSafe |
       MachineOperatorBuilder::kWord32Ctz | MachineOperatorBuilder::kWord64Ctz |
@@ -4715,12 +5001,18 @@ InstructionSelector::SupportedMachineOperatorFlags() {
 }
 
 // static
+template <typename Adapter>
 MachineOperatorBuilder::AlignmentRequirements
-InstructionSelector::AlignmentRequirements() {
+InstructionSelectorT<Adapter>::AlignmentRequirements() {
   return MachineOperatorBuilder::AlignmentRequirements::
       FullUnalignedAccessSupport();
 }
 
+template class EXPORT_TEMPLATE_DEFINE(V8_EXPORT_PRIVATE)
+    InstructionSelectorT<TurbofanAdapter>;
+template class EXPORT_TEMPLATE_DEFINE(V8_EXPORT_PRIVATE)
+    InstructionSelectorT<TurboshaftAdapter>;
+
 }  // namespace compiler
 }  // namespace internal
 }  // namespace v8
diff --git a/src/compiler/pipeline.cc b/src/compiler/pipeline.cc
index 0a8bc0b54d9..26da2d5ef97 100644
--- a/src/compiler/pipeline.cc
+++ b/src/compiler/pipeline.cc
@@ -2421,40 +2421,45 @@ struct InstructionSelectionPhase {
 
   base::Optional<BailoutReason> Run(PipelineData* data, Zone* temp_zone,
                                     Linkage* linkage) {
-    InstructionSelector selector(
-        temp_zone, data->graph()->NodeCount(), linkage, data->sequence(),
-        data->schedule(), data->source_positions(), data->frame(),
-        data->info()->switch_jump_table()
-            ? InstructionSelector::kEnableSwitchJumpTable
-            : InstructionSelector::kDisableSwitchJumpTable,
-        &data->info()->tick_counter(), data->broker(),
-        data->address_of_max_unoptimized_frame_height(),
-        data->address_of_max_pushed_argument_count(),
-        data->info()->source_positions()
-            ? InstructionSelector::kAllSourcePositions
-            : InstructionSelector::kCallSourcePositions,
-        InstructionSelector::SupportedFeatures(),
-        v8_flags.turbo_instruction_scheduling
-            ? InstructionSelector::kEnableScheduling
-            : InstructionSelector::kDisableScheduling,
-        data->assembler_options().enable_root_relative_access
-            ? InstructionSelector::kEnableRootsRelativeAddressing
-            : InstructionSelector::kDisableRootsRelativeAddressing,
-        data->info()->trace_turbo_json()
-            ? InstructionSelector::kEnableTraceTurboJson
-            : InstructionSelector::kDisableTraceTurboJson);
-    if (base::Optional<BailoutReason> bailout = selector.SelectInstructions()) {
-      return bailout;
-    }
-    if (data->info()->trace_turbo_json()) {
-      TurboJsonFile json_of(data->info(), std::ios_base::app);
-      json_of << "{\"name\":\"" << phase_name()
-              << "\",\"type\":\"instructions\""
-              << InstructionRangesAsJSON{data->sequence(),
-                                         &selector.instr_origins()}
-              << "},\n";
+    if (v8_flags.turboshaft && v8_flags.turboshaft_instruction_selection) {
+      UNIMPLEMENTED();
+    } else {
+      InstructionSelector selector(
+          temp_zone, data->graph()->NodeCount(), linkage, data->sequence(),
+          data->schedule(), data->source_positions(), data->frame(),
+          data->info()->switch_jump_table()
+              ? InstructionSelector::kEnableSwitchJumpTable
+              : InstructionSelector::kDisableSwitchJumpTable,
+          &data->info()->tick_counter(), data->broker(),
+          data->address_of_max_unoptimized_frame_height(),
+          data->address_of_max_pushed_argument_count(),
+          data->info()->source_positions()
+              ? InstructionSelector::kAllSourcePositions
+              : InstructionSelector::kCallSourcePositions,
+          InstructionSelector::SupportedFeatures(),
+          v8_flags.turbo_instruction_scheduling
+              ? InstructionSelector::kEnableScheduling
+              : InstructionSelector::kDisableScheduling,
+          data->assembler_options().enable_root_relative_access
+              ? InstructionSelector::kEnableRootsRelativeAddressing
+              : InstructionSelector::kDisableRootsRelativeAddressing,
+          data->info()->trace_turbo_json()
+              ? InstructionSelector::kEnableTraceTurboJson
+              : InstructionSelector::kDisableTraceTurboJson);
+      if (base::Optional<BailoutReason> bailout =
+              selector.SelectInstructions()) {
+        return bailout;
+      }
+      if (data->info()->trace_turbo_json()) {
+        TurboJsonFile json_of(data->info(), std::ios_base::app);
+        json_of << "{\"name\":\"" << phase_name()
+                << "\",\"type\":\"instructions\""
+                << InstructionRangesAsJSON{data->sequence(),
+                                           &selector.instr_origins()}
+                << "},\n";
+      }
+      return base::nullopt;
     }
-    return base::nullopt;
   }
 };
 
diff --git a/src/flags/flag-definitions.h b/src/flags/flag-definitions.h
index 97e091dc3cf..bb9882c5968 100644
--- a/src/flags/flag-definitions.h
+++ b/src/flags/flag-definitions.h
@@ -1152,6 +1152,9 @@ DEFINE_EXPERIMENTAL_FEATURE(turboshaft_wasm,
 DEFINE_EXPERIMENTAL_FEATURE(turboshaft_typed_optimizations,
                             "enable an additional Turboshaft phase that "
                             "performs optimizations based on type information")
+DEFINE_EXPERIMENTAL_FEATURE(
+    turboshaft_instruction_selection,
+    "run instruction selection on Turboshaft IR directly")
 #ifdef DEBUG
 DEFINE_UINT64(turboshaft_opt_bisect_limit, std::numeric_limits<uint64_t>::max(),
               "stop applying optional optimizations after a specified number "
-- 
2.35.1

