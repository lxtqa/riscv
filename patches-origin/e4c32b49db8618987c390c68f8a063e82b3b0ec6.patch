From e4c32b49db8618987c390c68f8a063e82b3b0ec6 Mon Sep 17 00:00:00 2001
From: Clemens Backes <clemensb@chromium.org>
Date: Thu, 15 Sep 2022 12:02:47 +0200
Subject: [PATCH] [compiler] Use v8_flags for accessing flag values

Avoid the deprecated FLAG_* syntax, access flag values via the
{v8_flags} struct instead.

R=thibaudm@chromium.org

Bug: v8:12887
Change-Id: Id2f457a1c0056d5015e2f9983d4599582d7189cd
Reviewed-on: https://chromium-review.googlesource.com/c/v8/v8/+/3876185
Reviewed-by: Thibaud Michaud <thibaudm@chromium.org>
Commit-Queue: Clemens Backes <clemensb@chromium.org>
Cr-Commit-Position: refs/heads/main@{#83219}
---
 .../backend/arm/code-generator-arm.cc         |  12 +-
 .../backend/arm/instruction-selector-arm.cc   |   5 +-
 .../backend/arm/unwinding-info-writer-arm.h   |   2 +-
 .../backend/arm64/code-generator-arm64.cc     |  12 +-
 .../arm64/instruction-selector-arm64.cc       |  10 +-
 .../arm64/unwinding-info-writer-arm64.h       |   2 +-
 src/compiler/backend/code-generator.cc        |   8 +-
 .../backend/ia32/code-generator-ia32.cc       |   8 +-
 .../backend/ia32/instruction-selector-ia32.cc |   5 +-
 src/compiler/backend/instruction-scheduler.cc |   8 +-
 src/compiler/backend/instruction-selector.h   |   2 +-
 src/compiler/backend/instruction.cc           |   2 +-
 src/compiler/backend/jump-threading.cc        |  10 +-
 .../backend/loong64/code-generator-loong64.cc |  14 +-
 .../loong64/instruction-selector-loong64.cc   |  10 +-
 .../backend/mips64/code-generator-mips64.cc   |  16 +-
 .../mips64/instruction-scheduler-mips64.cc    |   2 +-
 .../mips64/instruction-selector-mips64.cc     |  12 +-
 .../backend/ppc/code-generator-ppc.cc         |  24 +--
 .../backend/ppc/instruction-selector-ppc.cc   |   5 +-
 .../backend/ppc/unwinding-info-writer-ppc.h   |   2 +-
 .../backend/riscv/code-generator-riscv.cc     |  13 +-
 .../riscv/instruction-scheduler-riscv.cc      |   2 +-
 .../riscv/instruction-selector-riscv32.cc     |   2 +-
 .../riscv/instruction-selector-riscv64.cc     |   4 +-
 .../backend/s390/code-generator-s390.cc       |  14 +-
 .../backend/s390/instruction-selector-s390.cc |   5 +-
 .../backend/s390/unwinding-info-writer-s390.h |   2 +-
 src/compiler/backend/spill-placer.cc          |   2 +-
 src/compiler/backend/unwinding-info-writer.h  |   2 +-
 .../backend/x64/code-generator-x64.cc         |  20 +--
 .../backend/x64/instruction-selector-x64.cc   |   5 +-
 .../backend/x64/unwinding-info-writer-x64.h   |   2 +-
 src/compiler/basic-block-instrumentor.cc      |   2 +-
 src/compiler/bytecode-analysis.cc             |   2 +-
 src/compiler/code-assembler.cc                |   4 +-
 src/compiler/code-assembler.h                 |   6 +-
 src/compiler/compilation-dependencies.cc      |  20 +--
 src/compiler/constant-folding-reducer.cc      |   4 +-
 src/compiler/control-equivalence.cc           |   8 +-
 src/compiler/csa-load-elimination.cc          |   2 +-
 src/compiler/escape-analysis-reducer.cc       |   6 +-
 src/compiler/escape-analysis.cc               |   6 +-
 src/compiler/fast-api-calls.cc                |   2 +-
 src/compiler/frame.cc                         |   2 +-
 src/compiler/globals.h                        |   2 +-
 src/compiler/graph-reducer.cc                 |   4 +-
 src/compiler/graph-trimmer.cc                 |   2 +-
 src/compiler/graph-visualizer.cc              |  14 +-
 src/compiler/heap-refs.cc                     |   2 +-
 src/compiler/js-call-reducer.cc               |  16 +-
 src/compiler/js-heap-broker.cc                |   2 +-
 src/compiler/js-heap-broker.h                 |  18 +--
 src/compiler/js-inlining-heuristic.cc         |  17 ++-
 src/compiler/js-inlining-heuristic.h          |   4 +-
 src/compiler/js-inlining.cc                   |  10 +-
 src/compiler/js-intrinsic-lowering.cc         |   2 +-
 .../js-native-context-specialization.cc       |   4 +-
 src/compiler/load-elimination.cc              |   2 +-
 src/compiler/loop-analysis.cc                 |   4 +-
 src/compiler/loop-peeling.cc                  |   2 +-
 src/compiler/loop-variable-optimizer.cc       |  10 +-
 src/compiler/memory-lowering.cc               |   8 +-
 src/compiler/pipeline.cc                      | 143 +++++++++---------
 src/compiler/raw-machine-assembler.cc         |   8 +-
 src/compiler/schedule.cc                      |   4 +-
 src/compiler/scheduler.cc                     |  24 +--
 src/compiler/simplified-lowering.cc           |  12 +-
 src/compiler/store-store-elimination.cc       |   4 +-
 src/compiler/turboshaft/optimization-phase.h  |   2 +-
 src/compiler/typer.cc                         |   2 +-
 src/compiler/verifier.cc                      |   2 +-
 test/cctest/compiler/function-tester.cc       |   4 +-
 .../compiler/test-basic-block-profiler.cc     |   2 +-
 .../test-calls-with-arraylike-or-spread.cc    |   8 +-
 test/cctest/compiler/test-code-assembler.cc   |   2 +-
 test/cctest/compiler/test-code-generator.cc   |  20 +--
 .../test-concurrent-shared-function-info.cc   |   2 +-
 test/cctest/compiler/test-loop-analysis.cc    |   2 +-
 test/cctest/compiler/test-multiple-return.cc  |   4 +-
 .../test-run-bytecode-graph-builder.cc        |   4 +-
 test/cctest/compiler/test-run-native-calls.cc |   2 +-
 .../compiler/test-run-unwinding-info.cc       |   4 +-
 test/cctest/compiler/test-verify-type.cc      |   2 +-
 .../backend/instruction-selector-unittest.cc  |   6 +-
 .../compiler/bytecode-analysis-unittest.cc    |   4 +-
 test/unittests/compiler/compiler-unittest.cc  |  27 ++--
 .../compiler/control-equivalence-unittest.cc  |   2 +-
 test/unittests/compiler/function-tester.cc    |   4 +-
 .../compiler/loop-peeling-unittest.cc         |   4 +-
 .../instruction-selector-mips64-unittest.cc   |   4 +-
 .../compiler/regalloc/live-range-unittest.cc  |  13 +-
 .../mid-tier-register-allocator-unittest.cc   |   4 +-
 .../regalloc/move-optimizer-unittest.cc       |   4 +-
 .../instruction-selector-riscv32-unittest.cc  |   2 +-
 .../instruction-selector-riscv64-unittest.cc  |   2 +-
 test/unittests/compiler/run-deopt-unittest.cc |  10 +-
 .../compiler/run-jscalls-unittest.cc          |   2 +-
 test/unittests/compiler/scheduler-unittest.cc |   4 +-
 .../compiler/sloppy-equality-unittest.cc      |   4 +-
 100 files changed, 402 insertions(+), 382 deletions(-)

diff --git a/src/compiler/backend/arm/code-generator-arm.cc b/src/compiler/backend/arm/code-generator-arm.cc
index 8d8e5204b2..4c5accd7a8 100644
--- a/src/compiler/backend/arm/code-generator-arm.cc
+++ b/src/compiler/backend/arm/code-generator-arm.cc
@@ -747,7 +747,7 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
     }
     case kArchCallJSFunction: {
       Register func = i.InputRegister(0);
-      if (FLAG_debug_code) {
+      if (v8_flags.debug_code) {
         UseScratchRegisterScope temps(tasm());
         Register scratch = temps.Acquire();
         // Check the function's context matches the context argument.
@@ -938,7 +938,7 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
       Register object = i.InputRegister(0);
       Register value = i.InputRegister(2);
 
-      if (FLAG_debug_code) {
+      if (v8_flags.debug_code) {
         // Checking that |value| is not a cleared weakref: our write barrier
         // does not support that for now.
         __ cmp(value, Operand(kClearedWeakHeapObjectLower32));
@@ -3588,7 +3588,7 @@ void CodeGenerator::AssembleArchTrap(Instruction* instr,
         ReferenceMap* reference_map =
             gen_->zone()->New<ReferenceMap>(gen_->zone());
         gen_->RecordSafepoint(reference_map);
-        if (FLAG_debug_code) {
+        if (v8_flags.debug_code) {
           __ stop();
         }
       }
@@ -3743,7 +3743,7 @@ void CodeGenerator::AssembleConstructFrame() {
       // If the frame is bigger than the stack, we throw the stack overflow
       // exception unconditionally. Thereby we can avoid the integer overflow
       // check in the condition code.
-      if (required_slots * kSystemPointerSize < FLAG_stack_size * KB) {
+      if (required_slots * kSystemPointerSize < v8_flags.stack_size * KB) {
         UseScratchRegisterScope temps(tasm());
         Register scratch = temps.Acquire();
         __ ldr(scratch, FieldMemOperand(
@@ -3760,7 +3760,7 @@ void CodeGenerator::AssembleConstructFrame() {
       // define an empty safepoint.
       ReferenceMap* reference_map = zone()->New<ReferenceMap>(zone());
       RecordSafepoint(reference_map);
-      if (FLAG_debug_code) __ stop();
+      if (v8_flags.debug_code) __ stop();
 
       __ bind(&done);
     }
@@ -3824,7 +3824,7 @@ void CodeGenerator::AssembleReturn(InstructionOperand* additional_pop_count) {
   if (parameter_slots != 0) {
     if (additional_pop_count->IsImmediate()) {
       DCHECK_EQ(g.ToConstant(additional_pop_count).ToInt32(), 0);
-    } else if (FLAG_debug_code) {
+    } else if (v8_flags.debug_code) {
       __ cmp(g.ToRegister(additional_pop_count), Operand(0));
       __ Assert(eq, AbortReason::kUnexpectedAdditionalPopValue);
     }
diff --git a/src/compiler/backend/arm/instruction-selector-arm.cc b/src/compiler/backend/arm/instruction-selector-arm.cc
index 578581877e..8733aff787 100644
--- a/src/compiler/backend/arm/instruction-selector-arm.cc
+++ b/src/compiler/backend/arm/instruction-selector-arm.cc
@@ -700,11 +700,12 @@ void VisitStoreCommon(InstructionSelector* selector, Node* node,
   WriteBarrierKind write_barrier_kind = store_rep.write_barrier_kind();
   MachineRepresentation rep = store_rep.representation();
 
-  if (FLAG_enable_unconditional_write_barriers && CanBeTaggedPointer(rep)) {
+  if (v8_flags.enable_unconditional_write_barriers && CanBeTaggedPointer(rep)) {
     write_barrier_kind = kFullWriteBarrier;
   }
 
-  if (write_barrier_kind != kNoWriteBarrier && !FLAG_disable_write_barriers) {
+  if (write_barrier_kind != kNoWriteBarrier &&
+      !v8_flags.disable_write_barriers) {
     DCHECK(CanBeTaggedPointer(rep));
     AddressingMode addressing_mode;
     InstructionOperand inputs[3];
diff --git a/src/compiler/backend/arm/unwinding-info-writer-arm.h b/src/compiler/backend/arm/unwinding-info-writer-arm.h
index 6b9ade0c48..de81134bd1 100644
--- a/src/compiler/backend/arm/unwinding-info-writer-arm.h
+++ b/src/compiler/backend/arm/unwinding-info-writer-arm.h
@@ -49,7 +49,7 @@ class UnwindingInfoWriter {
   }
 
  private:
-  bool enabled() const { return FLAG_perf_prof_unwinding_info; }
+  bool enabled() const { return v8_flags.perf_prof_unwinding_info; }
 
   class BlockInitialState : public ZoneObject {
    public:
diff --git a/src/compiler/backend/arm64/code-generator-arm64.cc b/src/compiler/backend/arm64/code-generator-arm64.cc
index 4a9654e8b9..2a90535397 100644
--- a/src/compiler/backend/arm64/code-generator-arm64.cc
+++ b/src/compiler/backend/arm64/code-generator-arm64.cc
@@ -433,7 +433,7 @@ class WasmProtectedInstructionTrap final : public WasmOutOfLineTrap {
       : WasmOutOfLineTrap(gen, instr), pc_(pc) {}
 
   void Generate() override {
-    DCHECK(FLAG_wasm_bounds_checks && !FLAG_wasm_enforce_bounds_checks);
+    DCHECK(v8_flags.wasm_bounds_checks && !v8_flags.wasm_enforce_bounds_checks);
     gen_->AddProtectedInstructionLanding(pc_, __ pc_offset());
     GenerateWithTrapId(TrapId::kTrapMemOutOfBounds);
   }
@@ -767,7 +767,7 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
     }
     case kArchCallJSFunction: {
       Register func = i.InputRegister(0);
-      if (FLAG_debug_code) {
+      if (v8_flags.debug_code) {
         // Check the function's context matches the context argument.
         UseScratchRegisterScope scope(tasm());
         Register temp = scope.AcquireX();
@@ -961,7 +961,7 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
       }
       Register value = i.InputRegister(2);
 
-      if (FLAG_debug_code) {
+      if (v8_flags.debug_code) {
         // Checking that |value| is not a cleared weakref: our write barrier
         // does not support that for now.
         __ cmp(value, Operand(kClearedWeakHeapObjectLower32));
@@ -3144,7 +3144,7 @@ void CodeGenerator::AssembleConstructFrame() {
       // If the frame is bigger than the stack, we throw the stack overflow
       // exception unconditionally. Thereby we can avoid the integer overflow
       // check in the condition code.
-      if (required_slots * kSystemPointerSize < FLAG_stack_size * KB) {
+      if (required_slots * kSystemPointerSize < v8_flags.stack_size * KB) {
         UseScratchRegisterScope scope(tasm());
         Register scratch = scope.AcquireX();
         __ Ldr(scratch, FieldMemOperand(
@@ -3170,7 +3170,7 @@ void CodeGenerator::AssembleConstructFrame() {
       // define an empty safepoint.
       ReferenceMap* reference_map = zone()->New<ReferenceMap>(zone());
       RecordSafepoint(reference_map);
-      if (FLAG_debug_code) __ Brk(0);
+      if (v8_flags.debug_code) __ Brk(0);
       __ Bind(&done);
     }
 #endif  // V8_ENABLE_WEBASSEMBLY
@@ -3287,7 +3287,7 @@ void CodeGenerator::AssembleReturn(InstructionOperand* additional_pop_count) {
   if (parameter_slots != 0) {
     if (additional_pop_count->IsImmediate()) {
       DCHECK_EQ(g.ToConstant(additional_pop_count).ToInt32(), 0);
-    } else if (FLAG_debug_code) {
+    } else if (v8_flags.debug_code) {
       __ cmp(g.ToRegister(additional_pop_count), Operand(0));
       __ Assert(eq, AbortReason::kUnexpectedAdditionalPopValue);
     }
diff --git a/src/compiler/backend/arm64/instruction-selector-arm64.cc b/src/compiler/backend/arm64/instruction-selector-arm64.cc
index 66c39e6c9d..b79ec0e8ad 100644
--- a/src/compiler/backend/arm64/instruction-selector-arm64.cc
+++ b/src/compiler/backend/arm64/instruction-selector-arm64.cc
@@ -872,13 +872,14 @@ void InstructionSelector::VisitStore(Node* node) {
   WriteBarrierKind write_barrier_kind = store_rep.write_barrier_kind();
   MachineRepresentation rep = store_rep.representation();
 
-  if (FLAG_enable_unconditional_write_barriers &&
+  if (v8_flags.enable_unconditional_write_barriers &&
       CanBeTaggedOrCompressedPointer(rep)) {
     write_barrier_kind = kFullWriteBarrier;
   }
 
   // TODO(arm64): I guess this could be done in a better way.
-  if (write_barrier_kind != kNoWriteBarrier && !FLAG_disable_write_barriers) {
+  if (write_barrier_kind != kNoWriteBarrier &&
+      !v8_flags.disable_write_barriers) {
     DCHECK(CanBeTaggedOrCompressedPointer(rep));
     AddressingMode addressing_mode;
     InstructionOperand inputs[3];
@@ -2775,7 +2776,7 @@ void VisitAtomicStore(InstructionSelector* selector, Node* node,
   WriteBarrierKind write_barrier_kind = store_params.write_barrier_kind();
   MachineRepresentation rep = store_params.representation();
 
-  if (FLAG_enable_unconditional_write_barriers &&
+  if (v8_flags.enable_unconditional_write_barriers &&
       CanBeTaggedOrCompressedPointer(rep)) {
     write_barrier_kind = kFullWriteBarrier;
   }
@@ -2785,7 +2786,8 @@ void VisitAtomicStore(InstructionSelector* selector, Node* node,
   InstructionOperand temps[] = {g.TempRegister()};
   InstructionCode code;
 
-  if (write_barrier_kind != kNoWriteBarrier && !FLAG_disable_write_barriers) {
+  if (write_barrier_kind != kNoWriteBarrier &&
+      !v8_flags.disable_write_barriers) {
     DCHECK(CanBeTaggedOrCompressedPointer(rep));
     DCHECK_EQ(AtomicWidthSize(width), kTaggedSize);
 
diff --git a/src/compiler/backend/arm64/unwinding-info-writer-arm64.h b/src/compiler/backend/arm64/unwinding-info-writer-arm64.h
index 36788735de..e142e13d69 100644
--- a/src/compiler/backend/arm64/unwinding-info-writer-arm64.h
+++ b/src/compiler/backend/arm64/unwinding-info-writer-arm64.h
@@ -49,7 +49,7 @@ class UnwindingInfoWriter {
   }
 
  private:
-  bool enabled() const { return FLAG_perf_prof_unwinding_info; }
+  bool enabled() const { return v8_flags.perf_prof_unwinding_info; }
 
   class BlockInitialState : public ZoneObject {
    public:
diff --git a/src/compiler/backend/code-generator.cc b/src/compiler/backend/code-generator.cc
index 6747d710c1..788ee606dc 100644
--- a/src/compiler/backend/code-generator.cc
+++ b/src/compiler/backend/code-generator.cc
@@ -214,7 +214,7 @@ void CodeGenerator::AssembleCode() {
   tasm()->CodeEntry();
 
   // Check that {kJavaScriptCallCodeStartRegister} has been set correctly.
-  if (FLAG_debug_code && info->called_with_code_start_register()) {
+  if (v8_flags.debug_code && info->called_with_code_start_register()) {
     tasm()->RecordComment("-- Prologue: check code start register --");
     AssembleCodeStartRegisterCheck();
   }
@@ -274,7 +274,7 @@ void CodeGenerator::AssembleCode() {
     // Bind a label for a block.
     current_block_ = block->rpo_number();
     unwinding_info_writer_.BeginInstructionBlock(tasm()->pc_offset(), block);
-    if (FLAG_code_comments) {
+    if (v8_flags.code_comments) {
       std::ostringstream buffer;
       buffer << "-- B" << block->rpo_number().ToInt() << " start";
       if (block->IsDeferred()) buffer << " (deferred)";
@@ -307,7 +307,7 @@ void CodeGenerator::AssembleCode() {
       }
     }
 
-    if (FLAG_enable_embedded_constant_pool && !block->needs_frame()) {
+    if (v8_flags.enable_embedded_constant_pool && !block->needs_frame()) {
       ConstantPoolUnavailableScope constant_pool_unavailable(tasm());
       result_ = AssembleBlock(block);
     } else {
@@ -813,7 +813,7 @@ void CodeGenerator::AssembleSourcePosition(SourcePosition source_position) {
   if (!source_position.IsKnown()) return;
   source_position_table_builder_.AddPosition(tasm()->pc_offset(),
                                              source_position, false);
-  if (FLAG_code_comments) {
+  if (v8_flags.code_comments) {
     OptimizedCompilationInfo* info = this->info();
     if (!info->IsOptimizing()) {
 #if V8_ENABLE_WEBASSEMBLY
diff --git a/src/compiler/backend/ia32/code-generator-ia32.cc b/src/compiler/backend/ia32/code-generator-ia32.cc
index 8a53c5cd21..5afd119ff5 100644
--- a/src/compiler/backend/ia32/code-generator-ia32.cc
+++ b/src/compiler/backend/ia32/code-generator-ia32.cc
@@ -767,7 +767,7 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
     }
     case kArchCallJSFunction: {
       Register func = i.InputRegister(0);
-      if (FLAG_debug_code) {
+      if (v8_flags.debug_code) {
         // Check the function's context matches the context argument.
         __ cmp(esi, FieldOperand(func, JSFunction::kContextOffset));
         __ Assert(equal, AbortReason::kWrongFunctionContext);
@@ -962,7 +962,7 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
       Register scratch0 = i.TempRegister(0);
       Register scratch1 = i.TempRegister(1);
 
-      if (FLAG_debug_code) {
+      if (v8_flags.debug_code) {
         // Checking that |value| is not a cleared weakref: our write barrier
         // does not support that for now.
         __ cmp(value, Immediate(kClearedWeakHeapObjectLower32));
@@ -4066,7 +4066,7 @@ void CodeGenerator::AssembleConstructFrame() {
       // If the frame is bigger than the stack, we throw the stack overflow
       // exception unconditionally. Thereby we can avoid the integer overflow
       // check in the condition code.
-      if (required_slots * kSystemPointerSize < FLAG_stack_size * KB) {
+      if (required_slots * kSystemPointerSize < v8_flags.stack_size * KB) {
         Register scratch = esi;
         __ push(scratch);
         __ mov(scratch,
@@ -4134,7 +4134,7 @@ void CodeGenerator::AssembleReturn(InstructionOperand* additional_pop_count) {
   if (parameter_slots != 0) {
     if (additional_pop_count->IsImmediate()) {
       DCHECK_EQ(g.ToConstant(additional_pop_count).ToInt32(), 0);
-    } else if (FLAG_debug_code) {
+    } else if (v8_flags.debug_code) {
       __ cmp(g.ToRegister(additional_pop_count), Immediate(0));
       __ Assert(equal, AbortReason::kUnexpectedAdditionalPopValue);
     }
diff --git a/src/compiler/backend/ia32/instruction-selector-ia32.cc b/src/compiler/backend/ia32/instruction-selector-ia32.cc
index 84904062cf..bea1475584 100644
--- a/src/compiler/backend/ia32/instruction-selector-ia32.cc
+++ b/src/compiler/backend/ia32/instruction-selector-ia32.cc
@@ -695,11 +695,12 @@ void VisitStoreCommon(InstructionSelector* selector, Node* node,
   const bool is_seqcst =
       atomic_order && *atomic_order == AtomicMemoryOrder::kSeqCst;
 
-  if (FLAG_enable_unconditional_write_barriers && CanBeTaggedPointer(rep)) {
+  if (v8_flags.enable_unconditional_write_barriers && CanBeTaggedPointer(rep)) {
     write_barrier_kind = kFullWriteBarrier;
   }
 
-  if (write_barrier_kind != kNoWriteBarrier && !FLAG_disable_write_barriers) {
+  if (write_barrier_kind != kNoWriteBarrier &&
+      !v8_flags.disable_write_barriers) {
     DCHECK(CanBeTaggedPointer(rep));
     AddressingMode addressing_mode;
     InstructionOperand inputs[] = {
diff --git a/src/compiler/backend/instruction-scheduler.cc b/src/compiler/backend/instruction-scheduler.cc
index 3d0be78262..af339c1558 100644
--- a/src/compiler/backend/instruction-scheduler.cc
+++ b/src/compiler/backend/instruction-scheduler.cc
@@ -82,9 +82,9 @@ InstructionScheduler::InstructionScheduler(Zone* zone,
       last_live_in_reg_marker_(nullptr),
       last_deopt_or_trap_(nullptr),
       operands_map_(zone) {
-  if (FLAG_turbo_stress_instruction_scheduling) {
+  if (v8_flags.turbo_stress_instruction_scheduling) {
     random_number_generator_ =
-        base::Optional<base::RandomNumberGenerator>(FLAG_random_seed);
+        base::Optional<base::RandomNumberGenerator>(v8_flags.random_seed);
   }
 }
 
@@ -99,7 +99,7 @@ void InstructionScheduler::StartBlock(RpoNumber rpo) {
 }
 
 void InstructionScheduler::EndBlock(RpoNumber rpo) {
-  if (FLAG_turbo_stress_instruction_scheduling) {
+  if (v8_flags.turbo_stress_instruction_scheduling) {
     Schedule<StressSchedulerQueue>();
   } else {
     Schedule<CriticalPathFirstQueue>();
@@ -119,7 +119,7 @@ void InstructionScheduler::AddTerminator(Instruction* instr) {
 
 void InstructionScheduler::AddInstruction(Instruction* instr) {
   if (IsBarrier(instr)) {
-    if (FLAG_turbo_stress_instruction_scheduling) {
+    if (v8_flags.turbo_stress_instruction_scheduling) {
       Schedule<StressSchedulerQueue>();
     } else {
       Schedule<CriticalPathFirstQueue>();
diff --git a/src/compiler/backend/instruction-selector.h b/src/compiler/backend/instruction-selector.h
index 686ba68519..23cab58ced 100644
--- a/src/compiler/backend/instruction-selector.h
+++ b/src/compiler/backend/instruction-selector.h
@@ -292,7 +292,7 @@ class V8_EXPORT_PRIVATE InstructionSelector final {
       size_t* max_pushed_argument_count,
       SourcePositionMode source_position_mode = kCallSourcePositions,
       Features features = SupportedFeatures(),
-      EnableScheduling enable_scheduling = FLAG_turbo_instruction_scheduling
+      EnableScheduling enable_scheduling = v8_flags.turbo_instruction_scheduling
                                                ? kEnableScheduling
                                                : kDisableScheduling,
       EnableRootsRelativeAddressing enable_roots_relative_addressing =
diff --git a/src/compiler/backend/instruction.cc b/src/compiler/backend/instruction.cc
index 5477c9fb86..ea1aa2d9d7 100644
--- a/src/compiler/backend/instruction.cc
+++ b/src/compiler/backend/instruction.cc
@@ -803,7 +803,7 @@ void InstructionSequence::ComputeAssemblyOrder() {
     if (block->ao_number() != invalid) continue;  // loop rotated.
     if (block->IsLoopHeader()) {
       bool header_align = true;
-      if (FLAG_turbo_loop_rotation) {
+      if (v8_flags.turbo_loop_rotation) {
         // Perform loop rotation for non-deferred loops.
         InstructionBlock* loop_end =
             instruction_blocks_->at(block->loop_end().ToSize() - 1);
diff --git a/src/compiler/backend/jump-threading.cc b/src/compiler/backend/jump-threading.cc
index f056cdc945..14a3be024a 100644
--- a/src/compiler/backend/jump-threading.cc
+++ b/src/compiler/backend/jump-threading.cc
@@ -9,9 +9,9 @@ namespace v8 {
 namespace internal {
 namespace compiler {
 
-#define TRACE(...)                                \
-  do {                                            \
-    if (FLAG_trace_turbo_jt) PrintF(__VA_ARGS__); \
+#define TRACE(...)                                    \
+  do {                                                \
+    if (v8_flags.trace_turbo_jt) PrintF(__VA_ARGS__); \
   } while (false)
 
 namespace {
@@ -166,7 +166,7 @@ bool JumpThreading::ComputeForwarding(Zone* local_zone,
   }
 #endif
 
-  if (FLAG_trace_turbo_jt) {
+  if (v8_flags.trace_turbo_jt) {
     for (int i = 0; i < static_cast<int>(result->size()); i++) {
       TRACE("B%d ", i);
       int to = (*result)[i].ToInt();
@@ -184,7 +184,7 @@ bool JumpThreading::ComputeForwarding(Zone* local_zone,
 void JumpThreading::ApplyForwarding(Zone* local_zone,
                                     ZoneVector<RpoNumber> const& result,
                                     InstructionSequence* code) {
-  if (!FLAG_turbo_jt) return;
+  if (!v8_flags.turbo_jt) return;
 
   ZoneVector<bool> skip(static_cast<int>(result.size()), false, local_zone);
 
diff --git a/src/compiler/backend/loong64/code-generator-loong64.cc b/src/compiler/backend/loong64/code-generator-loong64.cc
index 5e54abd396..027910d910 100644
--- a/src/compiler/backend/loong64/code-generator-loong64.cc
+++ b/src/compiler/backend/loong64/code-generator-loong64.cc
@@ -627,7 +627,7 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
     }
     case kArchCallJSFunction: {
       Register func = i.InputRegister(0);
-      if (FLAG_debug_code) {
+      if (v8_flags.debug_code) {
         UseScratchRegisterScope temps(tasm());
         Register scratch = temps.Acquire();
         // Check the function's context matches the context argument.
@@ -692,7 +692,7 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
       int offset = __ root_array_available() ? 36 : 80;  // 9 or 20 instrs
 #endif  // V8_ENABLE_WEBASSEMBLY
 #if V8_HOST_ARCH_LOONG64
-      if (FLAG_debug_code) {
+      if (v8_flags.debug_code) {
         offset += 12;  // see CallCFunction
       }
 #endif
@@ -849,7 +849,7 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
           frame_access_state()->GetFrameOffset(i.InputInt32(0));
       Register base_reg = offset.from_stack_pointer() ? sp : fp;
       __ Add_d(i.OutputRegister(), base_reg, Operand(offset.offset()));
-      if (FLAG_debug_code) {
+      if (v8_flags.debug_code) {
         // Verify that the output_register is properly aligned
         __ And(scratch, i.OutputRegister(), Operand(kSystemPointerSize - 1));
         __ Assert(eq, AbortReason::kAllocationIsNotDoubleAligned, scratch,
@@ -1996,7 +1996,7 @@ void CodeGenerator::AssembleArchTrap(Instruction* instr,
         ReferenceMap* reference_map =
             gen_->zone()->New<ReferenceMap>(gen_->zone());
         gen_->RecordSafepoint(reference_map);
-        if (FLAG_debug_code) {
+        if (v8_flags.debug_code) {
           __ stop();
         }
       }
@@ -2275,7 +2275,7 @@ void CodeGenerator::AssembleConstructFrame() {
       // If the frame is bigger than the stack, we throw the stack overflow
       // exception unconditionally. Thereby we can avoid the integer overflow
       // check in the condition code.
-      if (required_slots * kSystemPointerSize < FLAG_stack_size * KB) {
+      if (required_slots * kSystemPointerSize < v8_flags.stack_size * KB) {
         UseScratchRegisterScope temps(tasm());
         Register scratch = temps.Acquire();
         __ Ld_d(scratch, FieldMemOperand(
@@ -2292,7 +2292,7 @@ void CodeGenerator::AssembleConstructFrame() {
       // define an empty safepoint.
       ReferenceMap* reference_map = zone()->New<ReferenceMap>(zone());
       RecordSafepoint(reference_map);
-      if (FLAG_debug_code) {
+      if (v8_flags.debug_code) {
         __ stop();
       }
 
@@ -2358,7 +2358,7 @@ void CodeGenerator::AssembleReturn(InstructionOperand* additional_pop_count) {
   if (parameter_slots != 0) {
     if (additional_pop_count->IsImmediate()) {
       DCHECK_EQ(g.ToConstant(additional_pop_count).ToInt32(), 0);
-    } else if (FLAG_debug_code) {
+    } else if (v8_flags.debug_code) {
       __ Assert(eq, AbortReason::kUnexpectedAdditionalPopValue,
                 g.ToRegister(additional_pop_count),
                 Operand(static_cast<int64_t>(0)));
diff --git a/src/compiler/backend/loong64/instruction-selector-loong64.cc b/src/compiler/backend/loong64/instruction-selector-loong64.cc
index 2498cdcb07..a3734c2c1f 100644
--- a/src/compiler/backend/loong64/instruction-selector-loong64.cc
+++ b/src/compiler/backend/loong64/instruction-selector-loong64.cc
@@ -492,12 +492,13 @@ void InstructionSelector::VisitStore(Node* node) {
   WriteBarrierKind write_barrier_kind = store_rep.write_barrier_kind();
   MachineRepresentation rep = store_rep.representation();
 
-  if (FLAG_enable_unconditional_write_barriers && CanBeTaggedPointer(rep)) {
+  if (v8_flags.enable_unconditional_write_barriers && CanBeTaggedPointer(rep)) {
     write_barrier_kind = kFullWriteBarrier;
   }
 
   // TODO(loong64): I guess this could be done in a better way.
-  if (write_barrier_kind != kNoWriteBarrier && !FLAG_disable_write_barriers) {
+  if (write_barrier_kind != kNoWriteBarrier &&
+      !v8_flags.disable_write_barriers) {
     DCHECK(CanBeTaggedPointer(rep));
     AddressingMode addressing_mode;
     InstructionOperand inputs[3];
@@ -2021,14 +2022,15 @@ void VisitAtomicStore(InstructionSelector* selector, Node* node,
   WriteBarrierKind write_barrier_kind = store_params.write_barrier_kind();
   MachineRepresentation rep = store_params.representation();
 
-  if (FLAG_enable_unconditional_write_barriers &&
+  if (v8_flags.enable_unconditional_write_barriers &&
       CanBeTaggedOrCompressedPointer(rep)) {
     write_barrier_kind = kFullWriteBarrier;
   }
 
   InstructionCode code;
 
-  if (write_barrier_kind != kNoWriteBarrier && !FLAG_disable_write_barriers) {
+  if (write_barrier_kind != kNoWriteBarrier &&
+      !v8_flags.disable_write_barriers) {
     DCHECK(CanBeTaggedPointer(rep));
     DCHECK_EQ(kTaggedSize, 8);
 
diff --git a/src/compiler/backend/mips64/code-generator-mips64.cc b/src/compiler/backend/mips64/code-generator-mips64.cc
index 00f2cb709b..3d91b751be 100644
--- a/src/compiler/backend/mips64/code-generator-mips64.cc
+++ b/src/compiler/backend/mips64/code-generator-mips64.cc
@@ -646,7 +646,7 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
     }
     case kArchCallJSFunction: {
       Register func = i.InputRegister(0);
-      if (FLAG_debug_code) {
+      if (v8_flags.debug_code) {
         // Check the function's context matches the context argument.
         __ Ld(kScratchReg, FieldMemOperand(func, JSFunction::kContextOffset));
         __ Assert(eq, AbortReason::kWrongFunctionContext, cp,
@@ -708,7 +708,7 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
       int offset = __ root_array_available() ? 64 : 112;
 #endif  // V8_ENABLE_WEBASSEMBLY
 #if V8_HOST_ARCH_MIPS64
-      if (FLAG_debug_code) {
+      if (v8_flags.debug_code) {
         offset += 16;
       }
 #endif
@@ -857,7 +857,7 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
           frame_access_state()->GetFrameOffset(i.InputInt32(0));
       Register base_reg = offset.from_stack_pointer() ? sp : fp;
       __ Daddu(i.OutputRegister(), base_reg, Operand(offset.offset()));
-      if (FLAG_debug_code) {
+      if (v8_flags.debug_code) {
         // Verify that the output_register is properly aligned
         __ And(kScratchReg, i.OutputRegister(),
                Operand(kSystemPointerSize - 1));
@@ -3426,7 +3426,7 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
       if (src0 == src1) {
         // Unary S32x4 shuffles are handled with shf.w instruction
         unsigned lane = shuffle & 0xFF;
-        if (FLAG_debug_code) {
+        if (v8_flags.debug_code) {
           // range of all four lanes, for unary instruction,
           // should belong to the same range, which can be one of these:
           // [0, 3] or [4, 7]
@@ -3926,7 +3926,7 @@ void CodeGenerator::AssembleArchTrap(Instruction* instr,
         ReferenceMap* reference_map =
             gen_->zone()->New<ReferenceMap>(gen_->zone());
         gen_->RecordSafepoint(reference_map);
-        if (FLAG_debug_code) {
+        if (v8_flags.debug_code) {
           __ stop();
         }
       }
@@ -4222,7 +4222,7 @@ void CodeGenerator::AssembleConstructFrame() {
       // If the frame is bigger than the stack, we throw the stack overflow
       // exception unconditionally. Thereby we can avoid the integer overflow
       // check in the condition code.
-      if (required_slots * kSystemPointerSize < FLAG_stack_size * KB) {
+      if (required_slots * kSystemPointerSize < v8_flags.stack_size * KB) {
         __ Ld(
              kScratchReg,
              FieldMemOperand(kWasmInstanceRegister,
@@ -4238,7 +4238,7 @@ void CodeGenerator::AssembleConstructFrame() {
       // define an empty safepoint.
       ReferenceMap* reference_map = zone()->New<ReferenceMap>(zone());
       RecordSafepoint(reference_map);
-      if (FLAG_debug_code) __ stop();
+      if (v8_flags.debug_code) __ stop();
 
       __ bind(&done);
     }
@@ -4302,7 +4302,7 @@ void CodeGenerator::AssembleReturn(InstructionOperand* additional_pop_count) {
   if (parameter_slots != 0) {
     if (additional_pop_count->IsImmediate()) {
       DCHECK_EQ(g.ToConstant(additional_pop_count).ToInt32(), 0);
-    } else if (FLAG_debug_code) {
+    } else if (v8_flags.debug_code) {
       __ Assert(eq, AbortReason::kUnexpectedAdditionalPopValue,
                 g.ToRegister(additional_pop_count),
                 Operand(static_cast<int64_t>(0)));
diff --git a/src/compiler/backend/mips64/instruction-scheduler-mips64.cc b/src/compiler/backend/mips64/instruction-scheduler-mips64.cc
index 597bb17570..59389edf91 100644
--- a/src/compiler/backend/mips64/instruction-scheduler-mips64.cc
+++ b/src/compiler/backend/mips64/instruction-scheduler-mips64.cc
@@ -1274,7 +1274,7 @@ int InstructionScheduler::GetInstructionLatency(const Instruction* instr) {
       return JumpLatency();
     case kArchCallJSFunction: {
       int latency = 0;
-      if (FLAG_debug_code) {
+      if (v8_flags.debug_code) {
         latency = 1 + AssertLatency();
       }
       return latency + 1 + DadduLatency(false) + CallLatency();
diff --git a/src/compiler/backend/mips64/instruction-selector-mips64.cc b/src/compiler/backend/mips64/instruction-selector-mips64.cc
index fc6972fa99..12ff76520a 100644
--- a/src/compiler/backend/mips64/instruction-selector-mips64.cc
+++ b/src/compiler/backend/mips64/instruction-selector-mips64.cc
@@ -528,12 +528,13 @@ void InstructionSelector::VisitStore(Node* node) {
   WriteBarrierKind write_barrier_kind = store_rep.write_barrier_kind();
   MachineRepresentation rep = store_rep.representation();
 
-  if (FLAG_enable_unconditional_write_barriers && CanBeTaggedPointer(rep)) {
+  if (v8_flags.enable_unconditional_write_barriers && CanBeTaggedPointer(rep)) {
     write_barrier_kind = kFullWriteBarrier;
   }
 
   // TODO(mips): I guess this could be done in a better way.
-  if (write_barrier_kind != kNoWriteBarrier && !FLAG_disable_write_barriers) {
+  if (write_barrier_kind != kNoWriteBarrier &&
+      !v8_flags.disable_write_barriers) {
     DCHECK(CanBeTaggedPointer(rep));
     InstructionOperand inputs[3];
     size_t input_count = 0;
@@ -2121,7 +2122,7 @@ void VisitFullWord32Compare(InstructionSelector* selector, Node* node,
 void VisitOptimizedWord32Compare(InstructionSelector* selector, Node* node,
                                  InstructionCode opcode,
                                  FlagsContinuation* cont) {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     Mips64OperandGenerator g(selector);
     InstructionOperand leftOp = g.TempRegister();
     InstructionOperand rightOp = g.TempRegister();
@@ -2256,14 +2257,15 @@ void VisitAtomicStore(InstructionSelector* selector, Node* node,
   WriteBarrierKind write_barrier_kind = store_params.write_barrier_kind();
   MachineRepresentation rep = store_params.representation();
 
-  if (FLAG_enable_unconditional_write_barriers &&
+  if (v8_flags.enable_unconditional_write_barriers &&
       CanBeTaggedOrCompressedPointer(rep)) {
     write_barrier_kind = kFullWriteBarrier;
   }
 
   InstructionCode code;
 
-  if (write_barrier_kind != kNoWriteBarrier && !FLAG_disable_write_barriers) {
+  if (write_barrier_kind != kNoWriteBarrier &&
+      !v8_flags.disable_write_barriers) {
     DCHECK(CanBeTaggedPointer(rep));
     DCHECK_EQ(AtomicWidthSize(width), kTaggedSize);
 
diff --git a/src/compiler/backend/ppc/code-generator-ppc.cc b/src/compiler/backend/ppc/code-generator-ppc.cc
index cd37671dd3..2421e994fc 100644
--- a/src/compiler/backend/ppc/code-generator-ppc.cc
+++ b/src/compiler/backend/ppc/code-generator-ppc.cc
@@ -785,7 +785,7 @@ void CodeGenerator::AssembleCodeStartRegisterCheck() {
 //    2. test kMarkedForDeoptimizationBit in those flags; and
 //    3. if it is not zero then it jumps to the builtin.
 void CodeGenerator::BailoutIfDeoptimized() {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     // Check that {kJavaScriptCallCodeStartRegister} is correct.
     __ ComputeCodeStartAddress(ip);
     __ CmpS64(ip, kJavaScriptCallCodeStartRegister);
@@ -908,7 +908,7 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
       v8::internal::Assembler::BlockTrampolinePoolScope block_trampoline_pool(
           tasm());
       Register func = i.InputRegister(0);
-      if (FLAG_debug_code) {
+      if (v8_flags.debug_code) {
         // Check the function's context matches the context argument.
         __ LoadTaggedPointerField(
             kScratchReg, FieldMemOperand(func, JSFunction::kContextOffset), r0);
@@ -1131,7 +1131,7 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
       Register scratch1 = i.TempRegister(1);
       OutOfLineRecordWrite* ool;
 
-      if (FLAG_debug_code) {
+      if (v8_flags.debug_code) {
         // Checking that |value| is not a cleared weakref: our write barrier
         // does not support that for now.
         __ CmpS64(value, Operand(kClearedWeakHeapObjectLower32), kScratchReg);
@@ -3561,7 +3561,7 @@ void CodeGenerator::AssembleArchTrap(Instruction* instr,
         ReferenceMap* reference_map =
             gen_->zone()->New<ReferenceMap>(gen_->zone());
         gen_->RecordSafepoint(reference_map);
-        if (FLAG_debug_code) {
+        if (v8_flags.debug_code) {
           __ stop();
         }
       }
@@ -3694,13 +3694,13 @@ void CodeGenerator::FinishFrame(Frame* frame) {
   }
   // Save callee-saved registers.
   const RegList saves =
-      FLAG_enable_embedded_constant_pool
+      v8_flags.enable_embedded_constant_pool
           ? call_descriptor->CalleeSavedRegisters() - kConstantPoolRegister
           : call_descriptor->CalleeSavedRegisters();
   if (!saves.is_empty()) {
     // register save area does not include the fp or constant pool pointer.
     const int num_saves =
-        kNumCalleeSaved - 1 - (FLAG_enable_embedded_constant_pool ? 1 : 0);
+        kNumCalleeSaved - 1 - (v8_flags.enable_embedded_constant_pool ? 1 : 0);
     frame->AllocateSavedCalleeRegisterSlots(num_saves);
   }
 }
@@ -3720,7 +3720,7 @@ void CodeGenerator::AssembleConstructFrame() {
 #endif  // V8_ENABLE_WEBASSEMBLY
       } else {
         __ mflr(r0);
-        if (FLAG_enable_embedded_constant_pool) {
+        if (v8_flags.enable_embedded_constant_pool) {
           __ Push(r0, fp, kConstantPoolRegister);
           // Adjust FP to point to saved FP.
           __ SubS64(fp, sp,
@@ -3769,7 +3769,7 @@ void CodeGenerator::AssembleConstructFrame() {
 
   const DoubleRegList saves_fp = call_descriptor->CalleeSavedFPRegisters();
   const RegList saves =
-      FLAG_enable_embedded_constant_pool
+      v8_flags.enable_embedded_constant_pool
           ? call_descriptor->CalleeSavedRegisters() - kConstantPoolRegister
           : call_descriptor->CalleeSavedRegisters();
 
@@ -3785,7 +3785,7 @@ void CodeGenerator::AssembleConstructFrame() {
       // If the frame is bigger than the stack, we throw the stack overflow
       // exception unconditionally. Thereby we can avoid the integer overflow
       // check in the condition code.
-      if (required_slots * kSystemPointerSize < FLAG_stack_size * KB) {
+      if (required_slots * kSystemPointerSize < v8_flags.stack_size * KB) {
         Register scratch = ip;
         __ LoadU64(
             scratch,
@@ -3804,7 +3804,7 @@ void CodeGenerator::AssembleConstructFrame() {
       // define an empty safepoint.
       ReferenceMap* reference_map = zone()->New<ReferenceMap>(zone());
       RecordSafepoint(reference_map);
-      if (FLAG_debug_code) __ stop();
+      if (v8_flags.debug_code) __ stop();
 
       __ bind(&done);
     }
@@ -3845,7 +3845,7 @@ void CodeGenerator::AssembleReturn(InstructionOperand* additional_pop_count) {
 
   // Restore registers.
   const RegList saves =
-      FLAG_enable_embedded_constant_pool
+      v8_flags.enable_embedded_constant_pool
           ? call_descriptor->CalleeSavedRegisters() - kConstantPoolRegister
           : call_descriptor->CalleeSavedRegisters();
   if (!saves.is_empty()) {
@@ -3869,7 +3869,7 @@ void CodeGenerator::AssembleReturn(InstructionOperand* additional_pop_count) {
   if (parameter_slots != 0) {
     if (additional_pop_count->IsImmediate()) {
       DCHECK_EQ(g.ToConstant(additional_pop_count).ToInt32(), 0);
-    } else if (FLAG_debug_code) {
+    } else if (v8_flags.debug_code) {
       __ cmpi(g.ToRegister(additional_pop_count), Operand(0));
       __ Assert(eq, AbortReason::kUnexpectedAdditionalPopValue);
     }
diff --git a/src/compiler/backend/ppc/instruction-selector-ppc.cc b/src/compiler/backend/ppc/instruction-selector-ppc.cc
index d1e492c228..067c7643da 100644
--- a/src/compiler/backend/ppc/instruction-selector-ppc.cc
+++ b/src/compiler/backend/ppc/instruction-selector-ppc.cc
@@ -286,12 +286,13 @@ void VisitStoreCommon(InstructionSelector* selector, Node* node,
     write_barrier_kind = store_rep.write_barrier_kind();
   }
 
-  if (FLAG_enable_unconditional_write_barriers &&
+  if (v8_flags.enable_unconditional_write_barriers &&
       CanBeTaggedOrCompressedPointer(rep)) {
     write_barrier_kind = kFullWriteBarrier;
   }
 
-  if (write_barrier_kind != kNoWriteBarrier && !FLAG_disable_write_barriers) {
+  if (write_barrier_kind != kNoWriteBarrier &&
+      !v8_flags.disable_write_barriers) {
     DCHECK(CanBeTaggedOrCompressedPointer(rep));
     AddressingMode addressing_mode;
     InstructionOperand inputs[3];
diff --git a/src/compiler/backend/ppc/unwinding-info-writer-ppc.h b/src/compiler/backend/ppc/unwinding-info-writer-ppc.h
index e96a48308f..b11e00be2d 100644
--- a/src/compiler/backend/ppc/unwinding-info-writer-ppc.h
+++ b/src/compiler/backend/ppc/unwinding-info-writer-ppc.h
@@ -49,7 +49,7 @@ class UnwindingInfoWriter {
   }
 
  private:
-  bool enabled() const { return FLAG_perf_prof_unwinding_info; }
+  bool enabled() const { return v8_flags.perf_prof_unwinding_info; }
 
   class BlockInitialState : public ZoneObject {
    public:
diff --git a/src/compiler/backend/riscv/code-generator-riscv.cc b/src/compiler/backend/riscv/code-generator-riscv.cc
index 7ff828b0ec..f307f16709 100644
--- a/src/compiler/backend/riscv/code-generator-riscv.cc
+++ b/src/compiler/backend/riscv/code-generator-riscv.cc
@@ -721,7 +721,7 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
     }
     case kArchCallJSFunction: {
       Register func = i.InputOrZeroRegister(0);
-      if (FLAG_debug_code) {
+      if (v8_flags.debug_code) {
         // Check the function's context matches the context argument.
         __ LoadTaggedPointerField(
             kScratchReg, FieldMemOperand(func, JSFunction::kContextOffset));
@@ -905,7 +905,7 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
       int alignment = i.InputInt32(1);
       DCHECK(alignment == 0 || alignment == 4 || alignment == 8 ||
              alignment == 16);
-      if (FLAG_debug_code && alignment > 0) {
+      if (v8_flags.debug_code && alignment > 0) {
         // Verify that the output_register is properly aligned
         __ And(kScratchReg, i.OutputRegister(),
                Operand(kSystemPointerSize - 1));
@@ -3855,7 +3855,7 @@ void CodeGenerator::AssembleArchTrap(Instruction* instr,
         ReferenceMap* reference_map =
             gen_->zone()->New<ReferenceMap>(gen_->zone());
         gen_->RecordSafepoint(reference_map);
-        if (FLAG_debug_code) {
+        if (v8_flags.debug_code) {
           __ stop();
         }
       }
@@ -4193,7 +4193,8 @@ void CodeGenerator::AssembleConstructFrame() {
       // If the frame is bigger than the stack, we throw the stack overflow
       // exception unconditionally. Thereby we can avoid the integer overflow
       // check in the condition code.
-      if ((required_slots * kSystemPointerSize) < (FLAG_stack_size * 1024)) {
+      if ((required_slots * kSystemPointerSize) <
+          (v8_flags.stack_size * 1024)) {
         __ LoadWord(
             kScratchReg,
             FieldMemOperand(kWasmInstanceRegister,
@@ -4208,7 +4209,7 @@ void CodeGenerator::AssembleConstructFrame() {
       // We come from WebAssembly, there are no references for the GC.
       ReferenceMap* reference_map = zone()->New<ReferenceMap>(zone());
       RecordSafepoint(reference_map);
-      if (FLAG_debug_code) {
+      if (v8_flags.debug_code) {
         __ stop();
       }
 
@@ -4273,7 +4274,7 @@ void CodeGenerator::AssembleReturn(InstructionOperand* additional_pop_count) {
   if (parameter_slots != 0) {
     if (additional_pop_count->IsImmediate()) {
       DCHECK_EQ(g.ToConstant(additional_pop_count).ToInt32(), 0);
-    } else if (FLAG_debug_code) {
+    } else if (v8_flags.debug_code) {
       __ Assert(eq, AbortReason::kUnexpectedAdditionalPopValue,
                 g.ToRegister(additional_pop_count),
                 Operand(static_cast<intptr_t>(0)));
diff --git a/src/compiler/backend/riscv/instruction-scheduler-riscv.cc b/src/compiler/backend/riscv/instruction-scheduler-riscv.cc
index 879ac4393f..db6d62a40c 100644
--- a/src/compiler/backend/riscv/instruction-scheduler-riscv.cc
+++ b/src/compiler/backend/riscv/instruction-scheduler-riscv.cc
@@ -1128,7 +1128,7 @@ int InstructionScheduler::GetInstructionLatency(const Instruction* instr) {
       return JumpLatency();
     case kArchCallJSFunction: {
       int latency = 0;
-      if (FLAG_debug_code) {
+      if (v8_flags.debug_code) {
         latency = 1 + AssertLatency();
       }
       return latency + 1 + Add64Latency(false) + CallLatency();
diff --git a/src/compiler/backend/riscv/instruction-selector-riscv32.cc b/src/compiler/backend/riscv/instruction-selector-riscv32.cc
index 4d01a47b70..a8db8248b3 100644
--- a/src/compiler/backend/riscv/instruction-selector-riscv32.cc
+++ b/src/compiler/backend/riscv/instruction-selector-riscv32.cc
@@ -204,7 +204,7 @@ void InstructionSelector::VisitStore(Node* node) {
 
   // TODO(riscv): I guess this could be done in a better way.
   if (write_barrier_kind != kNoWriteBarrier &&
-      V8_LIKELY(!FLAG_disable_write_barriers)) {
+      V8_LIKELY(!v8_flags.disable_write_barriers)) {
     DCHECK(CanBeTaggedPointer(rep));
     InstructionOperand inputs[3];
     size_t input_count = 0;
diff --git a/src/compiler/backend/riscv/instruction-selector-riscv64.cc b/src/compiler/backend/riscv/instruction-selector-riscv64.cc
index e1b6ff7eee..cffbfcb410 100644
--- a/src/compiler/backend/riscv/instruction-selector-riscv64.cc
+++ b/src/compiler/backend/riscv/instruction-selector-riscv64.cc
@@ -326,7 +326,7 @@ void InstructionSelector::VisitStore(Node* node) {
 
   // TODO(riscv): I guess this could be done in a better way.
   if (write_barrier_kind != kNoWriteBarrier &&
-      V8_LIKELY(!FLAG_disable_write_barriers)) {
+      V8_LIKELY(!v8_flags.disable_write_barriers)) {
     DCHECK(CanBeTaggedPointer(rep));
     InstructionOperand inputs[3];
     size_t input_count = 0;
@@ -1441,7 +1441,7 @@ void VisitFullWord32Compare(InstructionSelector* selector, Node* node,
 void VisitOptimizedWord32Compare(InstructionSelector* selector, Node* node,
                                  InstructionCode opcode,
                                  FlagsContinuation* cont) {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     RiscvOperandGenerator g(selector);
     InstructionOperand leftOp = g.TempRegister();
     InstructionOperand rightOp = g.TempRegister();
diff --git a/src/compiler/backend/s390/code-generator-s390.cc b/src/compiler/backend/s390/code-generator-s390.cc
index 6f5ed9b981..e7df15f318 100644
--- a/src/compiler/backend/s390/code-generator-s390.cc
+++ b/src/compiler/backend/s390/code-generator-s390.cc
@@ -1124,7 +1124,7 @@ void CodeGenerator::AssembleCodeStartRegisterCheck() {
 //    2. test kMarkedForDeoptimizationBit in those flags; and
 //    3. if it is not zero then it jumps to the builtin.
 void CodeGenerator::BailoutIfDeoptimized() {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     // Check that {kJavaScriptCallCodeStartRegister} is correct.
     __ ComputeCodeStartAddress(ip);
     __ CmpS64(ip, kJavaScriptCallCodeStartRegister);
@@ -1237,7 +1237,7 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
     }
     case kArchCallJSFunction: {
       Register func = i.InputRegister(0);
-      if (FLAG_debug_code) {
+      if (v8_flags.debug_code) {
         // Check the function's context matches the context argument.
         __ LoadTaggedPointerField(
             kScratchReg, FieldMemOperand(func, JSFunction::kContextOffset));
@@ -1417,7 +1417,7 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
       Register scratch1 = i.TempRegister(1);
       OutOfLineRecordWrite* ool;
 
-      if (FLAG_debug_code) {
+      if (v8_flags.debug_code) {
         // Checking that |value| is not a cleared weakref: our write barrier
         // does not support that for now.
         __ CmpS64(value, Operand(kClearedWeakHeapObjectLower32));
@@ -3240,7 +3240,7 @@ void CodeGenerator::AssembleArchTrap(Instruction* instr,
         ReferenceMap* reference_map =
             gen_->zone()->New<ReferenceMap>(gen_->zone());
         gen_->RecordSafepoint(reference_map);
-        if (FLAG_debug_code) {
+        if (v8_flags.debug_code) {
           __ stop();
         }
       }
@@ -3423,7 +3423,7 @@ void CodeGenerator::AssembleConstructFrame() {
       // If the frame is bigger than the stack, we throw the stack overflow
       // exception unconditionally. Thereby we can avoid the integer overflow
       // check in the condition code.
-      if (required_slots * kSystemPointerSize < FLAG_stack_size * KB) {
+      if (required_slots * kSystemPointerSize < v8_flags.stack_size * KB) {
         Register scratch = r1;
         __ LoadU64(
             scratch,
@@ -3441,7 +3441,7 @@ void CodeGenerator::AssembleConstructFrame() {
       // define an empty safepoint.
       ReferenceMap* reference_map = zone()->New<ReferenceMap>(zone());
       RecordSafepoint(reference_map);
-      if (FLAG_debug_code) __ stop();
+      if (v8_flags.debug_code) __ stop();
 
       __ bind(&done);
     }
@@ -3503,7 +3503,7 @@ void CodeGenerator::AssembleReturn(InstructionOperand* additional_pop_count) {
   if (parameter_slots != 0) {
     if (additional_pop_count->IsImmediate()) {
       DCHECK_EQ(g.ToConstant(additional_pop_count).ToInt32(), 0);
-    } else if (FLAG_debug_code) {
+    } else if (v8_flags.debug_code) {
       __ CmpS64(g.ToRegister(additional_pop_count), Operand(0));
       __ Assert(eq, AbortReason::kUnexpectedAdditionalPopValue);
     }
diff --git a/src/compiler/backend/s390/instruction-selector-s390.cc b/src/compiler/backend/s390/instruction-selector-s390.cc
index fc49a1376e..0899e1f7de 100644
--- a/src/compiler/backend/s390/instruction-selector-s390.cc
+++ b/src/compiler/backend/s390/instruction-selector-s390.cc
@@ -725,7 +725,8 @@ static void VisitGeneralStore(
   Node* base = node->InputAt(0);
   Node* offset = node->InputAt(1);
   Node* value = node->InputAt(2);
-  if (write_barrier_kind != kNoWriteBarrier && !FLAG_disable_write_barriers) {
+  if (write_barrier_kind != kNoWriteBarrier &&
+      !v8_flags.disable_write_barriers) {
     DCHECK(CanBeTaggedOrCompressedPointer(rep));
     AddressingMode addressing_mode;
     InstructionOperand inputs[3];
@@ -824,7 +825,7 @@ void InstructionSelector::VisitStore(Node* node) {
   WriteBarrierKind write_barrier_kind = store_rep.write_barrier_kind();
   MachineRepresentation rep = store_rep.representation();
 
-  if (FLAG_enable_unconditional_write_barriers &&
+  if (v8_flags.enable_unconditional_write_barriers &&
       CanBeTaggedOrCompressedPointer(rep)) {
     write_barrier_kind = kFullWriteBarrier;
   }
diff --git a/src/compiler/backend/s390/unwinding-info-writer-s390.h b/src/compiler/backend/s390/unwinding-info-writer-s390.h
index 2202c28595..7c472723c4 100644
--- a/src/compiler/backend/s390/unwinding-info-writer-s390.h
+++ b/src/compiler/backend/s390/unwinding-info-writer-s390.h
@@ -49,7 +49,7 @@ class UnwindingInfoWriter {
   }
 
  private:
-  bool enabled() const { return FLAG_perf_prof_unwinding_info; }
+  bool enabled() const { return v8_flags.perf_prof_unwinding_info; }
 
   class BlockInitialState : public ZoneObject {
    public:
diff --git a/src/compiler/backend/spill-placer.cc b/src/compiler/backend/spill-placer.cc
index 01d130ff59..5cf4861577 100644
--- a/src/compiler/backend/spill-placer.cc
+++ b/src/compiler/backend/spill-placer.cc
@@ -45,7 +45,7 @@ void SpillPlacer::Add(TopLevelLiveRange* range) {
   //   increasing the code size for no benefit.
   if (range->GetSpillMoveInsertionLocations(data()) == nullptr ||
       range->spilled() || top_start_block->IsDeferred() ||
-      (!FLAG_stress_turbo_late_spilling && !range->is_loop_phi())) {
+      (!v8_flags.stress_turbo_late_spilling && !range->is_loop_phi())) {
     range->CommitSpillMoves(data(), spill_operand);
     return;
   }
diff --git a/src/compiler/backend/unwinding-info-writer.h b/src/compiler/backend/unwinding-info-writer.h
index ecc9658d33..446bd82f57 100644
--- a/src/compiler/backend/unwinding-info-writer.h
+++ b/src/compiler/backend/unwinding-info-writer.h
@@ -33,7 +33,7 @@ namespace compiler {
 
 class InstructionBlock;
 
-static_assert(!FLAG_perf_prof_unwinding_info.value(),
+static_assert(!v8_flags.perf_prof_unwinding_info.value(),
               "--perf-prof-unwinding-info should be statically disabled if not "
               "supported");
 
diff --git a/src/compiler/backend/x64/code-generator-x64.cc b/src/compiler/backend/x64/code-generator-x64.cc
index 7d66748d13..6a36c54e04 100644
--- a/src/compiler/backend/x64/code-generator-x64.cc
+++ b/src/compiler/backend/x64/code-generator-x64.cc
@@ -481,7 +481,7 @@ class WasmProtectedInstructionTrap final : public WasmOutOfLineTrap {
       : WasmOutOfLineTrap(gen, instr), pc_(pc) {}
 
   void Generate() final {
-    DCHECK(FLAG_wasm_bounds_checks && !FLAG_wasm_enforce_bounds_checks);
+    DCHECK(v8_flags.wasm_bounds_checks && !v8_flags.wasm_enforce_bounds_checks);
     gen_->AddProtectedInstructionLanding(pc_, __ pc_offset());
     GenerateWithTrapId(TrapId::kTrapMemOutOfBounds);
   }
@@ -1318,7 +1318,7 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
     }
     case kArchCallJSFunction: {
       Register func = i.InputRegister(0);
-      if (FLAG_debug_code) {
+      if (v8_flags.debug_code) {
         // Check the function's context matches the context argument.
         __ cmp_tagged(rsi, FieldOperand(func, JSFunction::kContextOffset));
         __ Assert(equal, AbortReason::kWrongFunctionContext);
@@ -1515,7 +1515,7 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
       Register scratch0 = i.TempRegister(0);
       Register scratch1 = i.TempRegister(1);
 
-      if (FLAG_debug_code) {
+      if (v8_flags.debug_code) {
         // Checking that |value| is not a cleared weakref: our write barrier
         // does not support that for now.
         __ Cmp(value, kClearedWeakHeapObjectLower32);
@@ -4605,7 +4605,7 @@ void CodeGenerator::AssembleArchDeoptBranch(Instruction* instr,
   }
   __ j(FlagsConditionToCondition(branch->condition), tlabel);
 
-  if (FLAG_deopt_every_n_times > 0) {
+  if (v8_flags.deopt_every_n_times > 0) {
     ExternalReference counter =
         ExternalReference::stress_deopt_count(isolate());
 
@@ -4615,7 +4615,7 @@ void CodeGenerator::AssembleArchDeoptBranch(Instruction* instr,
     __ decl(rax);
     __ j(not_zero, &nodeopt, Label::kNear);
 
-    __ Move(rax, FLAG_deopt_every_n_times);
+    __ Move(rax, v8_flags.deopt_every_n_times);
     __ store_rax(counter);
     __ popq(rax);
     __ popfq();
@@ -4840,7 +4840,7 @@ void CodeGenerator::AssembleConstructFrame() {
       // If the frame is bigger than the stack, we throw the stack overflow
       // exception unconditionally. Thereby we can avoid the integer overflow
       // check in the condition code.
-      if (required_slots * kSystemPointerSize < FLAG_stack_size * KB) {
+      if (required_slots * kSystemPointerSize < v8_flags.stack_size * KB) {
         __ movq(kScratchRegister,
                 FieldOperand(kWasmInstanceRegister,
                              WasmInstanceObject::kRealStackLimitAddressOffset));
@@ -4934,7 +4934,7 @@ void CodeGenerator::AssembleReturn(InstructionOperand* additional_pop_count) {
   if (parameter_slots != 0) {
     if (additional_pop_count->IsImmediate()) {
       DCHECK_EQ(g.ToConstant(additional_pop_count).ToInt32(), 0);
-    } else if (FLAG_debug_code) {
+    } else if (v8_flags.debug_code) {
       __ cmpq(g.ToRegister(additional_pop_count), Immediate(0));
       __ Assert(equal, AbortReason::kUnexpectedAdditionalPopValue);
     }
@@ -5015,7 +5015,7 @@ void CodeGenerator::PrepareForDeoptimizationExits(
 
 void CodeGenerator::IncrementStackAccessCounter(
     InstructionOperand* source, InstructionOperand* destination) {
-  DCHECK(FLAG_trace_turbo_stack_accesses);
+  DCHECK(v8_flags.trace_turbo_stack_accesses);
   if (!info()->IsOptimizing()) {
 #if V8_ENABLE_WEBASSEMBLY
     if (!info()->IsWasm()) return;
@@ -5203,7 +5203,7 @@ void CodeGenerator::AssembleMove(InstructionOperand* source,
     __ movq(dst, kScratchRegister);
   };
 
-  if (FLAG_trace_turbo_stack_accesses) {
+  if (v8_flags.trace_turbo_stack_accesses) {
     IncrementStackAccessCounter(source, destination);
   }
 
@@ -5322,7 +5322,7 @@ void CodeGenerator::AssembleMove(InstructionOperand* source,
 
 void CodeGenerator::AssembleSwap(InstructionOperand* source,
                                  InstructionOperand* destination) {
-  if (FLAG_trace_turbo_stack_accesses) {
+  if (v8_flags.trace_turbo_stack_accesses) {
     IncrementStackAccessCounter(source, destination);
     IncrementStackAccessCounter(destination, source);
   }
diff --git a/src/compiler/backend/x64/instruction-selector-x64.cc b/src/compiler/backend/x64/instruction-selector-x64.cc
index 09d005bce9..2ba211669f 100644
--- a/src/compiler/backend/x64/instruction-selector-x64.cc
+++ b/src/compiler/backend/x64/instruction-selector-x64.cc
@@ -597,7 +597,7 @@ void VisitStoreCommon(InstructionSelector* selector, Node* node,
   const bool is_seqcst =
       atomic_order && *atomic_order == AtomicMemoryOrder::kSeqCst;
 
-  if (FLAG_enable_unconditional_write_barriers &&
+  if (v8_flags.enable_unconditional_write_barriers &&
       CanBeTaggedOrCompressedPointer(store_rep.representation())) {
     write_barrier_kind = kFullWriteBarrier;
   }
@@ -606,7 +606,8 @@ void VisitStoreCommon(InstructionSelector* selector, Node* node,
     ? MemoryAccessMode::kMemoryAccessProtected
     : MemoryAccessMode::kMemoryAccessDirect;
 
-  if (write_barrier_kind != kNoWriteBarrier && !FLAG_disable_write_barriers) {
+  if (write_barrier_kind != kNoWriteBarrier &&
+      !v8_flags.disable_write_barriers) {
     DCHECK(CanBeTaggedOrCompressedPointer(store_rep.representation()));
     AddressingMode addressing_mode;
     InstructionOperand inputs[] = {
diff --git a/src/compiler/backend/x64/unwinding-info-writer-x64.h b/src/compiler/backend/x64/unwinding-info-writer-x64.h
index c85ad46a63..188a66f56f 100644
--- a/src/compiler/backend/x64/unwinding-info-writer-x64.h
+++ b/src/compiler/backend/x64/unwinding-info-writer-x64.h
@@ -53,7 +53,7 @@ class UnwindingInfoWriter {
   }
 
  private:
-  bool enabled() const { return FLAG_perf_prof_unwinding_info; }
+  bool enabled() const { return v8_flags.perf_prof_unwinding_info; }
 
   class BlockInitialState : public ZoneObject {
    public:
diff --git a/src/compiler/basic-block-instrumentor.cc b/src/compiler/basic-block-instrumentor.cc
index 2d5a02985a..1e9d0246b6 100644
--- a/src/compiler/basic-block-instrumentor.cc
+++ b/src/compiler/basic-block-instrumentor.cc
@@ -64,7 +64,7 @@ BasicBlockProfilerData* BasicBlockInstrumentor::Instrument(
   // Set the function name.
   data->SetFunctionName(info->GetDebugName());
   // Capture the schedule string before instrumentation.
-  if (FLAG_turbo_profiling_verbose) {
+  if (v8_flags.turbo_profiling_verbose) {
     std::ostringstream os;
     os << *schedule;
     data->SetSchedule(os);
diff --git a/src/compiler/bytecode-analysis.cc b/src/compiler/bytecode-analysis.cc
index 0b5ee57767..71275f933b 100644
--- a/src/compiler/bytecode-analysis.cc
+++ b/src/compiler/bytecode-analysis.cc
@@ -704,7 +704,7 @@ void BytecodeAnalysis::Analyze() {
   }
 
   DCHECK(analyze_liveness_);
-  if (FLAG_trace_environment_liveness) {
+  if (v8_flags.trace_environment_liveness) {
     StdoutStream of;
     PrintLivenessTo(of);
   }
diff --git a/src/compiler/code-assembler.cc b/src/compiler/code-assembler.cc
index d07482f24c..7b69cea027 100644
--- a/src/compiler/code-assembler.cc
+++ b/src/compiler/code-assembler.cc
@@ -501,7 +501,7 @@ void CodeAssembler::Unreachable() {
 }
 
 void CodeAssembler::Comment(std::string str) {
-  if (!FLAG_code_comments) return;
+  if (!v8_flags.code_comments) return;
   raw_assembler()->Comment(str);
 }
 
@@ -1549,7 +1549,7 @@ void CodeAssemblerLabel::Bind(AssemblerDebugInfo debug_info) {
         << "\n#    previous: " << *label_->block();
     FATAL("%s", str.str().c_str());
   }
-  if (FLAG_enable_source_at_csa_bind) {
+  if (v8_flags.enable_source_at_csa_bind) {
     state_->raw_assembler_->SetCurrentExternalSourcePosition(
         {debug_info.file, debug_info.line});
   }
diff --git a/src/compiler/code-assembler.h b/src/compiler/code-assembler.h
index a071b31c60..82a02fb178 100644
--- a/src/compiler/code-assembler.h
+++ b/src/compiler/code-assembler.h
@@ -435,7 +435,7 @@ class V8_EXPORT_PRIVATE CodeAssembler {
               !std::is_convertible<TNode<PreviousType>, TNode<A>>::value,
           "Unnecessary CAST: types are convertible.");
 #ifdef DEBUG
-      if (FLAG_debug_code) {
+      if (v8_flags.debug_code) {
         TNode<ExternalReference> function = code_assembler_->ExternalConstant(
             ExternalReference::check_object_type());
         code_assembler_->CallCFunction(
@@ -618,13 +618,13 @@ class V8_EXPORT_PRIVATE CodeAssembler {
   void DebugBreak();
   void Unreachable();
   void Comment(const char* msg) {
-    if (!FLAG_code_comments) return;
+    if (!v8_flags.code_comments) return;
     Comment(std::string(msg));
   }
   void Comment(std::string msg);
   template <class... Args>
   void Comment(Args&&... args) {
-    if (!FLAG_code_comments) return;
+    if (!v8_flags.code_comments) return;
     std::ostringstream s;
     USE((s << std::forward<Args>(args))...);
     Comment(s.str());
diff --git a/src/compiler/compilation-dependencies.cc b/src/compiler/compilation-dependencies.cc
index 761e6731f3..abf9911284 100644
--- a/src/compiler/compilation-dependencies.cc
+++ b/src/compiler/compilation-dependencies.cc
@@ -124,7 +124,7 @@ class PendingDependencies final {
   }
 
   void InstallAll(Isolate* isolate, Handle<Code> code) {
-    if (V8_UNLIKELY(FLAG_predictable)) {
+    if (V8_UNLIKELY(v8_flags.predictable)) {
       InstallAllPredictable(isolate, code);
       return;
     }
@@ -139,7 +139,7 @@ class PendingDependencies final {
   }
 
   void InstallAllPredictable(Isolate* isolate, Handle<Code> code) {
-    CHECK(FLAG_predictable);
+    CHECK(v8_flags.predictable);
     // First, guarantee predictable iteration order.
     using HandleAndGroup =
         std::pair<Handle<HeapObject>, DependentCode::DependencyGroups>;
@@ -1028,7 +1028,7 @@ void CompilationDependencies::DependOnConstantInDictionaryPrototypeChain(
 
 AllocationType CompilationDependencies::DependOnPretenureMode(
     const AllocationSiteRef& site) {
-  if (!FLAG_allocation_site_pretenuring) return AllocationType::kYoung;
+  if (!v8_flags.allocation_site_pretenuring) return AllocationType::kYoung;
   AllocationType allocation = site.GetAllocationType();
   RecordDependency(zone_->New<PretenureModeDependency>(site, allocation));
   return allocation;
@@ -1141,7 +1141,7 @@ void CompilationDependencies::DependOnOwnConstantDictionaryProperty(
 
 V8_INLINE void TraceInvalidCompilationDependency(
     const CompilationDependency* d) {
-  DCHECK(FLAG_trace_compilation_dependencies);
+  DCHECK(v8_flags.trace_compilation_dependencies);
   DCHECK(!d->IsValid());
   PrintF("Compilation aborted due to invalid dependency: %s\n", d->ToString());
 }
@@ -1159,7 +1159,7 @@ bool CompilationDependencies::Commit(Handle<Code> code) {
       // can call EnsureHasInitialMap, which can invalidate a
       // StableMapDependency on the prototype object's map.
       if (!dep->IsValid()) {
-        if (FLAG_trace_compilation_dependencies) {
+        if (v8_flags.trace_compilation_dependencies) {
           TraceInvalidCompilationDependency(dep);
         }
         dependencies_.clear();
@@ -1183,7 +1183,7 @@ bool CompilationDependencies::Commit(Handle<Code> code) {
   //    deoptimization.
   // 2. since the function state was deemed consistent above, that means the
   //    compilation saw a self-consistent state of the jsfunction.
-  if (FLAG_stress_gc_during_compilation) {
+  if (v8_flags.stress_gc_during_compilation) {
     broker_->isolate()->heap()->PreciseCollectAllGarbage(
         Heap::kForcedGC, GarbageCollectionReason::kTesting, kNoGCCallbackFlags);
   }
@@ -1199,13 +1199,13 @@ bool CompilationDependencies::Commit(Handle<Code> code) {
 }
 
 bool CompilationDependencies::PrepareInstall() {
-  if (V8_UNLIKELY(FLAG_predictable)) {
+  if (V8_UNLIKELY(v8_flags.predictable)) {
     return PrepareInstallPredictable();
   }
 
   for (auto dep : dependencies_) {
     if (!dep->IsValid()) {
-      if (FLAG_trace_compilation_dependencies) {
+      if (v8_flags.trace_compilation_dependencies) {
         TraceInvalidCompilationDependency(dep);
       }
       dependencies_.clear();
@@ -1217,7 +1217,7 @@ bool CompilationDependencies::PrepareInstall() {
 }
 
 bool CompilationDependencies::PrepareInstallPredictable() {
-  CHECK(FLAG_predictable);
+  CHECK(v8_flags.predictable);
 
   std::vector<const CompilationDependency*> deps(dependencies_.begin(),
                                                  dependencies_.end());
@@ -1225,7 +1225,7 @@ bool CompilationDependencies::PrepareInstallPredictable() {
 
   for (auto dep : deps) {
     if (!dep->IsValid()) {
-      if (FLAG_trace_compilation_dependencies) {
+      if (v8_flags.trace_compilation_dependencies) {
         TraceInvalidCompilationDependency(dep);
       }
       dependencies_.clear();
diff --git a/src/compiler/constant-folding-reducer.cc b/src/compiler/constant-folding-reducer.cc
index c768441d29..5e74ba7535 100644
--- a/src/compiler/constant-folding-reducer.cc
+++ b/src/compiler/constant-folding-reducer.cc
@@ -42,7 +42,7 @@ Node* TryGetConstant(JSGraph* jsgraph, Node* node) {
 }
 
 bool IsAlreadyBeingFolded(Node* node) {
-  DCHECK(FLAG_assert_types);
+  DCHECK(v8_flags.assert_types);
   if (node->opcode() == IrOpcode::kFoldConstant) return true;
   for (Edge edge : node->use_edges()) {
     if (NodeProperties::IsValueEdge(edge) &&
@@ -70,7 +70,7 @@ Reduction ConstantFoldingReducer::Reduce(Node* node) {
     Node* constant = TryGetConstant(jsgraph(), node);
     if (constant != nullptr) {
       DCHECK(NodeProperties::IsTyped(constant));
-      if (!FLAG_assert_types) {
+      if (!v8_flags.assert_types) {
         DCHECK_EQ(node->op()->ControlOutputCount(), 0);
         ReplaceWithValue(node, constant);
         return Replace(constant);
diff --git a/src/compiler/control-equivalence.cc b/src/compiler/control-equivalence.cc
index 4649cf0d6b..f170813962 100644
--- a/src/compiler/control-equivalence.cc
+++ b/src/compiler/control-equivalence.cc
@@ -5,9 +5,9 @@
 #include "src/compiler/control-equivalence.h"
 #include "src/compiler/node-properties.h"
 
-#define TRACE(...)                                 \
-  do {                                             \
-    if (FLAG_trace_turbo_ceq) PrintF(__VA_ARGS__); \
+#define TRACE(...)                                     \
+  do {                                                 \
+    if (v8_flags.trace_turbo_ceq) PrintF(__VA_ARGS__); \
   } while (false)
 
 namespace v8 {
@@ -219,7 +219,7 @@ void ControlEquivalence::BracketListDelete(BracketList& blist, Node* to,
 
 
 void ControlEquivalence::BracketListTRACE(BracketList& blist) {
-  if (FLAG_trace_turbo_ceq) {
+  if (v8_flags.trace_turbo_ceq) {
     TRACE("  BList: ");
     for (Bracket bracket : blist) {
       TRACE("{%d->%d} ", bracket.from->id(), bracket.to->id());
diff --git a/src/compiler/csa-load-elimination.cc b/src/compiler/csa-load-elimination.cc
index 4cfce6fa51..43f5572e78 100644
--- a/src/compiler/csa-load-elimination.cc
+++ b/src/compiler/csa-load-elimination.cc
@@ -14,7 +14,7 @@ namespace internal {
 namespace compiler {
 
 Reduction CsaLoadElimination::Reduce(Node* node) {
-  if (FLAG_trace_turbo_load_elimination) {
+  if (v8_flags.trace_turbo_load_elimination) {
     if (node->op()->EffectInputCount() > 0) {
       PrintF(" visit #%d:%s", node->id(), node->op()->mnemonic());
       if (node->op()->ValueInputCount() > 0) {
diff --git a/src/compiler/escape-analysis-reducer.cc b/src/compiler/escape-analysis-reducer.cc
index d0e705610b..9f760f2c0b 100644
--- a/src/compiler/escape-analysis-reducer.cc
+++ b/src/compiler/escape-analysis-reducer.cc
@@ -15,9 +15,9 @@ namespace internal {
 namespace compiler {
 
 #ifdef DEBUG
-#define TRACE(...)                                    \
-  do {                                                \
-    if (FLAG_trace_turbo_escape) PrintF(__VA_ARGS__); \
+#define TRACE(...)                                        \
+  do {                                                    \
+    if (v8_flags.trace_turbo_escape) PrintF(__VA_ARGS__); \
   } while (false)
 #else
 #define TRACE(...)
diff --git a/src/compiler/escape-analysis.cc b/src/compiler/escape-analysis.cc
index 94e5c86f97..86f6a157fc 100644
--- a/src/compiler/escape-analysis.cc
+++ b/src/compiler/escape-analysis.cc
@@ -14,9 +14,9 @@
 #include "src/objects/map-inl.h"
 
 #ifdef DEBUG
-#define TRACE(...)                                    \
-  do {                                                \
-    if (FLAG_trace_turbo_escape) PrintF(__VA_ARGS__); \
+#define TRACE(...)                                        \
+  do {                                                    \
+    if (v8_flags.trace_turbo_escape) PrintF(__VA_ARGS__); \
   } while (false)
 #else
 #define TRACE(...)
diff --git a/src/compiler/fast-api-calls.cc b/src/compiler/fast-api-calls.cc
index 08f9554a6f..02faf86673 100644
--- a/src/compiler/fast-api-calls.cc
+++ b/src/compiler/fast-api-calls.cc
@@ -175,7 +175,7 @@ Node* FastApiCallBuilder::WrapFastCall(const CallDescriptor* call_descriptor,
       ExternalReference::javascript_execution_assert(isolate()));
   static_assert(sizeof(bool) == 1, "Wrong assumption about boolean size.");
 
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     auto do_store = __ MakeLabel();
     Node* old_scope_value =
         __ Load(MachineType::Int8(), javascript_execution_assert, 0);
diff --git a/src/compiler/frame.cc b/src/compiler/frame.cc
index 0f2a2b478b..dd4abbd840 100644
--- a/src/compiler/frame.cc
+++ b/src/compiler/frame.cc
@@ -46,7 +46,7 @@ void FrameAccessState::MarkHasFrame(bool state) {
 }
 
 void FrameAccessState::SetFrameAccessToDefault() {
-  if (has_frame() && !FLAG_turbo_sp_frame_access) {
+  if (has_frame() && !v8_flags.turbo_sp_frame_access) {
     SetFrameAccessToFP();
   } else {
     SetFrameAccessToSP();
diff --git a/src/compiler/globals.h b/src/compiler/globals.h
index c379ecf20a..e085194eeb 100644
--- a/src/compiler/globals.h
+++ b/src/compiler/globals.h
@@ -22,7 +22,7 @@ namespace compiler {
 // TODO(jgruber): Remove once we've made a decision whether to collect feedback
 // unconditionally.
 inline bool CollectFeedbackInGenericLowering() {
-  return FLAG_turbo_collect_feedback_in_generic_lowering;
+  return v8_flags.turbo_collect_feedback_in_generic_lowering;
 }
 
 enum class StackCheckKind : uint8_t {
diff --git a/src/compiler/graph-reducer.cc b/src/compiler/graph-reducer.cc
index 48d84c2179..043b92a53d 100644
--- a/src/compiler/graph-reducer.cc
+++ b/src/compiler/graph-reducer.cc
@@ -109,7 +109,7 @@ Reduction GraphReducer::Reduce(Node* const node) {
         // {replacement} == {node} represents an in-place reduction. Rerun
         // all the other reducers for this node, as now there may be more
         // opportunities for reduction.
-        if (FLAG_trace_turbo_reduction) {
+        if (v8_flags.trace_turbo_reduction) {
           UnparkedScopeIfNeeded unparked(broker_);
           // TODO(neis): Disallow racy handle dereference once we stop
           // supporting --no-local-heaps --no-concurrent-inlining.
@@ -122,7 +122,7 @@ Reduction GraphReducer::Reduce(Node* const node) {
         continue;
       } else {
         // {node} was replaced by another node.
-        if (FLAG_trace_turbo_reduction) {
+        if (v8_flags.trace_turbo_reduction) {
           UnparkedScopeIfNeeded unparked(broker_);
           // TODO(neis): Disallow racy handle dereference once we stop
           // supporting --no-local-heaps --no-concurrent-inlining.
diff --git a/src/compiler/graph-trimmer.cc b/src/compiler/graph-trimmer.cc
index e1dbfffff5..3a85a456da 100644
--- a/src/compiler/graph-trimmer.cc
+++ b/src/compiler/graph-trimmer.cc
@@ -33,7 +33,7 @@ void GraphTrimmer::TrimGraph() {
     for (Edge edge : live->use_edges()) {
       Node* const user = edge.from();
       if (!IsLive(user)) {
-        if (FLAG_trace_turbo_trimming) {
+        if (v8_flags.trace_turbo_trimming) {
           StdoutStream{} << "DeadLink: " << *user << "(" << edge.index()
                          << ") -> " << *live << std::endl;
         }
diff --git a/src/compiler/graph-visualizer.cc b/src/compiler/graph-visualizer.cc
index 9b8cc6a514..2cd220eb86 100644
--- a/src/compiler/graph-visualizer.cc
+++ b/src/compiler/graph-visualizer.cc
@@ -33,8 +33,8 @@ namespace compiler {
 
 const char* get_cached_trace_turbo_filename(OptimizedCompilationInfo* info) {
   if (!info->trace_turbo_filename()) {
-    info->set_trace_turbo_filename(
-        GetVisualizerLogFileName(info, FLAG_trace_turbo_path, nullptr, "json"));
+    info->set_trace_turbo_filename(GetVisualizerLogFileName(
+        info, v8_flags.trace_turbo_path, nullptr, "json"));
   }
   return info->trace_turbo_filename();
 }
@@ -232,7 +232,7 @@ std::unique_ptr<char[]> GetVisualizerLogFileName(OptimizedCompilationInfo* info,
                                                  const char* suffix) {
   base::EmbeddedVector<char, 256> filename(0);
   std::unique_ptr<char[]> debug_name = info->GetDebugName();
-  const char* file_prefix = FLAG_trace_turbo_file_prefix.value();
+  const char* file_prefix = v8_flags.trace_turbo_file_prefix.value();
   int optimization_id = info->IsOptimizing() ? info->optimization_id() : 0;
   if (strlen(debug_name.get()) > 0) {
     SNPrintF(filename, "%s-%s-%i", file_prefix, debug_name.get(),
@@ -246,7 +246,7 @@ std::unique_ptr<char[]> GetVisualizerLogFileName(OptimizedCompilationInfo* info,
   }
   base::EmbeddedVector<char, 256> source_file(0);
   bool source_available = false;
-  if (FLAG_trace_file_names && info->has_shared_info() &&
+  if (v8_flags.trace_file_names && info->has_shared_info() &&
       info->shared_info()->script().IsScript()) {
     Object source_name = Script::cast(info->shared_info()->script()).name();
     if (source_name.IsString()) {
@@ -680,7 +680,7 @@ void GraphC1Visualizer::PrintSchedule(const char* phase,
         PrintIndent();
         os_ << "0 " << uses << " ";
         PrintNode(node);
-        if (FLAG_trace_turbo_types) {
+        if (v8_flags.trace_turbo_types) {
           os_ << " ";
           PrintType(node);
         }
@@ -710,7 +710,7 @@ void GraphC1Visualizer::PrintSchedule(const char* phase,
         for (BasicBlock* successor : current->successors()) {
           os_ << " B" << successor->rpo_number();
         }
-        if (FLAG_trace_turbo_types && current->control_input() != nullptr) {
+        if (v8_flags.trace_turbo_types && current->control_input() != nullptr) {
           os_ << " ";
           PrintType(current->control_input());
         }
@@ -811,7 +811,7 @@ void GraphC1Visualizer::PrintLiveRange(const LiveRange* range, const char* type,
 
     UsePosition* current_pos = range->first_pos();
     while (current_pos != nullptr) {
-      if (current_pos->RegisterIsBeneficial() || FLAG_trace_all_uses) {
+      if (current_pos->RegisterIsBeneficial() || v8_flags.trace_all_uses) {
         os_ << " " << current_pos->pos().value() << " M";
       }
       current_pos = current_pos->next();
diff --git a/src/compiler/heap-refs.cc b/src/compiler/heap-refs.cc
index 391796d00e..c428f056ea 100644
--- a/src/compiler/heap-refs.cc
+++ b/src/compiler/heap-refs.cc
@@ -2250,7 +2250,7 @@ base::Optional<PropertyCellRef> JSGlobalObjectRef::GetPropertyCell(
 }
 
 std::ostream& operator<<(std::ostream& os, const ObjectRef& ref) {
-  if (!FLAG_concurrent_recompilation) {
+  if (!v8_flags.concurrent_recompilation) {
     // We cannot be in a background thread so it's safe to read the heap.
     AllowHandleDereference allow_handle_dereference;
     return os << ref.data() << " {" << ref.object() << "}";
diff --git a/src/compiler/js-call-reducer.cc b/src/compiler/js-call-reducer.cc
index 4825cbb72f..da53d19f98 100644
--- a/src/compiler/js-call-reducer.cc
+++ b/src/compiler/js-call-reducer.cc
@@ -681,7 +681,7 @@ class IteratingArrayBuiltinReducerAssembler : public JSCallReducerAssembler {
  public:
   IteratingArrayBuiltinReducerAssembler(JSCallReducer* reducer, Node* node)
       : JSCallReducerAssembler(reducer, node) {
-    DCHECK(FLAG_turbo_inline_array_builtins);
+    DCHECK(v8_flags.turbo_inline_array_builtins);
   }
 
   TNode<Object> ReduceArrayPrototypeForEach(
@@ -3400,7 +3400,7 @@ class IteratingArrayBuiltinHelper {
         effect_(NodeProperties::GetEffectInput(node)),
         control_(NodeProperties::GetControlInput(node)),
         inference_(broker, receiver_, effect_) {
-    if (!FLAG_turbo_inline_array_builtins) return;
+    if (!v8_flags.turbo_inline_array_builtins) return;
 
     DCHECK_EQ(IrOpcode::kJSCall, node->opcode());
     const CallParameters& p = CallParametersOf(node->op());
@@ -3698,7 +3698,7 @@ FastApiCallFunctionVector CanOptimizeFastCall(
     Zone* zone, const FunctionTemplateInfoRef& function_template_info,
     size_t argc) {
   FastApiCallFunctionVector result(zone);
-  if (!FLAG_turbo_fast_api_calls) return result;
+  if (!v8_flags.turbo_fast_api_calls) return result;
 
   static constexpr int kReceiver = 1;
 
@@ -4273,7 +4273,7 @@ Reduction JSCallReducer::ReduceCallOrConstructWithArrayLikeOrSpread(
         feedback_source, speculation_mode, feedback_relation);
   }
 
-  if (!FLAG_turbo_optimize_apply) return NoChange();
+  if (!v8_flags.turbo_optimize_apply) return NoChange();
 
   // Optimization of construct nodes not supported yet.
   if (!IsCallWithArrayLikeOrSpread(node)) return NoChange();
@@ -5618,7 +5618,7 @@ void JSCallReducer::CheckIfElementsKind(Node* receiver_elements_kind,
 
 // ES6 section 23.1.3.1 Array.prototype.at ( )
 Reduction JSCallReducer::ReduceArrayPrototypeAt(Node* node) {
-  if (!FLAG_turbo_inline_array_builtins) return NoChange();
+  if (!v8_flags.turbo_inline_array_builtins) return NoChange();
 
   JSCallNode n(node);
   CallParameters const& p = n.Parameters();
@@ -6075,7 +6075,7 @@ Reduction JSCallReducer::ReduceArrayPrototypeShift(Node* node) {
           ElementAccess const access =
               AccessBuilder::ForFixedArrayElement(kind);
 
-          // When disable FLAG_turbo_loop_variable, typer cannot infer index
+          // When disable v8_flags.turbo_loop_variable, typer cannot infer index
           // is in [1, kMaxCopyElements-1], and will break in representing
           // kRepFloat64 (Range(1, inf)) to kRepWord64 when converting
           // input for kLoadElement. So we need to add type guard here.
@@ -6196,7 +6196,7 @@ Reduction JSCallReducer::ReduceArrayPrototypeShift(Node* node) {
 
 // ES6 section 22.1.3.23 Array.prototype.slice ( )
 Reduction JSCallReducer::ReduceArrayPrototypeSlice(Node* node) {
-  if (!FLAG_turbo_inline_array_builtins) return NoChange();
+  if (!v8_flags.turbo_inline_array_builtins) return NoChange();
   JSCallNode n(node);
   CallParameters const& p = n.Parameters();
   if (p.speculation_mode() == SpeculationMode::kDisallowSpeculation) {
@@ -8236,7 +8236,7 @@ Reduction JSCallReducer::ReduceNumberParseInt(Node* node) {
 Reduction JSCallReducer::ReduceRegExpPrototypeTest(Node* node) {
   JSCallNode n(node);
   CallParameters const& p = n.Parameters();
-  if (FLAG_force_slow_path) return NoChange();
+  if (v8_flags.force_slow_path) return NoChange();
   if (n.ArgumentCount() < 1) return NoChange();
 
   if (p.speculation_mode() == SpeculationMode::kDisallowSpeculation) {
diff --git a/src/compiler/js-heap-broker.cc b/src/compiler/js-heap-broker.cc
index 83246ca90d..48b21bea31 100644
--- a/src/compiler/js-heap-broker.cc
+++ b/src/compiler/js-heap-broker.cc
@@ -486,7 +486,7 @@ ProcessedFeedback const& JSHeapBroker::ReadFeedbackForPropertyAccess(
       // if non-deprecation is important.
       if (map.is_deprecated()) {
         // TODO(ishell): support fast map updating if we enable it.
-        CHECK(!FLAG_fast_map_update);
+        CHECK(!v8_flags.fast_map_update);
         base::Optional<Map> maybe_map = MapUpdater::TryUpdateNoLock(
             isolate(), *map.object(), ConcurrencyMode::kConcurrent);
         if (maybe_map.has_value()) {
diff --git a/src/compiler/js-heap-broker.h b/src/compiler/js-heap-broker.h
index a841be74c3..1faa1004e9 100644
--- a/src/compiler/js-heap-broker.h
+++ b/src/compiler/js-heap-broker.h
@@ -40,16 +40,16 @@ class ObjectRef;
 
 std::ostream& operator<<(std::ostream& os, const ObjectRef& ref);
 
-#define TRACE_BROKER(broker, x)                                      \
-  do {                                                               \
-    if (broker->tracing_enabled() && FLAG_trace_heap_broker_verbose) \
-      StdoutStream{} << broker->Trace() << x << '\n';                \
+#define TRACE_BROKER(broker, x)                                          \
+  do {                                                                   \
+    if (broker->tracing_enabled() && v8_flags.trace_heap_broker_verbose) \
+      StdoutStream{} << broker->Trace() << x << '\n';                    \
   } while (false)
 
-#define TRACE_BROKER_MEMORY(broker, x)                              \
-  do {                                                              \
-    if (broker->tracing_enabled() && FLAG_trace_heap_broker_memory) \
-      StdoutStream{} << broker->Trace() << x << std::endl;          \
+#define TRACE_BROKER_MEMORY(broker, x)                                  \
+  do {                                                                  \
+    if (broker->tracing_enabled() && v8_flags.trace_heap_broker_memory) \
+      StdoutStream{} << broker->Trace() << x << std::endl;              \
   } while (false)
 
 #define TRACE_BROKER_MISSING(broker, x)                                        \
@@ -101,7 +101,7 @@ class V8_EXPORT_PRIVATE JSHeapBroker {
   // For use only in tests, sets default values for some arguments. Avoids
   // churn when new flags are added.
   JSHeapBroker(Isolate* isolate, Zone* broker_zone)
-      : JSHeapBroker(isolate, broker_zone, FLAG_trace_heap_broker,
+      : JSHeapBroker(isolate, broker_zone, v8_flags.trace_heap_broker,
                      CodeKind::TURBOFAN) {}
 
   ~JSHeapBroker();
diff --git a/src/compiler/js-inlining-heuristic.cc b/src/compiler/js-inlining-heuristic.cc
index 4ab224b6cd..e73a9c9f90 100644
--- a/src/compiler/js-inlining-heuristic.cc
+++ b/src/compiler/js-inlining-heuristic.cc
@@ -14,14 +14,15 @@ namespace v8 {
 namespace internal {
 namespace compiler {
 
-#define TRACE(...)                                                             \
-  do {                                                                         \
-    if (FLAG_trace_turbo_inlining) StdoutStream{} << __VA_ARGS__ << std::endl; \
+#define TRACE(...)                                \
+  do {                                            \
+    if (v8_flags.trace_turbo_inlining)            \
+      StdoutStream{} << __VA_ARGS__ << std::endl; \
   } while (false)
 
 namespace {
 bool IsSmall(int const size) {
-  return size <= FLAG_max_inlined_bytecode_size_small;
+  return size <= v8_flags.max_inlined_bytecode_size_small;
 }
 
 bool CanConsiderForInlining(JSHeapBroker* broker,
@@ -173,7 +174,7 @@ Reduction JSInliningHeuristic::Reduce(Node* node) {
   Candidate candidate = CollectFunctions(node, kMaxCallPolymorphism);
   if (candidate.num_functions == 0) {
     return NoChange();
-  } else if (candidate.num_functions > 1 && !FLAG_polymorphic_inlining) {
+  } else if (candidate.num_functions > 1 && !v8_flags.polymorphic_inlining) {
     TRACE("Not considering call site #"
           << node->id() << ":" << node->op()->mnemonic()
           << ", because polymorphic inlining is disabled");
@@ -248,7 +249,7 @@ Reduction JSInliningHeuristic::Reduce(Node* node) {
   // threshold, i.e. a call site that is only hit once every N
   // invocations of the caller.
   if (candidate.frequency.IsKnown() &&
-      candidate.frequency.value() < FLAG_min_inlining_frequency) {
+      candidate.frequency.value() < v8_flags.min_inlining_frequency) {
     return NoChange();
   }
 
@@ -275,7 +276,7 @@ Reduction JSInliningHeuristic::Reduce(Node* node) {
 
 void JSInliningHeuristic::Finalize() {
   if (candidates_.empty()) return;  // Nothing to do without candidates.
-  if (FLAG_trace_turbo_inlining) PrintCandidates();
+  if (v8_flags.trace_turbo_inlining) PrintCandidates();
 
   // We inline at most one candidate in every iteration of the fixpoint.
   // This is to ensure that we don't consume the full inlining budget
@@ -293,7 +294,7 @@ void JSInliningHeuristic::Finalize() {
     // Make sure we have some extra budget left, so that any small functions
     // exposed by this function would be given a chance to inline.
     double size_of_candidate =
-        candidate.total_size * FLAG_reserve_inline_budget_scale_factor;
+        candidate.total_size * v8_flags.reserve_inline_budget_scale_factor;
     int total_size =
         total_inlined_bytecode_size_ + static_cast<int>(size_of_candidate);
     if (total_size > max_inlined_bytecode_size_cumulative_) {
diff --git a/src/compiler/js-inlining-heuristic.h b/src/compiler/js-inlining-heuristic.h
index 9357165352..3a830943bd 100644
--- a/src/compiler/js-inlining-heuristic.h
+++ b/src/compiler/js-inlining-heuristic.h
@@ -30,9 +30,9 @@ class JSInliningHeuristic final : public AdvancedReducer {
         broker_(broker),
         mode_(mode),
         max_inlined_bytecode_size_cumulative_(
-            FLAG_max_inlined_bytecode_size_cumulative),
+            v8_flags.max_inlined_bytecode_size_cumulative),
         max_inlined_bytecode_size_absolute_(
-            FLAG_max_inlined_bytecode_size_absolute) {}
+            v8_flags.max_inlined_bytecode_size_absolute) {}
 
   const char* reducer_name() const override { return "JSInliningHeuristic"; }
 
diff --git a/src/compiler/js-inlining.cc b/src/compiler/js-inlining.cc
index a3aac2a032..4baabf3077 100644
--- a/src/compiler/js-inlining.cc
+++ b/src/compiler/js-inlining.cc
@@ -34,11 +34,11 @@ namespace {
 static const int kMaxDepthForInlining = 50;
 }  // namespace
 
-#define TRACE(x)                     \
-  do {                               \
-    if (FLAG_trace_turbo_inlining) { \
-      StdoutStream() << x << "\n";   \
-    }                                \
+#define TRACE(x)                         \
+  do {                                   \
+    if (v8_flags.trace_turbo_inlining) { \
+      StdoutStream() << x << "\n";       \
+    }                                    \
   } while (false)
 
 // Provides convenience accessors for the common layout of nodes having either
diff --git a/src/compiler/js-intrinsic-lowering.cc b/src/compiler/js-intrinsic-lowering.cc
index 831697d645..bcc4e43a47 100644
--- a/src/compiler/js-intrinsic-lowering.cc
+++ b/src/compiler/js-intrinsic-lowering.cc
@@ -277,7 +277,7 @@ Reduction JSIntrinsicLowering::ReduceIsJSReceiver(Node* node) {
 }
 
 Reduction JSIntrinsicLowering::ReduceTurbofanStaticAssert(Node* node) {
-  if (FLAG_always_turbofan) {
+  if (v8_flags.always_turbofan) {
     // Ignore static asserts, as we most likely won't have enough information
     RelaxEffectsAndControls(node);
   } else {
diff --git a/src/compiler/js-native-context-specialization.cc b/src/compiler/js-native-context-specialization.cc
index 39302152ed..05dcf59096 100644
--- a/src/compiler/js-native-context-specialization.cc
+++ b/src/compiler/js-native-context-specialization.cc
@@ -1164,7 +1164,7 @@ Reduction JSNativeContextSpecialization::ReduceMegaDOMPropertyAccess(
       simplified()->LoadField(AccessBuilder::ForMapInstanceType()),
       receiver_map, effect, control);
 
-  if (FLAG_embedder_instance_types && range_start != 0) {
+  if (v8_flags.embedder_instance_types && range_start != 0) {
     // Embedder instance ID is set, doing a simple range check.
     Node* diff_to_start =
         graph()->NewNode(simplified()->NumberSubtract(), receiver_instance_type,
@@ -1251,7 +1251,7 @@ Reduction JSNativeContextSpecialization::ReduceNamedAccess(
   // lookup_start_object = the object where we start looking for the property.
   Node* lookup_start_object;
   if (node->opcode() == IrOpcode::kJSLoadNamedFromSuper) {
-    DCHECK(FLAG_super_ic);
+    DCHECK(v8_flags.super_ic);
     JSLoadNamedFromSuperNode n(node);
     // Lookup start object is the __proto__ of the home object.
     lookup_start_object = effect =
diff --git a/src/compiler/load-elimination.cc b/src/compiler/load-elimination.cc
index d526dc1cae..cc56abe493 100644
--- a/src/compiler/load-elimination.cc
+++ b/src/compiler/load-elimination.cc
@@ -72,7 +72,7 @@ bool MustAlias(Node* a, Node* b) {
 }  // namespace
 
 Reduction LoadElimination::Reduce(Node* node) {
-  if (FLAG_trace_turbo_load_elimination) {
+  if (v8_flags.trace_turbo_load_elimination) {
     if (node->op()->EffectInputCount() > 0) {
       PrintF(" visit #%d:%s", node->id(), node->op()->mnemonic());
       if (node->op()->ValueInputCount() > 0) {
diff --git a/src/compiler/loop-analysis.cc b/src/compiler/loop-analysis.cc
index a9682e955d..182d6ab42a 100644
--- a/src/compiler/loop-analysis.cc
+++ b/src/compiler/loop-analysis.cc
@@ -542,7 +542,7 @@ LoopTree* LoopFinder::BuildLoopTree(Graph* graph, TickCounter* tick_counter,
       graph->zone()->New<LoopTree>(graph->NodeCount(), graph->zone());
   LoopFinderImpl finder(graph, loop_tree, tick_counter, zone);
   finder.Run();
-  if (FLAG_trace_turbo_loop) {
+  if (v8_flags.trace_turbo_loop) {
     finder.Print();
   }
   return loop_tree;
@@ -694,7 +694,7 @@ bool LoopFinder::HasMarkedExits(LoopTree* loop_tree,
             unmarked_exit = (use->opcode() != IrOpcode::kTerminate);
         }
         if (unmarked_exit) {
-          if (FLAG_trace_turbo_loop) {
+          if (v8_flags.trace_turbo_loop) {
             PrintF(
                 "Cannot peel loop %i. Loop exit without explicit mark: Node %i "
                 "(%s) is inside loop, but its use %i (%s) is outside.\n",
diff --git a/src/compiler/loop-peeling.cc b/src/compiler/loop-peeling.cc
index cfc149f639..ee46d5e494 100644
--- a/src/compiler/loop-peeling.cc
+++ b/src/compiler/loop-peeling.cc
@@ -225,7 +225,7 @@ void LoopPeeler::PeelInnerLoops(LoopTree::Loop* loop) {
   }
   // Only peel small-enough loops.
   if (loop->TotalSize() > LoopPeeler::kMaxPeeledNodes) return;
-  if (FLAG_trace_turbo_loop) {
+  if (v8_flags.trace_turbo_loop) {
     PrintF("Peeling loop with header: ");
     for (Node* node : loop_tree_->HeaderNodes(loop)) {
       PrintF("%i ", node->id());
diff --git a/src/compiler/loop-variable-optimizer.cc b/src/compiler/loop-variable-optimizer.cc
index 7f16e243c5..8d1caa22fe 100644
--- a/src/compiler/loop-variable-optimizer.cc
+++ b/src/compiler/loop-variable-optimizer.cc
@@ -17,9 +17,9 @@ namespace internal {
 namespace compiler {
 
 // Macro for outputting trace information from representation inference.
-#define TRACE(...)                                  \
-  do {                                              \
-    if (FLAG_trace_turbo_loop) PrintF(__VA_ARGS__); \
+#define TRACE(...)                                      \
+  do {                                                  \
+    if (v8_flags.trace_turbo_loop) PrintF(__VA_ARGS__); \
   } while (false)
 
 LoopVariableOptimizer::LoopVariableOptimizer(Graph* graph,
@@ -76,7 +76,7 @@ void LoopVariableOptimizer::Run() {
 
 void InductionVariable::AddUpperBound(Node* bound,
                                       InductionVariable::ConstraintKind kind) {
-  if (FLAG_trace_turbo_loop) {
+  if (v8_flags.trace_turbo_loop) {
     StdoutStream{} << "New upper bound for " << phi()->id() << " (loop "
                    << NodeProperties::GetControlInput(phi())->id()
                    << "): " << *bound << std::endl;
@@ -86,7 +86,7 @@ void InductionVariable::AddUpperBound(Node* bound,
 
 void InductionVariable::AddLowerBound(Node* bound,
                                       InductionVariable::ConstraintKind kind) {
-  if (FLAG_trace_turbo_loop) {
+  if (v8_flags.trace_turbo_loop) {
     StdoutStream{} << "New lower bound for " << phi()->id() << " (loop "
                    << NodeProperties::GetControlInput(phi())->id()
                    << "): " << *bound;
diff --git a/src/compiler/memory-lowering.cc b/src/compiler/memory-lowering.cc
index 6e7d0a2ce6..f9ac0adccc 100644
--- a/src/compiler/memory-lowering.cc
+++ b/src/compiler/memory-lowering.cc
@@ -48,7 +48,7 @@ class MemoryLowering::AllocationGroup final : public ZoneObject {
   static inline AllocationType CheckAllocationType(AllocationType allocation) {
     // For non-generational heap, all young allocations are redirected to old
     // space.
-    if (FLAG_single_generation && allocation == AllocationType::kYoung) {
+    if (v8_flags.single_generation && allocation == AllocationType::kYoung) {
       return AllocationType::kOld;
     }
     return allocation;
@@ -137,7 +137,7 @@ Reduction MemoryLowering::ReduceAllocateRaw(
   DCHECK_EQ(IrOpcode::kAllocateRaw, node->opcode());
   DCHECK_IMPLIES(allocation_folding_ == AllocationFolding::kDoAllocationFolding,
                  state_ptr != nullptr);
-  if (FLAG_single_generation && allocation_type == AllocationType::kYoung) {
+  if (v8_flags.single_generation && allocation_type == AllocationType::kYoung) {
     allocation_type = AllocationType::kOld;
   }
   // Code objects may have a maximum size smaller than kMaxHeapObjectSize due to
@@ -232,7 +232,7 @@ Reduction MemoryLowering::ReduceAllocateRaw(
   // Check if we can fold this allocation into a previous allocation represented
   // by the incoming {state}.
   IntPtrMatcher m(size);
-  if (m.IsInRange(0, kMaxRegularHeapObjectSize) && FLAG_inline_new &&
+  if (m.IsInRange(0, kMaxRegularHeapObjectSize) && v8_flags.inline_new &&
       allocation_folding_ == AllocationFolding::kDoAllocationFolding) {
     intptr_t const object_size = m.ResolvedValue();
     AllocationState const* state = *state_ptr;
@@ -645,7 +645,7 @@ WriteBarrierKind MemoryLowering::ComputeWriteBarrierKind(
   if (!ValueNeedsWriteBarrier(value, isolate())) {
     write_barrier_kind = kNoWriteBarrier;
   }
-  if (FLAG_disable_write_barriers) {
+  if (v8_flags.disable_write_barriers) {
     write_barrier_kind = kNoWriteBarrier;
   }
   if (write_barrier_kind == WriteBarrierKind::kAssertNoWriteBarrier) {
diff --git a/src/compiler/pipeline.cc b/src/compiler/pipeline.cc
index babd0e8dd0..55fe714295 100644
--- a/src/compiler/pipeline.cc
+++ b/src/compiler/pipeline.cc
@@ -160,7 +160,7 @@ class PipelineData {
         allocator_(isolate->allocator()),
         info_(info),
         debug_name_(info_->GetDebugName()),
-        may_have_unverifiable_graph_(FLAG_turboshaft),
+        may_have_unverifiable_graph_(v8_flags.turboshaft),
         zone_stats_(zone_stats),
         pipeline_statistics_(pipeline_statistics),
         graph_zone_scope_(zone_stats_, kGraphZoneName, kCompressGraphZone),
@@ -578,7 +578,7 @@ class PipelineData {
         osr_helper_, start_source_position_, jump_optimization_info_,
         assembler_options(), buffer_cache, info_->builtin(),
         max_unoptimized_frame_height(), max_pushed_argument_count(),
-        FLAG_trace_turbo_stack_accesses ? debug_name_.get() : nullptr);
+        v8_flags.trace_turbo_stack_accesses ? debug_name_.get() : nullptr);
   }
 
   void BeginPhaseKind(const char* phase_kind_name) {
@@ -919,15 +919,15 @@ void PrintParticipatingSource(OptimizedCompilationInfo* info,
 // Print the code after compiling it.
 void PrintCode(Isolate* isolate, Handle<Code> code,
                OptimizedCompilationInfo* info) {
-  if (FLAG_print_opt_source && info->IsOptimizing()) {
+  if (v8_flags.print_opt_source && info->IsOptimizing()) {
     PrintParticipatingSource(info, isolate);
   }
 
 #ifdef ENABLE_DISASSEMBLER
   const bool print_code =
-      FLAG_print_code ||
-      (info->IsOptimizing() && FLAG_print_opt_code &&
-       info->shared_info()->PassesFilter(FLAG_print_opt_code_filter));
+      v8_flags.print_code ||
+      (info->IsOptimizing() && v8_flags.print_opt_code &&
+       info->shared_info()->PassesFilter(v8_flags.print_opt_code_filter));
   if (print_code) {
     std::unique_ptr<char[]> debug_name = info->GetDebugName();
     CodeTracer::StreamScope tracing_scope(isolate->GetCodeTracer());
@@ -988,7 +988,7 @@ void TraceSchedule(OptimizedCompilationInfo* info, PipelineData* data,
     json_of << "\"},\n";
   }
 
-  if (info->trace_turbo_graph() || FLAG_trace_turbo_scheduler) {
+  if (info->trace_turbo_graph() || v8_flags.trace_turbo_scheduler) {
     UnparkedScopeIfNeeded scope(data->broker());
     AllowHandleDereference allow_deref;
 
@@ -1008,7 +1008,7 @@ void TraceScheduleAndVerify(OptimizedCompilationInfo* info, PipelineData* data,
 
   TraceSchedule(info, data, schedule, phase_name);
 
-  if (FLAG_turbo_verify) ScheduleVerifier::Run(schedule);
+  if (v8_flags.turbo_verify) ScheduleVerifier::Run(schedule);
 }
 
 void AddReducer(PipelineData* data, GraphReducer* graph_reducer,
@@ -1038,7 +1038,7 @@ PipelineStatistics* CreatePipelineStatistics(Handle<Script> script,
   bool tracing_enabled;
   TRACE_EVENT_CATEGORY_GROUP_ENABLED(TRACE_DISABLED_BY_DEFAULT("v8.turbofan"),
                                      &tracing_enabled);
-  if (tracing_enabled || FLAG_turbo_stats || FLAG_turbo_stats_nvp) {
+  if (tracing_enabled || v8_flags.turbo_stats || v8_flags.turbo_stats_nvp) {
     pipeline_statistics =
         new PipelineStatistics(info, isolate->GetTurboStatistics(), zone_stats);
     pipeline_statistics->BeginPhaseKind("V8.TFInitializing");
@@ -1064,7 +1064,7 @@ PipelineStatistics* CreatePipelineStatistics(
   bool tracing_enabled;
   TRACE_EVENT_CATEGORY_GROUP_ENABLED(
       TRACE_DISABLED_BY_DEFAULT("v8.wasm.turbofan"), &tracing_enabled);
-  if (tracing_enabled || FLAG_turbo_stats_wasm) {
+  if (tracing_enabled || v8_flags.turbo_stats_wasm) {
     pipeline_statistics = new PipelineStatistics(
         info, wasm::GetWasmEngine()->GetOrCreateTurboStatistics(), zone_stats);
     pipeline_statistics->BeginPhaseKind("V8.WasmInitializing");
@@ -1180,20 +1180,20 @@ PipelineCompilationJob::Status PipelineCompilationJob::PrepareJobImpl(
   PipelineJobScope scope(&data_, isolate->counters()->runtime_call_stats());
 
   if (compilation_info()->bytecode_array()->length() >
-      FLAG_max_optimized_bytecode_size) {
+      v8_flags.max_optimized_bytecode_size) {
     return AbortOptimization(BailoutReason::kFunctionTooBig);
   }
 
-  if (!FLAG_always_turbofan) {
+  if (!v8_flags.always_turbofan) {
     compilation_info()->set_bailout_on_uninitialized();
   }
-  if (FLAG_turbo_loop_peeling) {
+  if (v8_flags.turbo_loop_peeling) {
     compilation_info()->set_loop_peeling();
   }
-  if (FLAG_turbo_inlining) {
+  if (v8_flags.turbo_inlining) {
     compilation_info()->set_inlining();
   }
-  if (FLAG_turbo_allocation_folding) {
+  if (v8_flags.turbo_allocation_folding) {
     compilation_info()->set_allocation_folding();
   }
 
@@ -1461,7 +1461,7 @@ struct EarlyGraphTrimmingPhase {
     GraphTrimmer trimmer(temp_zone, data->graph());
     NodeVector roots(temp_zone);
     data->jsgraph()->GetCachedNodes(&roots);
-    UnparkedScopeIfNeeded scope(data->broker(), FLAG_trace_turbo_trimming);
+    UnparkedScopeIfNeeded scope(data->broker(), v8_flags.trace_turbo_trimming);
     trimmer.TrimGraph(roots.begin(), roots.end());
   }
 };
@@ -1479,7 +1479,7 @@ struct TyperPhase {
 
     LoopVariableOptimizer induction_vars(data->jsgraph()->graph(),
                                          data->common(), temp_zone);
-    if (FLAG_turbo_loop_variable) induction_vars.Run();
+    if (v8_flags.turbo_loop_variable) induction_vars.Run();
 
     // The typer inspects heap objects, so we need to unpark the local heap.
     UnparkedScopeIfNeeded scope(data->broker());
@@ -1634,7 +1634,8 @@ struct LoopPeelingPhase {
     NodeVector roots(temp_zone);
     data->jsgraph()->GetCachedNodes(&roots);
     {
-      UnparkedScopeIfNeeded scope(data->broker(), FLAG_trace_turbo_trimming);
+      UnparkedScopeIfNeeded scope(data->broker(),
+                                  v8_flags.trace_turbo_trimming);
       trimmer.TrimGraph(roots.begin(), roots.end());
     }
 
@@ -1726,15 +1727,15 @@ struct WasmLoopPeelingPhase {
       if (loop_info.can_be_innermost) {
         ZoneUnorderedSet<Node*>* loop =
             LoopFinder::FindSmallInnermostLoopFromHeader(
-                loop_info.header, temp_zone, FLAG_wasm_loop_peeling_max_size,
-                false);
+                loop_info.header, temp_zone,
+                v8_flags.wasm_loop_peeling_max_size, false);
         if (loop == nullptr) continue;
         PeelWasmLoop(loop_info.header, loop, data->graph(), data->common(),
                      temp_zone, data->source_positions(), data->node_origins());
       }
     }
     // If we are going to unroll later, keep loop exits.
-    if (!FLAG_wasm_loop_unrolling) EliminateLoopExits(loop_infos);
+    if (!v8_flags.wasm_loop_unrolling) EliminateLoopExits(loop_infos);
   }
 };
 #endif  // V8_ENABLE_WEBASSEMBLY
@@ -1815,7 +1816,8 @@ struct EffectControlLinearizationPhase {
       NodeVector roots(temp_zone);
       data->jsgraph()->GetCachedNodes(&roots);
       {
-        UnparkedScopeIfNeeded scope(data->broker(), FLAG_trace_turbo_trimming);
+        UnparkedScopeIfNeeded scope(data->broker(),
+                                    v8_flags.trace_turbo_trimming);
         trimmer.TrimGraph(roots.begin(), roots.end());
       }
 
@@ -1868,7 +1870,8 @@ struct StoreStoreEliminationPhase {
     NodeVector roots(temp_zone);
     data->jsgraph()->GetCachedNodes(&roots);
     {
-      UnparkedScopeIfNeeded scope(data->broker(), FLAG_trace_turbo_trimming);
+      UnparkedScopeIfNeeded scope(data->broker(),
+                                  v8_flags.trace_turbo_trimming);
       trimmer.TrimGraph(roots.begin(), roots.end());
     }
 
@@ -1930,7 +1933,8 @@ struct MemoryOptimizationPhase {
     NodeVector roots(temp_zone);
     data->jsgraph()->GetCachedNodes(&roots);
     {
-      UnparkedScopeIfNeeded scope(data->broker(), FLAG_trace_turbo_trimming);
+      UnparkedScopeIfNeeded scope(data->broker(),
+                                  v8_flags.trace_turbo_trimming);
       trimmer.TrimGraph(roots.begin(), roots.end());
     }
 
@@ -2121,7 +2125,7 @@ struct WasmOptimizationPhase {
     // then one around branch elimination. This is because those two
     // optimizations sometimes display quadratic complexity when run together.
     // We only need load elimination for managed objects.
-    if (FLAG_experimental_wasm_gc) {
+    if (v8_flags.experimental_wasm_gc) {
       GraphReducer graph_reducer(temp_zone, data->graph(),
                                  &data->info()->tick_counter(), data->broker(),
                                  data->jsgraph()->Dead(),
@@ -2314,7 +2318,7 @@ struct InstructionSelectionPhase {
             ? InstructionSelector::kAllSourcePositions
             : InstructionSelector::kCallSourcePositions,
         InstructionSelector::SupportedFeatures(),
-        FLAG_turbo_instruction_scheduling
+        v8_flags.turbo_instruction_scheduling
             ? InstructionSelector::kEnableScheduling
             : InstructionSelector::kDisableScheduling,
         data->assembler_options().enable_root_relative_access
@@ -2716,7 +2720,7 @@ CompilationJob::Status WasmHeapStubCompilationJob::PrepareJobImpl(
 CompilationJob::Status WasmHeapStubCompilationJob::ExecuteJobImpl(
     RuntimeCallStats* stats, LocalIsolate* local_isolate) {
   std::unique_ptr<PipelineStatistics> pipeline_statistics;
-  if (FLAG_turbo_stats || FLAG_turbo_stats_nvp) {
+  if (v8_flags.turbo_stats || v8_flags.turbo_stats_nvp) {
     pipeline_statistics.reset(new PipelineStatistics(
         &info_, wasm::GetWasmEngine()->GetOrCreateTurboStatistics(),
         &zone_stats_));
@@ -2759,7 +2763,7 @@ CompilationJob::Status WasmHeapStubCompilationJob::FinalizeJobImpl(
   if (pipeline_.CommitDependencies(code)) {
     info_.SetCode(code);
 #ifdef ENABLE_DISASSEMBLER
-    if (FLAG_print_opt_code) {
+    if (v8_flags.print_opt_code) {
       CodeTracer::StreamScope tracing_scope(isolate->GetCodeTracer());
       code->Disassemble(compilation_info()->GetDebugName().get(),
                         tracing_scope.stream(), isolate);
@@ -2778,7 +2782,7 @@ void PipelineImpl::RunPrintAndVerify(const char* phase, bool untyped) {
   if (info()->trace_turbo_json() || info()->trace_turbo_graph()) {
     Run<PrintGraphPhase>(phase);
   }
-  if (FLAG_turbo_verify) {
+  if (v8_flags.turbo_verify) {
     Run<VerifyGraphPhase>(untyped);
   }
 }
@@ -2871,18 +2875,18 @@ bool PipelineImpl::OptimizeGraph(Linkage* linkage) {
     RunPrintAndVerify(LoopExitEliminationPhase::phase_name(), true);
   }
 
-  if (FLAG_turbo_load_elimination) {
+  if (v8_flags.turbo_load_elimination) {
     Run<LoadEliminationPhase>();
     RunPrintAndVerify(LoadEliminationPhase::phase_name());
   }
   data->DeleteTyper();
 
-  if (FLAG_turbo_escape) {
+  if (v8_flags.turbo_escape) {
     Run<EscapeAnalysisPhase>();
     RunPrintAndVerify(EscapeAnalysisPhase::phase_name());
   }
 
-  if (FLAG_assert_types) {
+  if (v8_flags.assert_types) {
     Run<TypeAssertionsPhase>();
     RunPrintAndVerify(TypeAssertionsPhase::phase_name());
   }
@@ -2928,13 +2932,13 @@ bool PipelineImpl::OptimizeGraph(Linkage* linkage) {
   Run<EffectControlLinearizationPhase>();
   RunPrintAndVerify(EffectControlLinearizationPhase::phase_name(), true);
 
-  if (FLAG_turbo_store_elimination) {
+  if (v8_flags.turbo_store_elimination) {
     Run<StoreStoreEliminationPhase>();
     RunPrintAndVerify(StoreStoreEliminationPhase::phase_name(), true);
   }
 
   // Optimize control flow.
-  if (FLAG_turbo_cf_optimization) {
+  if (v8_flags.turbo_cf_optimization) {
     Run<ControlFlowOptimizationPhase>();
     RunPrintAndVerify(ControlFlowOptimizationPhase::phase_name(), true);
   }
@@ -2952,7 +2956,7 @@ bool PipelineImpl::OptimizeGraph(Linkage* linkage) {
   Run<MachineOperatorOptimizationPhase>();
   RunPrintAndVerify(MachineOperatorOptimizationPhase::phase_name(), true);
 
-  if (!FLAG_turboshaft) {
+  if (!v8_flags.turboshaft) {
     Run<DecompressionOptimizationPhase>();
     RunPrintAndVerify(DecompressionOptimizationPhase::phase_name(), true);
   }
@@ -2967,7 +2971,7 @@ bool PipelineImpl::OptimizeGraph(Linkage* linkage) {
 
   ComputeScheduledGraph();
 
-  if (FLAG_turboshaft) {
+  if (v8_flags.turboshaft) {
     if (base::Optional<BailoutReason> bailout = Run<BuildTurboshaftPhase>()) {
       info()->AbortOptimization(*bailout);
       data->EndPhaseKind();
@@ -3072,17 +3076,17 @@ MaybeHandle<Code> Pipeline::GenerateCodeForCodeStub(
   NodeOriginTable node_origins(graph);
   JumpOptimizationInfo jump_opt;
   bool should_optimize_jumps = isolate->serializer_enabled() &&
-                               FLAG_turbo_rewrite_far_jumps &&
-                               !FLAG_turbo_profiling;
+                               v8_flags.turbo_rewrite_far_jumps &&
+                               !v8_flags.turbo_profiling;
   PipelineData data(&zone_stats, &info, isolate, isolate->allocator(), graph,
                     jsgraph, nullptr, source_positions, &node_origins,
                     should_optimize_jumps ? &jump_opt : nullptr, options,
                     profile_data);
   PipelineJobScope scope(&data, isolate->counters()->runtime_call_stats());
   RCS_SCOPE(isolate, RuntimeCallCounterId::kOptimizeCode);
-  data.set_verify_graph(FLAG_verify_csa);
+  data.set_verify_graph(v8_flags.verify_csa);
   std::unique_ptr<PipelineStatistics> pipeline_statistics;
-  if (FLAG_turbo_stats || FLAG_turbo_stats_nvp) {
+  if (v8_flags.turbo_stats || v8_flags.turbo_stats_nvp) {
     pipeline_statistics.reset(new PipelineStatistics(
         &info, isolate->GetTurboStatistics(), &zone_stats));
     pipeline_statistics->BeginPhaseKind("V8.TFStubCodegen");
@@ -3127,13 +3131,13 @@ MaybeHandle<Code> Pipeline::GenerateCodeForCodeStub(
   pipeline.Run<VerifyGraphPhase>(true);
 
   int graph_hash_before_scheduling = 0;
-  if (FLAG_turbo_profiling || profile_data != nullptr) {
+  if (v8_flags.turbo_profiling || profile_data != nullptr) {
     graph_hash_before_scheduling = HashGraphForPGO(data.graph());
   }
 
   if (profile_data != nullptr &&
       profile_data->hash() != graph_hash_before_scheduling) {
-    if (FLAG_warn_about_builtin_profile_data) {
+    if (v8_flags.warn_about_builtin_profile_data) {
       PrintF("Rejected profile data for %s due to function change\n",
              debug_name);
       PrintF("Please use tools/builtins-pgo/generate.py to refresh it.\n");
@@ -3155,11 +3159,11 @@ MaybeHandle<Code> Pipeline::GenerateCodeForCodeStub(
                            profile_data);
   PipelineJobScope second_scope(&second_data,
                                 isolate->counters()->runtime_call_stats());
-  second_data.set_verify_graph(FLAG_verify_csa);
+  second_data.set_verify_graph(v8_flags.verify_csa);
   PipelineImpl second_pipeline(&second_data);
   second_pipeline.SelectInstructionsAndAssemble(call_descriptor);
 
-  if (FLAG_turbo_profiling) {
+  if (v8_flags.turbo_profiling) {
     info.profiler_data()->SetHash(graph_hash_before_scheduling);
   }
 
@@ -3206,7 +3210,7 @@ wasm::WasmCompilationResult Pipeline::GenerateCodeForWasmNativeStub(
   PipelineData data(&zone_stats, wasm_engine, &info, mcgraph, nullptr,
                     source_positions, node_positions, options, kNoBufferCache);
   std::unique_ptr<PipelineStatistics> pipeline_statistics;
-  if (FLAG_turbo_stats || FLAG_turbo_stats_nvp) {
+  if (v8_flags.turbo_stats || v8_flags.turbo_stats_nvp) {
     pipeline_statistics.reset(new PipelineStatistics(
         &info, wasm_engine->GetOrCreateTurboStatistics(), &zone_stats));
     pipeline_statistics->BeginPhaseKind("V8.WasmStubCodegen");
@@ -3304,7 +3308,7 @@ void Pipeline::GenerateCodeForWasmFunction(
     wasm::AssemblerBufferCache* buffer_cache) {
   auto* wasm_engine = wasm::GetWasmEngine();
   base::TimeTicks start_time;
-  if (V8_UNLIKELY(FLAG_trace_wasm_compilation_times)) {
+  if (V8_UNLIKELY(v8_flags.trace_wasm_compilation_times)) {
     start_time = base::TimeTicks::Now();
   }
   ZoneStats zone_stats(wasm_engine->allocator());
@@ -3327,38 +3331,39 @@ void Pipeline::GenerateCodeForWasmFunction(
   pipeline.RunPrintAndVerify("V8.WasmMachineCode", true);
 
   data.BeginPhaseKind("V8.WasmOptimization");
-  if (FLAG_wasm_inlining) {
+  if (v8_flags.wasm_inlining) {
     pipeline.Run<WasmInliningPhase>(env, function_index, wire_bytes_storage,
                                     loop_info);
     pipeline.RunPrintAndVerify(WasmInliningPhase::phase_name(), true);
   }
-  if (FLAG_wasm_loop_peeling) {
+  if (v8_flags.wasm_loop_peeling) {
     pipeline.Run<WasmLoopPeelingPhase>(loop_info);
     pipeline.RunPrintAndVerify(WasmLoopPeelingPhase::phase_name(), true);
   }
-  if (FLAG_wasm_loop_unrolling) {
+  if (v8_flags.wasm_loop_unrolling) {
     pipeline.Run<WasmLoopUnrollingPhase>(loop_info);
     pipeline.RunPrintAndVerify(WasmLoopUnrollingPhase::phase_name(), true);
   }
   const bool is_asm_js = is_asmjs_module(module);
 
-  if (FLAG_experimental_wasm_gc || FLAG_experimental_wasm_stringref) {
+  if (v8_flags.experimental_wasm_gc || v8_flags.experimental_wasm_stringref) {
     pipeline.Run<WasmTypingPhase>(function_index);
     pipeline.RunPrintAndVerify(WasmTypingPhase::phase_name(), true);
-    if (FLAG_wasm_opt) {
+    if (v8_flags.wasm_opt) {
       pipeline.Run<WasmGCOptimizationPhase>(module);
       pipeline.RunPrintAndVerify(WasmGCOptimizationPhase::phase_name(), true);
     }
   }
 
   // These proposals use gc nodes.
-  if (FLAG_experimental_wasm_gc || FLAG_experimental_wasm_typed_funcref ||
-      FLAG_experimental_wasm_stringref) {
+  if (v8_flags.experimental_wasm_gc ||
+      v8_flags.experimental_wasm_typed_funcref ||
+      v8_flags.experimental_wasm_stringref) {
     pipeline.Run<WasmGCLoweringPhase>();
     pipeline.RunPrintAndVerify(WasmGCLoweringPhase::phase_name(), true);
   }
 
-  if (FLAG_wasm_opt || is_asm_js) {
+  if (v8_flags.wasm_opt || is_asm_js) {
     pipeline.Run<WasmOptimizationPhase>(is_asm_js);
     pipeline.RunPrintAndVerify(WasmOptimizationPhase::phase_name(), true);
   } else {
@@ -3369,7 +3374,7 @@ void Pipeline::GenerateCodeForWasmFunction(
   pipeline.Run<MemoryOptimizationPhase>();
   pipeline.RunPrintAndVerify(MemoryOptimizationPhase::phase_name(), true);
 
-  if (FLAG_experimental_wasm_gc && FLAG_wasm_opt) {
+  if (v8_flags.experimental_wasm_gc && v8_flags.wasm_opt) {
     // Run value numbering and machine operator reducer to optimize load/store
     // address computation (in particular, reuse the address computation
     // whenever possible).
@@ -3381,13 +3386,13 @@ void Pipeline::GenerateCodeForWasmFunction(
                                true);
   }
 
-  if (FLAG_wasm_opt) {
+  if (v8_flags.wasm_opt) {
     pipeline.Run<BranchConditionDuplicationPhase>();
     pipeline.RunPrintAndVerify(BranchConditionDuplicationPhase::phase_name(),
                                true);
   }
 
-  if (FLAG_turbo_splitting && !is_asm_js) {
+  if (v8_flags.turbo_splitting && !is_asm_js) {
     data.info()->set_splitting();
   }
 
@@ -3443,7 +3448,7 @@ void Pipeline::GenerateCodeForWasmFunction(
         << " using TurboFan" << std::endl;
   }
 
-  if (V8_UNLIKELY(FLAG_trace_wasm_compilation_times)) {
+  if (V8_UNLIKELY(v8_flags.trace_wasm_compilation_times)) {
     base::TimeDelta time = base::TimeTicks::Now() - start_time;
     int codesize = result->code_desc.body_size();
     StdoutStream{} << "Compiled function "
@@ -3524,7 +3529,7 @@ MaybeHandle<Code> Pipeline::GenerateCodeForTesting(
                     nullptr, schedule, nullptr, node_positions, nullptr,
                     options, nullptr);
   std::unique_ptr<PipelineStatistics> pipeline_statistics;
-  if (FLAG_turbo_stats || FLAG_turbo_stats_nvp) {
+  if (v8_flags.turbo_stats || v8_flags.turbo_stats_nvp) {
     pipeline_statistics.reset(new PipelineStatistics(
         info, isolate->GetTurboStatistics(), &zone_stats));
     pipeline_statistics->BeginPhaseKind("V8.TFTestCodegen");
@@ -3604,7 +3609,7 @@ bool PipelineImpl::SelectInstructions(Linkage* linkage) {
   DCHECK_NOT_NULL(data->graph());
   DCHECK_NOT_NULL(data->schedule());
 
-  if (FLAG_turbo_profiling) {
+  if (v8_flags.turbo_profiling) {
     UnparkedScopeIfNeeded unparked_scope(data->broker());
     data->info()->set_profiler_data(BasicBlockInstrumentor::Instrument(
         info(), data->graph(), data->schedule(), data->isolate()));
@@ -3612,9 +3617,9 @@ bool PipelineImpl::SelectInstructions(Linkage* linkage) {
 
   bool verify_stub_graph =
       data->verify_graph() ||
-      (FLAG_turbo_verify_machine_graph != nullptr &&
-       (!strcmp(FLAG_turbo_verify_machine_graph, "*") ||
-        !strcmp(FLAG_turbo_verify_machine_graph, data->debug_name())));
+      (v8_flags.turbo_verify_machine_graph != nullptr &&
+       (!strcmp(v8_flags.turbo_verify_machine_graph, "*") ||
+        !strcmp(v8_flags.turbo_verify_machine_graph, data->debug_name())));
   // Jump optimization runs instruction selection twice, but the instruction
   // selector mutates nodes like swapping the inputs of a load, which can
   // violate the machine graph verification rules. So we skip the second
@@ -3624,7 +3629,7 @@ bool PipelineImpl::SelectInstructions(Linkage* linkage) {
     verify_stub_graph = false;
   }
   if (verify_stub_graph) {
-    if (FLAG_trace_verify_csa) {
+    if (v8_flags.trace_verify_csa) {
       UnparkedScopeIfNeeded scope(data->broker());
       AllowHandleDereference allow_deref;
       CodeTracer::StreamScope tracing_scope(data->GetCodeTracer());
@@ -3689,7 +3694,7 @@ bool PipelineImpl::SelectInstructions(Linkage* linkage) {
 
   data->BeginPhaseKind("V8.TFRegisterAllocation");
 
-  bool run_verifier = FLAG_turbo_verify_allocation;
+  bool run_verifier = v8_flags.turbo_verify_allocation;
 
   // Allocate registers.
 
@@ -3705,8 +3710,8 @@ bool PipelineImpl::SelectInstructions(Linkage* linkage) {
   // JS functions.
   bool use_mid_tier_register_allocator =
       data->info()->code_kind() == CodeKind::WASM_FUNCTION &&
-      (FLAG_turbo_force_mid_tier_regalloc ||
-       (FLAG_turbo_use_mid_tier_regalloc_for_huge_functions &&
+      (v8_flags.turbo_force_mid_tier_regalloc ||
+       (v8_flags.turbo_use_mid_tier_regalloc_for_huge_functions &&
         data->sequence()->VirtualRegisterCount() >
             kTopTierVirtualRegistersLimit));
 
@@ -3733,7 +3738,7 @@ bool PipelineImpl::SelectInstructions(Linkage* linkage) {
   bool generate_frame_at_start =
       data_->sequence()->instruction_blocks().front()->must_construct_frame();
   // Optimimize jumps.
-  if (FLAG_turbo_jt) {
+  if (v8_flags.turbo_jt) {
     Run<JumpThreadingPhase>(generate_frame_at_start);
   }
 
@@ -4004,7 +4009,7 @@ void PipelineImpl::AllocateRegistersForTopTier(
 
   Run<PopulateReferenceMapsPhase>();
 
-  if (FLAG_turbo_move_optimization) {
+  if (v8_flags.turbo_move_optimization) {
     Run<OptimizeMovesPhase>();
   }
 
diff --git a/src/compiler/raw-machine-assembler.cc b/src/compiler/raw-machine-assembler.cc
index cdd6b4a55a..7302e6f2fa 100644
--- a/src/compiler/raw-machine-assembler.cc
+++ b/src/compiler/raw-machine-assembler.cc
@@ -88,7 +88,7 @@ Node* RawMachineAssembler::OptimizedAllocate(
 Schedule* RawMachineAssembler::ExportForTest() {
   // Compute the correct codegen order.
   DCHECK(schedule_->rpo_order()->empty());
-  if (FLAG_trace_turbo_scheduler) {
+  if (v8_flags.trace_turbo_scheduler) {
     PrintF("--- RAW SCHEDULE -------------------------------------------\n");
     StdoutStream{} << *schedule_;
   }
@@ -96,7 +96,7 @@ Schedule* RawMachineAssembler::ExportForTest() {
   Scheduler::ComputeSpecialRPO(zone(), schedule_);
   Scheduler::GenerateDominatorTree(schedule_);
   schedule_->PropagateDeferredMark();
-  if (FLAG_trace_turbo_scheduler) {
+  if (v8_flags.trace_turbo_scheduler) {
     PrintF("--- EDGE SPLIT AND PROPAGATED DEFERRED SCHEDULE ------------\n");
     StdoutStream{} << *schedule_;
   }
@@ -110,14 +110,14 @@ Schedule* RawMachineAssembler::ExportForTest() {
 Graph* RawMachineAssembler::ExportForOptimization() {
   // Compute the correct codegen order.
   DCHECK(schedule_->rpo_order()->empty());
-  if (FLAG_trace_turbo_scheduler) {
+  if (v8_flags.trace_turbo_scheduler) {
     PrintF("--- RAW SCHEDULE -------------------------------------------\n");
     StdoutStream{} << *schedule_;
   }
   schedule_->EnsureCFGWellFormedness();
   OptimizeControlFlow(schedule_, graph(), common());
   Scheduler::ComputeSpecialRPO(zone(), schedule_);
-  if (FLAG_trace_turbo_scheduler) {
+  if (v8_flags.trace_turbo_scheduler) {
     PrintF("--- SCHEDULE BEFORE GRAPH CREATION -------------------------\n");
     StdoutStream{} << *schedule_;
   }
diff --git a/src/compiler/schedule.cc b/src/compiler/schedule.cc
index 5aeeff9123..c608dd63ad 100644
--- a/src/compiler/schedule.cc
+++ b/src/compiler/schedule.cc
@@ -198,7 +198,7 @@ BasicBlock* Schedule::NewBasicBlock() {
 }
 
 void Schedule::PlanNode(BasicBlock* block, Node* node) {
-  if (FLAG_trace_turbo_scheduler) {
+  if (v8_flags.trace_turbo_scheduler) {
     StdoutStream{} << "Planning #" << node->id() << ":"
                    << node->op()->mnemonic()
                    << " for future add to id:" << block->id() << "\n";
@@ -208,7 +208,7 @@ void Schedule::PlanNode(BasicBlock* block, Node* node) {
 }
 
 void Schedule::AddNode(BasicBlock* block, Node* node) {
-  if (FLAG_trace_turbo_scheduler) {
+  if (v8_flags.trace_turbo_scheduler) {
     StdoutStream{} << "Adding #" << node->id() << ":" << node->op()->mnemonic()
                    << " to id:" << block->id() << "\n";
   }
diff --git a/src/compiler/scheduler.cc b/src/compiler/scheduler.cc
index e28c848e5f..4da855cf6e 100644
--- a/src/compiler/scheduler.cc
+++ b/src/compiler/scheduler.cc
@@ -22,9 +22,9 @@ namespace v8 {
 namespace internal {
 namespace compiler {
 
-#define TRACE(...)                                       \
-  do {                                                   \
-    if (FLAG_trace_turbo_scheduler) PrintF(__VA_ARGS__); \
+#define TRACE(...)                                           \
+  do {                                                       \
+    if (v8_flags.trace_turbo_scheduler) PrintF(__VA_ARGS__); \
   } while (false)
 
 Scheduler::Scheduler(Zone* zone, Graph* graph, Schedule* schedule, Flags flags,
@@ -195,7 +195,7 @@ void Scheduler::IncrementUnscheduledUseCount(Node* node, Node* from) {
   }
 
   ++(GetData(node)->unscheduled_count_);
-  if (FLAG_trace_turbo_scheduler) {
+  if (v8_flags.trace_turbo_scheduler) {
     TRACE("  Use count of #%d:%s (used by #%d:%s)++ = %d\n", node->id(),
           node->op()->mnemonic(), from->id(), from->op()->mnemonic(),
           GetData(node)->unscheduled_count_);
@@ -215,7 +215,7 @@ void Scheduler::DecrementUnscheduledUseCount(Node* node, Node* from) {
 
   DCHECK_LT(0, GetData(node)->unscheduled_count_);
   --(GetData(node)->unscheduled_count_);
-  if (FLAG_trace_turbo_scheduler) {
+  if (v8_flags.trace_turbo_scheduler) {
     TRACE("  Use count of #%d:%s (used by #%d:%s)-- = %d\n", node->id(),
           node->op()->mnemonic(), from->id(), from->op()->mnemonic(),
           GetData(node)->unscheduled_count_);
@@ -495,7 +495,7 @@ class CFGBuilder : public ZoneObject {
         break;
     }
 
-    if (FLAG_warn_about_builtin_profile_data &&
+    if (v8_flags.warn_about_builtin_profile_data &&
         hint_from_profile != BranchHint::kNone &&
         BranchHintOf(branch->op()) != BranchHint::kNone &&
         hint_from_profile != BranchHintOf(branch->op())) {
@@ -704,7 +704,7 @@ class SpecialRPONumberer : public ZoneObject {
   // Print and verify the special reverse-post-order.
   void PrintAndVerifySpecialRPO() {
 #if DEBUG
-    if (FLAG_trace_turbo_scheduler) PrintRPO();
+    if (v8_flags.trace_turbo_scheduler) PrintRPO();
     VerifySpecialRPO();
 #endif
   }
@@ -1481,7 +1481,7 @@ void Scheduler::ScheduleEarly() {
   }
 
   TRACE("--- SCHEDULE EARLY -----------------------------------------\n");
-  if (FLAG_trace_turbo_scheduler) {
+  if (v8_flags.trace_turbo_scheduler) {
     TRACE("roots: ");
     for (Node* node : schedule_root_nodes_) {
       TRACE("#%d:%s ", node->id(), node->op()->mnemonic());
@@ -1847,7 +1847,7 @@ class ScheduleLateNodeVisitor {
 
 void Scheduler::ScheduleLate() {
   TRACE("--- SCHEDULE LATE ------------------------------------------\n");
-  if (FLAG_trace_turbo_scheduler) {
+  if (v8_flags.trace_turbo_scheduler) {
     TRACE("roots: ");
     for (Node* node : schedule_root_nodes_) {
       TRACE("#%d:%s ", node->id(), node->op()->mnemonic());
@@ -1891,7 +1891,7 @@ void Scheduler::SealFinalSchedule() {
 
 void Scheduler::FuseFloatingControl(BasicBlock* block, Node* node) {
   TRACE("--- FUSE FLOATING CONTROL ----------------------------------\n");
-  if (FLAG_trace_turbo_scheduler) {
+  if (v8_flags.trace_turbo_scheduler) {
     StdoutStream{} << "Schedule before control flow fusion:\n" << *schedule_;
   }
 
@@ -1919,7 +1919,7 @@ void Scheduler::FuseFloatingControl(BasicBlock* block, Node* node) {
       }
     }
   }
-  if (FLAG_trace_turbo_scheduler) {
+  if (v8_flags.trace_turbo_scheduler) {
     TRACE("propagation roots: ");
     for (Node* r : propagation_roots) {
       TRACE("#%d:%s ", r->id(), r->op()->mnemonic());
@@ -1934,7 +1934,7 @@ void Scheduler::FuseFloatingControl(BasicBlock* block, Node* node) {
   scheduled_nodes_.resize(schedule_->BasicBlockCount());
   MovePlannedNodes(block, schedule_->block(node));
 
-  if (FLAG_trace_turbo_scheduler) {
+  if (v8_flags.trace_turbo_scheduler) {
     StdoutStream{} << "Schedule after control flow fusion:\n" << *schedule_;
   }
 }
diff --git a/src/compiler/simplified-lowering.cc b/src/compiler/simplified-lowering.cc
index a0e4583f2e..842d043376 100644
--- a/src/compiler/simplified-lowering.cc
+++ b/src/compiler/simplified-lowering.cc
@@ -39,9 +39,9 @@ namespace internal {
 namespace compiler {
 
 // Macro for outputting trace information from representation inference.
-#define TRACE(...)                                      \
-  do {                                                  \
-    if (FLAG_trace_representation) PrintF(__VA_ARGS__); \
+#define TRACE(...)                                          \
+  do {                                                      \
+    if (v8_flags.trace_representation) PrintF(__VA_ARGS__); \
   } while (false)
 
 const char* kSimplifiedLoweringReducerName = "SimplifiedLowering";
@@ -515,7 +515,7 @@ class RepresentationSelector {
 
     if (!type.IsInvalid() && new_type.Is(type)) return false;
     GetInfo(node)->set_feedback_type(new_type);
-    if (FLAG_trace_representation) {
+    if (v8_flags.trace_representation) {
       PrintNodeFeedbackType(node);
     }
     return true;
@@ -4142,7 +4142,7 @@ class RepresentationSelector {
           if (inputType.CanBeAsserted()) {
             ChangeOp(node, simplified()->AssertType(inputType));
           } else {
-            if (!FLAG_fuzzing) {
+            if (!v8_flags.fuzzing) {
 #ifdef DEBUG
               inputType.Print();
 #endif
@@ -4440,7 +4440,7 @@ SimplifiedLowering::SimplifiedLowering(
 
 void SimplifiedLowering::LowerAllNodes() {
   SimplifiedLoweringVerifier* verifier = nullptr;
-  if (FLAG_verify_simplified_lowering) {
+  if (v8_flags.verify_simplified_lowering) {
     verifier = zone_->New<SimplifiedLoweringVerifier>(zone_, graph());
   }
   RepresentationChanger changer(jsgraph(), broker_, verifier);
diff --git a/src/compiler/store-store-elimination.cc b/src/compiler/store-store-elimination.cc
index 75a5ea421e..f6ef3d5242 100644
--- a/src/compiler/store-store-elimination.cc
+++ b/src/compiler/store-store-elimination.cc
@@ -18,7 +18,7 @@ namespace compiler {
 
 #define TRACE(fmt, ...)                                         \
   do {                                                          \
-    if (FLAG_trace_store_elimination) {                         \
+    if (v8_flags.trace_store_elimination) {                     \
       PrintF("RedundantStoreFinder: " fmt "\n", ##__VA_ARGS__); \
     }                                                           \
   } while (false)
@@ -553,7 +553,7 @@ void StoreStoreElimination::Run(JSGraph* js_graph, TickCounter* tick_counter,
 
   // Remove superfluous nodes
   for (Node* node : finder.to_remove_const()) {
-    if (FLAG_trace_store_elimination) {
+    if (v8_flags.trace_store_elimination) {
       PrintF("StoreStoreElimination::Run: Eliminating node #%d:%s\n",
              node->id(), node->op()->mnemonic());
     }
diff --git a/src/compiler/turboshaft/optimization-phase.h b/src/compiler/turboshaft/optimization-phase.h
index 074e517c81..0f9848a1e3 100644
--- a/src/compiler/turboshaft/optimization-phase.h
+++ b/src/compiler/turboshaft/optimization-phase.h
@@ -111,7 +111,7 @@ class OptimizationPhase {
   static void Run(Graph* input, Zone* phase_zone, NodeOriginTable* origins,
                   VisitOrder visit_order = VisitOrder::kAsEmitted) {
     Impl phase{*input, phase_zone, origins, visit_order};
-    if (FLAG_turboshaft_trace_reduction) {
+    if (v8_flags.turboshaft_trace_reduction) {
       phase.template Run<true>();
     } else {
       phase.template Run<false>();
diff --git a/src/compiler/typer.cc b/src/compiler/typer.cc
index b261b5e1cd..228e315be4 100644
--- a/src/compiler/typer.cc
+++ b/src/compiler/typer.cc
@@ -862,7 +862,7 @@ Type Typer::Visitor::TypeInductionVariablePhi(Node* node) {
     max = +V8_INFINITY;
   }
 
-  if (FLAG_trace_turbo_loop) {
+  if (v8_flags.trace_turbo_loop) {
     StdoutStream{} << std::setprecision(10) << "Loop ("
                    << NodeProperties::GetControlInput(node)->id()
                    << ") variable bounds in "
diff --git a/src/compiler/verifier.cc b/src/compiler/verifier.cc
index 9d1959a776..1404b8e6ae 100644
--- a/src/compiler/verifier.cc
+++ b/src/compiler/verifier.cc
@@ -48,7 +48,7 @@ class Verifier::Visitor {
  private:
   void CheckNotTyped(Node* node) {
     // Verification of simplified lowering sets types of many additional nodes.
-    if (FLAG_verify_simplified_lowering) return;
+    if (v8_flags.verify_simplified_lowering) return;
 
     if (NodeProperties::IsTyped(node)) {
       std::ostringstream str;
diff --git a/test/cctest/compiler/function-tester.cc b/test/cctest/compiler/function-tester.cc
index 220b4b6d43..5d104467c0 100644
--- a/test/cctest/compiler/function-tester.cc
+++ b/test/cctest/compiler/function-tester.cc
@@ -23,7 +23,7 @@ namespace compiler {
 FunctionTester::FunctionTester(const char* source, uint32_t flags)
     : isolate(main_isolate()),
       canonical(isolate),
-      function((FLAG_allow_natives_syntax = true, NewFunction(source))),
+      function((v8_flags.allow_natives_syntax = true, NewFunction(source))),
       flags_(flags) {
   Compile(function);
   const uint32_t supported_flags = OptimizedCompilationInfo::kInlining;
@@ -41,7 +41,7 @@ FunctionTester::FunctionTester(Graph* graph, int param_count)
 FunctionTester::FunctionTester(Handle<Code> code, int param_count)
     : isolate(main_isolate()),
       canonical(isolate),
-      function((FLAG_allow_natives_syntax = true,
+      function((v8_flags.allow_natives_syntax = true,
                 NewFunction(BuildFunction(param_count).c_str()))),
       flags_(0) {
   CHECK(!code.is_null());
diff --git a/test/cctest/compiler/test-basic-block-profiler.cc b/test/cctest/compiler/test-basic-block-profiler.cc
index 011bc1f11e..218944bbd1 100644
--- a/test/cctest/compiler/test-basic-block-profiler.cc
+++ b/test/cctest/compiler/test-basic-block-profiler.cc
@@ -15,7 +15,7 @@ class BasicBlockProfilerTest : public RawMachineAssemblerTester<int32_t> {
  public:
   BasicBlockProfilerTest()
       : RawMachineAssemblerTester<int32_t>(MachineType::Int32()) {
-    FLAG_turbo_profiling = true;
+    v8_flags.turbo_profiling = true;
   }
 
   void ResetCounts() {
diff --git a/test/cctest/compiler/test-calls-with-arraylike-or-spread.cc b/test/cctest/compiler/test-calls-with-arraylike-or-spread.cc
index 7e056f4e6c..dc9767865e 100644
--- a/test/cctest/compiler/test-calls-with-arraylike-or-spread.cc
+++ b/test/cctest/compiler/test-calls-with-arraylike-or-spread.cc
@@ -20,8 +20,8 @@ void CompileRunWithNodeObserver(const std::string& js_code,
   v8::Isolate* isolate = env->GetIsolate();
   v8::HandleScope handle_scope(isolate);
 
-  FLAG_allow_natives_syntax = true;
-  FLAG_turbo_optimize_apply = true;
+  v8_flags.allow_natives_syntax = true;
+  v8_flags.turbo_optimize_apply = true;
 
   // Note: Make sure to not capture stack locations (e.g. `this`) here since
   // these lambdas are executed on another thread.
@@ -146,8 +146,8 @@ TEST(ReduceCAPICallWithArrayLike) {
   v8::Isolate* isolate = env->GetIsolate();
   v8::HandleScope scope(isolate);
 
-  FLAG_allow_natives_syntax = true;
-  FLAG_turbo_optimize_apply = true;
+  v8_flags.allow_natives_syntax = true;
+  v8_flags.turbo_optimize_apply = true;
 
   Local<v8::FunctionTemplate> sum = v8::FunctionTemplate::New(isolate, SumF);
   CHECK(env->Global()
diff --git a/test/cctest/compiler/test-code-assembler.cc b/test/cctest/compiler/test-code-assembler.cc
index 780851a0ec..d37ec78c34 100644
--- a/test/cctest/compiler/test-code-assembler.cc
+++ b/test/cctest/compiler/test-code-assembler.cc
@@ -468,7 +468,7 @@ TEST(ExceptionHandler) {
 
 TEST(TestCodeAssemblerCodeComment) {
 #ifdef V8_CODE_COMMENTS
-  i::FLAG_code_comments = true;
+  i::v8_flags.code_comments = true;
   Isolate* isolate(CcTest::InitIsolateOnce());
   const int kNumParams = 0;
   CodeAssemblerTester asm_tester(isolate, kNumParams);
diff --git a/test/cctest/compiler/test-code-generator.cc b/test/cctest/compiler/test-code-generator.cc
index e686d2a1a5..26ca83a475 100644
--- a/test/cctest/compiler/test-code-generator.cc
+++ b/test/cctest/compiler/test-code-generator.cc
@@ -693,13 +693,13 @@ class TestEnvironment : public HandleAndZoneScope {
       // The "setup" and "teardown" functions are relatively big, and with
       // runtime assertions enabled they get so big that memory during register
       // allocation becomes a problem. Temporarily disable such assertions.
-      bool old_enable_slow_asserts = FLAG_enable_slow_asserts;
-      FLAG_enable_slow_asserts = false;
+      bool old_enable_slow_asserts = v8_flags.enable_slow_asserts;
+      v8_flags.enable_slow_asserts = false;
 #endif
       Handle<Code> setup =
           BuildSetupFunction(main_isolate(), test_descriptor_, layout_);
 #ifdef ENABLE_SLOW_DCHECKS
-      FLAG_enable_slow_asserts = old_enable_slow_asserts;
+      v8_flags.enable_slow_asserts = old_enable_slow_asserts;
 #endif
       // FunctionTester maintains its own HandleScope which means that its
       // return value will be freed along with it. Copy the result into
@@ -1270,7 +1270,7 @@ TEST(FuzzAssembleMove) {
     }
 
     Handle<Code> test = c.FinalizeForExecuting();
-    if (FLAG_print_code) {
+    if (v8_flags.print_code) {
       test->Print();
     }
 
@@ -1309,7 +1309,7 @@ TEST(FuzzAssembleParallelMove) {
     }
 
     Handle<Code> test = c.FinalizeForExecuting();
-    if (FLAG_print_code) {
+    if (v8_flags.print_code) {
       test->Print();
     }
 
@@ -1335,7 +1335,7 @@ TEST(FuzzAssembleSwap) {
     }
 
     Handle<Code> test = c.FinalizeForExecuting();
-    if (FLAG_print_code) {
+    if (v8_flags.print_code) {
       test->Print();
     }
 
@@ -1373,7 +1373,7 @@ TEST(FuzzAssembleMoveAndSwap) {
     }
 
     Handle<Code> test = c.FinalizeForExecuting();
-    if (FLAG_print_code) {
+    if (v8_flags.print_code) {
       test->Print();
     }
 
@@ -1454,7 +1454,7 @@ TEST(AssembleTailCallGap) {
     c.CheckAssembleTailCallGaps(instr, first_slot + 4,
                                 CodeGeneratorTester::kRegisterPush);
     Handle<Code> code = c.Finalize();
-    if (FLAG_print_code) {
+    if (v8_flags.print_code) {
       code->Print();
     }
   }
@@ -1483,7 +1483,7 @@ TEST(AssembleTailCallGap) {
     c.CheckAssembleTailCallGaps(instr, first_slot + 4,
                                 CodeGeneratorTester::kStackSlotPush);
     Handle<Code> code = c.Finalize();
-    if (FLAG_print_code) {
+    if (v8_flags.print_code) {
       code->Print();
     }
   }
@@ -1512,7 +1512,7 @@ TEST(AssembleTailCallGap) {
     c.CheckAssembleTailCallGaps(instr, first_slot + 4,
                                 CodeGeneratorTester::kScalarPush);
     Handle<Code> code = c.Finalize();
-    if (FLAG_print_code) {
+    if (v8_flags.print_code) {
       code->Print();
     }
   }
diff --git a/test/cctest/compiler/test-concurrent-shared-function-info.cc b/test/cctest/compiler/test-concurrent-shared-function-info.cc
index 1dfa2dbab7..d9aabbe0c9 100644
--- a/test/cctest/compiler/test-concurrent-shared-function-info.cc
+++ b/test/cctest/compiler/test-concurrent-shared-function-info.cc
@@ -91,7 +91,7 @@ class BackgroundCompilationThread final : public v8::base::Thread {
 };
 
 TEST(TestConcurrentSharedFunctionInfo) {
-  FlagScope<bool> allow_natives_syntax(&i::FLAG_allow_natives_syntax, true);
+  FlagScope<bool> allow_natives_syntax(&i::v8_flags.allow_natives_syntax, true);
 
   HandleAndZoneScope scope;
   Isolate* isolate = scope.main_isolate();
diff --git a/test/cctest/compiler/test-loop-analysis.cc b/test/cctest/compiler/test-loop-analysis.cc
index fe5009f231..ede4da40c0 100644
--- a/test/cctest/compiler/test-loop-analysis.cc
+++ b/test/cctest/compiler/test-loop-analysis.cc
@@ -126,7 +126,7 @@ class LoopFinderTester : HandleAndZoneScope {
 
   LoopTree* GetLoopTree() {
     if (loop_tree == nullptr) {
-      if (FLAG_trace_turbo_graph) {
+      if (v8_flags.trace_turbo_graph) {
         StdoutStream{} << AsRPO(graph);
       }
       Zone zone(main_isolate()->allocator(), ZONE_NAME);
diff --git a/test/cctest/compiler/test-multiple-return.cc b/test/cctest/compiler/test-multiple-return.cc
index c988422683..c21ddff33f 100644
--- a/test/cctest/compiler/test-multiple-return.cc
+++ b/test/cctest/compiler/test-multiple-return.cc
@@ -169,7 +169,7 @@ void TestReturnMultipleValues(MachineType type, int min_count, int max_count) {
                               m.ExportForTest())
                               .ToHandleChecked();
 #ifdef ENABLE_DISASSEMBLER
-      if (FLAG_print_code) {
+      if (v8_flags.print_code) {
         StdoutStream os;
         code->Disassemble("multi_value", os, handles.main_isolate());
       }
@@ -217,7 +217,7 @@ void TestReturnMultipleValues(MachineType type, int min_count, int max_count) {
       mt.Return(ToInt32(&mt, type, ret));
 #ifdef ENABLE_DISASSEMBLER
       Handle<Code> code2 = mt.GetCode();
-      if (FLAG_print_code) {
+      if (v8_flags.print_code) {
         StdoutStream os;
         code2->Disassemble("multi_value_call", os, handles.main_isolate());
       }
diff --git a/test/cctest/compiler/test-run-bytecode-graph-builder.cc b/test/cctest/compiler/test-run-bytecode-graph-builder.cc
index eb970693b9..6176263a0e 100644
--- a/test/cctest/compiler/test-run-bytecode-graph-builder.cc
+++ b/test/cctest/compiler/test-run-bytecode-graph-builder.cc
@@ -77,8 +77,8 @@ class BytecodeGraphTester {
   BytecodeGraphTester(Isolate* isolate, const char* script,
                       const char* filter = kFunctionName)
       : isolate_(isolate), script_(script) {
-    i::FLAG_always_turbofan = false;
-    i::FLAG_allow_natives_syntax = true;
+    i::v8_flags.always_turbofan = false;
+    i::v8_flags.allow_natives_syntax = true;
   }
   virtual ~BytecodeGraphTester() = default;
   BytecodeGraphTester(const BytecodeGraphTester&) = delete;
diff --git a/test/cctest/compiler/test-run-native-calls.cc b/test/cctest/compiler/test-run-native-calls.cc
index 126bdc4c1f..cf55b622e9 100644
--- a/test/cctest/compiler/test-run-native-calls.cc
+++ b/test/cctest/compiler/test-run-native-calls.cc
@@ -251,7 +251,7 @@ Handle<CodeT> CompileGraph(const char* name, CallDescriptor* call_descriptor,
                           AssemblerOptions::Default(isolate), schedule)
                           .ToHandleChecked();
 #ifdef ENABLE_DISASSEMBLER
-  if (FLAG_print_opt_code) {
+  if (v8_flags.print_opt_code) {
     StdoutStream os;
     code->Disassemble(name, os, isolate);
   }
diff --git a/test/cctest/compiler/test-run-unwinding-info.cc b/test/cctest/compiler/test-run-unwinding-info.cc
index c4f65b4a4b..b5bc43db43 100644
--- a/test/cctest/compiler/test-run-unwinding-info.cc
+++ b/test/cctest/compiler/test-run-unwinding-info.cc
@@ -17,8 +17,8 @@ namespace internal {
 namespace compiler {
 
 TEST(RunUnwindingInfo) {
-  FLAG_always_turbofan = true;
-  FLAG_perf_prof_unwinding_info = true;
+  v8_flags.always_turbofan = true;
+  v8_flags.perf_prof_unwinding_info = true;
 
   FunctionTester tester(
       "(function (x) {\n"
diff --git a/test/cctest/compiler/test-verify-type.cc b/test/cctest/compiler/test-verify-type.cc
index 8a3ea077dd..ffc69d5514 100644
--- a/test/cctest/compiler/test-verify-type.cc
+++ b/test/cctest/compiler/test-verify-type.cc
@@ -12,7 +12,7 @@ namespace internal {
 namespace compiler {
 
 TEST(TestVerifyType) {
-  FlagScope<bool> allow_natives_syntax(&i::FLAG_allow_natives_syntax, true);
+  FlagScope<bool> allow_natives_syntax(&i::v8_flags.allow_natives_syntax, true);
   HandleAndZoneScope handle_scope;
   Isolate* isolate = handle_scope.main_isolate();
   Zone* zone = handle_scope.main_zone();
diff --git a/test/unittests/compiler/backend/instruction-selector-unittest.cc b/test/unittests/compiler/backend/instruction-selector-unittest.cc
index e52661fae2..30928fe94a 100644
--- a/test/unittests/compiler/backend/instruction-selector-unittest.cc
+++ b/test/unittests/compiler/backend/instruction-selector-unittest.cc
@@ -19,7 +19,7 @@ namespace compiler {
 
 InstructionSelectorTest::InstructionSelectorTest()
     : TestWithNativeContextAndZone(kCompressGraphZone),
-      rng_(FLAG_random_seed) {}
+      rng_(v8_flags.random_seed) {}
 
 InstructionSelectorTest::~InstructionSelectorTest() = default;
 
@@ -28,7 +28,7 @@ InstructionSelectorTest::Stream InstructionSelectorTest::StreamBuilder::Build(
     InstructionSelectorTest::StreamBuilderMode mode,
     InstructionSelector::SourcePositionMode source_position_mode) {
   Schedule* schedule = ExportForTest();
-  if (FLAG_trace_turbo) {
+  if (v8_flags.trace_turbo) {
     StdoutStream{} << "=== Schedule before instruction selection ==="
                    << std::endl
                    << *schedule;
@@ -52,7 +52,7 @@ InstructionSelectorTest::Stream InstructionSelectorTest::StreamBuilder::Build(
       source_position_mode, features, InstructionSelector::kDisableScheduling,
       InstructionSelector::kEnableRootsRelativeAddressing);
   selector.SelectInstructions();
-  if (FLAG_trace_turbo) {
+  if (v8_flags.trace_turbo) {
     StdoutStream{} << "=== Code sequence after instruction selection ==="
                    << std::endl
                    << sequence;
diff --git a/test/unittests/compiler/bytecode-analysis-unittest.cc b/test/unittests/compiler/bytecode-analysis-unittest.cc
index 90a80b968e..1eb4320041 100644
--- a/test/unittests/compiler/bytecode-analysis-unittest.cc
+++ b/test/unittests/compiler/bytecode-analysis-unittest.cc
@@ -29,8 +29,8 @@ class BytecodeAnalysisTest : public TestWithIsolateAndZone {
   static void SetUpTestSuite() {
     CHECK_NULL(save_flags_);
     save_flags_ = new SaveFlags();
-    i::FLAG_ignition_elide_noneffectful_bytecodes = false;
-    i::FLAG_ignition_reo = false;
+    i::v8_flags.ignition_elide_noneffectful_bytecodes = false;
+    i::v8_flags.ignition_reo = false;
 
     TestWithIsolateAndZone::SetUpTestSuite();
   }
diff --git a/test/unittests/compiler/compiler-unittest.cc b/test/unittests/compiler/compiler-unittest.cc
index b5419dba41..abef44976c 100644
--- a/test/unittests/compiler/compiler-unittest.cc
+++ b/test/unittests/compiler/compiler-unittest.cc
@@ -206,7 +206,7 @@ using CompilerC2JSFramesTest = WithPrintExtensionMixin<v8::TestWithIsolate>;
 //   |      JS       |
 //   |   C-to-JS     |
 TEST_F(CompilerC2JSFramesTest, C2JSFrames) {
-  FLAG_expose_gc = true;
+  v8_flags.expose_gc = true;
   v8::HandleScope scope(isolate());
   const char* extension_names[2] = {
       "v8/gc", WithPrintExtensionMixin::kPrintExtensionName};
@@ -276,8 +276,8 @@ TEST_F(CompilerTest, GetScriptLineNumber) {
 }
 
 TEST_F(CompilerTest, FeedbackVectorPreservedAcrossRecompiles) {
-  if (i::FLAG_always_turbofan || !i::FLAG_turbofan) return;
-  i::FLAG_allow_natives_syntax = true;
+  if (i::v8_flags.always_turbofan || !i::v8_flags.turbofan) return;
+  i::v8_flags.allow_natives_syntax = true;
   if (!i_isolate()->use_optimizer()) return;
   v8::HandleScope scope(isolate());
 
@@ -319,7 +319,8 @@ TEST_F(CompilerTest, FeedbackVectorPreservedAcrossRecompiles) {
 }
 
 TEST_F(CompilerTest, FeedbackVectorUnaffectedByScopeChanges) {
-  if (i::FLAG_always_turbofan || !i::FLAG_lazy || i::FLAG_lite_mode) {
+  if (i::v8_flags.always_turbofan || !i::v8_flags.lazy ||
+      i::v8_flags.lite_mode) {
     return;
   }
   v8::HandleScope scope(isolate());
@@ -357,8 +358,8 @@ TEST_F(CompilerTest, FeedbackVectorUnaffectedByScopeChanges) {
 
 // Test that optimized code for different closures is actually shared.
 TEST_F(CompilerTest, OptimizedCodeSharing1) {
-  FLAG_stress_compaction = false;
-  FLAG_allow_natives_syntax = true;
+  v8_flags.stress_compaction = false;
+  v8_flags.allow_natives_syntax = true;
   v8::HandleScope scope(isolate());
   for (int i = 0; i < 3; i++) {
     context()
@@ -399,7 +400,7 @@ TEST_F(CompilerTest, OptimizedCodeSharing1) {
 }
 
 TEST_F(CompilerTest, CompileFunction) {
-  if (i::FLAG_always_turbofan) return;
+  if (i::v8_flags.always_turbofan) return;
   v8::HandleScope scope(isolate());
   RunJS("var r = 10;");
   v8::Local<v8::Object> math = v8::Local<v8::Object>::Cast(
@@ -724,9 +725,9 @@ TEST_F(CompilerTest, CompileFunctionFunctionToString) {
 }
 
 TEST_F(CompilerTest, InvocationCount) {
-  if (FLAG_lite_mode) return;
-  FLAG_allow_natives_syntax = true;
-  FLAG_always_turbofan = false;
+  if (v8_flags.lite_mode) return;
+  v8_flags.allow_natives_syntax = true;
+  v8_flags.always_turbofan = false;
   v8::HandleScope scope(isolate());
 
   RunJS(
@@ -746,7 +747,7 @@ TEST_F(CompilerTest, InvocationCount) {
 }
 
 TEST_F(CompilerTest, ShallowEagerCompilation) {
-  i::FLAG_always_turbofan = false;
+  i::v8_flags.always_turbofan = false;
   v8::HandleScope scope(isolate());
   v8::Local<v8::String> source = NewString(
       "function f(x) {"
@@ -766,7 +767,7 @@ TEST_F(CompilerTest, ShallowEagerCompilation) {
 }
 
 TEST_F(CompilerTest, DeepEagerCompilation) {
-  i::FLAG_always_turbofan = false;
+  i::v8_flags.always_turbofan = false;
   v8::HandleScope scope(isolate());
   v8::Local<v8::String> source = NewString(
       "function f(x) {"
@@ -792,7 +793,7 @@ TEST_F(CompilerTest, DeepEagerCompilation) {
 }
 
 TEST_F(CompilerTest, DeepEagerCompilationPeakMemory) {
-  i::FLAG_always_turbofan = false;
+  i::v8_flags.always_turbofan = false;
   v8::HandleScope scope(isolate());
   v8::Local<v8::String> source = NewString(
       "function f() {"
diff --git a/test/unittests/compiler/control-equivalence-unittest.cc b/test/unittests/compiler/control-equivalence-unittest.cc
index e56d18a8cb..1463f4ac8e 100644
--- a/test/unittests/compiler/control-equivalence-unittest.cc
+++ b/test/unittests/compiler/control-equivalence-unittest.cc
@@ -30,7 +30,7 @@ class ControlEquivalenceTest : public GraphTest {
  protected:
   void ComputeEquivalence(Node* end_node) {
     graph()->SetEnd(graph()->NewNode(common()->End(1), end_node));
-    if (FLAG_trace_turbo) {
+    if (v8_flags.trace_turbo) {
       SourcePositionTable table(graph());
       NodeOriginTable table2(graph());
       StdoutStream{} << AsJSON(*graph(), &table, &table2);
diff --git a/test/unittests/compiler/function-tester.cc b/test/unittests/compiler/function-tester.cc
index 1d8895d3d0..d6951da6f7 100644
--- a/test/unittests/compiler/function-tester.cc
+++ b/test/unittests/compiler/function-tester.cc
@@ -37,7 +37,7 @@ FunctionTester::FunctionTester(Isolate* isolate, const char* source,
                                uint32_t flags)
     : isolate(isolate),
       canonical(isolate),
-      function((FLAG_allow_natives_syntax = true, NewFunction(source))),
+      function((v8_flags.allow_natives_syntax = true, NewFunction(source))),
       flags_(flags) {
   Compile(function);
   const uint32_t supported_flags = OptimizedCompilationInfo::kInlining;
@@ -56,7 +56,7 @@ FunctionTester::FunctionTester(Isolate* isolate, Handle<Code> code,
                                int param_count)
     : isolate(isolate),
       canonical(isolate),
-      function((FLAG_allow_natives_syntax = true,
+      function((v8_flags.allow_natives_syntax = true,
                 NewFunction(BuildFunction(param_count).c_str()))),
       flags_(0) {
   CHECK(!code.is_null());
diff --git a/test/unittests/compiler/loop-peeling-unittest.cc b/test/unittests/compiler/loop-peeling-unittest.cc
index 4e3d238bcd..160064439c 100644
--- a/test/unittests/compiler/loop-peeling-unittest.cc
+++ b/test/unittests/compiler/loop-peeling-unittest.cc
@@ -59,7 +59,7 @@ class LoopPeelingTest : public GraphTest {
   MachineOperatorBuilder* machine() { return &machine_; }
 
   LoopTree* GetLoopTree() {
-    if (FLAG_trace_turbo_graph) {
+    if (v8_flags.trace_turbo_graph) {
       StdoutStream{} << AsRPO(*graph());
     }
     Zone zone(isolate()->allocator(), ZONE_NAME);
@@ -79,7 +79,7 @@ class LoopPeelingTest : public GraphTest {
   PeeledIteration* Peel(LoopPeeler peeler, LoopTree::Loop* loop) {
     EXPECT_TRUE(peeler.CanPeel(loop));
     PeeledIteration* peeled = peeler.Peel(loop);
-    if (FLAG_trace_turbo_graph) {
+    if (v8_flags.trace_turbo_graph) {
       StdoutStream{} << AsRPO(*graph());
     }
     return peeled;
diff --git a/test/unittests/compiler/mips64/instruction-selector-mips64-unittest.cc b/test/unittests/compiler/mips64/instruction-selector-mips64-unittest.cc
index c24ad5b48e..bb8698c91b 100644
--- a/test/unittests/compiler/mips64/instruction-selector-mips64-unittest.cc
+++ b/test/unittests/compiler/mips64/instruction-selector-mips64-unittest.cc
@@ -334,7 +334,7 @@ TEST_P(InstructionSelectorCmpTest, Parameter) {
   m.Return((m.*cmp.mi.constructor)(m.Parameter(0), m.Parameter(1)));
   Stream s = m.Build();
 
-  if (FLAG_debug_code &&
+  if (v8_flags.debug_code &&
       type.representation() == MachineRepresentation::kWord32) {
     ASSERT_EQ(6U, s.size());
 
@@ -1164,7 +1164,7 @@ TEST_P(InstructionSelectorElidedChangeUint32ToUint64Test, Parameter) {
       (m.*binop.constructor)(m.Parameter(0), m.Parameter(1))));
   Stream s = m.Build();
   // Make sure the `ChangeUint32ToUint64` node turned into a no-op.
-  if (FLAG_debug_code && binop.arch_opcode == kMips64Cmp) {
+  if (v8_flags.debug_code && binop.arch_opcode == kMips64Cmp) {
     ASSERT_EQ(6U, s.size());
     EXPECT_EQ(kMips64Cmp, s[0]->arch_opcode());
     EXPECT_EQ(kMips64Dshl, s[1]->arch_opcode());
diff --git a/test/unittests/compiler/regalloc/live-range-unittest.cc b/test/unittests/compiler/regalloc/live-range-unittest.cc
index b8fe329f69..d2785465ca 100644
--- a/test/unittests/compiler/regalloc/live-range-unittest.cc
+++ b/test/unittests/compiler/regalloc/live-range-unittest.cc
@@ -56,13 +56,13 @@ class TestRangeBuilder {
       LifetimePosition start = LifetimePosition::FromInt(pair.first);
       LifetimePosition end = LifetimePosition::FromInt(pair.second);
       CHECK(start < end);
-      range->AddUseInterval(start, end, zone_, FLAG_trace_turbo_alloc);
+      range->AddUseInterval(start, end, zone_, v8_flags.trace_turbo_alloc);
     }
     for (int pos : uses_) {
       UsePosition* use_position =
           zone_->New<UsePosition>(LifetimePosition::FromInt(pos), nullptr,
                                   nullptr, UsePositionHintType::kNone);
-      range->AddUsePosition(use_position, FLAG_trace_turbo_alloc);
+      range->AddUsePosition(use_position, v8_flags.trace_turbo_alloc);
     }
 
     pairs_.clear();
@@ -117,10 +117,11 @@ TEST_F(LiveRangeUnitTest, InvalidConstruction) {
   // Build a range manually, because the builder guards against empty cases.
   TopLevelLiveRange* range =
       zone()->New<TopLevelLiveRange>(1, MachineRepresentation::kTagged);
-  V8_ASSERT_DEBUG_DEATH(range->AddUseInterval(LifetimePosition::FromInt(0),
-                                              LifetimePosition::FromInt(0),
-                                              zone(), FLAG_trace_turbo_alloc),
-                        ".*");
+  V8_ASSERT_DEBUG_DEATH(
+      range->AddUseInterval(LifetimePosition::FromInt(0),
+                            LifetimePosition::FromInt(0), zone(),
+                            v8_flags.trace_turbo_alloc),
+      ".*");
 }
 
 TEST_F(LiveRangeUnitTest, SplitInvalidStart) {
diff --git a/test/unittests/compiler/regalloc/mid-tier-register-allocator-unittest.cc b/test/unittests/compiler/regalloc/mid-tier-register-allocator-unittest.cc
index 4218f66180..d879636e52 100644
--- a/test/unittests/compiler/regalloc/mid-tier-register-allocator-unittest.cc
+++ b/test/unittests/compiler/regalloc/mid-tier-register-allocator-unittest.cc
@@ -400,7 +400,7 @@ TEST_F(MidTierRegisterAllocatorTest, SpillPhiDueToRegisterPressure) {
 }
 
 TEST_F(MidTierRegisterAllocatorTest, MoveLotsOfConstants) {
-  FLAG_trace_turbo = true;
+  v8_flags.trace_turbo = true;
   StartBlock();
   VReg constants[Register::kNumRegisters];
   for (size_t i = 0; i < arraysize(constants); ++i) {
@@ -816,7 +816,7 @@ class MidTierRegAllocSlotConstraintTest
 }  // namespace
 
 TEST_P(MidTierRegAllocSlotConstraintTest, SlotConstraint) {
-  FLAG_trace_turbo = true;
+  v8_flags.trace_turbo = true;
   StartBlock();
   VReg p_0;
   switch (parameter_type()) {
diff --git a/test/unittests/compiler/regalloc/move-optimizer-unittest.cc b/test/unittests/compiler/regalloc/move-optimizer-unittest.cc
index 4a26bbc715..709ec6601b 100644
--- a/test/unittests/compiler/regalloc/move-optimizer-unittest.cc
+++ b/test/unittests/compiler/regalloc/move-optimizer-unittest.cc
@@ -52,14 +52,14 @@ class MoveOptimizerTest : public InstructionSequenceTest {
   // TODO(dcarney): add a verifier.
   void Optimize() {
     WireBlocks();
-    if (FLAG_trace_turbo) {
+    if (v8_flags.trace_turbo) {
       StdoutStream{}
           << "----- Instruction sequence before move optimization -----\n"
           << *sequence();
     }
     MoveOptimizer move_optimizer(zone(), sequence());
     move_optimizer.Run();
-    if (FLAG_trace_turbo) {
+    if (v8_flags.trace_turbo) {
       StdoutStream{}
           << "----- Instruction sequence after move optimization -----\n"
           << *sequence();
diff --git a/test/unittests/compiler/riscv32/instruction-selector-riscv32-unittest.cc b/test/unittests/compiler/riscv32/instruction-selector-riscv32-unittest.cc
index 0c7b6478fd..f9380ce8cc 100644
--- a/test/unittests/compiler/riscv32/instruction-selector-riscv32-unittest.cc
+++ b/test/unittests/compiler/riscv32/instruction-selector-riscv32-unittest.cc
@@ -264,7 +264,7 @@ TEST_P(InstructionSelectorCmpTest, Parameter) {
   m.Return((m.*cmp.mi.constructor)(m.Parameter(0), m.Parameter(1)));
   Stream s = m.Build();
 
-  if (FLAG_debug_code &&
+  if (v8_flags.debug_code &&
       type.representation() == MachineRepresentation::kWord32) {
     ASSERT_EQ(1U, s.size());
 
diff --git a/test/unittests/compiler/riscv64/instruction-selector-riscv64-unittest.cc b/test/unittests/compiler/riscv64/instruction-selector-riscv64-unittest.cc
index b56149b604..8458e4e7d5 100644
--- a/test/unittests/compiler/riscv64/instruction-selector-riscv64-unittest.cc
+++ b/test/unittests/compiler/riscv64/instruction-selector-riscv64-unittest.cc
@@ -313,7 +313,7 @@ TEST_P(InstructionSelectorCmpTest, Parameter) {
   m.Return((m.*cmp.mi.constructor)(m.Parameter(0), m.Parameter(1)));
   Stream s = m.Build();
 
-  if (FLAG_debug_code &&
+  if (v8_flags.debug_code &&
       type.representation() == MachineRepresentation::kWord32) {
 #ifndef V8_COMPRESS_POINTERS
     ASSERT_EQ(6U, s.size());
diff --git a/test/unittests/compiler/run-deopt-unittest.cc b/test/unittests/compiler/run-deopt-unittest.cc
index 8535d7feb1..2c75b0455f 100644
--- a/test/unittests/compiler/run-deopt-unittest.cc
+++ b/test/unittests/compiler/run-deopt-unittest.cc
@@ -31,7 +31,7 @@ class RunDeoptTest : public TestWithContext {
 };
 
 TEST_F(RunDeoptTest, DeoptSimple) {
-  FLAG_allow_natives_syntax = true;
+  v8_flags.allow_natives_syntax = true;
 
   FunctionTester T(i_isolate(),
                    "(function f(a) {"
@@ -47,7 +47,7 @@ TEST_F(RunDeoptTest, DeoptSimple) {
 }
 
 TEST_F(RunDeoptTest, DeoptSimpleInExpr) {
-  FLAG_allow_natives_syntax = true;
+  v8_flags.allow_natives_syntax = true;
 
   FunctionTester T(i_isolate(),
                    "(function f(a) {"
@@ -64,7 +64,7 @@ TEST_F(RunDeoptTest, DeoptSimpleInExpr) {
 }
 
 TEST_F(RunDeoptTest, DeoptExceptionHandlerCatch) {
-  FLAG_allow_natives_syntax = true;
+  v8_flags.allow_natives_syntax = true;
 
   FunctionTester T(i_isolate(),
                    "(function f() {"
@@ -82,7 +82,7 @@ TEST_F(RunDeoptTest, DeoptExceptionHandlerCatch) {
 }
 
 TEST_F(RunDeoptTest, DeoptExceptionHandlerFinally) {
-  FLAG_allow_natives_syntax = true;
+  v8_flags.allow_natives_syntax = true;
 
   FunctionTester T(i_isolate(),
                    "(function f() {"
@@ -100,7 +100,7 @@ TEST_F(RunDeoptTest, DeoptExceptionHandlerFinally) {
 }
 
 TEST_F(RunDeoptTest, DeoptTrivial) {
-  FLAG_allow_natives_syntax = true;
+  v8_flags.allow_natives_syntax = true;
 
   FunctionTester T(i_isolate(),
                    "(function foo() {"
diff --git a/test/unittests/compiler/run-jscalls-unittest.cc b/test/unittests/compiler/run-jscalls-unittest.cc
index 65135e8344..b651e8e640 100644
--- a/test/unittests/compiler/run-jscalls-unittest.cc
+++ b/test/unittests/compiler/run-jscalls-unittest.cc
@@ -137,7 +137,7 @@ TEST_F(RunJSCallsTest, ConstructorCall) {
 }
 
 TEST_F(RunJSCallsTest, RuntimeCall) {
-  FLAG_allow_natives_syntax = true;
+  v8_flags.allow_natives_syntax = true;
   FunctionTester T(i_isolate(), "(function(a) { return %IsJSReceiver(a); })");
 
   T.CheckCall(T.false_value(), T.NewNumber(23), T.undefined());
diff --git a/test/unittests/compiler/scheduler-unittest.cc b/test/unittests/compiler/scheduler-unittest.cc
index 2f2a6c3f4c..b7254e3938 100644
--- a/test/unittests/compiler/scheduler-unittest.cc
+++ b/test/unittests/compiler/scheduler-unittest.cc
@@ -37,7 +37,7 @@ class SchedulerTest : public TestWithIsolateAndZone {
         js_(zone()) {}
 
   Schedule* ComputeAndVerifySchedule(size_t expected) {
-    if (FLAG_trace_turbo) {
+    if (v8_flags.trace_turbo) {
       SourcePositionTable table(graph());
       NodeOriginTable table2(graph());
       StdoutStream{} << AsJSON(*graph(), &table, &table2);
@@ -46,7 +46,7 @@ class SchedulerTest : public TestWithIsolateAndZone {
     Schedule* schedule = Scheduler::ComputeSchedule(
         zone(), graph(), Scheduler::kSplitNodes, tick_counter(), nullptr);
 
-    if (FLAG_trace_turbo_scheduler) {
+    if (v8_flags.trace_turbo_scheduler) {
       StdoutStream{} << *schedule << std::endl;
     }
     ScheduleVerifier::Run(schedule);
diff --git a/test/unittests/compiler/sloppy-equality-unittest.cc b/test/unittests/compiler/sloppy-equality-unittest.cc
index a1b13ebb23..b019c58a87 100644
--- a/test/unittests/compiler/sloppy-equality-unittest.cc
+++ b/test/unittests/compiler/sloppy-equality-unittest.cc
@@ -61,8 +61,8 @@ class TestSloppyEqualityFactory {
 };
 
 TEST_F(SloppyEqualityTest, SloppyEqualityTest) {
-  FlagScope<bool> allow_natives_syntax(&i::FLAG_allow_natives_syntax, true);
-  FlagScope<bool> always_turbofan(&i::FLAG_always_turbofan, false);
+  FlagScope<bool> allow_natives_syntax(&i::v8_flags.allow_natives_syntax, true);
+  FlagScope<bool> always_turbofan(&i::v8_flags.always_turbofan, false);
   TestSloppyEqualityFactory f(zone());
   // TODO(nicohartmann@, v8:5660): Collect more precise feedback for some useful
   // cases.
-- 
2.35.1

