From 5f1ce563182104f761762c5cb52ff2a1edb6ab69 Mon Sep 17 00:00:00 2001
From: Lu Yahan <yahan@iscas.ac.cn>
Date: Fri, 4 Aug 2023 13:45:05 +0800
Subject: [PATCH] [riscv][sandbox] Reference Code objects (and their
 entrypoint) through the CPT

Also port: [riscv][tagged-ptr] Make Object methods static

Port commit fbb9df7218ae348b56104ff8fdd82b578e42d189
Port commit f20f342a3e275ae6442a53e34869f1c90f0db4a0

Bug: chromium:1395058
Bug: v8:12710

Change-Id: Ie4b7feb6157ed3846a743d4ef1e2adcfdc522f30
Reviewed-on: https://chromium-review.googlesource.com/c/v8/v8/+/4750761
Auto-Submit: Yahan Lu <yahan@iscas.ac.cn>
Reviewed-by: Ji Qiu <qiuji@iscas.ac.cn>
Commit-Queue: Yahan Lu <yahan@iscas.ac.cn>
Cr-Commit-Position: refs/heads/main@{#89388}
---
 src/builtins/riscv/builtins-riscv.cc          |  4 +-
 src/codegen/riscv/macro-assembler-riscv.cc    | 45 ++++++++++++--
 src/codegen/riscv/macro-assembler-riscv.h     |  5 ++
 .../backend/riscv/code-generator-riscv.cc     | 43 +++++++++-----
 .../backend/riscv/instruction-codes-riscv.h   |  6 +-
 .../riscv/instruction-scheduler-riscv.cc      |  6 +-
 .../riscv/instruction-selector-riscv.h        | 50 +++++++++++++---
 .../riscv/instruction-selector-riscv64.cc     | 58 +++++++++++--------
 test/cctest/test-helper-riscv32.cc            |  2 +-
 test/cctest/test-helper-riscv32.h             |  4 +-
 test/cctest/test-helper-riscv64.cc            |  2 +-
 test/cctest/test-helper-riscv64.h             |  4 +-
 test/cctest/test-macro-assembler-riscv32.cc   |  4 +-
 test/cctest/test-macro-assembler-riscv64.cc   |  4 +-
 test/cctest/test-simple-riscv32.cc            |  2 +-
 test/cctest/test-simple-riscv64.cc            |  4 +-
 16 files changed, 166 insertions(+), 77 deletions(-)

diff --git a/src/builtins/riscv/builtins-riscv.cc b/src/builtins/riscv/builtins-riscv.cc
index f7fc0799ebc..7ca504b0107 100644
--- a/src/builtins/riscv/builtins-riscv.cc
+++ b/src/builtins/riscv/builtins-riscv.cc
@@ -452,9 +452,7 @@ void Builtins::Generate_ResumeGeneratorTrampoline(MacroAssembler* masm) {
     // undefined because generator functions are non-constructable.
     __ Move(a3, a1);
     __ Move(a1, a4);
-    static_assert(kJavaScriptCallCodeStartRegister == a2, "ABI mismatch");
-    __ LoadTaggedField(a2, FieldMemOperand(a1, JSFunction::kCodeOffset));
-    __ JumpCodeObject(a2);
+    __ JumpJSFunction(a1);
   }
 
   __ bind(&prepare_step_in_if_stepping);
diff --git a/src/codegen/riscv/macro-assembler-riscv.cc b/src/codegen/riscv/macro-assembler-riscv.cc
index 276caa6facf..82a57065bb5 100644
--- a/src/codegen/riscv/macro-assembler-riscv.cc
+++ b/src/codegen/riscv/macro-assembler-riscv.cc
@@ -5068,14 +5068,12 @@ void MacroAssembler::InvokeFunctionCode(Register function, Register new_target,
   // We call indirectly through the code field in the function to
   // allow recompilation to take effect without changing any of the
   // call sites.
-  Register code = kJavaScriptCallCodeStartRegister;
-  LoadTaggedField(code, FieldMemOperand(function, JSFunction::kCodeOffset));
   switch (type) {
     case InvokeType::kCall:
-      CallCodeObject(code);
+      CallJSFunction(function);
       break;
     case InvokeType::kJump:
-      JumpCodeObject(code);
+      JumpJSFunction(function);
       break;
   }
 
@@ -6452,6 +6450,45 @@ void MacroAssembler::JumpCodeObject(Register code, JumpMode jump_mode) {
   Jump(code);
 }
 
+void MacroAssembler::CallJSFunction(Register function_object) {
+  Register code = kJavaScriptCallCodeStartRegister;
+#ifdef V8_CODE_POINTER_SANDBOXING
+  // When the sandbox is enabled, we can directly fetch the entrypoint pointer
+  // from the code pointer table instead of going through the Code object. In
+  // this way, we avoid one memory load on this code path.
+  LoadCodeEntrypointField(
+      code, FieldMemOperand(function_object, JSFunction::kCodeOffset));
+  Call(code);
+#else
+  LoadTaggedField(code,
+                  FieldMemOperand(function_object, JSFunction::kCodeOffset));
+  CallCodeObject(code);
+#endif
+}
+
+void MacroAssembler::JumpJSFunction(Register function_object,
+                                    JumpMode jump_mode) {
+  Register code = kJavaScriptCallCodeStartRegister;
+#ifdef V8_CODE_POINTER_SANDBOXING
+  // When the sandbox is enabled, we can directly fetch the entrypoint pointer
+  // from the code pointer table instead of going through the Code object. In
+  // this way, we avoid one memory load on this code path.
+  LoadCodeEntrypointField(
+      code, FieldMemOperand(function_object, JSFunction::kCodeOffset));
+  DCHECK_EQ(jump_mode, JumpMode::kJump);
+  // We jump through x17 here because for Branch Identification (BTI) we use
+  // "Call" (`bti c`) rather than "Jump" (`bti j`) landing pads for tail-called
+  // code. See TailCallBuiltin for more information.
+  DCHECK_NE(code, t6);
+  Mov(t6, code);
+  Jump(t6);
+#else
+  LoadTaggedField(code,
+                  FieldMemOperand(function_object, JSFunction::kCodeOffset));
+  JumpCodeObject(code, jump_mode);
+#endif
+}
+
 #if V8_TARGET_ARCH_RISCV64
 void MacroAssembler::LoadTaggedField(const Register& destination,
                                      const MemOperand& field_operand) {
diff --git a/src/codegen/riscv/macro-assembler-riscv.h b/src/codegen/riscv/macro-assembler-riscv.h
index b8f73d9e5c6..7c1d95b4592 100644
--- a/src/codegen/riscv/macro-assembler-riscv.h
+++ b/src/codegen/riscv/macro-assembler-riscv.h
@@ -310,6 +310,11 @@ class V8_EXPORT_PRIVATE MacroAssembler : public MacroAssemblerBase {
   void JumpCodeObject(Register code_object,
                       JumpMode jump_mode = JumpMode::kJump);
 
+  // Convenience functions to call/jmp to the code of a JSFunction object.
+  void CallJSFunction(Register function_object);
+  void JumpJSFunction(Register function_object,
+                      JumpMode jump_mode = JumpMode::kJump);
+
   // Load the builtin given by the Smi in |builtin| into the same
   // register.
   // Load the builtin given by the Smi in |builtin_index| into |target|.
diff --git a/src/compiler/backend/riscv/code-generator-riscv.cc b/src/compiler/backend/riscv/code-generator-riscv.cc
index da2323e4b01..63e2d1de905 100644
--- a/src/compiler/backend/riscv/code-generator-riscv.cc
+++ b/src/compiler/backend/riscv/code-generator-riscv.cc
@@ -735,9 +735,7 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
         __ Assert(eq, AbortReason::kWrongFunctionContext, cp,
                   Operand(kScratchReg));
       }
-      static_assert(kJavaScriptCallCodeStartRegister == a2, "ABI mismatch");
-      __ LoadTaggedField(a2, FieldMemOperand(func, JSFunction::kCodeOffset));
-      __ CallCodeObject(a2);
+      __ CallJSFunction(func);
       RecordCallPosition(instr);
       frame_access_state()->ClearSPDelta();
       break;
@@ -910,6 +908,8 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
       __ bind(ool->exit());
       break;
     }
+    case kArchStoreIndirectWithWriteBarrier:
+      UNREACHABLE();
     case kArchStackSlot: {
       FrameOffset offset =
           frame_access_state()->GetFrameOffset(i.InputInt32(0));
@@ -1539,12 +1539,34 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
     case kRiscvFloat64SilenceNaN:
       __ FPUCanonicalizeNaN(i.OutputDoubleRegister(), i.InputDoubleRegister(0));
       break;
-    case kRiscvCvtSD:
-      __ fcvt_s_d(i.OutputSingleRegister(), i.InputDoubleRegister(0));
+    case kRiscvCvtSD: {
+      Label done;
+      __ feq_d(kScratchReg, i.InputDoubleRegister(0), i.InputDoubleRegister(0));
+      __ fmv_x_d(kScratchReg2, i.InputDoubleRegister(0));
+      __ fcvt_s_d(i.OutputDoubleRegister(), i.InputDoubleRegister(0));
+      __ Branch(&done, eq, kScratchReg, Operand(1));
+      __ And(kScratchReg2, kScratchReg2, Operand(0x8000000000000000));
+      __ srai(kScratchReg2, kScratchReg2, 32);
+      __ fmv_d_x(kScratchDoubleReg, kScratchReg2);
+      __ fsgnj_s(i.OutputDoubleRegister(), i.OutputDoubleRegister(),
+                 kScratchDoubleReg);
+      __ bind(&done);
       break;
-    case kRiscvCvtDS:
+    }
+    case kRiscvCvtDS: {
+      Label done;
+      __ feq_s(kScratchReg, i.InputDoubleRegister(0), i.InputDoubleRegister(0));
+      __ fmv_x_d(kScratchReg2, i.InputDoubleRegister(0));
       __ fcvt_d_s(i.OutputDoubleRegister(), i.InputSingleRegister(0));
+      __ Branch(&done, eq, kScratchReg, Operand(1));
+      __ And(kScratchReg2, kScratchReg2, Operand(0x80000000));
+      __ slli(kScratchReg2, kScratchReg2, 32);
+      __ fmv_d_x(kScratchDoubleReg, kScratchReg2);
+      __ fsgnj_d(i.OutputDoubleRegister(), i.OutputDoubleRegister(),
+                 kScratchDoubleReg);
+      __ bind(&done);
       break;
+    }
     case kRiscvCvtDW: {
       __ fcvt_d_w(i.OutputDoubleRegister(), i.InputRegister(0));
       break;
@@ -2419,12 +2441,6 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
                  i.InputSimd128Register(1));
       break;
     }
-    case kRiscvI64x2Add: {
-      (__ VU).set(kScratchReg, VSew::E64, Vlmul::m1);
-      __ vadd_vv(i.OutputSimd128Register(), i.InputSimd128Register(0),
-                 i.InputSimd128Register(1));
-      break;
-    }
     case kRiscvVrgather: {
       Simd128Register index = i.InputSimd128Register(0);
       if (!(instr->InputAt(1)->IsImmediate())) {
@@ -3759,7 +3775,6 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
       ASSEMBLE_RVV_BINOP_INTEGER(MinU, vminu_vv)
       ASSEMBLE_RVV_BINOP_INTEGER(MinS, vmin_vv)
       ASSEMBLE_RVV_UNOP_INTEGER_VR(Splat, vmv_vx)
-      ASSEMBLE_RVV_BINOP_INTEGER(Add, vadd_vv)
       ASSEMBLE_RVV_BINOP_INTEGER(Sub, vsub_vv)
 #if V8_TARGET_ARCH_RISCV64
     case kRiscvI64x2Splat: {
@@ -3826,7 +3841,7 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
       }
       break;
     }
-    case kRiscvVaddVv: {
+    case kRiscvVadd: {
       __ VU.set(kScratchReg, i.InputInt8(2), i.InputInt8(3));
       __ vadd_vv(i.OutputSimd128Register(), i.InputSimd128Register(0),
                  i.InputSimd128Register(1));
diff --git a/src/compiler/backend/riscv/instruction-codes-riscv.h b/src/compiler/backend/riscv/instruction-codes-riscv.h
index ab3d8ed7770..64c74d00753 100644
--- a/src/compiler/backend/riscv/instruction-codes-riscv.h
+++ b/src/compiler/backend/riscv/instruction-codes-riscv.h
@@ -217,7 +217,6 @@ namespace compiler {
   V(RiscvI32x4Splat)                      \
   V(RiscvI32x4ExtractLane)                \
   V(RiscvI32x4ReplaceLane)                \
-  V(RiscvI32x4Add)                        \
   V(RiscvI32x4Sub)                        \
   V(RiscvF64x2Abs)                        \
   V(RiscvF64x2Neg)                        \
@@ -272,7 +271,6 @@ namespace compiler {
   V(RiscvI64x2ExtractLane)                \
   V(RiscvI64x2ReplaceLane)                \
   V(RiscvI64x2ReplaceLaneI32Pair)         \
-  V(RiscvI64x2Add)                        \
   V(RiscvI64x2Sub)                        \
   V(RiscvI64x2Mul)                        \
   V(RiscvI64x2Abs)                        \
@@ -324,7 +322,6 @@ namespace compiler {
   V(RiscvI16x8Shl)                        \
   V(RiscvI16x8ShrS)                       \
   V(RiscvI16x8ShrU)                       \
-  V(RiscvI16x8Add)                        \
   V(RiscvI16x8AddSatS)                    \
   V(RiscvI16x8Sub)                        \
   V(RiscvI16x8SubSatS)                    \
@@ -352,7 +349,6 @@ namespace compiler {
   V(RiscvI8x16Neg)                        \
   V(RiscvI8x16Shl)                        \
   V(RiscvI8x16ShrS)                       \
-  V(RiscvI8x16Add)                        \
   V(RiscvI8x16AddSatS)                    \
   V(RiscvI8x16Sub)                        \
   V(RiscvI8x16SubSatS)                    \
@@ -435,7 +431,7 @@ namespace compiler {
   V(RiscvVwmulu)                          \
   V(RiscvVmvSx)                           \
   V(RiscvVcompress)                       \
-  V(RiscvVaddVv)                          \
+  V(RiscvVadd)                            \
   V(RiscvVwadd)                           \
   V(RiscvVwaddu)                          \
   V(RiscvVrgather)                        \
diff --git a/src/compiler/backend/riscv/instruction-scheduler-riscv.cc b/src/compiler/backend/riscv/instruction-scheduler-riscv.cc
index 7f18032ab72..51b439d3b87 100644
--- a/src/compiler/backend/riscv/instruction-scheduler-riscv.cc
+++ b/src/compiler/backend/riscv/instruction-scheduler-riscv.cc
@@ -127,7 +127,6 @@ int InstructionScheduler::GetTargetInstructionFlags(
     case kRiscvI64x2ExtractLane:
     case kRiscvI64x2ReplaceLane:
     case kRiscvI64x2ReplaceLaneI32Pair:
-    case kRiscvI64x2Add:
     case kRiscvI64x2Sub:
     case kRiscvI64x2Mul:
     case kRiscvI64x2Neg:
@@ -191,7 +190,6 @@ int InstructionScheduler::GetTargetInstructionFlags(
     case kRiscvI64x2SConvertI32x4High:
     case kRiscvI64x2UConvertI32x4Low:
     case kRiscvI64x2UConvertI32x4High:
-    case kRiscvI16x8Add:
     case kRiscvI16x8AddSatS:
     case kRiscvI16x8AddSatU:
     case kRiscvI16x8Eq:
@@ -230,7 +228,6 @@ int InstructionScheduler::GetTargetInstructionFlags(
     case kRiscvI16x8Q15MulRSatS:
     case kRiscvI16x8Abs:
     case kRiscvI16x8BitMask:
-    case kRiscvI32x4Add:
     case kRiscvI32x4Eq:
     case kRiscvI32x4ExtractLane:
     case kRiscvI32x4GeS:
@@ -258,7 +255,6 @@ int InstructionScheduler::GetTargetInstructionFlags(
     case kRiscvI32x4UConvertI16x8Low:
     case kRiscvI32x4Abs:
     case kRiscvI32x4BitMask:
-    case kRiscvI8x16Add:
     case kRiscvI8x16AddSatS:
     case kRiscvI8x16AddSatU:
     case kRiscvI8x16Eq:
@@ -354,7 +350,7 @@ int InstructionScheduler::GetTargetInstructionFlags(
     case kRiscvVwmulu:
     case kRiscvVmvSx:
     case kRiscvVcompress:
-    case kRiscvVaddVv:
+    case kRiscvVadd:
     case kRiscvVwadd:
     case kRiscvVwaddu:
     case kRiscvVrgather:
diff --git a/src/compiler/backend/riscv/instruction-selector-riscv.h b/src/compiler/backend/riscv/instruction-selector-riscv.h
index 636a90ae8cf..91e79b2e1d8 100644
--- a/src/compiler/backend/riscv/instruction-selector-riscv.h
+++ b/src/compiler/backend/riscv/instruction-selector-riscv.h
@@ -1137,6 +1137,8 @@ void InstructionSelectorT<Adapter>::VisitI16x8ExtAddPairwiseI8x16U(Node* node) {
   V(I8x16ShrS)                \
   V(I8x16ShrU)
 
+#define SIMD_INT_OP_LIST(V) V(Add, kRiscvVadd)
+
 #define SIMD_BINOP_LIST(V)                              \
   V(F64x2Add, kRiscvF64x2Add)                           \
   V(F64x2Sub, kRiscvF64x2Sub)                           \
@@ -1152,7 +1154,6 @@ void InstructionSelectorT<Adapter>::VisitI16x8ExtAddPairwiseI8x16U(Node* node) {
   V(I64x2Ne, kRiscvI64x2Ne)                             \
   V(I64x2GtS, kRiscvI64x2GtS)                           \
   V(I64x2GeS, kRiscvI64x2GeS)                           \
-  V(I64x2Add, kRiscvI64x2Add)                           \
   V(I64x2Sub, kRiscvI64x2Sub)                           \
   V(I64x2Mul, kRiscvI64x2Mul)                           \
   V(F32x4Add, kRiscvF32x4Add)                           \
@@ -1169,7 +1170,6 @@ void InstructionSelectorT<Adapter>::VisitI16x8ExtAddPairwiseI8x16U(Node* node) {
   V(F32x4RelaxedMax, kRiscvF32x4Max)                    \
   V(F64x2RelaxedMin, kRiscvF64x2Min)                    \
   V(F64x2RelaxedMax, kRiscvF64x2Max)                    \
-  V(I32x4Add, kRiscvI32x4Add)                           \
   V(I32x4Sub, kRiscvI32x4Sub)                           \
   V(I32x4Mul, kRiscvI32x4Mul)                           \
   V(I32x4MaxS, kRiscvI32x4MaxS)                         \
@@ -1182,7 +1182,6 @@ void InstructionSelectorT<Adapter>::VisitI16x8ExtAddPairwiseI8x16U(Node* node) {
   V(I32x4GeS, kRiscvI32x4GeS)                           \
   V(I32x4GtU, kRiscvI32x4GtU)                           \
   V(I32x4GeU, kRiscvI32x4GeU)                           \
-  V(I16x8Add, kRiscvI16x8Add)                           \
   V(I16x8AddSatS, kRiscvI16x8AddSatS)                   \
   V(I16x8AddSatU, kRiscvI16x8AddSatU)                   \
   V(I16x8Sub, kRiscvI16x8Sub)                           \
@@ -1204,7 +1203,6 @@ void InstructionSelectorT<Adapter>::VisitI16x8ExtAddPairwiseI8x16U(Node* node) {
   V(I16x8RelaxedQ15MulRS, kRiscvI16x8Q15MulRSatS)       \
   V(I16x8SConvertI32x4, kRiscvI16x8SConvertI32x4)       \
   V(I16x8UConvertI32x4, kRiscvI16x8UConvertI32x4)       \
-  V(I8x16Add, kRiscvI8x16Add)                           \
   V(I8x16AddSatS, kRiscvI8x16AddSatS)                   \
   V(I8x16AddSatU, kRiscvI8x16AddSatU)                   \
   V(I8x16Sub, kRiscvI8x16Sub)                           \
@@ -1313,6 +1311,42 @@ SIMD_SHIFT_OP_LIST(SIMD_VISIT_SHIFT_OP)
 SIMD_BINOP_LIST(SIMD_VISIT_BINOP)
 #undef SIMD_VISIT_BINOP
 
+#define SIMD_VISIT_INT(Name, instruction)                            \
+  template <typename Adapter>                                        \
+  void InstructionSelectorT<Adapter>::VisitI32x4##Name(Node* node) { \
+    RiscvOperandGeneratorT<Adapter> g(this);                         \
+    this->Emit(instruction, g.DefineAsRegister(node),                \
+               g.UseRegister(node->InputAt(0)),                      \
+               g.UseRegister(node->InputAt(1)), g.UseImmediate(E32), \
+               g.UseImmediate(m1));                                  \
+  }                                                                  \
+  template <typename Adapter>                                        \
+  void InstructionSelectorT<Adapter>::VisitI64x2##Name(Node* node) { \
+    RiscvOperandGeneratorT<Adapter> g(this);                         \
+    this->Emit(instruction, g.DefineAsRegister(node),                \
+               g.UseRegister(node->InputAt(0)),                      \
+               g.UseRegister(node->InputAt(1)), g.UseImmediate(E64), \
+               g.UseImmediate(m1));                                  \
+  }                                                                  \
+  template <typename Adapter>                                        \
+  void InstructionSelectorT<Adapter>::VisitI16x8##Name(Node* node) { \
+    RiscvOperandGeneratorT<Adapter> g(this);                         \
+    this->Emit(instruction, g.DefineAsRegister(node),                \
+               g.UseRegister(node->InputAt(0)),                      \
+               g.UseRegister(node->InputAt(1)), g.UseImmediate(E16), \
+               g.UseImmediate(m1));                                  \
+  }                                                                  \
+  template <typename Adapter>                                        \
+  void InstructionSelectorT<Adapter>::VisitI8x16##Name(Node* node) { \
+    RiscvOperandGeneratorT<Adapter> g(this);                         \
+    this->Emit(instruction, g.DefineAsRegister(node),                \
+               g.UseRegister(node->InputAt(0)),                      \
+               g.UseRegister(node->InputAt(1)), g.UseImmediate(E8),  \
+               g.UseImmediate(m1));                                  \
+  }
+SIMD_INT_OP_LIST(SIMD_VISIT_INT)
+#undef SIMD_VISIT_INT
+
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::VisitS128Select(Node* node) {
   VisitRRRR(this, kRiscvS128Select, node);
@@ -1356,7 +1390,7 @@ void InstructionSelectorT<Adapter>::VisitI32x4DotI16x8S(Node* node) {
              g.UseImmediate(E32), g.UseImmediate(m2));
   this->Emit(kRiscvVcompress, temp1, temp, g.UseImmediate(SECOND_INDEX),
              g.UseImmediate(E32), g.UseImmediate(m2));
-  this->Emit(kRiscvVaddVv, dst, temp1, temp2, g.UseImmediate(E32),
+  this->Emit(kRiscvVadd, dst, temp1, temp2, g.UseImmediate(E32),
              g.UseImmediate(m1));
 }
 
@@ -1376,7 +1410,7 @@ void InstructionSelectorT<Adapter>::VisitI16x8DotI8x16I7x16S(Node* node) {
              g.UseImmediate(E16), g.UseImmediate(m2));
   this->Emit(kRiscvVcompress, temp1, temp, g.UseImmediate(SECOND_INDEX),
              g.UseImmediate(E16), g.UseImmediate(m2));
-  this->Emit(kRiscvVaddVv, dst, temp1, temp2, g.UseImmediate(E16),
+  this->Emit(kRiscvVadd, dst, temp1, temp2, g.UseImmediate(E16),
              g.UseImmediate(m1));
 }
 
@@ -1419,9 +1453,9 @@ void InstructionSelectorT<Adapter>::VisitI32x4DotI8x16I7x16AddS(Node* node) {
 
   InstructionOperand mul_result = g.TempFpRegister(v16);
   InstructionOperand dst = g.DefineAsRegister(node);
-  this->Emit(kRiscvVaddVv, mul_result, temp2, temp, g.UseImmediate(E32),
+  this->Emit(kRiscvVadd, mul_result, temp2, temp, g.UseImmediate(E32),
              g.UseImmediate(m1));
-  this->Emit(kRiscvVaddVv, dst, mul_result, g.UseRegister(node->InputAt(2)),
+  this->Emit(kRiscvVadd, dst, mul_result, g.UseRegister(node->InputAt(2)),
              g.UseImmediate(E32), g.UseImmediate(m1));
 }
 
diff --git a/src/compiler/backend/riscv/instruction-selector-riscv64.cc b/src/compiler/backend/riscv/instruction-selector-riscv64.cc
index bbe886a2bdb..2dd1597bb7d 100644
--- a/src/compiler/backend/riscv/instruction-selector-riscv64.cc
+++ b/src/compiler/backend/riscv/instruction-selector-riscv64.cc
@@ -341,6 +341,7 @@ void InstructionSelectorT<Adapter>::VisitLoad(node_t node) {
         break;
       case MachineRepresentation::kSimd256:  // Fall through.
       case MachineRepresentation::kMapWord:  // Fall through.
+      case MachineRepresentation::kIndirectPointer:  // Fall through.
       case MachineRepresentation::kNone:
         UNREACHABLE();
     }
@@ -431,6 +432,7 @@ void InstructionSelectorT<TurbofanAdapter>::VisitStore(Node* node) {
         break;
       case MachineRepresentation::kSimd256:  // Fall through.
       case MachineRepresentation::kMapWord:  // Fall through.
+      case MachineRepresentation::kIndirectPointer:  // Fall through.
       case MachineRepresentation::kNone:
         UNREACHABLE();
     }
@@ -1460,37 +1462,41 @@ void InstructionSelectorT<Adapter>::VisitChangeUint32ToUint64(node_t node) {
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitTruncateInt64ToInt32(Node* node) {
-  RiscvOperandGeneratorT<Adapter> g(this);
-  Node* value = node->InputAt(0);
-  if (CanCover(node, value)) {
-    switch (value->opcode()) {
-      case IrOpcode::kWord64Sar: {
-        if (CanCover(value, value->InputAt(0)) &&
-            TryEmitExtendingLoad(this, value, node)) {
-          return;
-        } else {
-          Int64BinopMatcher m(value);
-          if (m.right().IsInRange(32, 63)) {
-            // After smi untagging no need for truncate. Combine sequence.
-            Emit(kRiscvSar64, g.DefineSameAsFirst(node),
-                 g.UseRegister(m.left().node()),
-                 g.UseImmediate(m.right().node()));
+void InstructionSelectorT<Adapter>::VisitTruncateInt64ToInt32(node_t node) {
+  if constexpr (Adapter::IsTurboshaft) {
+    UNIMPLEMENTED();
+  } else {
+    RiscvOperandGeneratorT<Adapter> g(this);
+    Node* value = node->InputAt(0);
+    if (CanCover(node, value)) {
+      switch (value->opcode()) {
+        case IrOpcode::kWord64Sar: {
+          if (CanCover(value, value->InputAt(0)) &&
+              TryEmitExtendingLoad(this, value, node)) {
             return;
+          } else {
+            Int64BinopMatcher m(value);
+            if (m.right().IsInRange(32, 63)) {
+              // After smi untagging no need for truncate. Combine sequence.
+              Emit(kRiscvSar64, g.DefineSameAsFirst(node),
+                   g.UseRegister(m.left().node()),
+                   g.UseImmediate(m.right().node()));
+              return;
+            }
           }
+          break;
         }
-        break;
+        default:
+          break;
       }
-      default:
-        break;
     }
+    // Semantics of this machine IR is not clear. For example, x86 zero-extend
+    // the truncated value; arm treats it as nop thus the upper 32-bit as
+    // undefined; Riscv emits ext instruction which zero-extend the 32-bit
+    // value; for riscv, we do sign-extension of the truncated value
+    Emit(kRiscvSignExtendWord, g.DefineAsRegister(node),
+         g.UseRegister(node->InputAt(0)), g.TempImmediate(0));
   }
-  // Semantics of this machine IR is not clear. For example, x86 zero-extend the
-  // truncated value; arm treats it as nop thus the upper 32-bit as undefined;
-  // Riscv emits ext instruction which zero-extend the 32-bit value; for riscv,
-  // we do sign-extension of the truncated value
-  Emit(kRiscvSignExtendWord, g.DefineAsRegister(node),
-       g.UseRegister(node->InputAt(0)), g.TempImmediate(0));
 }
 
 template <typename Adapter>
@@ -1761,6 +1767,7 @@ void InstructionSelectorT<Adapter>::VisitUnalignedLoad(Node* node) {
     case MachineRepresentation::kCompressed:         // Fall through.
     case MachineRepresentation::kSandboxedPointer:   // Fall through.
     case MachineRepresentation::kMapWord:            // Fall through.
+    case MachineRepresentation::kIndirectPointer:    // Fall through.
     case MachineRepresentation::kNone:
       UNREACHABLE();
   }
@@ -1818,6 +1825,7 @@ void InstructionSelectorT<Adapter>::VisitUnalignedStore(Node* node) {
     case MachineRepresentation::kCompressed:         // Fall through.
     case MachineRepresentation::kSandboxedPointer:   // Fall through.
     case MachineRepresentation::kMapWord:            // Fall through.
+    case MachineRepresentation::kIndirectPointer:    // Fall through.
     case MachineRepresentation::kNone:
       UNREACHABLE();
   }
diff --git a/test/cctest/test-helper-riscv32.cc b/test/cctest/test-helper-riscv32.cc
index 79468c73734..07ca9983a60 100644
--- a/test/cctest/test-helper-riscv32.cc
+++ b/test/cctest/test-helper-riscv32.cc
@@ -39,7 +39,7 @@ Handle<Code> AssembleCodeImpl(Isolate* isolate, Func assemble) {
   Handle<Code> code =
       Factory::CodeBuilder(isolate, desc, CodeKind::FOR_TESTING).Build();
   if (v8_flags.print_code) {
-    code->Print();
+    Print(*code);
   }
   return code;
 }
diff --git a/test/cctest/test-helper-riscv32.h b/test/cctest/test-helper-riscv32.h
index 91fe1835dcd..8e2028917cc 100644
--- a/test/cctest/test-helper-riscv32.h
+++ b/test/cctest/test-helper-riscv32.h
@@ -249,7 +249,7 @@ void GenAndRunTestForLRSC(T value, Func test_generator) {
   Handle<Code> code =
       Factory::CodeBuilder(isolate, desc, CodeKind::FOR_TESTING).Build();
 #if defined(DEBUG)
-  code->Print();
+  Print(*code);
 #endif
   using INT_T =
       typename std::conditional<sizeof(T) == 4, int32_t, int64_t>::type;
@@ -312,7 +312,7 @@ OUTPUT_T GenAndRunTestForAMO(INPUT_T input0, INPUT_T input1,
   Handle<Code> code =
       Factory::CodeBuilder(isolate, desc, CodeKind::FOR_TESTING).Build();
 #if defined(DEBUG)
-  code->Print();
+  Print(*code);
 #endif
   OUTPUT_T tmp = 0;
   auto f = GeneratedCode<OUTPUT_T(void* base, INPUT_T, INPUT_T)>::FromCode(
diff --git a/test/cctest/test-helper-riscv64.cc b/test/cctest/test-helper-riscv64.cc
index e4cf6bd29b8..a7841da9c82 100644
--- a/test/cctest/test-helper-riscv64.cc
+++ b/test/cctest/test-helper-riscv64.cc
@@ -38,7 +38,7 @@ Handle<Code> AssembleCodeImpl(Isolate* isolate, Func assemble) {
   Handle<Code> code =
       Factory::CodeBuilder(isolate, desc, CodeKind::FOR_TESTING).Build();
   if (v8_flags.print_code) {
-    code->Print();
+    Print(*code);
   }
   return code;
 }
diff --git a/test/cctest/test-helper-riscv64.h b/test/cctest/test-helper-riscv64.h
index a6914e20c28..9c4fccf287d 100644
--- a/test/cctest/test-helper-riscv64.h
+++ b/test/cctest/test-helper-riscv64.h
@@ -254,7 +254,7 @@ void GenAndRunTestForLRSC(T value, Func test_generator) {
   Handle<Code> code =
       Factory::CodeBuilder(isolate, desc, CodeKind::FOR_TESTING).Build();
 #if defined(DEBUG)
-  code->Print();
+  Print(*code);
 #endif
   using INT_T =
       typename std::conditional<sizeof(T) == 4, int32_t, int64_t>::type;
@@ -318,7 +318,7 @@ OUTPUT_T GenAndRunTestForAMO(INPUT_T input0, INPUT_T input1,
   Handle<Code> code =
       Factory::CodeBuilder(isolate, desc, CodeKind::FOR_TESTING).Build();
 #if defined(DEBUG)
-  code->Print();
+  Print(*code);
 #endif
   OUTPUT_T tmp = 0;
   auto f = GeneratedCode<OUTPUT_T(void* base, INPUT_T, INPUT_T)>::FromCode(
diff --git a/test/cctest/test-macro-assembler-riscv32.cc b/test/cctest/test-macro-assembler-riscv32.cc
index 533e1ac0434..68c1e72d3aa 100644
--- a/test/cctest/test-macro-assembler-riscv32.cc
+++ b/test/cctest/test-macro-assembler-riscv32.cc
@@ -230,7 +230,7 @@ TEST(jump_tables4) {
   Handle<Code> code =
       Factory::CodeBuilder(isolate, desc, CodeKind::FOR_TESTING).Build();
 #ifdef OBJECT_PRINT
-  code->Print(std::cout);
+  Print(*code, std::cout);
 #endif
   auto f = GeneratedCode<F1>::FromCode(isolate, *code);
   for (int i = 0; i < kNumCases; ++i) {
@@ -317,7 +317,7 @@ TEST(jump_tables6) {
   Handle<Code> code =
       Factory::CodeBuilder(isolate, desc, CodeKind::FOR_TESTING).Build();
 #ifdef OBJECT_PRINT
-  code->Print(std::cout);
+  Print(*code, std::cout);
 #endif
   auto f = GeneratedCode<F1>::FromCode(isolate, *code);
   for (int i = 0; i < kSwitchTableCases; ++i) {
diff --git a/test/cctest/test-macro-assembler-riscv64.cc b/test/cctest/test-macro-assembler-riscv64.cc
index d42c8ee5295..09cfa32ff96 100644
--- a/test/cctest/test-macro-assembler-riscv64.cc
+++ b/test/cctest/test-macro-assembler-riscv64.cc
@@ -228,7 +228,7 @@ TEST(jump_tables4) {
   Handle<Code> code =
       Factory::CodeBuilder(isolate, desc, CodeKind::FOR_TESTING).Build();
 #ifdef OBJECT_PRINT
-  code->Print(std::cout);
+  Print(*code, std::cout);
 #endif
   auto f = GeneratedCode<F1>::FromCode(isolate, *code);
   for (int i = 0; i < kNumCases; ++i) {
@@ -315,7 +315,7 @@ TEST(jump_tables6) {
   Handle<Code> code =
       Factory::CodeBuilder(isolate, desc, CodeKind::FOR_TESTING).Build();
 #ifdef OBJECT_PRINT
-  code->Print(std::cout);
+  Print(*code, std::cout);
 #endif
   auto f = GeneratedCode<F1>::FromCode(isolate, *code);
   for (int i = 0; i < kSwitchTableCases; ++i) {
diff --git a/test/cctest/test-simple-riscv32.cc b/test/cctest/test-simple-riscv32.cc
index be8919f0c73..dccd7ff857a 100644
--- a/test/cctest/test-simple-riscv32.cc
+++ b/test/cctest/test-simple-riscv32.cc
@@ -116,7 +116,7 @@ TEST(RISCV_SIMPLE2) {
   Handle<Code> code =
       Factory::CodeBuilder(isolate, desc, CodeKind::FOR_TESTING).Build();
 #ifdef DEBUG
-  code->Print();
+  Print(*code);
 #endif
   auto f = GeneratedCode<F1>::FromCode(isolate, *code);
   int32_t res = reinterpret_cast<int32_t>(f.Call(100, 0, 0, 0, 0));
diff --git a/test/cctest/test-simple-riscv64.cc b/test/cctest/test-simple-riscv64.cc
index 3cb1dddcec4..5bacfb5be8a 100644
--- a/test/cctest/test-simple-riscv64.cc
+++ b/test/cctest/test-simple-riscv64.cc
@@ -115,8 +115,8 @@ TEST(RISCV_SIMPLE2) {
   assm.GetCode(isolate, &desc);
   Handle<Code> code =
       Factory::CodeBuilder(isolate, desc, CodeKind::FOR_TESTING).Build();
-#ifdef DEBUG
-  code->Print();
+#ifdef OBJECT_PRINT
+  Print(*code);
 #endif
   auto f = GeneratedCode<F1>::FromCode(isolate, *code);
   int64_t res = reinterpret_cast<int64_t>(f.Call(100, 0, 0, 0, 0));
-- 
2.35.1

