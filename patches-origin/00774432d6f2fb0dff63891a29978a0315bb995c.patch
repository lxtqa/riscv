From 00774432d6f2fb0dff63891a29978a0315bb995c Mon Sep 17 00:00:00 2001
From: Stephen Roettger <sroettger@google.com>
Date: Wed, 22 Mar 2023 10:09:53 +0100
Subject: [PATCH] [heap] Pad code pages once and remove padding from IStream
 header
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

The instruction start is aligned to kCodeAlignment bytes, e.g. 64 bytes on x64.
Currently, this is done by adding padding to the header and end of every InstructionStream object. Due to the end padding, the object itself is 64 byte aligned and the header is then padded so that the insn start is also 64 byte aligned.
We can replace the header padding with a single padding in the MemoryChunk. In particular, if we start the first object at offset 52, its instruction start will be at 52+12 == 64 bytes and be properly aligned. Since all IStream objects are multiple of 64 bytes, every following object will have the same alignment automatically.

On x64, this saves 52 byte per InstructionStream obj.

Bug: v8:13784, chromium:1425374, chromium:1425293
Change-Id: I49a74f436a7dab8bd6a0a335bd248f4ae9c58b7c
Reviewed-on: https://chromium-review.googlesource.com/c/v8/v8/+/4350325
Commit-Queue: Stephen Röttger <sroettger@google.com>
Reviewed-by: Jakob Linke <jgruber@chromium.org>
Reviewed-by: Dominik Inführ <dinfuehr@chromium.org>
Cr-Commit-Position: refs/heads/main@{#86635}
---
 src/heap/factory.cc             |  4 +--
 src/heap/memory-allocator.cc    | 32 +++++++++++--------
 src/heap/memory-chunk-layout.cc |  9 +++++-
 src/heap/memory-chunk-layout.h  |  3 ++
 src/heap/memory-chunk.cc        | 10 ++++--
 src/objects/code-inl.h          |  7 -----
 src/objects/code.h              | 55 +++++++++------------------------
 src/snapshot/serializer.cc      |  9 ++++--
 test/cctest/heap/test-spaces.cc |  7 +++--
 9 files changed, 65 insertions(+), 71 deletions(-)

diff --git a/src/heap/factory.cc b/src/heap/factory.cc
index 333ba7c64ee..c952862340d 100644
--- a/src/heap/factory.cc
+++ b/src/heap/factory.cc
@@ -279,7 +279,7 @@ MaybeHandle<InstructionStream> Factory::CodeBuilder::AllocateInstructionStream(
       *isolate_->factory()->instruction_stream_map(), SKIP_WRITE_BARRIER);
   Handle<InstructionStream> istream =
       handle(InstructionStream::cast(result), isolate_);
-  DCHECK(IsAligned(istream->address(), kCodeAlignment));
+  DCHECK(IsAligned(istream->instruction_start(), kCodeAlignment));
   DCHECK_IMPLIES(
       !V8_ENABLE_THIRD_PARTY_HEAP_BOOL && !heap->code_region().is_empty(),
       heap->code_region().contains(istream->address()));
@@ -304,7 +304,7 @@ Factory::CodeBuilder::AllocateConcurrentSparkplugInstructionStream(
       *local_isolate_->factory()->instruction_stream_map(), SKIP_WRITE_BARRIER);
   Handle<InstructionStream> istream =
       handle(InstructionStream::cast(result), local_isolate_);
-  DCHECK(IsAligned(istream->address(), kCodeAlignment));
+  DCHECK(IsAligned(istream->instruction_start(), kCodeAlignment));
   return istream;
 }
 
diff --git a/src/heap/memory-allocator.cc b/src/heap/memory-allocator.cc
index 0fe950323b7..69284078b07 100644
--- a/src/heap/memory-allocator.cc
+++ b/src/heap/memory-allocator.cc
@@ -271,9 +271,8 @@ Address MemoryAllocator::AllocateAlignedMemory(
   Address base = reservation.address();
 
   if (executable == EXECUTABLE) {
-    const size_t aligned_area_size = ::RoundUp(area_size, GetCommitPageSize());
-    if (!SetPermissionsOnExecutableMemoryChunk(&reservation, base,
-                                               aligned_area_size, chunk_size)) {
+    if (!SetPermissionsOnExecutableMemoryChunk(&reservation, base, area_size,
+                                               chunk_size)) {
       return HandleAllocationFailure(executable);
     }
   } else {
@@ -694,17 +693,24 @@ bool MemoryAllocator::SetPermissionsOnExecutableMemoryChunk(VirtualMemory* vm,
                                                             size_t chunk_size) {
   const size_t page_size = GetCommitPageSize();
 
+  // The code area starts at an offset on the first page. To calculate the page
+  // aligned size of the area, we have to add that offset and then round up to
+  // commit page size.
+  size_t area_offset = MemoryChunkLayout::ObjectStartOffsetInCodePage() -
+                       MemoryChunkLayout::ObjectPageOffsetInCodePage();
+  size_t aligned_area_size = RoundUp(area_offset + area_size, page_size);
+
   // All addresses and sizes must be aligned to the commit page size.
   DCHECK(IsAligned(start, page_size));
-  DCHECK_EQ(0, area_size % page_size);
   DCHECK_EQ(0, chunk_size % page_size);
 
   const size_t guard_size = MemoryChunkLayout::CodePageGuardSize();
   const size_t pre_guard_offset = MemoryChunkLayout::CodePageGuardStartOffset();
   const size_t code_area_offset =
-      MemoryChunkLayout::ObjectStartOffsetInCodePage();
+      MemoryChunkLayout::ObjectPageOffsetInCodePage();
 
-  DCHECK_EQ(pre_guard_offset + guard_size + area_size + guard_size, chunk_size);
+  DCHECK_EQ(pre_guard_offset + guard_size + aligned_area_size + guard_size,
+            chunk_size);
 
   const Address pre_guard_page = start + pre_guard_offset;
   const Address code_area = start + code_area_offset;
@@ -722,15 +728,15 @@ bool MemoryAllocator::SetPermissionsOnExecutableMemoryChunk(VirtualMemory* vm,
       // Create the pre-code guard page, following the header.
       if (vm->DiscardSystemPages(pre_guard_page, page_size)) {
         // Commit the executable code body.
-        if (vm->RecommitPages(code_area, area_size,
+        if (vm->RecommitPages(code_area, aligned_area_size,
                               PageAllocator::kReadWriteExecute)) {
           // Create the post-code guard page.
           if (vm->DiscardSystemPages(post_guard_page, page_size)) {
-            UpdateAllocatedSpaceLimits(start, code_area + area_size);
+            UpdateAllocatedSpaceLimits(start, code_area + aligned_area_size);
             return true;
           }
 
-          vm->DiscardSystemPages(code_area, area_size);
+          vm->DiscardSystemPages(code_area, aligned_area_size);
         }
       }
       vm->DiscardSystemPages(start, pre_guard_offset);
@@ -747,7 +753,7 @@ bool MemoryAllocator::SetPermissionsOnExecutableMemoryChunk(VirtualMemory* vm,
         bool set_permission_successed = false;
 #if V8_HEAP_USE_PKU_JIT_WRITE_PROTECT
         if (!jitless && RwxMemoryWriteScope::IsSupported()) {
-          base::AddressRegion region(code_area, area_size);
+          base::AddressRegion region(code_area, aligned_area_size);
           set_permission_successed =
               base::MemoryProtectionKey::SetPermissionsAndKey(
                   code_page_allocator_, region,
@@ -757,7 +763,7 @@ bool MemoryAllocator::SetPermissionsOnExecutableMemoryChunk(VirtualMemory* vm,
 #endif
         {
           set_permission_successed = vm->SetPermissions(
-              code_area, area_size,
+              code_area, aligned_area_size,
               jitless ? PageAllocator::kReadWrite
                       : MemoryChunk::GetCodeModificationPermission());
         }
@@ -765,11 +771,11 @@ bool MemoryAllocator::SetPermissionsOnExecutableMemoryChunk(VirtualMemory* vm,
           // Create the post-code guard page.
           if (vm->SetPermissions(post_guard_page, page_size,
                                  PageAllocator::kNoAccess)) {
-            UpdateAllocatedSpaceLimits(start, code_area + area_size);
+            UpdateAllocatedSpaceLimits(start, code_area + aligned_area_size);
             return true;
           }
 
-          CHECK(vm->SetPermissions(code_area, area_size,
+          CHECK(vm->SetPermissions(code_area, aligned_area_size,
                                    PageAllocator::kNoAccess));
         }
       }
diff --git a/src/heap/memory-chunk-layout.cc b/src/heap/memory-chunk-layout.cc
index c85eb2e8865..1fb265f39fb 100644
--- a/src/heap/memory-chunk-layout.cc
+++ b/src/heap/memory-chunk-layout.cc
@@ -24,6 +24,12 @@ size_t MemoryChunkLayout::CodePageGuardSize() {
 }
 
 intptr_t MemoryChunkLayout::ObjectStartOffsetInCodePage() {
+  // The first page also includes padding for code alignment.
+  return ObjectPageOffsetInCodePage() +
+         InstructionStream::kCodeAlignmentMinusCodeHeader;
+}
+
+intptr_t MemoryChunkLayout::ObjectPageOffsetInCodePage() {
   // We are guarding code pages: the first OS page after the header
   // will be protected as non-writable.
   return CodePageGuardStartOffset() + CodePageGuardSize();
@@ -86,7 +92,8 @@ size_t MemoryChunkLayout::AllocatableMemoryInMemoryChunk(
 }
 
 int MemoryChunkLayout::MaxRegularCodeObjectSize() {
-  int size = static_cast<int>(AllocatableMemoryInCodePage() / 2);
+  int size = static_cast<int>(
+      RoundDown(AllocatableMemoryInCodePage() / 2, kTaggedSize));
   DCHECK_LE(size, kMaxRegularHeapObjectSize);
   return size;
 }
diff --git a/src/heap/memory-chunk-layout.h b/src/heap/memory-chunk-layout.h
index b8a87cd9554..05a44d6291c 100644
--- a/src/heap/memory-chunk-layout.h
+++ b/src/heap/memory-chunk-layout.h
@@ -83,6 +83,9 @@ class V8_EXPORT_PRIVATE MemoryChunkLayout {
 #undef FIELD
   static size_t CodePageGuardStartOffset();
   static size_t CodePageGuardSize();
+  // Code pages have padding on the first page for code alignment, so the
+  // ObjectStartOffset will not be page aligned.
+  static intptr_t ObjectPageOffsetInCodePage();
   static intptr_t ObjectStartOffsetInCodePage();
   static intptr_t ObjectEndOffsetInCodePage();
   static size_t AllocatableMemoryInCodePage();
diff --git a/src/heap/memory-chunk.cc b/src/heap/memory-chunk.cc
index 1c68e8d762b..7277852deb2 100644
--- a/src/heap/memory-chunk.cc
+++ b/src/heap/memory-chunk.cc
@@ -163,9 +163,13 @@ MemoryChunk::MemoryChunk(Heap* heap, BaseSpace* space, size_t chunk_size,
           heap->code_space_memory_modification_scope_depth();
     } else if (!V8_HEAP_USE_PTHREAD_JIT_WRITE_PROTECT) {
       size_t page_size = MemoryAllocator::GetCommitPageSize();
-      DCHECK(IsAligned(area_start_, page_size));
-      size_t area_size = RoundUp(area_end_ - area_start_, page_size);
-      CHECK(reservation_.SetPermissions(area_start_, area_size,
+      // On executable chunks, area_start_ points past padding used for code
+      // alignment.
+      Address start_before_padding =
+          address() + MemoryChunkLayout::ObjectPageOffsetInCodePage();
+      DCHECK(IsAligned(start_before_padding, page_size));
+      size_t area_size = RoundUp(area_end_ - start_before_padding, page_size);
+      CHECK(reservation_.SetPermissions(start_before_padding, area_size,
                                         DefaultWritableCodePermissions()));
     }
   }
diff --git a/src/objects/code-inl.h b/src/objects/code-inl.h
index 9d082eecc23..e864d57ce03 100644
--- a/src/objects/code-inl.h
+++ b/src/objects/code-inl.h
@@ -368,13 +368,6 @@ void InstructionStream::WipeOutHeader() {
 }
 
 void Code::ClearInstructionStreamPadding() {
-  // Clear the padding between the header and `body_start`.
-  if (FIELD_SIZE(InstructionStream::kOptionalPaddingOffset) != 0) {
-    memset(reinterpret_cast<void*>(instruction_stream().address() +
-                                   InstructionStream::kOptionalPaddingOffset),
-           0, FIELD_SIZE(InstructionStream::kOptionalPaddingOffset));
-  }
-
   // Clear the padding after `body_end`.
   size_t trailing_padding_size =
       CodeSize() - InstructionStream::kHeaderSize - body_size();
diff --git a/src/objects/code.h b/src/objects/code.h
index 7aa515e5b0b..e4cba409486 100644
--- a/src/objects/code.h
+++ b/src/objects/code.h
@@ -648,51 +648,26 @@ class InstructionStream : public HeapObject {
   inline HandlerTable::CatchPrediction GetBuiltinCatchPrediction() const;
 
   // Layout description.
-#define ISTREAM_FIELDS(V)                                                 \
-  V(kCodeOffset, kTaggedSize)                                             \
-  /* Data or code not directly visited by GC directly starts here. */     \
-  V(kDataStart, 0)                                                        \
-  V(kMainCageBaseUpper32BitsOffset,                                       \
-    V8_EXTERNAL_CODE_SPACE_BOOL ? kTaggedSize : 0)                        \
-  V(kUnalignedHeaderSize, 0)                                              \
-  /* Add padding to align the instruction start following right after */  \
-  /* the InstructionStream object header. */                              \
-  V(kOptionalPaddingOffset, CODE_POINTER_PADDING(kOptionalPaddingOffset)) \
+#define ISTREAM_FIELDS(V)                                             \
+  V(kCodeOffset, kTaggedSize)                                         \
+  /* Data or code not directly visited by GC directly starts here. */ \
+  V(kDataStart, 0)                                                    \
+  V(kMainCageBaseUpper32BitsOffset,                                   \
+    V8_EXTERNAL_CODE_SPACE_BOOL ? kTaggedSize : 0)                    \
   V(kHeaderSize, 0)
 
   DEFINE_FIELD_OFFSET_CONSTANTS(HeapObject::kHeaderSize, ISTREAM_FIELDS)
 #undef ISTREAM_FIELDS
 
-  // This documents the amount of free space we have in each InstructionStream
-  // object header due to padding for code alignment.
-#if V8_TARGET_ARCH_ARM64
-  static constexpr int kHeaderPaddingSize =
-      V8_EXTERNAL_CODE_SPACE_BOOL ? 20 : (COMPRESS_POINTERS_BOOL ? 24 : 16);
-#elif V8_TARGET_ARCH_MIPS64
-  static constexpr int kHeaderPaddingSize = 16;
-#elif V8_TARGET_ARCH_LOONG64
-  static constexpr int kHeaderPaddingSize = (COMPRESS_POINTERS_BOOL ? 24 : 16);
-#elif V8_TARGET_ARCH_X64
-  static constexpr int kHeaderPaddingSize =
-      V8_EXTERNAL_CODE_SPACE_BOOL ? 52 : (COMPRESS_POINTERS_BOOL ? 56 : 48);
-#elif V8_TARGET_ARCH_ARM
-  static constexpr int kHeaderPaddingSize = 24;
-#elif V8_TARGET_ARCH_IA32
-  static constexpr int kHeaderPaddingSize = 24;
-#elif V8_TARGET_ARCH_PPC64
-  static constexpr int kHeaderPaddingSize =
-      V8_EMBEDDED_CONSTANT_POOL_BOOL ? (COMPRESS_POINTERS_BOOL ? 4 : 48)
-                                     : (COMPRESS_POINTERS_BOOL ? 8 : 52);
-#elif V8_TARGET_ARCH_S390X
-  static constexpr int kHeaderPaddingSize = COMPRESS_POINTERS_BOOL ? 24 : 20;
-#elif V8_TARGET_ARCH_RISCV64
-  static constexpr int kHeaderPaddingSize = (COMPRESS_POINTERS_BOOL ? 24 : 16);
-#elif V8_TARGET_ARCH_RISCV32
-  static constexpr int kHeaderPaddingSize = 24;
-#else
-#error Unknown architecture.
-#endif
-  static_assert(FIELD_SIZE(kOptionalPaddingOffset) == kHeaderPaddingSize);
+  static_assert(kCodeAlignment > kHeaderSize);
+  // We do two things to ensure kCodeAlignment of the entry address:
+  // 1) add kCodeAlignmentMinusCodeHeader padding once in the beginning of every
+  //    MemoryChunk
+  // 2) Round up all IStream allocations to a multiple of kCodeAlignment
+  // Together, the IStream object itself will always start at offset
+  // kCodeAlignmentMinusCodeHeader, which aligns the entry to kCodeAlignment.
+  static constexpr int kCodeAlignmentMinusCodeHeader =
+      kCodeAlignment - kHeaderSize;
 
   class BodyDescriptor;
 
diff --git a/src/snapshot/serializer.cc b/src/snapshot/serializer.cc
index bf4b8503302..99f7ee1cebf 100644
--- a/src/snapshot/serializer.cc
+++ b/src/snapshot/serializer.cc
@@ -422,14 +422,19 @@ void Serializer::InitializeCodeAddressMap() {
 
 InstructionStream Serializer::CopyCode(InstructionStream code) {
   code_buffer_.clear();  // Clear buffer without deleting backing store.
+  // Add InstructionStream padding which is usually added by the allocator.
+  // While this doesn't guarantee the exact same alignment, it's enough to
+  // fulfill the alignment requirements of writes during relocation.
+  code_buffer_.resize(InstructionStream::kCodeAlignmentMinusCodeHeader);
   int size = code.CodeSize();
   code_buffer_.insert(code_buffer_.end(),
                       reinterpret_cast<byte*>(code.address()),
                       reinterpret_cast<byte*>(code.address() + size));
   // When pointer compression is enabled the checked cast will try to
   // decompress map field of off-heap InstructionStream object.
-  return InstructionStream::unchecked_cast(HeapObject::FromAddress(
-      reinterpret_cast<Address>(&code_buffer_.front())));
+  return InstructionStream::unchecked_cast(
+      HeapObject::FromAddress(reinterpret_cast<Address>(
+          &code_buffer_[InstructionStream::kCodeAlignmentMinusCodeHeader])));
 }
 
 void Serializer::ObjectSerializer::SerializePrologue(SnapshotSpace space,
diff --git a/test/cctest/heap/test-spaces.cc b/test/cctest/heap/test-spaces.cc
index 423181cf1c0..5ed8781d13b 100644
--- a/test/cctest/heap/test-spaces.cc
+++ b/test/cctest/heap/test-spaces.cc
@@ -136,9 +136,10 @@ static void VerifyMemoryChunk(Isolate* isolate, Heap* heap,
       memory_allocator->AllocateLargePage(space, area_size, executable);
   size_t reserved_size =
       ((executable == EXECUTABLE))
-          ? allocatable_memory_area_offset +
-                RoundUp(area_size, page_allocator->CommitPageSize()) +
-                guard_size
+          ? RoundUp(allocatable_memory_area_offset +
+                        RoundUp(area_size, page_allocator->CommitPageSize()) +
+                        guard_size,
+                    page_allocator->CommitPageSize())
           : RoundUp(allocatable_memory_area_offset + area_size,
                     page_allocator->CommitPageSize());
   CHECK(memory_chunk->size() == reserved_size);
-- 
2.35.1

