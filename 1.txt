From 45b2cc3fa78df088db4362b92a61101eea333f35 Mon Sep 17 00:00:00 2001
From: Paolo Severini <paolosev@microsoft.com>
Date: Wed, 24 Apr 2024 17:48:43 +0200
Subject: [PATCH] [wasm][memory64] Fix bounds check for memory64 guard region
 on MacOS

On MacOS invalid memory accesses can cause both a SIGBUS or a SIGSEGV.

Bug: 42204673
Change-Id: I66c67239f0d255e2da539c5b17da754f3182d4d6
Reviewed-on: https://chromium-review.googlesource.com/c/v8/v8/+/5468146
Reviewed-by: Andreas Haas <ahaas@chromium.org>
Reviewed-by: Matthias Liedtke <mliedtke@chromium.org>
Reviewed-by: Jakob Kummerow <jkummerow@chromium.org>
Commit-Queue: Paolo Severini <paolosev@microsoft.com>
Cr-Commit-Position: refs/heads/main@{#93583}
---
 include/v8-internal.h                         | 11 +-------
 src/compiler/wasm-compiler.cc                 | 21 ++++++++------
 src/execution/isolate-data.h                  | 28 -------------------
 src/execution/isolate.cc                      |  3 --
 src/flags/flag-definitions.h                  |  2 +-
 src/trap-handler/handler-inside.cc            | 10 ++-----
 .../baseline/arm/liftoff-assembler-arm-inl.h  |  4 +--
 .../arm64/liftoff-assembler-arm64-inl.h       | 16 +++++------
 .../ia32/liftoff-assembler-ia32-inl.h         |  4 +--
 src/wasm/baseline/liftoff-assembler.h         |  4 +--
 src/wasm/baseline/liftoff-compiler.cc         |  5 ++--
 .../loong64/liftoff-assembler-loong64-inl.h   |  4 +--
 .../mips64/liftoff-assembler-mips64-inl.h     |  4 +--
 .../baseline/ppc/liftoff-assembler-ppc-inl.h  |  4 +--
 .../riscv/liftoff-assembler-riscv-inl.h       |  4 +--
 .../s390/liftoff-assembler-s390-inl.h         |  4 +--
 .../baseline/x64/liftoff-assembler-x64-inl.h  | 13 +++++----
 src/wasm/turboshaft-graph-interface.cc        | 24 +++++++++-------
 src/wasm/wasm-module.h                        |  3 ++
 test/mjsunit/mjsunit.status                   |  7 +++--
 20 files changed, 70 insertions(+), 105 deletions(-)

diff --git a/include/v8-internal.h b/include/v8-internal.h
index aad86fa5127..0f18887c09a 100644
--- a/include/v8-internal.h
+++ b/include/v8-internal.h
@@ -851,17 +851,8 @@ class Internals {
 #endif  // V8_COMPRESS_POINTERS
   static const int kContinuationPreservedEmbedderDataOffset =
       kIsolateApiCallbackThunkArgumentOffset + kApiSystemPointerSize;
-
-// #if V8_HOST_ARCH_64_BIT
-  static const int kWasm64OOBOffsetAlignmentPaddingSize = 0;
-// #else
-//   static const int kWasm64OOBOffsetAlignmentPaddingSize = 4;
-// #endif  // V8_HOST_ARCH_64_BIT
-  static const int kWasm64OOBOffsetOffset =
-      kContinuationPreservedEmbedderDataOffset + kApiSystemPointerSize +
-      kWasm64OOBOffsetAlignmentPaddingSize;
   static const int kIsolateRootsOffset =
-      kWasm64OOBOffsetOffset + sizeof(int64_t);
+      kContinuationPreservedEmbedderDataOffset + kApiSystemPointerSize;
 
 #if V8_STATIC_ROOTS_BOOL
 
diff --git a/src/compiler/wasm-compiler.cc b/src/compiler/wasm-compiler.cc
index bceb1bb3f68..4793e09b445 100644
--- a/src/compiler/wasm-compiler.cc
+++ b/src/compiler/wasm-compiler.cc
@@ -3664,15 +3664,18 @@ std::pair<Node*, BoundsCheckResult> WasmGraphBuilder::BoundsCheckMem(
   if (bounds_checks == wasm::kTrapHandler &&
       enforce_check == EnforceBoundsCheck::kCanOmitBoundsCheck) {
     if (memory->is_memory64) {
-      Node* true_node =
-          gasm_->LoadImmutable(MachineType::Uint64(), BuildLoadIsolateRoot(),
-                               IsolateData::wasm64_oob_offset_offset());
-      Node* cond = gasm_->Word64Shr(
-          converted_index, Int64Constant(memory->GetMemory64GuardsShift()));
-      Node* modified_index =
-          mcgraph()->graph()->NewNode(mcgraph()->machine()->Word64Select().op(),
-                                      cond, true_node, converted_index);
-      return {modified_index, BoundsCheckResult::kTrapHandler};
+      auto done = gasm_->MakeLabel(MachineRepresentation::kWord64);
+      Node* cond = gasm_->Uint64LessThan(
+          converted_index, Int64Constant(memory->GetMemory64GuardsSize()));
+      gasm_->GotoIf(cond, &done, BranchHint::kTrue, converted_index);
+
+      // This will cause a memory access at memory[max_memory_size + offset],
+      // which is guaranteeed to cause an access to hit the memory guard region
+      // because we checked that offset < max_memory_size.
+      gasm_->Goto(&done, Int64Constant(memory->max_memory_size));
+
+      gasm_->Bind(&done);
+      return {done.PhiAt(0), BoundsCheckResult::kTrapHandler};
     } else {
       return {converted_index, BoundsCheckResult::kTrapHandler};
     }
diff --git a/src/execution/isolate-data.h b/src/execution/isolate-data.h
index be96645fd42..c7c43749052 100644
--- a/src/execution/isolate-data.h
+++ b/src/execution/isolate-data.h
@@ -28,22 +28,11 @@ class Isolate;
 // No padding is currently required for fast_c_call_XXX and wasm64_oob_offset_
 // fields.
 #define ISOLATE_DATA_FAST_C_CALL_PADDING(V)
-#define ISOLATE_DATA_WASM64_OOB_PADDING(V)
-
 #else
 // Aligns fast_c_call_XXX fields so that they stay in the same CPU cache line.
 #define ISOLATE_DATA_FAST_C_CALL_PADDING(V)               \
   V(kFastCCallAlignmentPaddingOffset, kSystemPointerSize, \
     fast_c_call_alignment_padding)
-
-// Aligns wasm64_oob_offset_ field to 8 bytes to avoid issues with different
-// field alignment vs cross-compilation.
-// The wasm64_oob_offset_ is currently aligned, so don't add the padding.
-#define ISOLATE_DATA_WASM64_OOB_PADDING(V)
-// #define ISOLATE_DATA_WASM64_OOB_PADDING(V)                      \
-//   V(kWasm64OOBOffsetAlignmentPaddingOffset, kSystemPointerSize, \
-//     wasm64_oob_offset_alignment_padding)
-
 #endif  // V8_HOST_ARCH_64_BIT
 
 // IsolateData fields, defined as: V(Offset, Size, Name)
@@ -85,8 +74,6 @@ class Isolate;
     api_callback_thunk_argument)                                               \
   V(kContinuationPreservedEmbedderDataOffset, kSystemPointerSize,              \
     continuation_preserved_embedder_data)                                      \
-  ISOLATE_DATA_WASM64_OOB_PADDING(V)                                           \
-  V(kWasm64OOBOffset, kInt64Size, wasm64_oob_offset)                           \
   /* Full tables (arbitrary size, potentially slower access). */               \
   V(kRootsTableOffset, RootsTable::kEntriesCount* kSystemPointerSize,          \
     roots_table)                                                               \
@@ -365,16 +352,6 @@ class IsolateData final {
   // This is data that should be preserved on newly created continuations.
   Tagged<Object> continuation_preserved_embedder_data_ = Smi::zero();
 
-#if !V8_HOST_ARCH_64_BIT
-  // Aligns wasm64_oob_offset_ field to 8 bytes to avoid cross-compilation
-  // issues on some 32-bit configurations.
-  // Address wasm64_oob_offset_alignment_padding_;
-#endif
-  // An offset that always generates an invalid address when added to any
-  // start address of a Wasm memory. This is used to force an out-of-bounds
-  // access on Wasm memory64.
-  int64_t wasm64_oob_offset_ = 0xf000'0000'0000'0000;
-
   RootsTable roots_table_;
   ExternalReferenceTable external_reference_table_;
 
@@ -421,11 +398,6 @@ void IsolateData::AssertPredictableLayout() {
   static_assert(offsetof(IsolateData, Name##_) == Offset);
   ISOLATE_DATA_FIELDS(V)
 #undef V
-  // Some C++ compilers on some 32-bits configurations want to align |int64_t|
-  // field to 8 while normally, Clang aligns this field to 4. In particular,
-  // when building for Android/arm or Windows/ia32. Catch this issue early.
-  static_assert(IsAligned(offsetof(IsolateData, wasm64_oob_offset_),
-                          sizeof(IsolateData::wasm64_oob_offset_)));
   static_assert(sizeof(IsolateData) == IsolateData::kSize);
 }
 
diff --git a/src/execution/isolate.cc b/src/execution/isolate.cc
index e8a620fda43..dc57104540f 100644
--- a/src/execution/isolate.cc
+++ b/src/execution/isolate.cc
@@ -4009,9 +4009,6 @@ void Isolate::CheckIsolateLayout() {
   CHECK_EQ(static_cast<int>(OFFSET_OF(
                Isolate, isolate_data_.continuation_preserved_embedder_data_)),
            Internals::kContinuationPreservedEmbedderDataOffset);
-  CHECK_EQ(
-      static_cast<int>(OFFSET_OF(Isolate, isolate_data_.wasm64_oob_offset_)),
-      Internals::kWasm64OOBOffsetOffset);
 
   CHECK_EQ(static_cast<int>(OFFSET_OF(Isolate, isolate_data_.roots_table_)),
            Internals::kIsolateRootsOffset);
diff --git a/src/flags/flag-definitions.h b/src/flags/flag-definitions.h
index 0236f48698f..13c86a0eaec 100644
--- a/src/flags/flag-definitions.h
+++ b/src/flags/flag-definitions.h
@@ -1698,7 +1698,7 @@ DEFINE_EXPERIMENTAL_FEATURE(
 DEFINE_BOOL(trace_wasm_revectorize, false, "trace wasm revectorize")
 #endif  // V8_ENABLE_WASM_SIMD256_REVEC
 
-#if (V8_TARGET_ARCH_ARM64 || V8_TARGET_ARCH_X64) && !V8_OS_DARWIN
+#if V8_TARGET_ARCH_ARM64 || V8_TARGET_ARCH_X64
 DEFINE_EXPERIMENTAL_FEATURE(wasm_memory64_trap_handling,
                             "Use trap handling for Wasm memory64 bounds checks")
 #else
diff --git a/src/trap-handler/handler-inside.cc b/src/trap-handler/handler-inside.cc
index 606d1e2a1fd..7fee744af95 100644
--- a/src/trap-handler/handler-inside.cc
+++ b/src/trap-handler/handler-inside.cc
@@ -75,14 +75,8 @@ bool IsFaultAddressCovered(uintptr_t fault_addr) {
 }
 
 bool IsAccessedMemoryCovered(uintptr_t addr) {
-  // For Wasm Memory64, we sometimes signal out-of-bounds accesses by accessing
-  // a non-canonical address (see IsolateData::wasm64_oob_offset_). In that
-  // case, the address reported by the kernel will be nullptr, so we also
-  // handle that here.
-  if (!addr) return true;
-
-  // Otherwise, check if the access is inside the V8 sandbox (if it is enabled)
-  // as all Wasm Memory objects must be located inside the sandbox.
+  // Check if the access is inside the V8 sandbox (if it is enabled) as all Wasm
+  // Memory objects must be located inside the sandbox.
   if (gV8SandboxSize > 0) {
     return addr >= gV8SandboxBase && addr < (gV8SandboxBase + gV8SandboxSize);
   }
diff --git a/src/wasm/baseline/arm/liftoff-assembler-arm-inl.h b/src/wasm/baseline/arm/liftoff-assembler-arm-inl.h
index 1f53bedbbfe..06b63f2fcd7 100644
--- a/src/wasm/baseline/arm/liftoff-assembler-arm-inl.h
+++ b/src/wasm/baseline/arm/liftoff-assembler-arm-inl.h
@@ -4415,8 +4415,8 @@ void LiftoffAssembler::emit_f64x2_qfms(LiftoffRegister dst,
   vsub(dst.high_fp(), src3.high_fp(), scratch.high());
 }
 
-void LiftoffAssembler::set_trap_on_oob_mem64(Register index, int oob_shift,
-                                             MemOperand oob_offset) {
+void LiftoffAssembler::set_trap_on_oob_mem64(Register index, uint64_t oob_size,
+                                             uint64_t oob_index) {
   UNREACHABLE();
 }
 
diff --git a/src/wasm/baseline/arm64/liftoff-assembler-arm64-inl.h b/src/wasm/baseline/arm64/liftoff-assembler-arm64-inl.h
index 5d6b063ec53..86078e8a991 100644
--- a/src/wasm/baseline/arm64/liftoff-assembler-arm64-inl.h
+++ b/src/wasm/baseline/arm64/liftoff-assembler-arm64-inl.h
@@ -3696,15 +3696,13 @@ void LiftoffAssembler::emit_f64x2_qfms(LiftoffRegister dst,
 
 #undef EMIT_QFMOP
 
-void LiftoffAssembler::set_trap_on_oob_mem64(Register index, int oob_shift,
-                                             MemOperand oob_offset) {
-  UseScratchRegisterScope temps(this);
-  Register scratch = temps.AcquireX();
-  Lsr(scratch.X(), index.X(), oob_shift);
-
-  Register scratch2 = temps.AcquireX();
-  ldr(scratch2, oob_offset);
-  Csel(scratch.X(), scratch2, index.X(), ne);
+void LiftoffAssembler::set_trap_on_oob_mem64(Register index, uint64_t oob_size,
+                                             uint64_t oob_index) {
+  Label done;
+  Cmp(index, oob_size);
+  B(&done, kUnsignedLessThan);
+  Mov(index, oob_index);
+  bind(&done);
 }
 
 void LiftoffAssembler::StackCheck(Label* ool_code) {
diff --git a/src/wasm/baseline/ia32/liftoff-assembler-ia32-inl.h b/src/wasm/baseline/ia32/liftoff-assembler-ia32-inl.h
index 46f912333f3..014cbdd4697 100644
--- a/src/wasm/baseline/ia32/liftoff-assembler-ia32-inl.h
+++ b/src/wasm/baseline/ia32/liftoff-assembler-ia32-inl.h
@@ -4640,8 +4640,8 @@ void LiftoffAssembler::emit_f64x2_qfms(LiftoffRegister dst,
             liftoff::kScratchDoubleReg);
 }
 
-void LiftoffAssembler::set_trap_on_oob_mem64(Register index, int oob_shift,
-                                             MemOperand oob_offset) {
+void LiftoffAssembler::set_trap_on_oob_mem64(Register index, uint64_t oob_size,
+                                             uint64_t oob_index) {
   UNREACHABLE();
 }
 
diff --git a/src/wasm/baseline/liftoff-assembler.h b/src/wasm/baseline/liftoff-assembler.h
index 80c12370cb0..f4f4da126eb 100644
--- a/src/wasm/baseline/liftoff-assembler.h
+++ b/src/wasm/baseline/liftoff-assembler.h
@@ -1424,8 +1424,8 @@ class LiftoffAssembler : public MacroAssembler {
   inline void emit_f64x2_qfms(LiftoffRegister dst, LiftoffRegister src1,
                               LiftoffRegister src2, LiftoffRegister src3);
 
-  inline void set_trap_on_oob_mem64(Register index, int oob_shift,
-                                    MemOperand oob_offset);
+  inline void set_trap_on_oob_mem64(Register index, uint64_t oob_size,
+                                    uint64_t oob_index);
 
   inline void StackCheck(Label* ool_code);
 
diff --git a/src/wasm/baseline/liftoff-compiler.cc b/src/wasm/baseline/liftoff-compiler.cc
index 5d0c594af7f..0174f011934 100644
--- a/src/wasm/baseline/liftoff-compiler.cc
+++ b/src/wasm/baseline/liftoff-compiler.cc
@@ -3349,9 +3349,8 @@ class LiftoffCompiler {
         // If index is outside the guards pages, sets index to a value that will
         // certainly cause (memory_start + offset + index) to be not accessible,
         // to make sure that the OOB access will be caught by the trap handler.
-        __ set_trap_on_oob_mem64(
-            index_ptrsize, memory->GetMemory64GuardsShift(),
-            MemOperand(kRootRegister, IsolateData::wasm64_oob_offset_offset()));
+        __ set_trap_on_oob_mem64(index_ptrsize, memory->GetMemory64GuardsSize(),
+                                 memory->max_memory_size);
       }
 
       // With trap handlers we should not have a register pair as input (we
diff --git a/src/wasm/baseline/loong64/liftoff-assembler-loong64-inl.h b/src/wasm/baseline/loong64/liftoff-assembler-loong64-inl.h
index c096a5f5e94..827b9efdf64 100644
--- a/src/wasm/baseline/loong64/liftoff-assembler-loong64-inl.h
+++ b/src/wasm/baseline/loong64/liftoff-assembler-loong64-inl.h
@@ -3116,8 +3116,8 @@ void LiftoffAssembler::emit_f64x2_qfms(LiftoffRegister dst,
   bailout(kRelaxedSimd, "emit_f64x2_qfms");
 }
 
-void LiftoffAssembler::set_trap_on_oob_mem64(Register index, int oob_shift,
-                                             MemOperand oob_offset) {
+void LiftoffAssembler::set_trap_on_oob_mem64(Register index, uint64_t oob_size,
+                                             uint64_t oob_index) {
   UNREACHABLE();
 }
 
diff --git a/src/wasm/baseline/mips64/liftoff-assembler-mips64-inl.h b/src/wasm/baseline/mips64/liftoff-assembler-mips64-inl.h
index fc12c80e8ef..b23bf0b839b 100644
--- a/src/wasm/baseline/mips64/liftoff-assembler-mips64-inl.h
+++ b/src/wasm/baseline/mips64/liftoff-assembler-mips64-inl.h
@@ -3634,8 +3634,8 @@ void LiftoffAssembler::emit_f64x2_qfms(LiftoffRegister dst,
   bailout(kRelaxedSimd, "emit_f64x2_qfms");
 }
 
-void LiftoffAssembler::set_trap_on_oob_mem64(Register index, int oob_shift,
-                                             MemOperand oob_offset) {
+void LiftoffAssembler::set_trap_on_oob_mem64(Register index, uint64_t oob_size,
+                                             uint64_t oob_index) {
   UNREACHABLE();
 }
 
diff --git a/src/wasm/baseline/ppc/liftoff-assembler-ppc-inl.h b/src/wasm/baseline/ppc/liftoff-assembler-ppc-inl.h
index a806a30e194..e1fd7af97dd 100644
--- a/src/wasm/baseline/ppc/liftoff-assembler-ppc-inl.h
+++ b/src/wasm/baseline/ppc/liftoff-assembler-ppc-inl.h
@@ -2605,8 +2605,8 @@ void LiftoffAssembler::emit_i32x4_uconvert_i16x8_high(LiftoffRegister dst,
                          kScratchSimd128Reg);
 }
 
-void LiftoffAssembler::set_trap_on_oob_mem64(Register index, int oob_shift,
-                                             MemOperand oob_offset) {
+void LiftoffAssembler::set_trap_on_oob_mem64(Register index, uint64_t oob_size,
+                                             uint64_t oob_index) {
   UNREACHABLE();
 }
 
diff --git a/src/wasm/baseline/riscv/liftoff-assembler-riscv-inl.h b/src/wasm/baseline/riscv/liftoff-assembler-riscv-inl.h
index 4a30e5b99fb..41b93768beb 100644
--- a/src/wasm/baseline/riscv/liftoff-assembler-riscv-inl.h
+++ b/src/wasm/baseline/riscv/liftoff-assembler-riscv-inl.h
@@ -2194,8 +2194,8 @@ void LiftoffAssembler::emit_f64x2_qfms(LiftoffRegister dst,
   vmv_vv(dst.fp().toV(), src1.fp().toV());
 }
 
-void LiftoffAssembler::set_trap_on_oob_mem64(Register index, int oob_shift,
-                                             MemOperand oob_offset) {
+void LiftoffAssembler::set_trap_on_oob_mem64(Register index, uint64_t oob_size,
+                                             uint64_t oob_index) {
   UNREACHABLE();
 }
 
diff --git a/src/wasm/baseline/s390/liftoff-assembler-s390-inl.h b/src/wasm/baseline/s390/liftoff-assembler-s390-inl.h
index 2a9c892001d..63b4119d571 100644
--- a/src/wasm/baseline/s390/liftoff-assembler-s390-inl.h
+++ b/src/wasm/baseline/s390/liftoff-assembler-s390-inl.h
@@ -2980,8 +2980,8 @@ void LiftoffAssembler::emit_s128_relaxed_laneselect(LiftoffRegister dst,
   emit_s128_select(dst, src1, src2, mask);
 }
 
-void LiftoffAssembler::set_trap_on_oob_mem64(Register index, int oob_shift,
-                                             MemOperand oob_offset) {
+void LiftoffAssembler::set_trap_on_oob_mem64(Register index, uint64_t oob_size,
+                                             uint64_t oob_index) {
   UNREACHABLE();
 }
 
diff --git a/src/wasm/baseline/x64/liftoff-assembler-x64-inl.h b/src/wasm/baseline/x64/liftoff-assembler-x64-inl.h
index 837de8146b2..166acf48d60 100644
--- a/src/wasm/baseline/x64/liftoff-assembler-x64-inl.h
+++ b/src/wasm/baseline/x64/liftoff-assembler-x64-inl.h
@@ -4188,11 +4188,14 @@ void LiftoffAssembler::emit_f64x2_qfms(LiftoffRegister dst,
   F64x2Qfms(dst.fp(), src1.fp(), src2.fp(), src3.fp(), kScratchDoubleReg);
 }
 
-void LiftoffAssembler::set_trap_on_oob_mem64(Register index, int oob_shift,
-                                             MemOperand oob_offset) {
-  movq(kScratchRegister, index);
-  shrq(kScratchRegister, Immediate(oob_shift));
-  cmovq(not_equal, index, oob_offset);
+void LiftoffAssembler::set_trap_on_oob_mem64(Register index, uint64_t oob_size,
+                                             uint64_t oob_index) {
+  Label done;
+  movq(kScratchRegister, Immediate64(oob_size));
+  cmpq(index, kScratchRegister);
+  j(below, &done);
+  movq(index, Immediate64(oob_index));
+  bind(&done);
 }
 
 void LiftoffAssembler::StackCheck(Label* ool_code) {
diff --git a/src/wasm/turboshaft-graph-interface.cc b/src/wasm/turboshaft-graph-interface.cc
index 21bb004a008..9717e4df900 100644
--- a/src/wasm/turboshaft-graph-interface.cc
+++ b/src/wasm/turboshaft-graph-interface.cc
@@ -6293,16 +6293,20 @@ class TurboshaftGraphBuildingInterface : public WasmGraphBuilderBase {
         enforce_bounds_check ==
             compiler::EnforceBoundsCheck::kCanOmitBoundsCheck) {
       if (memory->is_memory64) {
-        V<Word32> cond = __ Word64Equal(
-            __ Word64ShiftRightLogical(V<Word64>::Cast(converted_index),
-                                       memory->GetMemory64GuardsShift()),
-            0);
-        V<WordPtr> vtrue = converted_index;
-        V<WordPtr> vfalse =
-            __ Load(__ LoadRootRegister(), LoadOp::Kind::RawAligned(),
-                    MemoryRepresentation::UintPtr(),
-                    IsolateData::wasm64_oob_offset_offset());
-        converted_index = __ WordPtrSelect(cond, vtrue, vfalse);
+        Label<WordPtr> no_oom(&asm_);
+        V<Word32> cond = __ UintPtrLessThan(
+            converted_index,
+            __ UintPtrConstant(memory->GetMemory64GuardsSize()));
+        GOTO_IF(LIKELY(cond), no_oom, converted_index);
+
+        // This will cause a memory access at memory[max_memory_size + offset],
+        // which is guaranteeed to cause an access to hit the memory guard
+        // region because we checked that offset < max_memory_size.
+        converted_index = __ UintPtrConstant(memory->max_memory_size);
+        GOTO(no_oom, converted_index);
+
+        BIND(no_oom, modified_index);
+        return {modified_index, compiler::BoundsCheckResult::kTrapHandler};
       }
       return {converted_index, compiler::BoundsCheckResult::kTrapHandler};
     }
diff --git a/src/wasm/wasm-module.h b/src/wasm/wasm-module.h
index 7323a7f0d58..32e7cc6771a 100644
--- a/src/wasm/wasm-module.h
+++ b/src/wasm/wasm-module.h
@@ -137,6 +137,9 @@ struct WasmMemory {
     return GetMemory64GuardsShift(maximum_pages * kWasmPageSize);
   }
   static int GetMemory64GuardsShift(uint64_t max_memory_size);
+  inline uint64_t GetMemory64GuardsSize() const {
+    return 1ull << GetMemory64GuardsShift();
+  }
 };
 
 inline void UpdateComputedInformation(WasmMemory* memory, ModuleOrigin origin) {
diff --git a/test/mjsunit/mjsunit.status b/test/mjsunit/mjsunit.status
index 7b5ec972765..828636a562f 100644
--- a/test/mjsunit/mjsunit.status
+++ b/test/mjsunit/mjsunit.status
@@ -1965,9 +1965,10 @@
 }],  # arch != x64 and arch != arm64 and arch != loong64 and arch != mips64'
 
 ##############################################################################
-# TODO(14716): memory64 trap handling doesn't work on mac nor arm64.
-['(arch != x64) or system == macos', {
-  # --wasm-memory64-trap-handling only supported on non-macos x64 and arm64.
+# TODO(14716): memory64 trap handling doesn't work on arm64 without pointer
+# compression.
+['(arch != x64 and arch != arm64) or (arch == arm64 and not pointer_compression)', {
+  # --wasm-memory64-trap-handling only supported on x64 and arm64.
   'regress/wasm/regress-332939161': [SKIP],
 }],
 
-- 
2.35.1

